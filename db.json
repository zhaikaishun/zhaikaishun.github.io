{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/CNAME","path":"CNAME","modified":0,"renderable":0},{"_id":"themes/hexo-theme-xups/source/css/style.styl","path":"css/style.styl","modified":0,"renderable":1},{"_id":"themes/hexo-theme-xups/source/img/default.png","path":"img/default.png","modified":0,"renderable":1},{"_id":"themes/hexo-theme-xups/source/img/jelon.jpg","path":"img/jelon.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-xups/source/img/loading.gif","path":"img/loading.gif","modified":0,"renderable":1},{"_id":"themes/hexo-theme-xups/source/img/scrolltoparrow.png","path":"img/scrolltoparrow.png","modified":0,"renderable":1},{"_id":"themes/hexo-theme-xups/source/img/share.png","path":"img/share.png","modified":0,"renderable":1},{"_id":"themes/hexo-theme-xups/source/img/url.png","path":"img/url.png","modified":0,"renderable":1},{"_id":"themes/hexo-theme-xups/source/img/wechat_mp.jpg","path":"img/wechat_mp.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-xups/source/img/wechat_pay.png","path":"img/wechat_pay.png","modified":0,"renderable":1},{"_id":"themes/hexo-theme-xups/source/img/wechat_jelon.png","path":"img/wechat_jelon.png","modified":0,"renderable":1},{"_id":"themes/hexo-theme-xups/source/js/html5.js","path":"js/html5.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-xups/source/js/main.js","path":"js/main.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-xups/source/js/comment.js","path":"js/comment.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-xups/source/img/no_found.png","path":"img/no_found.png","modified":0,"renderable":1},{"_id":"themes/hexo-theme-xups/source/img/unsigned_avatar.jpg","path":"img/unsigned_avatar.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-xups/source/img/lab/banner.jpg","path":"img/lab/banner.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-xups/source/img/thumbnail/0.jpg","path":"img/thumbnail/0.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-xups/source/img/thumbnail/1.jpg","path":"img/thumbnail/1.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-xups/source/img/thumbnail/4.jpg","path":"img/thumbnail/4.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-xups/source/img/thumbnail/5.jpg","path":"img/thumbnail/5.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-xups/source/img/thumbnail/6.jpg","path":"img/thumbnail/6.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-xups/source/img/thumbnail/7.jpg","path":"img/thumbnail/7.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-xups/source/img/thumbnail/8.jpg","path":"img/thumbnail/8.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-xups/source/img/thumbnail/9.jpg","path":"img/thumbnail/9.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-xups/source/css/fonts/icomoon/icomoon.ttf","path":"css/fonts/icomoon/icomoon.ttf","modified":0,"renderable":1},{"_id":"themes/hexo-theme-xups/source/img/thumbnail/2.jpg","path":"img/thumbnail/2.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-xups/source/img/thumbnail/3.jpg","path":"img/thumbnail/3.jpg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-xups/source/css/fonts/icomoon/icomoon.eot","path":"css/fonts/icomoon/icomoon.eot","modified":0,"renderable":1},{"_id":"themes/hexo-theme-xups/source/css/fonts/icomoon/icomoon.svg","path":"css/fonts/icomoon/icomoon.svg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-xups/source/css/fonts/homizio-nova/italic.otf","path":"css/fonts/homizio-nova/italic.otf","modified":0,"renderable":1},{"_id":"themes/hexo-theme-xups/source/css/fonts/homizio-nova/italic.ttf","path":"css/fonts/homizio-nova/italic.ttf","modified":0,"renderable":1},{"_id":"themes/hexo-theme-xups/source/css/fonts/homizio-nova/light.otf","path":"css/fonts/homizio-nova/light.otf","modified":0,"renderable":1},{"_id":"themes/hexo-theme-xups/source/css/fonts/icomoon/icomoon.woff","path":"css/fonts/icomoon/icomoon.woff","modified":0,"renderable":1},{"_id":"themes/hexo-theme-xups/source/css/fonts/homizio-nova/light_italic.otf","path":"css/fonts/homizio-nova/light_italic.otf","modified":0,"renderable":1},{"_id":"themes/hexo-theme-xups/source/css/fonts/homizio-nova/light_italic.ttf","path":"css/fonts/homizio-nova/light_italic.ttf","modified":0,"renderable":1},{"_id":"themes/hexo-theme-xups/source/css/fonts/homizio-nova/regular.otf","path":"css/fonts/homizio-nova/regular.otf","modified":0,"renderable":1},{"_id":"themes/hexo-theme-xups/source/css/fonts/homizio-nova/light.ttf","path":"css/fonts/homizio-nova/light.ttf","modified":0,"renderable":1},{"_id":"themes/hexo-theme-xups/source/css/fonts/homizio-nova/regular.ttf","path":"css/fonts/homizio-nova/regular.ttf","modified":0,"renderable":1}],"Cache":[{"_id":"source/CNAME","hash":"8ca8892815c10a14cc43f14c16148261bf2ed477","modified":1516443964056},{"_id":"themes/hexo-theme-xups/.gitignore","hash":"0d5c2fdbdc974f10150baa12e1fc171a34960ed8","modified":1516716030900},{"_id":"themes/hexo-theme-xups/README.md","hash":"7bc48e43dad4fb62776b150bbaf37c7cb4a4c5e2","modified":1516716030916},{"_id":"themes/hexo-theme-xups/_config.yml","hash":"9beca44a3da197b08c947b40c66e1064bba3ab2b","modified":1516716030933},{"_id":"source/_posts/HDFS java操作（一）FileSystem 常用操作.md","hash":"90d7f14332fe30ee3a174922dd86f1905575f7f1","modified":1516718168344},{"_id":"source/_posts/HDFS java操作（二）FileStatus 获取文件属性，globStatus 进行路径过滤.md","hash":"05268c69953c7b5c70e00ee9f0d344786dbdc16f","modified":1516718175600},{"_id":"source/_posts/HDFS 指令（四）find,help,setfatter,truncate,usage.md","hash":"76d9b228444a56907000d3e2ac775a5b9e6d14fa","modified":1516718184217},{"_id":"source/_posts/HDFS 指令（三）touchz，test，text，stat，appendToFile，checksum，count，chmod.md","hash":"2d57f52a7f954e833829e23610e268bbbd7089aa","modified":1516718151723},{"_id":"source/_posts/HDFS获取目录大小API.md","hash":"c1b1dc1cf477708434a1a82c6c476cf6f5ab26e3","modified":1516717263234},{"_id":"source/_posts/HDFS 指令（一） version，mkdir，ls，put，copyFromLocal，get，copyToLocal，cat，mv，cp.md","hash":"17997e1784736e3a50ebb5b8cd97331b552808d0","modified":1516718145119},{"_id":"source/_posts/HDFS 指令（二）moveFromLocal，moveToLocal，tail，rm，expunge，chown，chgrp，setrep，du，df.md","hash":"ff927581d6f9988e04fb37db24f678fb6cf5ab75","modified":1516718163384},{"_id":"source/_posts/Hadoop入门案例（三）全排序之自定义分区 数字排序.md","hash":"127b18a3e2f3db428e418515fd804cbbe2d76f66","modified":1516716907260},{"_id":"source/_posts/HDFS合并文件.md","hash":"b0c21a63e58635a6a1500f8ff097eb55e09aa456","modified":1516718528512},{"_id":"source/_posts/Hadoop入门案例（七）之TOP K.md","hash":"b5c46d74914eaf94dbb42b9b4595045107056aae","modified":1516716853441},{"_id":"source/_posts/Hadoop入门案例（五）全排序之TotalOrderPartitioner工具类+自动采样.md","hash":"93d9df0b7561e6fb8aee0ebc1de09fc1c7d9380c","modified":1516716995001},{"_id":"source/_posts/Hadoop入门案例（八）之 表 关联.md","hash":"1e957711e65dbd4da724a5c618f4b3dc56cdd1a1","modified":1516717069742},{"_id":"source/_posts/Linux  定时任务Crontab.md","hash":"6733ea9277e5306bd8a5ec7acd97e89a0b4be54d","modified":1516716487047},{"_id":"source/_posts/Hadoop入门案例（六）之二次排序，全排序基础下的二次排序.md","hash":"15f9d972fec8328a4428cd41c95c42a956f944b4","modified":1516717343657},{"_id":"source/_posts/Hadoop入门案例（四）全排序之自定义分区 字符串（单词）排序.md","hash":"9a59b8d3f067e3750c3cec1c6cbbf1edc619dd7b","modified":1516717123182},{"_id":"source/_posts/Linux 常见的查找相关笔记.md","hash":"17a99cb36bacd0d12691109a9d39a8e7b22e1c71","modified":1516637221975},{"_id":"source/_posts/JDBC批量插入与更新.md","hash":"2003234d83a7f92a8bb0197ea0a95365e9b0b30c","modified":1516637122412},{"_id":"source/_posts/Spark集群环境的搭建.md","hash":"bcfd78cdba2cc1a61e643687ccbb99fee94f1228","modified":1516634328304},{"_id":"source/_posts/hello-world.md","hash":"dcad3a175440091908b24107a9dc38e8e0c13280","modified":1516631680778},{"_id":"source/_posts/postName.md","hash":"767f3a71a4f1490fd2f84a4315221159220dfb6f","modified":1501681530688},{"_id":"source/_posts/hadoop集群搭建教程.md","hash":"1defed509a34cf63c08e4a3a7408dcb75154eb23","modified":1516717146419},{"_id":"source/_posts/scala编程基础.md","hash":"9bbd877aab3a01f7d02e4dd3b52cb04cbd9d0834","modified":1516636496256},{"_id":"source/_posts/Hadoop入门案例（二） 单词去重.md","hash":"9638e59f314eaf807e2a300c0d03e77c300610d4","modified":1516716973408},{"_id":"source/_posts/java曲线拟合commons-math3-3.6.1函数.md","hash":"fc935d46d331be69cbb2833b7411315cb4837669","modified":1516636396807},{"_id":"source/_posts/spark RDD算子（一）  parallelize，makeRDD，textFile.md","hash":"146c9e118f7da1d456d1e784e6063b4cedfa368d","modified":1516634559900},{"_id":"source/_posts/scala集合.md","hash":"504456d7736a0b8559e63a7cae572f48cceaee7a","modified":1516636531087},{"_id":"source/_posts/hello-world1.md","hash":"029f91fccbe8f87c0f3ea10c4042300d2fa7edc0","modified":1516029991953},{"_id":"source/_posts/spark RDD算子（七）之键值对分组操作 groupByKey，cogroup.md","hash":"8411bf60b00d21bcfa1af413e09a3c70038e6412","modified":1516634787931},{"_id":"source/_posts/spark RDD算子（九）之基本的Action操作 first, take, collect, count, countByValue, reduce, aggregate, fold,top.md","hash":"35141550816122589879638fb6eb7017efe60f32","modified":1516634860590},{"_id":"source/_posts/scala高阶函数学习.md","hash":"6d9d9b4b3720d2408e776d0f51b009c4a7f65191","modified":1516636633935},{"_id":"source/_posts/spark RDD算子（二） filter,map ,flatMap.md","hash":"a8ba50d94aa91f59bd72d76e0db1a1d2d8fcb7a3","modified":1516634604935},{"_id":"source/_posts/spark RDD算子（五）之键值对聚合操作 combineByKey.md","hash":"be1a017b4b6eeae31f0f9510e48490729d323f00","modified":1516634727593},{"_id":"source/_posts/spark RDD算子（八）之键值对关联操作 subtractByKey, join, rightOuterJoin, leftOuterJoin.md","hash":"6feaba71c9e1e57f5fdba60b27efae830b603d2d","modified":1516634822310},{"_id":"source/_posts/spark RDD算子（六）之键值对聚合操作reduceByKey，foldByKey，排序操作sortByKey.md","hash":"42bb4b4102621c1e7170b6c55f25cadc3c08c893","modified":1516634753238},{"_id":"source/_posts/Hadoop入门案例（一） wordcount.md","hash":"637fb464f24585ee44c63bd2a061dd19ec6e046a","modified":1516716765072},{"_id":"source/_posts/spark RDD算子（十一）之RDD Action 保存操作saveAsTextFile,saveAsSequenceFile,saveAsObjectFile,saveAsHadoopFile 等.md","hash":"1a5cb894894467e29fd5245bc95cb12efc4c0939","modified":1516634959847},{"_id":"source/_posts/spark RDD算子（四）之创建键值对RDD mapToPair flatMapToPair.md","hash":"039060fec14fbb2af53ed5e5512dea28d7f1f376","modified":1516634680859},{"_id":"source/_posts/spark RDD算子（十三）之RDD 分区 HashPartitioner，RangePartitioner，自定义分区.md","hash":"47e16f21a7a2445939becf8bf95891f034251b21","modified":1516635310859},{"_id":"source/_posts/spark RDD算子（十二）之RDD 分区操作上mapPartitions, mapPartitionsWithIndex.md","hash":"18583600da2f81bff0b3f613332531abc8e6ce61","modified":1516635048134},{"_id":"source/_posts/spark 从1.x 转到2.x，编写程序的的一些区别.md","hash":"cef11e37c857a9b346360f7a5a5f8832be6fd966","modified":1516634417654},{"_id":"source/_posts/spark RDD算子（十）之PairRDD的Action操作countByKey, collectAsMap.md","hash":"e73fd5cfd516908891a24d7317a64be7c098b8f6","modified":1516634925931},{"_id":"source/categories/index.md","hash":"e2373be5d4bf6a8db69f4a0725ca37d669c238f5","modified":1516443964076},{"_id":"source/_posts/虚心竹有低头叶 傲骨梅无仰面花.md","hash":"7ea7fe1943b921e5ad964c57f92e9373b35e3052","modified":1516716690549},{"_id":"source/about/index.md","hash":"45a3f30318df1c7ff75d4ba3ff77213648d36768","modified":1516443964073},{"_id":"themes/hexo-theme-xups/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1516716030853},{"_id":"source/_posts/时隔半年从wordpress再次转回hexo.md","hash":"7334c7b0913caaab105909a2b11dd1caba12a14c","modified":1516631570845},{"_id":"themes/hexo-theme-xups/.git/config","hash":"b1eac456c8f1f7cfa9eaa7e773f078fd9af0fb16","modified":1516716030869},{"_id":"source/_posts/花了2个小时.md","hash":"1139a173871d9710ba039e8eb82dd36ef6b6f6bd","modified":1516443964070},{"_id":"themes/hexo-theme-xups/.git/index","hash":"38da780d77924066cfcf59a7264814dcefe50c7a","modified":1516716031026},{"_id":"themes/hexo-theme-xups/.git/packed-refs","hash":"1b66f143972c70c1f25998ce74e34158dcabc789","modified":1516716030806},{"_id":"themes/hexo-theme-xups/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1516715993850},{"_id":"source/tags/index.md","hash":"7387b1b50e94d881b0afba07cbdf151ca7f55109","modified":1516443964078},{"_id":"source/pageName/index.md","hash":"33dcafd7fd7c39c50517759413b9c0ff0629aef2","modified":1501681557989},{"_id":"themes/hexo-theme-xups/__scaffolds/page.md","hash":"892cedeb7b62a73e72a3b369daf2bc63dfee47dc","modified":1516716030931},{"_id":"themes/hexo-theme-xups/__scaffolds/draft.md","hash":"eef222e6da6ad30e31ca264743f3e0c37db97ad0","modified":1516716030916},{"_id":"themes/hexo-theme-xups/__scaffolds/post.md","hash":"eef222e6da6ad30e31ca264743f3e0c37db97ad0","modified":1516716030932},{"_id":"themes/hexo-theme-xups/layout/archive.ejs","hash":"12b1b2f8ea19eb71e60b6ef5c015aaee6edec568","modified":1516716030933},{"_id":"themes/hexo-theme-xups/layout/index.ejs","hash":"73e85788bc6a985fbbdf9c37279ccf611d53e9ee","modified":1516716030948},{"_id":"themes/hexo-theme-xups/layout/layout.ejs","hash":"73f60bed976cc4e55eb2f4c1d3bec1e3975e86ba","modified":1516716030948},{"_id":"themes/hexo-theme-xups/layout/category.ejs","hash":"2f270f509a0f5d73e5f847fdc4351ccc1e76cef3","modified":1516716030933},{"_id":"themes/hexo-theme-xups/layout/tags.ejs","hash":"7a1c39c132a15b3aabd2092948741152b6232b05","modified":1516716030948},{"_id":"themes/hexo-theme-xups/layout/tag.ejs","hash":"0a500f20b93139859120443e2e14592cce81e21c","modified":1516716030948},{"_id":"themes/hexo-theme-xups/layout/post.ejs","hash":"6690fe7adb736e9f3d9d7f65a6b964cdc3c21f20","modified":1516716030948},{"_id":"themes/hexo-theme-xups/source/.DS_Store","hash":"fba98dc5c9096d5dba9d35076164c00012d98e11","modified":1516716030948},{"_id":"themes/hexo-theme-xups/layout/page.ejs","hash":"66c8c314d4083fff38568f2900d98223c407d5ef","modified":1516716030948},{"_id":"source/_posts/spark RDD算子（三） distinct，union，intersection，subtract，cartesian.md","hash":"70f0dd26913e459ca23f9f2ab0ce00c6bd151117","modified":1516634642192},{"_id":"source/_posts/异常Exception in thread -AWT-EventQueue-0- java.lang.NullPointerException.md","hash":"2a4a721783a1f1beb63dbdbf27886e0ed158d7ef","modified":1516716706514},{"_id":"source/_posts/spark lost task 异常 笔记.md","hash":"e373f599ab1c5dd5ded62530a7467c7dc460c099","modified":1516634486130},{"_id":"themes/hexo-theme-xups/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1516715993850},{"_id":"themes/hexo-theme-xups/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1516715993866},{"_id":"themes/hexo-theme-xups/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1516715993866},{"_id":"themes/hexo-theme-xups/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1516715993866},{"_id":"themes/hexo-theme-xups/.git/hooks/pre-commit.sample","hash":"36aed8976dcc08b5076844f0ec645b18bc37758f","modified":1516715993866},{"_id":"themes/hexo-theme-xups/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1516715993866},{"_id":"themes/hexo-theme-xups/.git/hooks/pre-rebase.sample","hash":"5885a56ab4fca8075a05a562d005e922cde9853b","modified":1516715993866},{"_id":"themes/hexo-theme-xups/.git/hooks/prepare-commit-msg.sample","hash":"2b6275eda365cad50d167fe3a387c9bc9fedd54f","modified":1516715993866},{"_id":"themes/hexo-theme-xups/.git/hooks/update.sample","hash":"e729cd61b27c128951d139de8e7c63d1a3758dde","modified":1516715993866},{"_id":"themes/hexo-theme-xups/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1516715993866},{"_id":"themes/hexo-theme-xups/.git/logs/HEAD","hash":"dd060242c9524ad17767075f329531d332dd8713","modified":1516716030853},{"_id":"themes/hexo-theme-xups/__source/about/index.md","hash":"32707ad1a27069c7f30fdbd645da15d25910fb69","modified":1516716030933},{"_id":"themes/hexo-theme-xups/__source/comment/index.md","hash":"8ebc21dec895c59375e511f921c8eef99cf192e7","modified":1516716030933},{"_id":"themes/hexo-theme-xups/__source/lab/index.md","hash":"756591670a1f2a6819add9f344e59d312eeb4d40","modified":1516716030933},{"_id":"themes/hexo-theme-xups/layout/_custom/about.ejs","hash":"0e4a7785f263ec748cee614349068c33229938a4","modified":1516716030933},{"_id":"themes/hexo-theme-xups/layout/_custom/comment.ejs","hash":"dfe71439cacc9a4d5eff76f24f04a7dd044f0aaa","modified":1516716030933},{"_id":"themes/hexo-theme-xups/layout/_custom/lab.ejs","hash":"f6f601ddf3e97ffaec9ce85db41b1fdfdf7c3fcc","modified":1516716030933},{"_id":"themes/hexo-theme-xups/layout/_partial/after-footer.ejs","hash":"51e748ed5cdb4abce01a80501e702fbfedd5f147","modified":1516716030933},{"_id":"themes/hexo-theme-xups/layout/_partial/archive-post.ejs","hash":"42cf85f3a2e2da26b64ee5029073b60dcf02e120","modified":1516716030933},{"_id":"themes/hexo-theme-xups/layout/_partial/archive.ejs","hash":"beec255420d7e3ce5a5e681660d3d521859eeb5e","modified":1516716030933},{"_id":"themes/hexo-theme-xups/layout/_partial/article.ejs","hash":"da86797a3b484c730d838f50a11e56bde16df0d8","modified":1516720624585},{"_id":"themes/hexo-theme-xups/layout/_partial/baidu-analytics.ejs","hash":"9e9e8da02174f708aaefa4385b5b1d6fe45c807e","modified":1516716030933},{"_id":"themes/hexo-theme-xups/layout/_partial/footer.ejs","hash":"8c56a4994d7cccc1694081f43ece229c8d85e930","modified":1516716030933},{"_id":"themes/hexo-theme-xups/layout/_partial/head.ejs","hash":"b74173aa97f1bb843bc3a2091a616f5dd4e723f2","modified":1516716030933},{"_id":"themes/hexo-theme-xups/layout/_partial/header.ejs","hash":"6cb479039ee016469b45da77e61dfb62c5d3bce6","modified":1516716030933},{"_id":"themes/hexo-theme-xups/layout/_partial/sidebar.ejs","hash":"1ff3c93c5c4f169ee9c26590034187a2c02de095","modified":1516716030933},{"_id":"themes/hexo-theme-xups/layout/_widget/categorys.ejs","hash":"29687d96d26fb41f0e78ef8fecc3be6da3d0da29","modified":1516716030933},{"_id":"themes/hexo-theme-xups/layout/_widget/friend_links.ejs","hash":"4ff96fbfb9d70b4636108c5deaaf041c6681ceeb","modified":1516716030933},{"_id":"themes/hexo-theme-xups/layout/_widget/tags.ejs","hash":"bc7b5e50b19e5a7f0f5c81d653407b26d8881c25","modified":1516716030933},{"_id":"themes/hexo-theme-xups/layout/_widget/weibo.ejs","hash":"5803954fea40637eabfa96bfb6d38328d3c33005","modified":1516716030933},{"_id":"themes/hexo-theme-xups/source/css/.DS_Store","hash":"7a2750dc6206c6f07303809521f5eb6654cfc3b3","modified":1516716030948},{"_id":"themes/hexo-theme-xups/source/css/_extend.styl","hash":"4942d3ad93832e5b1ad559a1b66e89e145e982b9","modified":1516716030948},{"_id":"themes/hexo-theme-xups/source/css/_markdown.styl","hash":"0fd8cff5a5bbf93507817758478cd22efd3cfc88","modified":1516716030979},{"_id":"themes/hexo-theme-xups/source/css/_variables.styl","hash":"d0314bd375ba70110ae95a716b7f53a949545f69","modified":1516716030979},{"_id":"themes/hexo-theme-xups/source/css/style.styl","hash":"ae25cdd604a9e4643caa04c704593fd4cc70dfd6","modified":1516716030995},{"_id":"themes/hexo-theme-xups/source/img/.DS_Store","hash":"a1bdef3a79e0c0f0c4f92e81c00a875f6422dab5","modified":1516716030995},{"_id":"themes/hexo-theme-xups/source/img/default.png","hash":"a0e8360a185c7516bc2dddb25241daee67b42972","modified":1516716030995},{"_id":"themes/hexo-theme-xups/source/img/jelon.jpg","hash":"5a698f21552a8365ef44f7113665bff8121635cb","modified":1516716030995},{"_id":"themes/hexo-theme-xups/source/img/loading.gif","hash":"92dcf9179379355f05cf2d1c8cceba930a2a674f","modified":1516716030995},{"_id":"themes/hexo-theme-xups/source/img/scrolltoparrow.png","hash":"53bd140adb85b23d535d1e488550eda4624a58bd","modified":1516716030995},{"_id":"themes/hexo-theme-xups/source/img/share.png","hash":"20a860aeb842a556d067deabb99f6bfc024c97d0","modified":1516716030995},{"_id":"themes/hexo-theme-xups/source/img/url.png","hash":"be1f35666ed5bf03aa3f6db121bd03c407b158a7","modified":1516716031011},{"_id":"themes/hexo-theme-xups/source/img/wechat_mp.jpg","hash":"d58b7796093ca8cdd4034b2dd28021c147199099","modified":1516716031026},{"_id":"themes/hexo-theme-xups/source/img/wechat_pay.png","hash":"3fb3bbd4b1fae4259578dce0ca838e0696283041","modified":1516716031026},{"_id":"themes/hexo-theme-xups/source/img/wechat_jelon.png","hash":"f1e1cab54bfb69e7816b1d5f355100ea8b272ce4","modified":1516716031011},{"_id":"themes/hexo-theme-xups/source/js/html5.js","hash":"4fefd28dcad6c973c44719ce8b3a8b82fe670a59","modified":1516716031026},{"_id":"themes/hexo-theme-xups/source/js/main.js","hash":"f82044bfaee5911d88d987fbe105fc48cfc96d22","modified":1516716031026},{"_id":"themes/hexo-theme-xups/source/js/comment.js","hash":"25e364e0d900842227da85df81984be84b1df3cd","modified":1516716031026},{"_id":"themes/hexo-theme-xups/source/img/no_found.png","hash":"aa383a9a73b3468b6a77ea3e3676b3f9d6e6ef0d","modified":1516716030995},{"_id":"themes/hexo-theme-xups/source/img/unsigned_avatar.jpg","hash":"8343a845110fca8317440fcf61aae209eec57edf","modified":1516716031011},{"_id":"themes/hexo-theme-xups/.git/objects/pack/pack-b3e1421dcf4796f7c9f20b059e9828472251a8b9.idx","hash":"b53758c8aee947a4ae3a46d990e971b8be4287fc","modified":1516716030371},{"_id":"themes/hexo-theme-xups/.git/refs/heads/master","hash":"408223402da8f31cd6366e7a8bcf19111575bbe5","modified":1516716030853},{"_id":"themes/hexo-theme-xups/layout/_partial/post/category.ejs","hash":"2908913f083b0d95a5cd09a7996cacfe722a3118","modified":1516716030933},{"_id":"themes/hexo-theme-xups/layout/_partial/post/comment.ejs","hash":"d3f2eceb848e750c7a677319cd9ff4904ab61c45","modified":1516716030933},{"_id":"themes/hexo-theme-xups/layout/_partial/post/date.ejs","hash":"848f362936b3f21e067e2a0783efbdbdab338985","modified":1516716030933},{"_id":"themes/hexo-theme-xups/layout/_partial/post/gallery.ejs","hash":"ed531d1970eedabd2a454d51ac92fffcc0697bec","modified":1516716030933},{"_id":"themes/hexo-theme-xups/layout/_partial/post/nav.ejs","hash":"fa8cb2d5c593fa402e19e1089007812219596481","modified":1516716030933},{"_id":"themes/hexo-theme-xups/layout/_partial/post/share.ejs","hash":"c0f861fb0bd74aced347e0bccee5fa7ff37fa7b4","modified":1516716030933},{"_id":"themes/hexo-theme-xups/layout/_partial/post/tag.ejs","hash":"38a0810b82e69ddd105211e7ff4ee9f2fb832ce9","modified":1516716030933},{"_id":"themes/hexo-theme-xups/layout/_partial/post/title.ejs","hash":"4819520155fc4e389b09c80310af5c61fcd57c03","modified":1516716030933},{"_id":"themes/hexo-theme-xups/source/css/_base/font.styl","hash":"52895afc3fd1e250d3359d9250f9f6a33e0c797a","modified":1516716030948},{"_id":"themes/hexo-theme-xups/source/css/_base/global.styl","hash":"5ba865047a3438ab6c2347e7403c67a9f2085619","modified":1516716030948},{"_id":"themes/hexo-theme-xups/source/css/_base/normalize.styl","hash":"3f6122f583b1590d51fe8181ccd310786c04a4c2","modified":1516716030948},{"_id":"themes/hexo-theme-xups/source/css/_base/reset.styl","hash":"ee2a445efc1fbfad1fcdf3cd141e81323171bd25","modified":1516716030948},{"_id":"themes/hexo-theme-xups/source/css/_partial/.DS_Store","hash":"16121b987e7a7a6c89f78f512e6e63eea583e25a","modified":1516716030979},{"_id":"themes/hexo-theme-xups/source/css/_partial/article.styl","hash":"127dd055a12c250b13add12cd0615b707fe21fab","modified":1516720192497},{"_id":"themes/hexo-theme-xups/source/css/_partial/comment.styl","hash":"ee549c816d38b5ebcdb1c319d4646a97317cb364","modified":1516716030979},{"_id":"themes/hexo-theme-xups/source/css/_partial/footer.styl","hash":"e4891d0642ab0816f29b8d42e014b9818def2a62","modified":1516716030979},{"_id":"themes/hexo-theme-xups/source/css/_partial/header.styl","hash":"796da02640d3501e269a828e634dcbc89ef41a11","modified":1516716030979},{"_id":"themes/hexo-theme-xups/source/css/_partial/highlight.styl","hash":"c1c9655afa9739a4987dc196fff5af50688ea445","modified":1516716030979},{"_id":"themes/hexo-theme-xups/source/css/_partial/post.styl","hash":"163988e3cb92deab32674175cb25048ca2f96ae6","modified":1516716030979},{"_id":"themes/hexo-theme-xups/source/css/_partial/responsive.styl","hash":"5f262fc7f37cab2c1a8868ef10ab652b6ddd0d5c","modified":1516716030979},{"_id":"themes/hexo-theme-xups/source/css/_partial/searchform.styl","hash":"2a2ae54716da54d90eb1970ddec92a33e6fde6b5","modified":1516716030979},{"_id":"themes/hexo-theme-xups/source/css/_partial/sidebar.styl","hash":"0e64e0a73a6befe0be43a9cb9458e7c690fe4883","modified":1516716030979},{"_id":"themes/hexo-theme-xups/source/css/_partial/theme.styl","hash":"e1eb40c93a39f2212b042fcb4481be2e901240af","modified":1516716030979},{"_id":"themes/hexo-theme-xups/source/css/fonts/.DS_Store","hash":"db2379b0a524b084530079a6a32ba976f2892009","modified":1516716030979},{"_id":"themes/hexo-theme-xups/source/img/lab/banner.jpg","hash":"daa0b3c13831b17182c3d587920f035c80816ae3","modified":1516716030995},{"_id":"themes/hexo-theme-xups/source/img/thumbnail/0.jpg","hash":"7285bbd93d25a0810df38b499fa2b9c201dd88f2","modified":1516716030995},{"_id":"themes/hexo-theme-xups/source/img/thumbnail/1.jpg","hash":"d56409526e114f07de70b9f6be036ee8939626bb","modified":1516716030995},{"_id":"themes/hexo-theme-xups/source/img/thumbnail/4.jpg","hash":"4ba83c3c4937b8068bcb9ed7002c186053482b86","modified":1516716031011},{"_id":"themes/hexo-theme-xups/source/img/thumbnail/5.jpg","hash":"b30fe3c75c445789d83b3d0f2f7b8d456802d306","modified":1516716031011},{"_id":"themes/hexo-theme-xups/source/img/thumbnail/6.jpg","hash":"767c9f7b42b7faf9966fcf87afc732045bf87167","modified":1516716031011},{"_id":"themes/hexo-theme-xups/source/img/thumbnail/7.jpg","hash":"74a4ed49b8823f7e93cad8dc53ed2bcc07f33254","modified":1516716031011},{"_id":"themes/hexo-theme-xups/source/img/thumbnail/8.jpg","hash":"ed356882b0a3015e7aec619bf22dd98e76a2dac8","modified":1516716031011},{"_id":"themes/hexo-theme-xups/source/img/thumbnail/9.jpg","hash":"64aa3752db2dd53e3dbbc7a84967afd2cdedd43e","modified":1516716031011},{"_id":"themes/hexo-theme-xups/source/css/fonts/icomoon/icomoon.ttf","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1516716030995},{"_id":"themes/hexo-theme-xups/source/img/thumbnail/2.jpg","hash":"edbd90ed95301d390a8aa8243cd4c08a4d862402","modified":1516716030995},{"_id":"themes/hexo-theme-xups/source/img/thumbnail/3.jpg","hash":"7870fb47e46f645332174f356148bbc8163dfe13","modified":1516716031011},{"_id":"themes/hexo-theme-xups/.git/logs/refs/heads/master","hash":"dd060242c9524ad17767075f329531d332dd8713","modified":1516716030853},{"_id":"themes/hexo-theme-xups/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1516716030853},{"_id":"themes/hexo-theme-xups/source/css/fonts/icomoon/icomoon.eot","hash":"26de148529b318cb8159c70ef5adbe3667fb3e6d","modified":1516716030995},{"_id":"themes/hexo-theme-xups/source/css/fonts/icomoon/icomoon.svg","hash":"2e3227e11e13917eecb676062ee953b37ea957c0","modified":1516716030995},{"_id":"themes/hexo-theme-xups/source/css/fonts/homizio-nova/.DS_Store","hash":"94c491df2256ef547252eed029a8bfb2961da5fe","modified":1516716030979},{"_id":"themes/hexo-theme-xups/source/css/fonts/homizio-nova/italic.otf","hash":"5b50719da49c1ac26793384b1b68055f01dd4e85","modified":1516716030979},{"_id":"themes/hexo-theme-xups/source/css/fonts/homizio-nova/italic.ttf","hash":"c62e2ba058f3dc49125dc514bcedde065ce64c5f","modified":1516716030979},{"_id":"themes/hexo-theme-xups/source/css/fonts/homizio-nova/light.otf","hash":"4afcc511fe6937b8e8d79e649d70dfb4fff6b406","modified":1516716030979},{"_id":"themes/hexo-theme-xups/source/css/fonts/icomoon/icomoon.woff","hash":"a5a4ffc5ecda60e59beac6b0f096b25b243a2acb","modified":1516716030995},{"_id":"themes/hexo-theme-xups/source/css/fonts/homizio-nova/light_italic.otf","hash":"29c648cf7caa087f36b1e1e1c479eb63da35af73","modified":1516716030979},{"_id":"themes/hexo-theme-xups/source/css/fonts/homizio-nova/light_italic.ttf","hash":"de9be5ed440fc380f2cbaa1683e94518d3d44e02","modified":1516716030979},{"_id":"themes/hexo-theme-xups/source/css/fonts/homizio-nova/regular.otf","hash":"c6e47b88647ac2b729c05d220cc5be19787b7835","modified":1516716030995},{"_id":"themes/hexo-theme-xups/source/css/fonts/homizio-nova/light.ttf","hash":"2ba5f36f461b540823978956af8c33b57b0b8ab1","modified":1516716030979},{"_id":"themes/hexo-theme-xups/source/css/fonts/homizio-nova/regular.ttf","hash":"0ab6cf8e5883d5693ba56065e4973cd8b3e896bc","modified":1516716030995},{"_id":"themes/hexo-theme-xups/.git/logs/refs/remotes/origin/HEAD","hash":"dd060242c9524ad17767075f329531d332dd8713","modified":1516716030853},{"_id":"themes/hexo-theme-xups/xups.png","hash":"22f1fc32dea794370b4e791b885c2afb03c886c6","modified":1516716031026},{"_id":"themes/hexo-theme-xups/.git/objects/pack/pack-b3e1421dcf4796f7c9f20b059e9828472251a8b9.pack","hash":"8bf55eafb8abe01ad93b4bcc7143b299f905fce8","modified":1516716030496},{"_id":"themes/hexo-theme-xups/layout/toc.ejs","hash":"1cf2078c4123432b9e7437c65f680dbcd7a9456e","modified":1516720499108}],"Category":[{"name":"大数据","_id":"cjcrpnzn800042wv37h6lvkzr"},{"name":"hdfs","parent":"cjcrpnzn800042wv37h6lvkzr","_id":"cjcrpnzoj000n2wv3t8z6zp0g"},{"name":"hadoop","parent":"cjcrpnzn800042wv37h6lvkzr","_id":"cjcrpnzpt001v2wv3aqr3z0fa"},{"name":"Linux","_id":"cjcrpnzr400352wv3ky4aitoj"},{"name":"programme","_id":"cjcrpnzsm00422wv3sa1xu923"},{"name":"spark","parent":"cjcrpnzn800042wv37h6lvkzr","_id":"cjcrpnzt4004b2wv3rpm0qt0r"},{"name":"再测试sss","_id":"cjcrpnztd004h2wv3a3xtnnhw"},{"name":"随笔","_id":"cjcrpnzuu006g2wv386v09ito"},{"name":"测试一下目录","_id":"cjcrpnzuu006j2wv3im8bnerx"},{"name":"大数据","parent":"cjcrpnzuu006j2wv3im8bnerx","_id":"cjcrpnzuu006m2wv3mj5sij65"}],"Data":[],"Page":[{"title":"categories","date":"2018-01-16T15:23:48.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2018-01-16 23:23:48\ntype: \"categories\"\n---\n","updated":"2018-01-20T10:26:04.076Z","path":"categories/index.html","comments":1,"layout":"page","_id":"cjcrpnzn800012wv3m9n1zikp","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"about","date":"2018-01-16T15:27:32.000Z","_content":"一只学习前端的小菜鸟，欢迎分享知识。","source":"about/index.md","raw":"---\ntitle: about\ndate: 2018-01-16 23:27:32\n---\n一只学习前端的小菜鸟，欢迎分享知识。","updated":"2018-01-20T10:26:04.073Z","path":"about/index.html","comments":1,"layout":"page","_id":"cjcrpnzn800032wv3g3w4zq0s","content":"<p>一只学习前端的小菜鸟，欢迎分享知识。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>一只学习前端的小菜鸟，欢迎分享知识。</p>\n"},{"title":"tags","date":"2018-01-16T15:17:45.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2018-01-16 23:17:45\ntype: \"tags\"\n---\n","updated":"2018-01-20T10:26:04.078Z","path":"tags/index.html","comments":1,"layout":"page","_id":"cjcrpnznn00072wv3rsqmnedo","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"pageName","date":"2017-08-02T13:45:57.000Z","_content":"","source":"pageName/index.md","raw":"---\ntitle: pageName\ndate: 2017-08-02 21:45:57\n---\n","updated":"2017-08-02T13:45:57.989Z","path":"pageName/index.html","comments":1,"layout":"page","_id":"cjcrpnznn00092wv39v3jqb2k","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"HDFS java操作（一）FileSystem 常用操作","date":"2016-06-26T13:25:21.000Z","author":"kaishun","id":"10","blogexcerpt":"本文主要讲解如何用java去操作hdfs，以下是我整理的常用的一些方法，本文主要介绍的是FileSystem，我把其集合到了一个工具类当中，下面的操作主要有 检查文件是否存在，创建文件，创建文件夹，复制（上传）本地文件到hdfs指定目录，复制（上传）本地文件夹到hdfs指定目录，从hdfs下载文件，移动hdfs上的文件或者文件夹，删除文件或者文件夹，HDFS到 HDFS的合并，列出所有DataNode的名字信息，检测是否是备用节点等操作","_content":"\n# **简介:**\n本文主要讲解如何用java去操作hdfs，以下是我整理的常用的一些方法，本文主要介绍的是FileSystem，我把其集合到了一个工具类当中，下面的操作主要有 检查文件是否存在，创建文件，创建文件夹，复制（上传）本地文件到hdfs指定目录，复制（上传）本地文件夹到hdfs指定目录，从hdfs下载文件，移动hdfs上的文件或者文件夹，删除文件或者文件夹，HDFS 到 HDFS 的合并，列出所有DataNode的名字信息，检测是否是备用节点等操作\n![hdfs-commands](http://or49tneld.bkt.clouddn.com/18-1-23/63075597.jpg)\n\n```java\npackage hdfs.tutorial;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.*;\nimport org.apache.hadoop.hdfs.DistributedFileSystem;\nimport org.apache.hadoop.hdfs.protocol.DatanodeInfo;\n\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.net.URI;\nimport java.net.URISyntaxException;\nimport java.util.Date;\nimport java.util.List;\n\n/**\n * Created by Administrator on 2017/5/25.\n */\npublic class HdfsOper {\n    private FileSystem hdfs;\n\n    /**\n     * @return 得到hdfs的连接 FileSystem类\n     * @throws URISyntaxException\n     * @throws IOException\n     * @throws InterruptedException\n     */\n    public static FileSystem getFileSystem() throws URISyntaxException, IOException, InterruptedException {\n        //获取FileSystem类的方法有很多种，这里只写一种\n        Configuration config = new Configuration();\n        URI uri = new URI(\"hdfs://192.xxx.x.xxx:xxx\");\n        return FileSystem.get(uri,config,\"your user\");// 第一位为uri，第二位为config，第三位是登录的用户\n    }\n\n    /**\n     * 检查文件或者文件夹是否存在\n     * @param filename\n     * @return\n     */\n    public boolean checkFileExist(String filename)\n    {\n        try\n        {\n            Path f = new Path(filename);\n            return hdfs.exists(f);\n        }\n        catch (Exception e)\n        {\n            e.printStackTrace();\n        }\n        return false;\n    }\n\n    /**\n     * 创建文件夹\n     * @param dirName\n     * @return\n     */\n    public boolean mkdir(String dirName)\n    {\n        if (checkFileExist(dirName))\n            return true;\n        try\n        {\n            Path f = new Path(dirName);\n            System.out.println(\"Create and Write :\" + f.getName() + \" to hdfs\");\n            return hdfs.mkdirs(f);\n        }\n        catch (Exception e)\n        {\n            e.printStackTrace();\n        }\n\n        return false;\n    }\n\n    /**\n     * 创建一个空文件\n     * @param filePath  文件的完整路径名称\n     * @return\n     */\n    public boolean mkfile(String filePath)\n    {\n        try\n        {\n            Path f = new Path(filePath);\n            FSDataOutputStream os = hdfs.create(f, true);\n            os.close();\n            return true;\n        }\n        catch (IllegalArgumentException e)\n        {\n            e.printStackTrace();\n        }\n        catch (IOException e)\n        {\n            e.printStackTrace();\n        }\n        return false;\n    }\n\n    /**\n     * 复制文件到指定目录\n     * @param srcfile  复制的文件路径\n     * @param desfile  粘贴的路径\n     * @return\n     */\n    public boolean hdfsCopyUtils(String srcfile, String desfile) {\n        Configuration conf = new Configuration();\n        Path src = new Path(srcfile);\n        Path dst = new Path(desfile);\n        try {\n            FileUtil.copy(src.getFileSystem(conf), src,\n                    dst.getFileSystem(conf), dst, false, conf);\n        } catch (IOException e) {\n            return false;\n        }\n        return true;\n    }\n\n    /**\n     * 移动文件或者文件夹\n     * @param src  初始路径\n     * @param dst   移动结束路径\n     * @throws Exception\n     */\n    public void movefile(String src, String dst) throws Exception\n    {\n        Path p1 = new Path(src);\n        Path p2 = new Path(dst);\n        hdfs.rename(p1, p2);\n    }\n\n    /**\n     * 删除文件或者文件夹\n     * @param src\n     * @throws Exception\n     */\n    public void delete(String src) throws Exception\n    {\n        Path p1 = new Path(src);\n        if (hdfs.isDirectory(p1))\n        {\n            hdfs.delete(p1, true);\n            System.out.println(\"删除文件夹成功: \" + src);\n        }\n        else if (hdfs.isFile(p1))\n        {\n            hdfs.delete(p1, false);\n            System.out.println(\"删除文件成功: \" + src);\n        }\n    }\n\n    /**\n     * 读取本地文件到HDFS系统, 保证文件格式是utf-8\n     * @param localFilename\n     * @param hdfsPath\n     * @return\n     */\n    public boolean copyLocalFileToHDFS(String localFilename, String hdfsPath)\n    {\n        try\n        {\n            // 如果路径不存在就创建文件夹\n            mkdir(hdfsPath);\n\n            File file = new File(localFilename);\n            FileInputStream is = new FileInputStream(file);\n\n            // 如果hdfs上已经存在文件，那么先删除该文件\n            if (this.checkFileExist(hdfsPath + \"/\" + file.getName()))\n            {\n                delete(hdfsPath + \"/\" + file.getName());\n            }\n\n            Path f = new Path(hdfsPath + \"/\" + file.getName());\n\n            FSDataOutputStream os = hdfs.create(f, true);\n            byte[] buffer = new byte[10240000];\n            int nCount = 0;\n\n            while (true)\n            {\n                int bytesRead = is.read(buffer);\n                if (bytesRead <= 0)\n                {\n                    break;\n                }\n\n                os.write(buffer, 0, bytesRead);\n                nCount++;\n                if (nCount % (100) == 0)\n                    System.out.println((new Date()).toLocaleString() + \": Have move \" + nCount + \" blocks\");\n            }\n\n            is.close();\n            os.close();\n            System.out.println((new Date()).toLocaleString() + \": Write content of file \" + file.getName()\n                    + \" to hdfs file \" + f.getName() + \" success\");\n            return true;\n        }\n        catch (Exception e)\n        {\n            e.printStackTrace();\n        }\n        return false;\n    }\n\n    /**\n     * 复制本地文件夹到hdfs的文件\n     * @param localPath\n     * @param hdfsPath\n     * @return\n     */\n    public boolean CopyLocalDirTohdfs(String localPath, String hdfsPath)\n    {\n        try\n        {\n            File root = new File(localPath);\n            File[] files = root.listFiles();\n\n            for (File file : files)\n            {\n                if (file.isFile())\n                {\n                    copyLocalFileToHDFS(file.getPath().toString(), hdfsPath);\n\n                }\n                else if(file.isDirectory())\n                {\n                    CopyLocalDirTohdfs(localPath+\"/\"+file.getName(), hdfsPath+\"/\"+file.getName());\n                }\n            }\n            return true;\n        }\n        catch (Exception e)\n        {\n            e.printStackTrace();\n        }\n        return false;\n    }\n\n\n    /**\n     * 从hdfs下载\n     * @param hdfsFilename\n     * @param localPath\n     * @return\n     */\n    public boolean downloadFileFromHdfs(String hdfsFilename, String localPath)\n    {\n        try\n        {\n            Path f = new Path(hdfsFilename);\n\n            FSDataInputStream dis = hdfs.open(f);\n            File file = new File(localPath + \"/\" + f.getName());\n            FileOutputStream os = new FileOutputStream(file);\n\n            byte[] buffer = new byte[1024000];\n            int length = 0;\n            while ((length = dis.read(buffer)) > 0)\n            {\n                os.write(buffer, 0, length);\n            }\n\n            os.close();\n            dis.close();\n\n            return true;\n        }\n        catch (Exception e)\n        {\n            e.printStackTrace();\n        }\n        return false;\n    }\n\n    /**\n     * HDFS 到 HDFS 的合并\n     * hdfs提供了一种FileUtil.copyMerge（）的方法， 注意下面的 false 这个，如果改为true，就会删除这个目录\n     * @param folder 需要合并的目录\n     * @param file  要合并成的文件，完整路径名称\n     */\n    public void copyMerge(String folder, String file) {\n        Configuration conf = new Configuration();\n        Path src = new Path(folder);\n        Path dst = new Path(file);\n\n        try {\n            FileUtil.copyMerge(src.getFileSystem(conf), src,\n                    dst.getFileSystem(conf), dst, false, conf, null);\n        } catch (IOException e) {\n            // TODO Auto-generated catch block\n            e.printStackTrace();\n        }\n    }\n\n\n    /**\n     * 列出所有DataNode的名字信息\n     */\n    public void listDataNodeInfo()\n    {\n        try\n        {\n            DistributedFileSystem fs =null;\n            fs = (DistributedFileSystem) hdfs;\n            DatanodeInfo[] dataNodeStats = fs.getDataNodeStats();\n            String[] names = new String[dataNodeStats.length];\n            System.out.println(\"List of all the datanode in the HDFS cluster:\");\n\n            for (int i = 0; i < names.length; i++)\n            {\n                names[i] = dataNodeStats[i].getHostName();\n                System.out.println(names[i]);\n            }\n            System.out.println(hdfs.getUri().toString());\n        }\n        catch (Exception e)\n        {\n            e.printStackTrace();\n        }\n    }\n\n    /**\n     * 检测是否是备用节点\n     * @throws Exception\n     */\n    public boolean checkStandbyException(String filename)\n    {\n        try {\n            Path f = new Path(filename);\n            hdfs.exists(f);\n        } catch (org.apache.hadoop.ipc.RemoteException e) {\n            if(e.getClassName().equals(\"org.apache.hadoop.ipc.StandbyException\"))\n            {\n                return true;\n            }\n        } catch (Exception e) {\n\n        }\n        return false;\n    }\n\n\n    public boolean mergeDirFiles(List<FileStatus> fileList, String tarPath, String rowTerminateFlag)\n    {\n        //rowTerminateFlag   \\n\n        FSDataOutputStream tarFileOutputStream = null;\n        FSDataInputStream srcFileInputStream = null;\n\n        try\n        {\n            Path tarFile = new Path(tarPath);\n            tarFileOutputStream = hdfs.create(tarFile, true);\n\n            byte[] buffer = new byte[1024000];\n            int length = 0;\n            long nTotalLength = 0;\n            int nCount = 0;\n            boolean bfirst = true;\n            for(FileStatus file : fileList)\n            {\n                if(file.getPath().equals(tarFile))\n                {\n                    continue;\n                }\n                System.out.println(\" merging file from  \" + file.getPath() + \" to \" + tarPath);\n\n                if(!bfirst)\n                {\n                    //添加换行符\n                    tarFileOutputStream.write(rowTerminateFlag.getBytes(), 0, rowTerminateFlag.length());\n                }\n\n                srcFileInputStream = hdfs.open(file.getPath(), buffer.length);\n                while ((length = srcFileInputStream.read(buffer)) > 0)\n                {\n                    nCount++;\n                    tarFileOutputStream.write(buffer, 0, length);\n                    nTotalLength += length;\n                    //System.out.println(\" file length \" + file.getLen() + \" read \" + length);\n                    if (nCount % 1000 == 0)\n                    {\n                        tarFileOutputStream.flush();\n                        System.out.println((new Date()).toLocaleString() + \": Have move \" + (nTotalLength / 1024000) + \" MB\");\n                    }\n\n                }\n\n                srcFileInputStream.close();\n\n                bfirst = false;\n            }\n\n        }\n        catch (Exception e)\n        {\n            e.printStackTrace();\n            try\n            {\n                delete(tarPath);\n            }\n            catch (Exception e2)\n            {\n                // TODO: handle exception\n            }\n            return false;\n        }\n        finally\n        {\n            try\n            {\n                if(tarFileOutputStream != null)\n                {\n                    tarFileOutputStream.flush();\n                    tarFileOutputStream.close();\n                    srcFileInputStream.close();\n                }\n            }\n            catch (Exception e2)\n            {\n                // TODO: handle exception\n            }\n        }\n        return true;\n    }\n\n\n}\n\n\t/**\n\t * 将一个字符串写入某个路径\n\t *\n\t * @param text 要保存的字符串\n\t * @param path 要保存的路径\n\t */\n\tpublic void writerString(String text,String path){\n\n\t\ttry\n\t\t{\n\t\t\tPath f = new Path(path);\n\t\t\tFSDataOutputStream os = fs.create(f, true);\n\t\t\tBufferedWriter  writer = new BufferedWriter(new OutputStreamWriter(os, \"utf-8\"));// 以UTF-8格式写入文件，不乱码\n\t\t\twriter.write(text);\n\t\t\twriter.close();\n\t\t\tos.close();\n\t\t}\n\t\tcatch (Exception e)\n\t\t{\n\t\t\te.printStackTrace();\n\n\t\t}\n\n\t}\n\n\t/**\n\t * 按行读取文件内容，并且防止乱码\n\t * @param hdfsFilename\n\t * @return\n\t */\n\tpublic boolean readByLine(String hdfsFilename)\n\t{\n\t\ttry\n\t\t{\n\t\t\tPath f = new Path(hdfsFilename);\n\n\t\t\tFSDataInputStream dis = hdfs.open(f);\n\n\t\t\tBufferedReader bf=new BufferedReader(new InputStreamReader(dis));//防止中文乱码\n\t\t\tString line = null;\n\t\t\twhile ((line=bf.readLine())!=null)\n\t\t\t{\n\t\t\t\tSystem.out.println(new String(line.getBytes(),\"utf-8\"));\n\t\t\t}\n\n\t\t\tdis.close();\n\t\t\treturn true;\n\t\t}\n\t\tcatch (Exception e)\n\t\t{\n\t\t\te.printStackTrace();\n\t\t}\n\t\treturn false;\n\t}\n\n\tpublic void reNameExistsPath(String srcPath, String tarPath) throws Exception\n\t{\n\t    //检测输出目录是否存在，存在就改名\n\t    if(checkFileExist(srcPath))\n\t    {\n\t    \ttarPath = srcPath.trim();\n\t    \twhile(tarPath.charAt(tarPath.length()-1) == '/')\n\t    \t{\n\t    \t\ttarPath = tarPath.substring(0, tarPath.length()-1);\n\t    \t}\n\t    \tDate now = new Date(); \n\t    \tSimpleDateFormat dateFormat = new SimpleDateFormat(\"yyMMddHHmmss\");\n\t    \tString nowStr = dateFormat.format(now); \n\t    \ttarPath += \"_\" + nowStr;\n\t    \tmovefile(srcPath, tarPath);\n\t    }\n\t    else \n\t    {\n\t    \ttarPath = srcPath;\n\t    }\n\t}\n```","source":"_posts/HDFS java操作（一）FileSystem 常用操作.md","raw":"---\ntitle: HDFS java操作（一）FileSystem 常用操作\ndate: 2016-06-26 21:25:21\ntags: [hdfs]\ncategories: [大数据,hdfs]\nauthor: kaishun\nid: 10\npermalink: hdfs-filesystem-1\nblogexcerpt: 本文主要讲解如何用java去操作hdfs，以下是我整理的常用的一些方法，本文主要介绍的是FileSystem，我把其集合到了一个工具类当中，下面的操作主要有 检查文件是否存在，创建文件，创建文件夹，复制（上传）本地文件到hdfs指定目录，复制（上传）本地文件夹到hdfs指定目录，从hdfs下载文件，移动hdfs上的文件或者文件夹，删除文件或者文件夹，HDFS到 HDFS的合并，列出所有DataNode的名字信息，检测是否是备用节点等操作\n---\n\n# **简介:**\n本文主要讲解如何用java去操作hdfs，以下是我整理的常用的一些方法，本文主要介绍的是FileSystem，我把其集合到了一个工具类当中，下面的操作主要有 检查文件是否存在，创建文件，创建文件夹，复制（上传）本地文件到hdfs指定目录，复制（上传）本地文件夹到hdfs指定目录，从hdfs下载文件，移动hdfs上的文件或者文件夹，删除文件或者文件夹，HDFS 到 HDFS 的合并，列出所有DataNode的名字信息，检测是否是备用节点等操作\n![hdfs-commands](http://or49tneld.bkt.clouddn.com/18-1-23/63075597.jpg)\n\n```java\npackage hdfs.tutorial;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.*;\nimport org.apache.hadoop.hdfs.DistributedFileSystem;\nimport org.apache.hadoop.hdfs.protocol.DatanodeInfo;\n\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.net.URI;\nimport java.net.URISyntaxException;\nimport java.util.Date;\nimport java.util.List;\n\n/**\n * Created by Administrator on 2017/5/25.\n */\npublic class HdfsOper {\n    private FileSystem hdfs;\n\n    /**\n     * @return 得到hdfs的连接 FileSystem类\n     * @throws URISyntaxException\n     * @throws IOException\n     * @throws InterruptedException\n     */\n    public static FileSystem getFileSystem() throws URISyntaxException, IOException, InterruptedException {\n        //获取FileSystem类的方法有很多种，这里只写一种\n        Configuration config = new Configuration();\n        URI uri = new URI(\"hdfs://192.xxx.x.xxx:xxx\");\n        return FileSystem.get(uri,config,\"your user\");// 第一位为uri，第二位为config，第三位是登录的用户\n    }\n\n    /**\n     * 检查文件或者文件夹是否存在\n     * @param filename\n     * @return\n     */\n    public boolean checkFileExist(String filename)\n    {\n        try\n        {\n            Path f = new Path(filename);\n            return hdfs.exists(f);\n        }\n        catch (Exception e)\n        {\n            e.printStackTrace();\n        }\n        return false;\n    }\n\n    /**\n     * 创建文件夹\n     * @param dirName\n     * @return\n     */\n    public boolean mkdir(String dirName)\n    {\n        if (checkFileExist(dirName))\n            return true;\n        try\n        {\n            Path f = new Path(dirName);\n            System.out.println(\"Create and Write :\" + f.getName() + \" to hdfs\");\n            return hdfs.mkdirs(f);\n        }\n        catch (Exception e)\n        {\n            e.printStackTrace();\n        }\n\n        return false;\n    }\n\n    /**\n     * 创建一个空文件\n     * @param filePath  文件的完整路径名称\n     * @return\n     */\n    public boolean mkfile(String filePath)\n    {\n        try\n        {\n            Path f = new Path(filePath);\n            FSDataOutputStream os = hdfs.create(f, true);\n            os.close();\n            return true;\n        }\n        catch (IllegalArgumentException e)\n        {\n            e.printStackTrace();\n        }\n        catch (IOException e)\n        {\n            e.printStackTrace();\n        }\n        return false;\n    }\n\n    /**\n     * 复制文件到指定目录\n     * @param srcfile  复制的文件路径\n     * @param desfile  粘贴的路径\n     * @return\n     */\n    public boolean hdfsCopyUtils(String srcfile, String desfile) {\n        Configuration conf = new Configuration();\n        Path src = new Path(srcfile);\n        Path dst = new Path(desfile);\n        try {\n            FileUtil.copy(src.getFileSystem(conf), src,\n                    dst.getFileSystem(conf), dst, false, conf);\n        } catch (IOException e) {\n            return false;\n        }\n        return true;\n    }\n\n    /**\n     * 移动文件或者文件夹\n     * @param src  初始路径\n     * @param dst   移动结束路径\n     * @throws Exception\n     */\n    public void movefile(String src, String dst) throws Exception\n    {\n        Path p1 = new Path(src);\n        Path p2 = new Path(dst);\n        hdfs.rename(p1, p2);\n    }\n\n    /**\n     * 删除文件或者文件夹\n     * @param src\n     * @throws Exception\n     */\n    public void delete(String src) throws Exception\n    {\n        Path p1 = new Path(src);\n        if (hdfs.isDirectory(p1))\n        {\n            hdfs.delete(p1, true);\n            System.out.println(\"删除文件夹成功: \" + src);\n        }\n        else if (hdfs.isFile(p1))\n        {\n            hdfs.delete(p1, false);\n            System.out.println(\"删除文件成功: \" + src);\n        }\n    }\n\n    /**\n     * 读取本地文件到HDFS系统, 保证文件格式是utf-8\n     * @param localFilename\n     * @param hdfsPath\n     * @return\n     */\n    public boolean copyLocalFileToHDFS(String localFilename, String hdfsPath)\n    {\n        try\n        {\n            // 如果路径不存在就创建文件夹\n            mkdir(hdfsPath);\n\n            File file = new File(localFilename);\n            FileInputStream is = new FileInputStream(file);\n\n            // 如果hdfs上已经存在文件，那么先删除该文件\n            if (this.checkFileExist(hdfsPath + \"/\" + file.getName()))\n            {\n                delete(hdfsPath + \"/\" + file.getName());\n            }\n\n            Path f = new Path(hdfsPath + \"/\" + file.getName());\n\n            FSDataOutputStream os = hdfs.create(f, true);\n            byte[] buffer = new byte[10240000];\n            int nCount = 0;\n\n            while (true)\n            {\n                int bytesRead = is.read(buffer);\n                if (bytesRead <= 0)\n                {\n                    break;\n                }\n\n                os.write(buffer, 0, bytesRead);\n                nCount++;\n                if (nCount % (100) == 0)\n                    System.out.println((new Date()).toLocaleString() + \": Have move \" + nCount + \" blocks\");\n            }\n\n            is.close();\n            os.close();\n            System.out.println((new Date()).toLocaleString() + \": Write content of file \" + file.getName()\n                    + \" to hdfs file \" + f.getName() + \" success\");\n            return true;\n        }\n        catch (Exception e)\n        {\n            e.printStackTrace();\n        }\n        return false;\n    }\n\n    /**\n     * 复制本地文件夹到hdfs的文件\n     * @param localPath\n     * @param hdfsPath\n     * @return\n     */\n    public boolean CopyLocalDirTohdfs(String localPath, String hdfsPath)\n    {\n        try\n        {\n            File root = new File(localPath);\n            File[] files = root.listFiles();\n\n            for (File file : files)\n            {\n                if (file.isFile())\n                {\n                    copyLocalFileToHDFS(file.getPath().toString(), hdfsPath);\n\n                }\n                else if(file.isDirectory())\n                {\n                    CopyLocalDirTohdfs(localPath+\"/\"+file.getName(), hdfsPath+\"/\"+file.getName());\n                }\n            }\n            return true;\n        }\n        catch (Exception e)\n        {\n            e.printStackTrace();\n        }\n        return false;\n    }\n\n\n    /**\n     * 从hdfs下载\n     * @param hdfsFilename\n     * @param localPath\n     * @return\n     */\n    public boolean downloadFileFromHdfs(String hdfsFilename, String localPath)\n    {\n        try\n        {\n            Path f = new Path(hdfsFilename);\n\n            FSDataInputStream dis = hdfs.open(f);\n            File file = new File(localPath + \"/\" + f.getName());\n            FileOutputStream os = new FileOutputStream(file);\n\n            byte[] buffer = new byte[1024000];\n            int length = 0;\n            while ((length = dis.read(buffer)) > 0)\n            {\n                os.write(buffer, 0, length);\n            }\n\n            os.close();\n            dis.close();\n\n            return true;\n        }\n        catch (Exception e)\n        {\n            e.printStackTrace();\n        }\n        return false;\n    }\n\n    /**\n     * HDFS 到 HDFS 的合并\n     * hdfs提供了一种FileUtil.copyMerge（）的方法， 注意下面的 false 这个，如果改为true，就会删除这个目录\n     * @param folder 需要合并的目录\n     * @param file  要合并成的文件，完整路径名称\n     */\n    public void copyMerge(String folder, String file) {\n        Configuration conf = new Configuration();\n        Path src = new Path(folder);\n        Path dst = new Path(file);\n\n        try {\n            FileUtil.copyMerge(src.getFileSystem(conf), src,\n                    dst.getFileSystem(conf), dst, false, conf, null);\n        } catch (IOException e) {\n            // TODO Auto-generated catch block\n            e.printStackTrace();\n        }\n    }\n\n\n    /**\n     * 列出所有DataNode的名字信息\n     */\n    public void listDataNodeInfo()\n    {\n        try\n        {\n            DistributedFileSystem fs =null;\n            fs = (DistributedFileSystem) hdfs;\n            DatanodeInfo[] dataNodeStats = fs.getDataNodeStats();\n            String[] names = new String[dataNodeStats.length];\n            System.out.println(\"List of all the datanode in the HDFS cluster:\");\n\n            for (int i = 0; i < names.length; i++)\n            {\n                names[i] = dataNodeStats[i].getHostName();\n                System.out.println(names[i]);\n            }\n            System.out.println(hdfs.getUri().toString());\n        }\n        catch (Exception e)\n        {\n            e.printStackTrace();\n        }\n    }\n\n    /**\n     * 检测是否是备用节点\n     * @throws Exception\n     */\n    public boolean checkStandbyException(String filename)\n    {\n        try {\n            Path f = new Path(filename);\n            hdfs.exists(f);\n        } catch (org.apache.hadoop.ipc.RemoteException e) {\n            if(e.getClassName().equals(\"org.apache.hadoop.ipc.StandbyException\"))\n            {\n                return true;\n            }\n        } catch (Exception e) {\n\n        }\n        return false;\n    }\n\n\n    public boolean mergeDirFiles(List<FileStatus> fileList, String tarPath, String rowTerminateFlag)\n    {\n        //rowTerminateFlag   \\n\n        FSDataOutputStream tarFileOutputStream = null;\n        FSDataInputStream srcFileInputStream = null;\n\n        try\n        {\n            Path tarFile = new Path(tarPath);\n            tarFileOutputStream = hdfs.create(tarFile, true);\n\n            byte[] buffer = new byte[1024000];\n            int length = 0;\n            long nTotalLength = 0;\n            int nCount = 0;\n            boolean bfirst = true;\n            for(FileStatus file : fileList)\n            {\n                if(file.getPath().equals(tarFile))\n                {\n                    continue;\n                }\n                System.out.println(\" merging file from  \" + file.getPath() + \" to \" + tarPath);\n\n                if(!bfirst)\n                {\n                    //添加换行符\n                    tarFileOutputStream.write(rowTerminateFlag.getBytes(), 0, rowTerminateFlag.length());\n                }\n\n                srcFileInputStream = hdfs.open(file.getPath(), buffer.length);\n                while ((length = srcFileInputStream.read(buffer)) > 0)\n                {\n                    nCount++;\n                    tarFileOutputStream.write(buffer, 0, length);\n                    nTotalLength += length;\n                    //System.out.println(\" file length \" + file.getLen() + \" read \" + length);\n                    if (nCount % 1000 == 0)\n                    {\n                        tarFileOutputStream.flush();\n                        System.out.println((new Date()).toLocaleString() + \": Have move \" + (nTotalLength / 1024000) + \" MB\");\n                    }\n\n                }\n\n                srcFileInputStream.close();\n\n                bfirst = false;\n            }\n\n        }\n        catch (Exception e)\n        {\n            e.printStackTrace();\n            try\n            {\n                delete(tarPath);\n            }\n            catch (Exception e2)\n            {\n                // TODO: handle exception\n            }\n            return false;\n        }\n        finally\n        {\n            try\n            {\n                if(tarFileOutputStream != null)\n                {\n                    tarFileOutputStream.flush();\n                    tarFileOutputStream.close();\n                    srcFileInputStream.close();\n                }\n            }\n            catch (Exception e2)\n            {\n                // TODO: handle exception\n            }\n        }\n        return true;\n    }\n\n\n}\n\n\t/**\n\t * 将一个字符串写入某个路径\n\t *\n\t * @param text 要保存的字符串\n\t * @param path 要保存的路径\n\t */\n\tpublic void writerString(String text,String path){\n\n\t\ttry\n\t\t{\n\t\t\tPath f = new Path(path);\n\t\t\tFSDataOutputStream os = fs.create(f, true);\n\t\t\tBufferedWriter  writer = new BufferedWriter(new OutputStreamWriter(os, \"utf-8\"));// 以UTF-8格式写入文件，不乱码\n\t\t\twriter.write(text);\n\t\t\twriter.close();\n\t\t\tos.close();\n\t\t}\n\t\tcatch (Exception e)\n\t\t{\n\t\t\te.printStackTrace();\n\n\t\t}\n\n\t}\n\n\t/**\n\t * 按行读取文件内容，并且防止乱码\n\t * @param hdfsFilename\n\t * @return\n\t */\n\tpublic boolean readByLine(String hdfsFilename)\n\t{\n\t\ttry\n\t\t{\n\t\t\tPath f = new Path(hdfsFilename);\n\n\t\t\tFSDataInputStream dis = hdfs.open(f);\n\n\t\t\tBufferedReader bf=new BufferedReader(new InputStreamReader(dis));//防止中文乱码\n\t\t\tString line = null;\n\t\t\twhile ((line=bf.readLine())!=null)\n\t\t\t{\n\t\t\t\tSystem.out.println(new String(line.getBytes(),\"utf-8\"));\n\t\t\t}\n\n\t\t\tdis.close();\n\t\t\treturn true;\n\t\t}\n\t\tcatch (Exception e)\n\t\t{\n\t\t\te.printStackTrace();\n\t\t}\n\t\treturn false;\n\t}\n\n\tpublic void reNameExistsPath(String srcPath, String tarPath) throws Exception\n\t{\n\t    //检测输出目录是否存在，存在就改名\n\t    if(checkFileExist(srcPath))\n\t    {\n\t    \ttarPath = srcPath.trim();\n\t    \twhile(tarPath.charAt(tarPath.length()-1) == '/')\n\t    \t{\n\t    \t\ttarPath = tarPath.substring(0, tarPath.length()-1);\n\t    \t}\n\t    \tDate now = new Date(); \n\t    \tSimpleDateFormat dateFormat = new SimpleDateFormat(\"yyMMddHHmmss\");\n\t    \tString nowStr = dateFormat.format(now); \n\t    \ttarPath += \"_\" + nowStr;\n\t    \tmovefile(srcPath, tarPath);\n\t    }\n\t    else \n\t    {\n\t    \ttarPath = srcPath;\n\t    }\n\t}\n```","slug":"hdfs-filesystem-1","published":1,"updated":"2018-01-23T14:36:08.344Z","_id":"cjcrpnzms00002wv3fyr62dhb","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介:\"></a><strong>简介:</strong></h1><p>本文主要讲解如何用java去操作hdfs，以下是我整理的常用的一些方法，本文主要介绍的是FileSystem，我把其集合到了一个工具类当中，下面的操作主要有 检查文件是否存在，创建文件，创建文件夹，复制（上传）本地文件到hdfs指定目录，复制（上传）本地文件夹到hdfs指定目录，从hdfs下载文件，移动hdfs上的文件或者文件夹，删除文件或者文件夹，HDFS 到 HDFS 的合并，列出所有DataNode的名字信息，检测是否是备用节点等操作<br><img src=\"http://or49tneld.bkt.clouddn.com/18-1-23/63075597.jpg\" alt=\"hdfs-commands\"></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div><div class=\"line\">106</div><div class=\"line\">107</div><div class=\"line\">108</div><div class=\"line\">109</div><div class=\"line\">110</div><div class=\"line\">111</div><div class=\"line\">112</div><div class=\"line\">113</div><div class=\"line\">114</div><div class=\"line\">115</div><div class=\"line\">116</div><div class=\"line\">117</div><div class=\"line\">118</div><div class=\"line\">119</div><div class=\"line\">120</div><div class=\"line\">121</div><div class=\"line\">122</div><div class=\"line\">123</div><div class=\"line\">124</div><div class=\"line\">125</div><div class=\"line\">126</div><div class=\"line\">127</div><div class=\"line\">128</div><div class=\"line\">129</div><div class=\"line\">130</div><div class=\"line\">131</div><div class=\"line\">132</div><div class=\"line\">133</div><div class=\"line\">134</div><div class=\"line\">135</div><div class=\"line\">136</div><div class=\"line\">137</div><div class=\"line\">138</div><div class=\"line\">139</div><div class=\"line\">140</div><div class=\"line\">141</div><div class=\"line\">142</div><div class=\"line\">143</div><div class=\"line\">144</div><div class=\"line\">145</div><div class=\"line\">146</div><div class=\"line\">147</div><div class=\"line\">148</div><div class=\"line\">149</div><div class=\"line\">150</div><div class=\"line\">151</div><div class=\"line\">152</div><div class=\"line\">153</div><div class=\"line\">154</div><div class=\"line\">155</div><div class=\"line\">156</div><div class=\"line\">157</div><div class=\"line\">158</div><div class=\"line\">159</div><div class=\"line\">160</div><div class=\"line\">161</div><div class=\"line\">162</div><div class=\"line\">163</div><div class=\"line\">164</div><div class=\"line\">165</div><div class=\"line\">166</div><div class=\"line\">167</div><div class=\"line\">168</div><div class=\"line\">169</div><div class=\"line\">170</div><div class=\"line\">171</div><div class=\"line\">172</div><div class=\"line\">173</div><div class=\"line\">174</div><div class=\"line\">175</div><div class=\"line\">176</div><div class=\"line\">177</div><div class=\"line\">178</div><div class=\"line\">179</div><div class=\"line\">180</div><div class=\"line\">181</div><div class=\"line\">182</div><div class=\"line\">183</div><div class=\"line\">184</div><div class=\"line\">185</div><div class=\"line\">186</div><div class=\"line\">187</div><div class=\"line\">188</div><div class=\"line\">189</div><div class=\"line\">190</div><div class=\"line\">191</div><div class=\"line\">192</div><div class=\"line\">193</div><div class=\"line\">194</div><div class=\"line\">195</div><div class=\"line\">196</div><div class=\"line\">197</div><div class=\"line\">198</div><div class=\"line\">199</div><div class=\"line\">200</div><div class=\"line\">201</div><div class=\"line\">202</div><div class=\"line\">203</div><div class=\"line\">204</div><div class=\"line\">205</div><div class=\"line\">206</div><div class=\"line\">207</div><div class=\"line\">208</div><div class=\"line\">209</div><div class=\"line\">210</div><div class=\"line\">211</div><div class=\"line\">212</div><div class=\"line\">213</div><div class=\"line\">214</div><div class=\"line\">215</div><div class=\"line\">216</div><div class=\"line\">217</div><div class=\"line\">218</div><div class=\"line\">219</div><div class=\"line\">220</div><div class=\"line\">221</div><div class=\"line\">222</div><div class=\"line\">223</div><div class=\"line\">224</div><div class=\"line\">225</div><div class=\"line\">226</div><div class=\"line\">227</div><div class=\"line\">228</div><div class=\"line\">229</div><div class=\"line\">230</div><div class=\"line\">231</div><div class=\"line\">232</div><div class=\"line\">233</div><div class=\"line\">234</div><div class=\"line\">235</div><div class=\"line\">236</div><div class=\"line\">237</div><div class=\"line\">238</div><div class=\"line\">239</div><div class=\"line\">240</div><div class=\"line\">241</div><div class=\"line\">242</div><div class=\"line\">243</div><div class=\"line\">244</div><div class=\"line\">245</div><div class=\"line\">246</div><div class=\"line\">247</div><div class=\"line\">248</div><div class=\"line\">249</div><div class=\"line\">250</div><div class=\"line\">251</div><div class=\"line\">252</div><div class=\"line\">253</div><div class=\"line\">254</div><div class=\"line\">255</div><div class=\"line\">256</div><div class=\"line\">257</div><div class=\"line\">258</div><div class=\"line\">259</div><div class=\"line\">260</div><div class=\"line\">261</div><div class=\"line\">262</div><div class=\"line\">263</div><div class=\"line\">264</div><div class=\"line\">265</div><div class=\"line\">266</div><div class=\"line\">267</div><div class=\"line\">268</div><div class=\"line\">269</div><div class=\"line\">270</div><div class=\"line\">271</div><div class=\"line\">272</div><div class=\"line\">273</div><div class=\"line\">274</div><div class=\"line\">275</div><div class=\"line\">276</div><div class=\"line\">277</div><div class=\"line\">278</div><div class=\"line\">279</div><div class=\"line\">280</div><div class=\"line\">281</div><div class=\"line\">282</div><div class=\"line\">283</div><div class=\"line\">284</div><div class=\"line\">285</div><div class=\"line\">286</div><div class=\"line\">287</div><div class=\"line\">288</div><div class=\"line\">289</div><div class=\"line\">290</div><div class=\"line\">291</div><div class=\"line\">292</div><div class=\"line\">293</div><div class=\"line\">294</div><div class=\"line\">295</div><div class=\"line\">296</div><div class=\"line\">297</div><div class=\"line\">298</div><div class=\"line\">299</div><div class=\"line\">300</div><div class=\"line\">301</div><div class=\"line\">302</div><div class=\"line\">303</div><div class=\"line\">304</div><div class=\"line\">305</div><div class=\"line\">306</div><div class=\"line\">307</div><div class=\"line\">308</div><div class=\"line\">309</div><div class=\"line\">310</div><div class=\"line\">311</div><div class=\"line\">312</div><div class=\"line\">313</div><div class=\"line\">314</div><div class=\"line\">315</div><div class=\"line\">316</div><div class=\"line\">317</div><div class=\"line\">318</div><div class=\"line\">319</div><div class=\"line\">320</div><div class=\"line\">321</div><div class=\"line\">322</div><div class=\"line\">323</div><div class=\"line\">324</div><div class=\"line\">325</div><div class=\"line\">326</div><div class=\"line\">327</div><div class=\"line\">328</div><div class=\"line\">329</div><div class=\"line\">330</div><div class=\"line\">331</div><div class=\"line\">332</div><div class=\"line\">333</div><div class=\"line\">334</div><div class=\"line\">335</div><div class=\"line\">336</div><div class=\"line\">337</div><div class=\"line\">338</div><div class=\"line\">339</div><div class=\"line\">340</div><div class=\"line\">341</div><div class=\"line\">342</div><div class=\"line\">343</div><div class=\"line\">344</div><div class=\"line\">345</div><div class=\"line\">346</div><div class=\"line\">347</div><div class=\"line\">348</div><div class=\"line\">349</div><div class=\"line\">350</div><div class=\"line\">351</div><div class=\"line\">352</div><div class=\"line\">353</div><div class=\"line\">354</div><div class=\"line\">355</div><div class=\"line\">356</div><div class=\"line\">357</div><div class=\"line\">358</div><div class=\"line\">359</div><div class=\"line\">360</div><div class=\"line\">361</div><div class=\"line\">362</div><div class=\"line\">363</div><div class=\"line\">364</div><div class=\"line\">365</div><div class=\"line\">366</div><div class=\"line\">367</div><div class=\"line\">368</div><div class=\"line\">369</div><div class=\"line\">370</div><div class=\"line\">371</div><div class=\"line\">372</div><div class=\"line\">373</div><div class=\"line\">374</div><div class=\"line\">375</div><div class=\"line\">376</div><div class=\"line\">377</div><div class=\"line\">378</div><div class=\"line\">379</div><div class=\"line\">380</div><div class=\"line\">381</div><div class=\"line\">382</div><div class=\"line\">383</div><div class=\"line\">384</div><div class=\"line\">385</div><div class=\"line\">386</div><div class=\"line\">387</div><div class=\"line\">388</div><div class=\"line\">389</div><div class=\"line\">390</div><div class=\"line\">391</div><div class=\"line\">392</div><div class=\"line\">393</div><div class=\"line\">394</div><div class=\"line\">395</div><div class=\"line\">396</div><div class=\"line\">397</div><div class=\"line\">398</div><div class=\"line\">399</div><div class=\"line\">400</div><div class=\"line\">401</div><div class=\"line\">402</div><div class=\"line\">403</div><div class=\"line\">404</div><div class=\"line\">405</div><div class=\"line\">406</div><div class=\"line\">407</div><div class=\"line\">408</div><div class=\"line\">409</div><div class=\"line\">410</div><div class=\"line\">411</div><div class=\"line\">412</div><div class=\"line\">413</div><div class=\"line\">414</div><div class=\"line\">415</div><div class=\"line\">416</div><div class=\"line\">417</div><div class=\"line\">418</div><div class=\"line\">419</div><div class=\"line\">420</div><div class=\"line\">421</div><div class=\"line\">422</div><div class=\"line\">423</div><div class=\"line\">424</div><div class=\"line\">425</div><div class=\"line\">426</div><div class=\"line\">427</div><div class=\"line\">428</div><div class=\"line\">429</div><div class=\"line\">430</div><div class=\"line\">431</div><div class=\"line\">432</div><div class=\"line\">433</div><div class=\"line\">434</div><div class=\"line\">435</div><div class=\"line\">436</div><div class=\"line\">437</div><div class=\"line\">438</div><div class=\"line\">439</div><div class=\"line\">440</div><div class=\"line\">441</div><div class=\"line\">442</div><div class=\"line\">443</div><div class=\"line\">444</div><div class=\"line\">445</div><div class=\"line\">446</div><div class=\"line\">447</div><div class=\"line\">448</div><div class=\"line\">449</div><div class=\"line\">450</div><div class=\"line\">451</div><div class=\"line\">452</div><div class=\"line\">453</div><div class=\"line\">454</div><div class=\"line\">455</div><div class=\"line\">456</div><div class=\"line\">457</div><div class=\"line\">458</div><div class=\"line\">459</div><div class=\"line\">460</div><div class=\"line\">461</div><div class=\"line\">462</div><div class=\"line\">463</div><div class=\"line\">464</div><div class=\"line\">465</div><div class=\"line\">466</div><div class=\"line\">467</div><div class=\"line\">468</div><div class=\"line\">469</div><div class=\"line\">470</div><div class=\"line\">471</div><div class=\"line\">472</div><div class=\"line\">473</div><div class=\"line\">474</div><div class=\"line\">475</div><div class=\"line\">476</div><div class=\"line\">477</div><div class=\"line\">478</div><div class=\"line\">479</div><div class=\"line\">480</div><div class=\"line\">481</div><div class=\"line\">482</div><div class=\"line\">483</div><div class=\"line\">484</div><div class=\"line\">485</div><div class=\"line\">486</div><div class=\"line\">487</div><div class=\"line\">488</div><div class=\"line\">489</div><div class=\"line\">490</div><div class=\"line\">491</div><div class=\"line\">492</div><div class=\"line\">493</div><div class=\"line\">494</div><div class=\"line\">495</div><div class=\"line\">496</div><div class=\"line\">497</div><div class=\"line\">498</div><div class=\"line\">499</div><div class=\"line\">500</div><div class=\"line\">501</div><div class=\"line\">502</div><div class=\"line\">503</div><div class=\"line\">504</div><div class=\"line\">505</div><div class=\"line\">506</div><div class=\"line\">507</div><div class=\"line\">508</div><div class=\"line\">509</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">package</span> hdfs.tutorial;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.conf.Configuration;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.fs.*;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.hdfs.DistributedFileSystem;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.hdfs.protocol.DatanodeInfo;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">import</span> java.io.File;</div><div class=\"line\"><span class=\"keyword\">import</span> java.io.FileInputStream;</div><div class=\"line\"><span class=\"keyword\">import</span> java.io.FileOutputStream;</div><div class=\"line\"><span class=\"keyword\">import</span> java.io.IOException;</div><div class=\"line\"><span class=\"keyword\">import</span> java.net.URI;</div><div class=\"line\"><span class=\"keyword\">import</span> java.net.URISyntaxException;</div><div class=\"line\"><span class=\"keyword\">import</span> java.util.Date;</div><div class=\"line\"><span class=\"keyword\">import</span> java.util.List;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\"> * Created by Administrator on 2017/5/25.</span></div><div class=\"line\"><span class=\"comment\"> */</span></div><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">HdfsOper</span> </span>&#123;</div><div class=\"line\">    <span class=\"keyword\">private</span> FileSystem hdfs;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span> 得到hdfs的连接 FileSystem类</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@throws</span> URISyntaxException</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@throws</span> IOException</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@throws</span> InterruptedException</span></div><div class=\"line\"><span class=\"comment\">     */</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> FileSystem <span class=\"title\">getFileSystem</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> URISyntaxException, IOException, InterruptedException </span>&#123;</div><div class=\"line\">        <span class=\"comment\">//获取FileSystem类的方法有很多种，这里只写一种</span></div><div class=\"line\">        Configuration config = <span class=\"keyword\">new</span> Configuration();</div><div class=\"line\">        URI uri = <span class=\"keyword\">new</span> URI(<span class=\"string\">\"hdfs://192.xxx.x.xxx:xxx\"</span>);</div><div class=\"line\">        <span class=\"keyword\">return</span> FileSystem.get(uri,config,<span class=\"string\">\"your user\"</span>);<span class=\"comment\">// 第一位为uri，第二位为config，第三位是登录的用户</span></div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\">     * 检查文件或者文件夹是否存在</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> filename</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span></span></div><div class=\"line\"><span class=\"comment\">     */</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">checkFileExist</span><span class=\"params\">(String filename)</span></span></div><div class=\"line\"><span class=\"function\">    </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">try</span></div><div class=\"line\">        &#123;</div><div class=\"line\">            Path f = <span class=\"keyword\">new</span> Path(filename);</div><div class=\"line\">            <span class=\"keyword\">return</span> hdfs.exists(f);</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">catch</span> (Exception e)</div><div class=\"line\">        &#123;</div><div class=\"line\">            e.printStackTrace();</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\">     * 创建文件夹</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> dirName</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span></span></div><div class=\"line\"><span class=\"comment\">     */</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">mkdir</span><span class=\"params\">(String dirName)</span></span></div><div class=\"line\"><span class=\"function\">    </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">if</span> (checkFileExist(dirName))</div><div class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</div><div class=\"line\">        <span class=\"keyword\">try</span></div><div class=\"line\">        &#123;</div><div class=\"line\">            Path f = <span class=\"keyword\">new</span> Path(dirName);</div><div class=\"line\">            System.out.println(<span class=\"string\">\"Create and Write :\"</span> + f.getName() + <span class=\"string\">\" to hdfs\"</span>);</div><div class=\"line\">            <span class=\"keyword\">return</span> hdfs.mkdirs(f);</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">catch</span> (Exception e)</div><div class=\"line\">        &#123;</div><div class=\"line\">            e.printStackTrace();</div><div class=\"line\">        &#125;</div><div class=\"line\"></div><div class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\">     * 创建一个空文件</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> filePath  文件的完整路径名称</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span></span></div><div class=\"line\"><span class=\"comment\">     */</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">mkfile</span><span class=\"params\">(String filePath)</span></span></div><div class=\"line\"><span class=\"function\">    </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">try</span></div><div class=\"line\">        &#123;</div><div class=\"line\">            Path f = <span class=\"keyword\">new</span> Path(filePath);</div><div class=\"line\">            FSDataOutputStream os = hdfs.create(f, <span class=\"keyword\">true</span>);</div><div class=\"line\">            os.close();</div><div class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">catch</span> (IllegalArgumentException e)</div><div class=\"line\">        &#123;</div><div class=\"line\">            e.printStackTrace();</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">catch</span> (IOException e)</div><div class=\"line\">        &#123;</div><div class=\"line\">            e.printStackTrace();</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\">     * 复制文件到指定目录</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> srcfile  复制的文件路径</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> desfile  粘贴的路径</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span></span></div><div class=\"line\"><span class=\"comment\">     */</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">hdfsCopyUtils</span><span class=\"params\">(String srcfile, String desfile)</span> </span>&#123;</div><div class=\"line\">        Configuration conf = <span class=\"keyword\">new</span> Configuration();</div><div class=\"line\">        Path src = <span class=\"keyword\">new</span> Path(srcfile);</div><div class=\"line\">        Path dst = <span class=\"keyword\">new</span> Path(desfile);</div><div class=\"line\">        <span class=\"keyword\">try</span> &#123;</div><div class=\"line\">            FileUtil.copy(src.getFileSystem(conf), src,</div><div class=\"line\">                    dst.getFileSystem(conf), dst, <span class=\"keyword\">false</span>, conf);</div><div class=\"line\">        &#125; <span class=\"keyword\">catch</span> (IOException e) &#123;</div><div class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\">     * 移动文件或者文件夹</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> src  初始路径</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> dst   移动结束路径</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@throws</span> Exception</span></div><div class=\"line\"><span class=\"comment\">     */</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">movefile</span><span class=\"params\">(String src, String dst)</span> <span class=\"keyword\">throws</span> Exception</span></div><div class=\"line\"><span class=\"function\">    </span>&#123;</div><div class=\"line\">        Path p1 = <span class=\"keyword\">new</span> Path(src);</div><div class=\"line\">        Path p2 = <span class=\"keyword\">new</span> Path(dst);</div><div class=\"line\">        hdfs.rename(p1, p2);</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\">     * 删除文件或者文件夹</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> src</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@throws</span> Exception</span></div><div class=\"line\"><span class=\"comment\">     */</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">delete</span><span class=\"params\">(String src)</span> <span class=\"keyword\">throws</span> Exception</span></div><div class=\"line\"><span class=\"function\">    </span>&#123;</div><div class=\"line\">        Path p1 = <span class=\"keyword\">new</span> Path(src);</div><div class=\"line\">        <span class=\"keyword\">if</span> (hdfs.isDirectory(p1))</div><div class=\"line\">        &#123;</div><div class=\"line\">            hdfs.delete(p1, <span class=\"keyword\">true</span>);</div><div class=\"line\">            System.out.println(<span class=\"string\">\"删除文件夹成功: \"</span> + src);</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (hdfs.isFile(p1))</div><div class=\"line\">        &#123;</div><div class=\"line\">            hdfs.delete(p1, <span class=\"keyword\">false</span>);</div><div class=\"line\">            System.out.println(<span class=\"string\">\"删除文件成功: \"</span> + src);</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\">     * 读取本地文件到HDFS系统, 保证文件格式是utf-8</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> localFilename</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> hdfsPath</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span></span></div><div class=\"line\"><span class=\"comment\">     */</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">copyLocalFileToHDFS</span><span class=\"params\">(String localFilename, String hdfsPath)</span></span></div><div class=\"line\"><span class=\"function\">    </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">try</span></div><div class=\"line\">        &#123;</div><div class=\"line\">            <span class=\"comment\">// 如果路径不存在就创建文件夹</span></div><div class=\"line\">            mkdir(hdfsPath);</div><div class=\"line\"></div><div class=\"line\">            File file = <span class=\"keyword\">new</span> File(localFilename);</div><div class=\"line\">            FileInputStream is = <span class=\"keyword\">new</span> FileInputStream(file);</div><div class=\"line\"></div><div class=\"line\">            <span class=\"comment\">// 如果hdfs上已经存在文件，那么先删除该文件</span></div><div class=\"line\">            <span class=\"keyword\">if</span> (<span class=\"keyword\">this</span>.checkFileExist(hdfsPath + <span class=\"string\">\"/\"</span> + file.getName()))</div><div class=\"line\">            &#123;</div><div class=\"line\">                delete(hdfsPath + <span class=\"string\">\"/\"</span> + file.getName());</div><div class=\"line\">            &#125;</div><div class=\"line\"></div><div class=\"line\">            Path f = <span class=\"keyword\">new</span> Path(hdfsPath + <span class=\"string\">\"/\"</span> + file.getName());</div><div class=\"line\"></div><div class=\"line\">            FSDataOutputStream os = hdfs.create(f, <span class=\"keyword\">true</span>);</div><div class=\"line\">            <span class=\"keyword\">byte</span>[] buffer = <span class=\"keyword\">new</span> <span class=\"keyword\">byte</span>[<span class=\"number\">10240000</span>];</div><div class=\"line\">            <span class=\"keyword\">int</span> nCount = <span class=\"number\">0</span>;</div><div class=\"line\"></div><div class=\"line\">            <span class=\"keyword\">while</span> (<span class=\"keyword\">true</span>)</div><div class=\"line\">            &#123;</div><div class=\"line\">                <span class=\"keyword\">int</span> bytesRead = is.read(buffer);</div><div class=\"line\">                <span class=\"keyword\">if</span> (bytesRead &lt;= <span class=\"number\">0</span>)</div><div class=\"line\">                &#123;</div><div class=\"line\">                    <span class=\"keyword\">break</span>;</div><div class=\"line\">                &#125;</div><div class=\"line\"></div><div class=\"line\">                os.write(buffer, <span class=\"number\">0</span>, bytesRead);</div><div class=\"line\">                nCount++;</div><div class=\"line\">                <span class=\"keyword\">if</span> (nCount % (<span class=\"number\">100</span>) == <span class=\"number\">0</span>)</div><div class=\"line\">                    System.out.println((<span class=\"keyword\">new</span> Date()).toLocaleString() + <span class=\"string\">\": Have move \"</span> + nCount + <span class=\"string\">\" blocks\"</span>);</div><div class=\"line\">            &#125;</div><div class=\"line\"></div><div class=\"line\">            is.close();</div><div class=\"line\">            os.close();</div><div class=\"line\">            System.out.println((<span class=\"keyword\">new</span> Date()).toLocaleString() + <span class=\"string\">\": Write content of file \"</span> + file.getName()</div><div class=\"line\">                    + <span class=\"string\">\" to hdfs file \"</span> + f.getName() + <span class=\"string\">\" success\"</span>);</div><div class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">catch</span> (Exception e)</div><div class=\"line\">        &#123;</div><div class=\"line\">            e.printStackTrace();</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\">     * 复制本地文件夹到hdfs的文件</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> localPath</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> hdfsPath</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span></span></div><div class=\"line\"><span class=\"comment\">     */</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">CopyLocalDirTohdfs</span><span class=\"params\">(String localPath, String hdfsPath)</span></span></div><div class=\"line\"><span class=\"function\">    </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">try</span></div><div class=\"line\">        &#123;</div><div class=\"line\">            File root = <span class=\"keyword\">new</span> File(localPath);</div><div class=\"line\">            File[] files = root.listFiles();</div><div class=\"line\"></div><div class=\"line\">            <span class=\"keyword\">for</span> (File file : files)</div><div class=\"line\">            &#123;</div><div class=\"line\">                <span class=\"keyword\">if</span> (file.isFile())</div><div class=\"line\">                &#123;</div><div class=\"line\">                    copyLocalFileToHDFS(file.getPath().toString(), hdfsPath);</div><div class=\"line\"></div><div class=\"line\">                &#125;</div><div class=\"line\">                <span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(file.isDirectory())</div><div class=\"line\">                &#123;</div><div class=\"line\">                    CopyLocalDirTohdfs(localPath+<span class=\"string\">\"/\"</span>+file.getName(), hdfsPath+<span class=\"string\">\"/\"</span>+file.getName());</div><div class=\"line\">                &#125;</div><div class=\"line\">            &#125;</div><div class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">catch</span> (Exception e)</div><div class=\"line\">        &#123;</div><div class=\"line\">            e.printStackTrace();</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\">     * 从hdfs下载</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> hdfsFilename</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> localPath</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span></span></div><div class=\"line\"><span class=\"comment\">     */</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">downloadFileFromHdfs</span><span class=\"params\">(String hdfsFilename, String localPath)</span></span></div><div class=\"line\"><span class=\"function\">    </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">try</span></div><div class=\"line\">        &#123;</div><div class=\"line\">            Path f = <span class=\"keyword\">new</span> Path(hdfsFilename);</div><div class=\"line\"></div><div class=\"line\">            FSDataInputStream dis = hdfs.open(f);</div><div class=\"line\">            File file = <span class=\"keyword\">new</span> File(localPath + <span class=\"string\">\"/\"</span> + f.getName());</div><div class=\"line\">            FileOutputStream os = <span class=\"keyword\">new</span> FileOutputStream(file);</div><div class=\"line\"></div><div class=\"line\">            <span class=\"keyword\">byte</span>[] buffer = <span class=\"keyword\">new</span> <span class=\"keyword\">byte</span>[<span class=\"number\">1024000</span>];</div><div class=\"line\">            <span class=\"keyword\">int</span> length = <span class=\"number\">0</span>;</div><div class=\"line\">            <span class=\"keyword\">while</span> ((length = dis.read(buffer)) &gt; <span class=\"number\">0</span>)</div><div class=\"line\">            &#123;</div><div class=\"line\">                os.write(buffer, <span class=\"number\">0</span>, length);</div><div class=\"line\">            &#125;</div><div class=\"line\"></div><div class=\"line\">            os.close();</div><div class=\"line\">            dis.close();</div><div class=\"line\"></div><div class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">catch</span> (Exception e)</div><div class=\"line\">        &#123;</div><div class=\"line\">            e.printStackTrace();</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\">     * HDFS 到 HDFS 的合并</span></div><div class=\"line\"><span class=\"comment\">     * hdfs提供了一种FileUtil.copyMerge（）的方法， 注意下面的 false 这个，如果改为true，就会删除这个目录</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> folder 需要合并的目录</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> file  要合并成的文件，完整路径名称</span></div><div class=\"line\"><span class=\"comment\">     */</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">copyMerge</span><span class=\"params\">(String folder, String file)</span> </span>&#123;</div><div class=\"line\">        Configuration conf = <span class=\"keyword\">new</span> Configuration();</div><div class=\"line\">        Path src = <span class=\"keyword\">new</span> Path(folder);</div><div class=\"line\">        Path dst = <span class=\"keyword\">new</span> Path(file);</div><div class=\"line\"></div><div class=\"line\">        <span class=\"keyword\">try</span> &#123;</div><div class=\"line\">            FileUtil.copyMerge(src.getFileSystem(conf), src,</div><div class=\"line\">                    dst.getFileSystem(conf), dst, <span class=\"keyword\">false</span>, conf, <span class=\"keyword\">null</span>);</div><div class=\"line\">        &#125; <span class=\"keyword\">catch</span> (IOException e) &#123;</div><div class=\"line\">            <span class=\"comment\">// TODO Auto-generated catch block</span></div><div class=\"line\">            e.printStackTrace();</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\">     * 列出所有DataNode的名字信息</span></div><div class=\"line\"><span class=\"comment\">     */</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">listDataNodeInfo</span><span class=\"params\">()</span></span></div><div class=\"line\"><span class=\"function\">    </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">try</span></div><div class=\"line\">        &#123;</div><div class=\"line\">            DistributedFileSystem fs =<span class=\"keyword\">null</span>;</div><div class=\"line\">            fs = (DistributedFileSystem) hdfs;</div><div class=\"line\">            DatanodeInfo[] dataNodeStats = fs.getDataNodeStats();</div><div class=\"line\">            String[] names = <span class=\"keyword\">new</span> String[dataNodeStats.length];</div><div class=\"line\">            System.out.println(<span class=\"string\">\"List of all the datanode in the HDFS cluster:\"</span>);</div><div class=\"line\"></div><div class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; names.length; i++)</div><div class=\"line\">            &#123;</div><div class=\"line\">                names[i] = dataNodeStats[i].getHostName();</div><div class=\"line\">                System.out.println(names[i]);</div><div class=\"line\">            &#125;</div><div class=\"line\">            System.out.println(hdfs.getUri().toString());</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">catch</span> (Exception e)</div><div class=\"line\">        &#123;</div><div class=\"line\">            e.printStackTrace();</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\">     * 检测是否是备用节点</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@throws</span> Exception</span></div><div class=\"line\"><span class=\"comment\">     */</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">checkStandbyException</span><span class=\"params\">(String filename)</span></span></div><div class=\"line\"><span class=\"function\">    </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">try</span> &#123;</div><div class=\"line\">            Path f = <span class=\"keyword\">new</span> Path(filename);</div><div class=\"line\">            hdfs.exists(f);</div><div class=\"line\">        &#125; <span class=\"keyword\">catch</span> (org.apache.hadoop.ipc.RemoteException e) &#123;</div><div class=\"line\">            <span class=\"keyword\">if</span>(e.getClassName().equals(<span class=\"string\">\"org.apache.hadoop.ipc.StandbyException\"</span>))</div><div class=\"line\">            &#123;</div><div class=\"line\">                <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125; <span class=\"keyword\">catch</span> (Exception e) &#123;</div><div class=\"line\"></div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">mergeDirFiles</span><span class=\"params\">(List&lt;FileStatus&gt; fileList, String tarPath, String rowTerminateFlag)</span></span></div><div class=\"line\"><span class=\"function\">    </span>&#123;</div><div class=\"line\">        <span class=\"comment\">//rowTerminateFlag   \\n</span></div><div class=\"line\">        FSDataOutputStream tarFileOutputStream = <span class=\"keyword\">null</span>;</div><div class=\"line\">        FSDataInputStream srcFileInputStream = <span class=\"keyword\">null</span>;</div><div class=\"line\"></div><div class=\"line\">        <span class=\"keyword\">try</span></div><div class=\"line\">        &#123;</div><div class=\"line\">            Path tarFile = <span class=\"keyword\">new</span> Path(tarPath);</div><div class=\"line\">            tarFileOutputStream = hdfs.create(tarFile, <span class=\"keyword\">true</span>);</div><div class=\"line\"></div><div class=\"line\">            <span class=\"keyword\">byte</span>[] buffer = <span class=\"keyword\">new</span> <span class=\"keyword\">byte</span>[<span class=\"number\">1024000</span>];</div><div class=\"line\">            <span class=\"keyword\">int</span> length = <span class=\"number\">0</span>;</div><div class=\"line\">            <span class=\"keyword\">long</span> nTotalLength = <span class=\"number\">0</span>;</div><div class=\"line\">            <span class=\"keyword\">int</span> nCount = <span class=\"number\">0</span>;</div><div class=\"line\">            <span class=\"keyword\">boolean</span> bfirst = <span class=\"keyword\">true</span>;</div><div class=\"line\">            <span class=\"keyword\">for</span>(FileStatus file : fileList)</div><div class=\"line\">            &#123;</div><div class=\"line\">                <span class=\"keyword\">if</span>(file.getPath().equals(tarFile))</div><div class=\"line\">                &#123;</div><div class=\"line\">                    <span class=\"keyword\">continue</span>;</div><div class=\"line\">                &#125;</div><div class=\"line\">                System.out.println(<span class=\"string\">\" merging file from  \"</span> + file.getPath() + <span class=\"string\">\" to \"</span> + tarPath);</div><div class=\"line\"></div><div class=\"line\">                <span class=\"keyword\">if</span>(!bfirst)</div><div class=\"line\">                &#123;</div><div class=\"line\">                    <span class=\"comment\">//添加换行符</span></div><div class=\"line\">                    tarFileOutputStream.write(rowTerminateFlag.getBytes(), <span class=\"number\">0</span>, rowTerminateFlag.length());</div><div class=\"line\">                &#125;</div><div class=\"line\"></div><div class=\"line\">                srcFileInputStream = hdfs.open(file.getPath(), buffer.length);</div><div class=\"line\">                <span class=\"keyword\">while</span> ((length = srcFileInputStream.read(buffer)) &gt; <span class=\"number\">0</span>)</div><div class=\"line\">                &#123;</div><div class=\"line\">                    nCount++;</div><div class=\"line\">                    tarFileOutputStream.write(buffer, <span class=\"number\">0</span>, length);</div><div class=\"line\">                    nTotalLength += length;</div><div class=\"line\">                    <span class=\"comment\">//System.out.println(\" file length \" + file.getLen() + \" read \" + length);</span></div><div class=\"line\">                    <span class=\"keyword\">if</span> (nCount % <span class=\"number\">1000</span> == <span class=\"number\">0</span>)</div><div class=\"line\">                    &#123;</div><div class=\"line\">                        tarFileOutputStream.flush();</div><div class=\"line\">                        System.out.println((<span class=\"keyword\">new</span> Date()).toLocaleString() + <span class=\"string\">\": Have move \"</span> + (nTotalLength / <span class=\"number\">1024000</span>) + <span class=\"string\">\" MB\"</span>);</div><div class=\"line\">                    &#125;</div><div class=\"line\"></div><div class=\"line\">                &#125;</div><div class=\"line\"></div><div class=\"line\">                srcFileInputStream.close();</div><div class=\"line\"></div><div class=\"line\">                bfirst = <span class=\"keyword\">false</span>;</div><div class=\"line\">            &#125;</div><div class=\"line\"></div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">catch</span> (Exception e)</div><div class=\"line\">        &#123;</div><div class=\"line\">            e.printStackTrace();</div><div class=\"line\">            <span class=\"keyword\">try</span></div><div class=\"line\">            &#123;</div><div class=\"line\">                delete(tarPath);</div><div class=\"line\">            &#125;</div><div class=\"line\">            <span class=\"keyword\">catch</span> (Exception e2)</div><div class=\"line\">            &#123;</div><div class=\"line\">                <span class=\"comment\">// <span class=\"doctag\">TODO:</span> handle exception</span></div><div class=\"line\">            &#125;</div><div class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">finally</span></div><div class=\"line\">        &#123;</div><div class=\"line\">            <span class=\"keyword\">try</span></div><div class=\"line\">            &#123;</div><div class=\"line\">                <span class=\"keyword\">if</span>(tarFileOutputStream != <span class=\"keyword\">null</span>)</div><div class=\"line\">                &#123;</div><div class=\"line\">                    tarFileOutputStream.flush();</div><div class=\"line\">                    tarFileOutputStream.close();</div><div class=\"line\">                    srcFileInputStream.close();</div><div class=\"line\">                &#125;</div><div class=\"line\">            &#125;</div><div class=\"line\">            <span class=\"keyword\">catch</span> (Exception e2)</div><div class=\"line\">            &#123;</div><div class=\"line\">                <span class=\"comment\">// <span class=\"doctag\">TODO:</span> handle exception</span></div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">\t<span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\">\t * 将一个字符串写入某个路径</span></div><div class=\"line\"><span class=\"comment\">\t *</span></div><div class=\"line\"><span class=\"comment\">\t * <span class=\"doctag\">@param</span> text 要保存的字符串</span></div><div class=\"line\"><span class=\"comment\">\t * <span class=\"doctag\">@param</span> path 要保存的路径</span></div><div class=\"line\"><span class=\"comment\">\t */</span></div><div class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">writerString</span><span class=\"params\">(String text,String path)</span></span>&#123;</div><div class=\"line\"></div><div class=\"line\">\t\t<span class=\"keyword\">try</span></div><div class=\"line\">\t\t&#123;</div><div class=\"line\">\t\t\tPath f = <span class=\"keyword\">new</span> Path(path);</div><div class=\"line\">\t\t\tFSDataOutputStream os = fs.create(f, <span class=\"keyword\">true</span>);</div><div class=\"line\">\t\t\tBufferedWriter  writer = <span class=\"keyword\">new</span> BufferedWriter(<span class=\"keyword\">new</span> OutputStreamWriter(os, <span class=\"string\">\"utf-8\"</span>));<span class=\"comment\">// 以UTF-8格式写入文件，不乱码</span></div><div class=\"line\">\t\t\twriter.write(text);</div><div class=\"line\">\t\t\twriter.close();</div><div class=\"line\">\t\t\tos.close();</div><div class=\"line\">\t\t&#125;</div><div class=\"line\">\t\t<span class=\"keyword\">catch</span> (Exception e)</div><div class=\"line\">\t\t&#123;</div><div class=\"line\">\t\t\te.printStackTrace();</div><div class=\"line\"></div><div class=\"line\">\t\t&#125;</div><div class=\"line\"></div><div class=\"line\">\t&#125;</div><div class=\"line\"></div><div class=\"line\">\t<span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\">\t * 按行读取文件内容，并且防止乱码</span></div><div class=\"line\"><span class=\"comment\">\t * <span class=\"doctag\">@param</span> hdfsFilename</span></div><div class=\"line\"><span class=\"comment\">\t * <span class=\"doctag\">@return</span></span></div><div class=\"line\"><span class=\"comment\">\t */</span></div><div class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">readByLine</span><span class=\"params\">(String hdfsFilename)</span></span></div><div class=\"line\"><span class=\"function\">\t</span>&#123;</div><div class=\"line\">\t\t<span class=\"keyword\">try</span></div><div class=\"line\">\t\t&#123;</div><div class=\"line\">\t\t\tPath f = <span class=\"keyword\">new</span> Path(hdfsFilename);</div><div class=\"line\"></div><div class=\"line\">\t\t\tFSDataInputStream dis = hdfs.open(f);</div><div class=\"line\"></div><div class=\"line\">\t\t\tBufferedReader bf=<span class=\"keyword\">new</span> BufferedReader(<span class=\"keyword\">new</span> InputStreamReader(dis));<span class=\"comment\">//防止中文乱码</span></div><div class=\"line\">\t\t\tString line = <span class=\"keyword\">null</span>;</div><div class=\"line\">\t\t\t<span class=\"keyword\">while</span> ((line=bf.readLine())!=<span class=\"keyword\">null</span>)</div><div class=\"line\">\t\t\t&#123;</div><div class=\"line\">\t\t\t\tSystem.out.println(<span class=\"keyword\">new</span> String(line.getBytes(),<span class=\"string\">\"utf-8\"</span>));</div><div class=\"line\">\t\t\t&#125;</div><div class=\"line\"></div><div class=\"line\">\t\t\tdis.close();</div><div class=\"line\">\t\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</div><div class=\"line\">\t\t&#125;</div><div class=\"line\">\t\t<span class=\"keyword\">catch</span> (Exception e)</div><div class=\"line\">\t\t&#123;</div><div class=\"line\">\t\t\te.printStackTrace();</div><div class=\"line\">\t\t&#125;</div><div class=\"line\">\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</div><div class=\"line\">\t&#125;</div><div class=\"line\"></div><div class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">reNameExistsPath</span><span class=\"params\">(String srcPath, String tarPath)</span> <span class=\"keyword\">throws</span> Exception</span></div><div class=\"line\"><span class=\"function\">\t</span>&#123;</div><div class=\"line\">\t    <span class=\"comment\">//检测输出目录是否存在，存在就改名</span></div><div class=\"line\">\t    <span class=\"keyword\">if</span>(checkFileExist(srcPath))</div><div class=\"line\">\t    &#123;</div><div class=\"line\">\t    \ttarPath = srcPath.trim();</div><div class=\"line\">\t    \t<span class=\"keyword\">while</span>(tarPath.charAt(tarPath.length()-<span class=\"number\">1</span>) == <span class=\"string\">'/'</span>)</div><div class=\"line\">\t    \t&#123;</div><div class=\"line\">\t    \t\ttarPath = tarPath.substring(<span class=\"number\">0</span>, tarPath.length()-<span class=\"number\">1</span>);</div><div class=\"line\">\t    \t&#125;</div><div class=\"line\">\t    \tDate now = <span class=\"keyword\">new</span> Date(); </div><div class=\"line\">\t    \tSimpleDateFormat dateFormat = <span class=\"keyword\">new</span> SimpleDateFormat(<span class=\"string\">\"yyMMddHHmmss\"</span>);</div><div class=\"line\">\t    \tString nowStr = dateFormat.format(now); </div><div class=\"line\">\t    \ttarPath += <span class=\"string\">\"_\"</span> + nowStr;</div><div class=\"line\">\t    \tmovefile(srcPath, tarPath);</div><div class=\"line\">\t    &#125;</div><div class=\"line\">\t    <span class=\"keyword\">else</span> </div><div class=\"line\">\t    &#123;</div><div class=\"line\">\t    \ttarPath = srcPath;</div><div class=\"line\">\t    &#125;</div><div class=\"line\">\t&#125;</div></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介:\"></a><strong>简介:</strong></h1><p>本文主要讲解如何用java去操作hdfs，以下是我整理的常用的一些方法，本文主要介绍的是FileSystem，我把其集合到了一个工具类当中，下面的操作主要有 检查文件是否存在，创建文件，创建文件夹，复制（上传）本地文件到hdfs指定目录，复制（上传）本地文件夹到hdfs指定目录，从hdfs下载文件，移动hdfs上的文件或者文件夹，删除文件或者文件夹，HDFS 到 HDFS 的合并，列出所有DataNode的名字信息，检测是否是备用节点等操作<br><img src=\"http://or49tneld.bkt.clouddn.com/18-1-23/63075597.jpg\" alt=\"hdfs-commands\"></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div><div class=\"line\">106</div><div class=\"line\">107</div><div class=\"line\">108</div><div class=\"line\">109</div><div class=\"line\">110</div><div class=\"line\">111</div><div class=\"line\">112</div><div class=\"line\">113</div><div class=\"line\">114</div><div class=\"line\">115</div><div class=\"line\">116</div><div class=\"line\">117</div><div class=\"line\">118</div><div class=\"line\">119</div><div class=\"line\">120</div><div class=\"line\">121</div><div class=\"line\">122</div><div class=\"line\">123</div><div class=\"line\">124</div><div class=\"line\">125</div><div class=\"line\">126</div><div class=\"line\">127</div><div class=\"line\">128</div><div class=\"line\">129</div><div class=\"line\">130</div><div class=\"line\">131</div><div class=\"line\">132</div><div class=\"line\">133</div><div class=\"line\">134</div><div class=\"line\">135</div><div class=\"line\">136</div><div class=\"line\">137</div><div class=\"line\">138</div><div class=\"line\">139</div><div class=\"line\">140</div><div class=\"line\">141</div><div class=\"line\">142</div><div class=\"line\">143</div><div class=\"line\">144</div><div class=\"line\">145</div><div class=\"line\">146</div><div class=\"line\">147</div><div class=\"line\">148</div><div class=\"line\">149</div><div class=\"line\">150</div><div class=\"line\">151</div><div class=\"line\">152</div><div class=\"line\">153</div><div class=\"line\">154</div><div class=\"line\">155</div><div class=\"line\">156</div><div class=\"line\">157</div><div class=\"line\">158</div><div class=\"line\">159</div><div class=\"line\">160</div><div class=\"line\">161</div><div class=\"line\">162</div><div class=\"line\">163</div><div class=\"line\">164</div><div class=\"line\">165</div><div class=\"line\">166</div><div class=\"line\">167</div><div class=\"line\">168</div><div class=\"line\">169</div><div class=\"line\">170</div><div class=\"line\">171</div><div class=\"line\">172</div><div class=\"line\">173</div><div class=\"line\">174</div><div class=\"line\">175</div><div class=\"line\">176</div><div class=\"line\">177</div><div class=\"line\">178</div><div class=\"line\">179</div><div class=\"line\">180</div><div class=\"line\">181</div><div class=\"line\">182</div><div class=\"line\">183</div><div class=\"line\">184</div><div class=\"line\">185</div><div class=\"line\">186</div><div class=\"line\">187</div><div class=\"line\">188</div><div class=\"line\">189</div><div class=\"line\">190</div><div class=\"line\">191</div><div class=\"line\">192</div><div class=\"line\">193</div><div class=\"line\">194</div><div class=\"line\">195</div><div class=\"line\">196</div><div class=\"line\">197</div><div class=\"line\">198</div><div class=\"line\">199</div><div class=\"line\">200</div><div class=\"line\">201</div><div class=\"line\">202</div><div class=\"line\">203</div><div class=\"line\">204</div><div class=\"line\">205</div><div class=\"line\">206</div><div class=\"line\">207</div><div class=\"line\">208</div><div class=\"line\">209</div><div class=\"line\">210</div><div class=\"line\">211</div><div class=\"line\">212</div><div class=\"line\">213</div><div class=\"line\">214</div><div class=\"line\">215</div><div class=\"line\">216</div><div class=\"line\">217</div><div class=\"line\">218</div><div class=\"line\">219</div><div class=\"line\">220</div><div class=\"line\">221</div><div class=\"line\">222</div><div class=\"line\">223</div><div class=\"line\">224</div><div class=\"line\">225</div><div class=\"line\">226</div><div class=\"line\">227</div><div class=\"line\">228</div><div class=\"line\">229</div><div class=\"line\">230</div><div class=\"line\">231</div><div class=\"line\">232</div><div class=\"line\">233</div><div class=\"line\">234</div><div class=\"line\">235</div><div class=\"line\">236</div><div class=\"line\">237</div><div class=\"line\">238</div><div class=\"line\">239</div><div class=\"line\">240</div><div class=\"line\">241</div><div class=\"line\">242</div><div class=\"line\">243</div><div class=\"line\">244</div><div class=\"line\">245</div><div class=\"line\">246</div><div class=\"line\">247</div><div class=\"line\">248</div><div class=\"line\">249</div><div class=\"line\">250</div><div class=\"line\">251</div><div class=\"line\">252</div><div class=\"line\">253</div><div class=\"line\">254</div><div class=\"line\">255</div><div class=\"line\">256</div><div class=\"line\">257</div><div class=\"line\">258</div><div class=\"line\">259</div><div class=\"line\">260</div><div class=\"line\">261</div><div class=\"line\">262</div><div class=\"line\">263</div><div class=\"line\">264</div><div class=\"line\">265</div><div class=\"line\">266</div><div class=\"line\">267</div><div class=\"line\">268</div><div class=\"line\">269</div><div class=\"line\">270</div><div class=\"line\">271</div><div class=\"line\">272</div><div class=\"line\">273</div><div class=\"line\">274</div><div class=\"line\">275</div><div class=\"line\">276</div><div class=\"line\">277</div><div class=\"line\">278</div><div class=\"line\">279</div><div class=\"line\">280</div><div class=\"line\">281</div><div class=\"line\">282</div><div class=\"line\">283</div><div class=\"line\">284</div><div class=\"line\">285</div><div class=\"line\">286</div><div class=\"line\">287</div><div class=\"line\">288</div><div class=\"line\">289</div><div class=\"line\">290</div><div class=\"line\">291</div><div class=\"line\">292</div><div class=\"line\">293</div><div class=\"line\">294</div><div class=\"line\">295</div><div class=\"line\">296</div><div class=\"line\">297</div><div class=\"line\">298</div><div class=\"line\">299</div><div class=\"line\">300</div><div class=\"line\">301</div><div class=\"line\">302</div><div class=\"line\">303</div><div class=\"line\">304</div><div class=\"line\">305</div><div class=\"line\">306</div><div class=\"line\">307</div><div class=\"line\">308</div><div class=\"line\">309</div><div class=\"line\">310</div><div class=\"line\">311</div><div class=\"line\">312</div><div class=\"line\">313</div><div class=\"line\">314</div><div class=\"line\">315</div><div class=\"line\">316</div><div class=\"line\">317</div><div class=\"line\">318</div><div class=\"line\">319</div><div class=\"line\">320</div><div class=\"line\">321</div><div class=\"line\">322</div><div class=\"line\">323</div><div class=\"line\">324</div><div class=\"line\">325</div><div class=\"line\">326</div><div class=\"line\">327</div><div class=\"line\">328</div><div class=\"line\">329</div><div class=\"line\">330</div><div class=\"line\">331</div><div class=\"line\">332</div><div class=\"line\">333</div><div class=\"line\">334</div><div class=\"line\">335</div><div class=\"line\">336</div><div class=\"line\">337</div><div class=\"line\">338</div><div class=\"line\">339</div><div class=\"line\">340</div><div class=\"line\">341</div><div class=\"line\">342</div><div class=\"line\">343</div><div class=\"line\">344</div><div class=\"line\">345</div><div class=\"line\">346</div><div class=\"line\">347</div><div class=\"line\">348</div><div class=\"line\">349</div><div class=\"line\">350</div><div class=\"line\">351</div><div class=\"line\">352</div><div class=\"line\">353</div><div class=\"line\">354</div><div class=\"line\">355</div><div class=\"line\">356</div><div class=\"line\">357</div><div class=\"line\">358</div><div class=\"line\">359</div><div class=\"line\">360</div><div class=\"line\">361</div><div class=\"line\">362</div><div class=\"line\">363</div><div class=\"line\">364</div><div class=\"line\">365</div><div class=\"line\">366</div><div class=\"line\">367</div><div class=\"line\">368</div><div class=\"line\">369</div><div class=\"line\">370</div><div class=\"line\">371</div><div class=\"line\">372</div><div class=\"line\">373</div><div class=\"line\">374</div><div class=\"line\">375</div><div class=\"line\">376</div><div class=\"line\">377</div><div class=\"line\">378</div><div class=\"line\">379</div><div class=\"line\">380</div><div class=\"line\">381</div><div class=\"line\">382</div><div class=\"line\">383</div><div class=\"line\">384</div><div class=\"line\">385</div><div class=\"line\">386</div><div class=\"line\">387</div><div class=\"line\">388</div><div class=\"line\">389</div><div class=\"line\">390</div><div class=\"line\">391</div><div class=\"line\">392</div><div class=\"line\">393</div><div class=\"line\">394</div><div class=\"line\">395</div><div class=\"line\">396</div><div class=\"line\">397</div><div class=\"line\">398</div><div class=\"line\">399</div><div class=\"line\">400</div><div class=\"line\">401</div><div class=\"line\">402</div><div class=\"line\">403</div><div class=\"line\">404</div><div class=\"line\">405</div><div class=\"line\">406</div><div class=\"line\">407</div><div class=\"line\">408</div><div class=\"line\">409</div><div class=\"line\">410</div><div class=\"line\">411</div><div class=\"line\">412</div><div class=\"line\">413</div><div class=\"line\">414</div><div class=\"line\">415</div><div class=\"line\">416</div><div class=\"line\">417</div><div class=\"line\">418</div><div class=\"line\">419</div><div class=\"line\">420</div><div class=\"line\">421</div><div class=\"line\">422</div><div class=\"line\">423</div><div class=\"line\">424</div><div class=\"line\">425</div><div class=\"line\">426</div><div class=\"line\">427</div><div class=\"line\">428</div><div class=\"line\">429</div><div class=\"line\">430</div><div class=\"line\">431</div><div class=\"line\">432</div><div class=\"line\">433</div><div class=\"line\">434</div><div class=\"line\">435</div><div class=\"line\">436</div><div class=\"line\">437</div><div class=\"line\">438</div><div class=\"line\">439</div><div class=\"line\">440</div><div class=\"line\">441</div><div class=\"line\">442</div><div class=\"line\">443</div><div class=\"line\">444</div><div class=\"line\">445</div><div class=\"line\">446</div><div class=\"line\">447</div><div class=\"line\">448</div><div class=\"line\">449</div><div class=\"line\">450</div><div class=\"line\">451</div><div class=\"line\">452</div><div class=\"line\">453</div><div class=\"line\">454</div><div class=\"line\">455</div><div class=\"line\">456</div><div class=\"line\">457</div><div class=\"line\">458</div><div class=\"line\">459</div><div class=\"line\">460</div><div class=\"line\">461</div><div class=\"line\">462</div><div class=\"line\">463</div><div class=\"line\">464</div><div class=\"line\">465</div><div class=\"line\">466</div><div class=\"line\">467</div><div class=\"line\">468</div><div class=\"line\">469</div><div class=\"line\">470</div><div class=\"line\">471</div><div class=\"line\">472</div><div class=\"line\">473</div><div class=\"line\">474</div><div class=\"line\">475</div><div class=\"line\">476</div><div class=\"line\">477</div><div class=\"line\">478</div><div class=\"line\">479</div><div class=\"line\">480</div><div class=\"line\">481</div><div class=\"line\">482</div><div class=\"line\">483</div><div class=\"line\">484</div><div class=\"line\">485</div><div class=\"line\">486</div><div class=\"line\">487</div><div class=\"line\">488</div><div class=\"line\">489</div><div class=\"line\">490</div><div class=\"line\">491</div><div class=\"line\">492</div><div class=\"line\">493</div><div class=\"line\">494</div><div class=\"line\">495</div><div class=\"line\">496</div><div class=\"line\">497</div><div class=\"line\">498</div><div class=\"line\">499</div><div class=\"line\">500</div><div class=\"line\">501</div><div class=\"line\">502</div><div class=\"line\">503</div><div class=\"line\">504</div><div class=\"line\">505</div><div class=\"line\">506</div><div class=\"line\">507</div><div class=\"line\">508</div><div class=\"line\">509</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">package</span> hdfs.tutorial;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.conf.Configuration;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.fs.*;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.hdfs.DistributedFileSystem;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.hdfs.protocol.DatanodeInfo;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">import</span> java.io.File;</div><div class=\"line\"><span class=\"keyword\">import</span> java.io.FileInputStream;</div><div class=\"line\"><span class=\"keyword\">import</span> java.io.FileOutputStream;</div><div class=\"line\"><span class=\"keyword\">import</span> java.io.IOException;</div><div class=\"line\"><span class=\"keyword\">import</span> java.net.URI;</div><div class=\"line\"><span class=\"keyword\">import</span> java.net.URISyntaxException;</div><div class=\"line\"><span class=\"keyword\">import</span> java.util.Date;</div><div class=\"line\"><span class=\"keyword\">import</span> java.util.List;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\"> * Created by Administrator on 2017/5/25.</span></div><div class=\"line\"><span class=\"comment\"> */</span></div><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">HdfsOper</span> </span>&#123;</div><div class=\"line\">    <span class=\"keyword\">private</span> FileSystem hdfs;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span> 得到hdfs的连接 FileSystem类</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@throws</span> URISyntaxException</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@throws</span> IOException</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@throws</span> InterruptedException</span></div><div class=\"line\"><span class=\"comment\">     */</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> FileSystem <span class=\"title\">getFileSystem</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> URISyntaxException, IOException, InterruptedException </span>&#123;</div><div class=\"line\">        <span class=\"comment\">//获取FileSystem类的方法有很多种，这里只写一种</span></div><div class=\"line\">        Configuration config = <span class=\"keyword\">new</span> Configuration();</div><div class=\"line\">        URI uri = <span class=\"keyword\">new</span> URI(<span class=\"string\">\"hdfs://192.xxx.x.xxx:xxx\"</span>);</div><div class=\"line\">        <span class=\"keyword\">return</span> FileSystem.get(uri,config,<span class=\"string\">\"your user\"</span>);<span class=\"comment\">// 第一位为uri，第二位为config，第三位是登录的用户</span></div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\">     * 检查文件或者文件夹是否存在</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> filename</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span></span></div><div class=\"line\"><span class=\"comment\">     */</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">checkFileExist</span><span class=\"params\">(String filename)</span></span></div><div class=\"line\"><span class=\"function\">    </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">try</span></div><div class=\"line\">        &#123;</div><div class=\"line\">            Path f = <span class=\"keyword\">new</span> Path(filename);</div><div class=\"line\">            <span class=\"keyword\">return</span> hdfs.exists(f);</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">catch</span> (Exception e)</div><div class=\"line\">        &#123;</div><div class=\"line\">            e.printStackTrace();</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\">     * 创建文件夹</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> dirName</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span></span></div><div class=\"line\"><span class=\"comment\">     */</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">mkdir</span><span class=\"params\">(String dirName)</span></span></div><div class=\"line\"><span class=\"function\">    </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">if</span> (checkFileExist(dirName))</div><div class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</div><div class=\"line\">        <span class=\"keyword\">try</span></div><div class=\"line\">        &#123;</div><div class=\"line\">            Path f = <span class=\"keyword\">new</span> Path(dirName);</div><div class=\"line\">            System.out.println(<span class=\"string\">\"Create and Write :\"</span> + f.getName() + <span class=\"string\">\" to hdfs\"</span>);</div><div class=\"line\">            <span class=\"keyword\">return</span> hdfs.mkdirs(f);</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">catch</span> (Exception e)</div><div class=\"line\">        &#123;</div><div class=\"line\">            e.printStackTrace();</div><div class=\"line\">        &#125;</div><div class=\"line\"></div><div class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\">     * 创建一个空文件</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> filePath  文件的完整路径名称</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span></span></div><div class=\"line\"><span class=\"comment\">     */</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">mkfile</span><span class=\"params\">(String filePath)</span></span></div><div class=\"line\"><span class=\"function\">    </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">try</span></div><div class=\"line\">        &#123;</div><div class=\"line\">            Path f = <span class=\"keyword\">new</span> Path(filePath);</div><div class=\"line\">            FSDataOutputStream os = hdfs.create(f, <span class=\"keyword\">true</span>);</div><div class=\"line\">            os.close();</div><div class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">catch</span> (IllegalArgumentException e)</div><div class=\"line\">        &#123;</div><div class=\"line\">            e.printStackTrace();</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">catch</span> (IOException e)</div><div class=\"line\">        &#123;</div><div class=\"line\">            e.printStackTrace();</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\">     * 复制文件到指定目录</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> srcfile  复制的文件路径</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> desfile  粘贴的路径</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span></span></div><div class=\"line\"><span class=\"comment\">     */</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">hdfsCopyUtils</span><span class=\"params\">(String srcfile, String desfile)</span> </span>&#123;</div><div class=\"line\">        Configuration conf = <span class=\"keyword\">new</span> Configuration();</div><div class=\"line\">        Path src = <span class=\"keyword\">new</span> Path(srcfile);</div><div class=\"line\">        Path dst = <span class=\"keyword\">new</span> Path(desfile);</div><div class=\"line\">        <span class=\"keyword\">try</span> &#123;</div><div class=\"line\">            FileUtil.copy(src.getFileSystem(conf), src,</div><div class=\"line\">                    dst.getFileSystem(conf), dst, <span class=\"keyword\">false</span>, conf);</div><div class=\"line\">        &#125; <span class=\"keyword\">catch</span> (IOException e) &#123;</div><div class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\">     * 移动文件或者文件夹</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> src  初始路径</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> dst   移动结束路径</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@throws</span> Exception</span></div><div class=\"line\"><span class=\"comment\">     */</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">movefile</span><span class=\"params\">(String src, String dst)</span> <span class=\"keyword\">throws</span> Exception</span></div><div class=\"line\"><span class=\"function\">    </span>&#123;</div><div class=\"line\">        Path p1 = <span class=\"keyword\">new</span> Path(src);</div><div class=\"line\">        Path p2 = <span class=\"keyword\">new</span> Path(dst);</div><div class=\"line\">        hdfs.rename(p1, p2);</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\">     * 删除文件或者文件夹</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> src</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@throws</span> Exception</span></div><div class=\"line\"><span class=\"comment\">     */</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">delete</span><span class=\"params\">(String src)</span> <span class=\"keyword\">throws</span> Exception</span></div><div class=\"line\"><span class=\"function\">    </span>&#123;</div><div class=\"line\">        Path p1 = <span class=\"keyword\">new</span> Path(src);</div><div class=\"line\">        <span class=\"keyword\">if</span> (hdfs.isDirectory(p1))</div><div class=\"line\">        &#123;</div><div class=\"line\">            hdfs.delete(p1, <span class=\"keyword\">true</span>);</div><div class=\"line\">            System.out.println(<span class=\"string\">\"删除文件夹成功: \"</span> + src);</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (hdfs.isFile(p1))</div><div class=\"line\">        &#123;</div><div class=\"line\">            hdfs.delete(p1, <span class=\"keyword\">false</span>);</div><div class=\"line\">            System.out.println(<span class=\"string\">\"删除文件成功: \"</span> + src);</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\">     * 读取本地文件到HDFS系统, 保证文件格式是utf-8</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> localFilename</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> hdfsPath</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span></span></div><div class=\"line\"><span class=\"comment\">     */</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">copyLocalFileToHDFS</span><span class=\"params\">(String localFilename, String hdfsPath)</span></span></div><div class=\"line\"><span class=\"function\">    </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">try</span></div><div class=\"line\">        &#123;</div><div class=\"line\">            <span class=\"comment\">// 如果路径不存在就创建文件夹</span></div><div class=\"line\">            mkdir(hdfsPath);</div><div class=\"line\"></div><div class=\"line\">            File file = <span class=\"keyword\">new</span> File(localFilename);</div><div class=\"line\">            FileInputStream is = <span class=\"keyword\">new</span> FileInputStream(file);</div><div class=\"line\"></div><div class=\"line\">            <span class=\"comment\">// 如果hdfs上已经存在文件，那么先删除该文件</span></div><div class=\"line\">            <span class=\"keyword\">if</span> (<span class=\"keyword\">this</span>.checkFileExist(hdfsPath + <span class=\"string\">\"/\"</span> + file.getName()))</div><div class=\"line\">            &#123;</div><div class=\"line\">                delete(hdfsPath + <span class=\"string\">\"/\"</span> + file.getName());</div><div class=\"line\">            &#125;</div><div class=\"line\"></div><div class=\"line\">            Path f = <span class=\"keyword\">new</span> Path(hdfsPath + <span class=\"string\">\"/\"</span> + file.getName());</div><div class=\"line\"></div><div class=\"line\">            FSDataOutputStream os = hdfs.create(f, <span class=\"keyword\">true</span>);</div><div class=\"line\">            <span class=\"keyword\">byte</span>[] buffer = <span class=\"keyword\">new</span> <span class=\"keyword\">byte</span>[<span class=\"number\">10240000</span>];</div><div class=\"line\">            <span class=\"keyword\">int</span> nCount = <span class=\"number\">0</span>;</div><div class=\"line\"></div><div class=\"line\">            <span class=\"keyword\">while</span> (<span class=\"keyword\">true</span>)</div><div class=\"line\">            &#123;</div><div class=\"line\">                <span class=\"keyword\">int</span> bytesRead = is.read(buffer);</div><div class=\"line\">                <span class=\"keyword\">if</span> (bytesRead &lt;= <span class=\"number\">0</span>)</div><div class=\"line\">                &#123;</div><div class=\"line\">                    <span class=\"keyword\">break</span>;</div><div class=\"line\">                &#125;</div><div class=\"line\"></div><div class=\"line\">                os.write(buffer, <span class=\"number\">0</span>, bytesRead);</div><div class=\"line\">                nCount++;</div><div class=\"line\">                <span class=\"keyword\">if</span> (nCount % (<span class=\"number\">100</span>) == <span class=\"number\">0</span>)</div><div class=\"line\">                    System.out.println((<span class=\"keyword\">new</span> Date()).toLocaleString() + <span class=\"string\">\": Have move \"</span> + nCount + <span class=\"string\">\" blocks\"</span>);</div><div class=\"line\">            &#125;</div><div class=\"line\"></div><div class=\"line\">            is.close();</div><div class=\"line\">            os.close();</div><div class=\"line\">            System.out.println((<span class=\"keyword\">new</span> Date()).toLocaleString() + <span class=\"string\">\": Write content of file \"</span> + file.getName()</div><div class=\"line\">                    + <span class=\"string\">\" to hdfs file \"</span> + f.getName() + <span class=\"string\">\" success\"</span>);</div><div class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">catch</span> (Exception e)</div><div class=\"line\">        &#123;</div><div class=\"line\">            e.printStackTrace();</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\">     * 复制本地文件夹到hdfs的文件</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> localPath</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> hdfsPath</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span></span></div><div class=\"line\"><span class=\"comment\">     */</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">CopyLocalDirTohdfs</span><span class=\"params\">(String localPath, String hdfsPath)</span></span></div><div class=\"line\"><span class=\"function\">    </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">try</span></div><div class=\"line\">        &#123;</div><div class=\"line\">            File root = <span class=\"keyword\">new</span> File(localPath);</div><div class=\"line\">            File[] files = root.listFiles();</div><div class=\"line\"></div><div class=\"line\">            <span class=\"keyword\">for</span> (File file : files)</div><div class=\"line\">            &#123;</div><div class=\"line\">                <span class=\"keyword\">if</span> (file.isFile())</div><div class=\"line\">                &#123;</div><div class=\"line\">                    copyLocalFileToHDFS(file.getPath().toString(), hdfsPath);</div><div class=\"line\"></div><div class=\"line\">                &#125;</div><div class=\"line\">                <span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(file.isDirectory())</div><div class=\"line\">                &#123;</div><div class=\"line\">                    CopyLocalDirTohdfs(localPath+<span class=\"string\">\"/\"</span>+file.getName(), hdfsPath+<span class=\"string\">\"/\"</span>+file.getName());</div><div class=\"line\">                &#125;</div><div class=\"line\">            &#125;</div><div class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">catch</span> (Exception e)</div><div class=\"line\">        &#123;</div><div class=\"line\">            e.printStackTrace();</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\">     * 从hdfs下载</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> hdfsFilename</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> localPath</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span></span></div><div class=\"line\"><span class=\"comment\">     */</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">downloadFileFromHdfs</span><span class=\"params\">(String hdfsFilename, String localPath)</span></span></div><div class=\"line\"><span class=\"function\">    </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">try</span></div><div class=\"line\">        &#123;</div><div class=\"line\">            Path f = <span class=\"keyword\">new</span> Path(hdfsFilename);</div><div class=\"line\"></div><div class=\"line\">            FSDataInputStream dis = hdfs.open(f);</div><div class=\"line\">            File file = <span class=\"keyword\">new</span> File(localPath + <span class=\"string\">\"/\"</span> + f.getName());</div><div class=\"line\">            FileOutputStream os = <span class=\"keyword\">new</span> FileOutputStream(file);</div><div class=\"line\"></div><div class=\"line\">            <span class=\"keyword\">byte</span>[] buffer = <span class=\"keyword\">new</span> <span class=\"keyword\">byte</span>[<span class=\"number\">1024000</span>];</div><div class=\"line\">            <span class=\"keyword\">int</span> length = <span class=\"number\">0</span>;</div><div class=\"line\">            <span class=\"keyword\">while</span> ((length = dis.read(buffer)) &gt; <span class=\"number\">0</span>)</div><div class=\"line\">            &#123;</div><div class=\"line\">                os.write(buffer, <span class=\"number\">0</span>, length);</div><div class=\"line\">            &#125;</div><div class=\"line\"></div><div class=\"line\">            os.close();</div><div class=\"line\">            dis.close();</div><div class=\"line\"></div><div class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">catch</span> (Exception e)</div><div class=\"line\">        &#123;</div><div class=\"line\">            e.printStackTrace();</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\">     * HDFS 到 HDFS 的合并</span></div><div class=\"line\"><span class=\"comment\">     * hdfs提供了一种FileUtil.copyMerge（）的方法， 注意下面的 false 这个，如果改为true，就会删除这个目录</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> folder 需要合并的目录</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> file  要合并成的文件，完整路径名称</span></div><div class=\"line\"><span class=\"comment\">     */</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">copyMerge</span><span class=\"params\">(String folder, String file)</span> </span>&#123;</div><div class=\"line\">        Configuration conf = <span class=\"keyword\">new</span> Configuration();</div><div class=\"line\">        Path src = <span class=\"keyword\">new</span> Path(folder);</div><div class=\"line\">        Path dst = <span class=\"keyword\">new</span> Path(file);</div><div class=\"line\"></div><div class=\"line\">        <span class=\"keyword\">try</span> &#123;</div><div class=\"line\">            FileUtil.copyMerge(src.getFileSystem(conf), src,</div><div class=\"line\">                    dst.getFileSystem(conf), dst, <span class=\"keyword\">false</span>, conf, <span class=\"keyword\">null</span>);</div><div class=\"line\">        &#125; <span class=\"keyword\">catch</span> (IOException e) &#123;</div><div class=\"line\">            <span class=\"comment\">// TODO Auto-generated catch block</span></div><div class=\"line\">            e.printStackTrace();</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\">     * 列出所有DataNode的名字信息</span></div><div class=\"line\"><span class=\"comment\">     */</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">listDataNodeInfo</span><span class=\"params\">()</span></span></div><div class=\"line\"><span class=\"function\">    </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">try</span></div><div class=\"line\">        &#123;</div><div class=\"line\">            DistributedFileSystem fs =<span class=\"keyword\">null</span>;</div><div class=\"line\">            fs = (DistributedFileSystem) hdfs;</div><div class=\"line\">            DatanodeInfo[] dataNodeStats = fs.getDataNodeStats();</div><div class=\"line\">            String[] names = <span class=\"keyword\">new</span> String[dataNodeStats.length];</div><div class=\"line\">            System.out.println(<span class=\"string\">\"List of all the datanode in the HDFS cluster:\"</span>);</div><div class=\"line\"></div><div class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; names.length; i++)</div><div class=\"line\">            &#123;</div><div class=\"line\">                names[i] = dataNodeStats[i].getHostName();</div><div class=\"line\">                System.out.println(names[i]);</div><div class=\"line\">            &#125;</div><div class=\"line\">            System.out.println(hdfs.getUri().toString());</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">catch</span> (Exception e)</div><div class=\"line\">        &#123;</div><div class=\"line\">            e.printStackTrace();</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\">     * 检测是否是备用节点</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@throws</span> Exception</span></div><div class=\"line\"><span class=\"comment\">     */</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">checkStandbyException</span><span class=\"params\">(String filename)</span></span></div><div class=\"line\"><span class=\"function\">    </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">try</span> &#123;</div><div class=\"line\">            Path f = <span class=\"keyword\">new</span> Path(filename);</div><div class=\"line\">            hdfs.exists(f);</div><div class=\"line\">        &#125; <span class=\"keyword\">catch</span> (org.apache.hadoop.ipc.RemoteException e) &#123;</div><div class=\"line\">            <span class=\"keyword\">if</span>(e.getClassName().equals(<span class=\"string\">\"org.apache.hadoop.ipc.StandbyException\"</span>))</div><div class=\"line\">            &#123;</div><div class=\"line\">                <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125; <span class=\"keyword\">catch</span> (Exception e) &#123;</div><div class=\"line\"></div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">mergeDirFiles</span><span class=\"params\">(List&lt;FileStatus&gt; fileList, String tarPath, String rowTerminateFlag)</span></span></div><div class=\"line\"><span class=\"function\">    </span>&#123;</div><div class=\"line\">        <span class=\"comment\">//rowTerminateFlag   \\n</span></div><div class=\"line\">        FSDataOutputStream tarFileOutputStream = <span class=\"keyword\">null</span>;</div><div class=\"line\">        FSDataInputStream srcFileInputStream = <span class=\"keyword\">null</span>;</div><div class=\"line\"></div><div class=\"line\">        <span class=\"keyword\">try</span></div><div class=\"line\">        &#123;</div><div class=\"line\">            Path tarFile = <span class=\"keyword\">new</span> Path(tarPath);</div><div class=\"line\">            tarFileOutputStream = hdfs.create(tarFile, <span class=\"keyword\">true</span>);</div><div class=\"line\"></div><div class=\"line\">            <span class=\"keyword\">byte</span>[] buffer = <span class=\"keyword\">new</span> <span class=\"keyword\">byte</span>[<span class=\"number\">1024000</span>];</div><div class=\"line\">            <span class=\"keyword\">int</span> length = <span class=\"number\">0</span>;</div><div class=\"line\">            <span class=\"keyword\">long</span> nTotalLength = <span class=\"number\">0</span>;</div><div class=\"line\">            <span class=\"keyword\">int</span> nCount = <span class=\"number\">0</span>;</div><div class=\"line\">            <span class=\"keyword\">boolean</span> bfirst = <span class=\"keyword\">true</span>;</div><div class=\"line\">            <span class=\"keyword\">for</span>(FileStatus file : fileList)</div><div class=\"line\">            &#123;</div><div class=\"line\">                <span class=\"keyword\">if</span>(file.getPath().equals(tarFile))</div><div class=\"line\">                &#123;</div><div class=\"line\">                    <span class=\"keyword\">continue</span>;</div><div class=\"line\">                &#125;</div><div class=\"line\">                System.out.println(<span class=\"string\">\" merging file from  \"</span> + file.getPath() + <span class=\"string\">\" to \"</span> + tarPath);</div><div class=\"line\"></div><div class=\"line\">                <span class=\"keyword\">if</span>(!bfirst)</div><div class=\"line\">                &#123;</div><div class=\"line\">                    <span class=\"comment\">//添加换行符</span></div><div class=\"line\">                    tarFileOutputStream.write(rowTerminateFlag.getBytes(), <span class=\"number\">0</span>, rowTerminateFlag.length());</div><div class=\"line\">                &#125;</div><div class=\"line\"></div><div class=\"line\">                srcFileInputStream = hdfs.open(file.getPath(), buffer.length);</div><div class=\"line\">                <span class=\"keyword\">while</span> ((length = srcFileInputStream.read(buffer)) &gt; <span class=\"number\">0</span>)</div><div class=\"line\">                &#123;</div><div class=\"line\">                    nCount++;</div><div class=\"line\">                    tarFileOutputStream.write(buffer, <span class=\"number\">0</span>, length);</div><div class=\"line\">                    nTotalLength += length;</div><div class=\"line\">                    <span class=\"comment\">//System.out.println(\" file length \" + file.getLen() + \" read \" + length);</span></div><div class=\"line\">                    <span class=\"keyword\">if</span> (nCount % <span class=\"number\">1000</span> == <span class=\"number\">0</span>)</div><div class=\"line\">                    &#123;</div><div class=\"line\">                        tarFileOutputStream.flush();</div><div class=\"line\">                        System.out.println((<span class=\"keyword\">new</span> Date()).toLocaleString() + <span class=\"string\">\": Have move \"</span> + (nTotalLength / <span class=\"number\">1024000</span>) + <span class=\"string\">\" MB\"</span>);</div><div class=\"line\">                    &#125;</div><div class=\"line\"></div><div class=\"line\">                &#125;</div><div class=\"line\"></div><div class=\"line\">                srcFileInputStream.close();</div><div class=\"line\"></div><div class=\"line\">                bfirst = <span class=\"keyword\">false</span>;</div><div class=\"line\">            &#125;</div><div class=\"line\"></div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">catch</span> (Exception e)</div><div class=\"line\">        &#123;</div><div class=\"line\">            e.printStackTrace();</div><div class=\"line\">            <span class=\"keyword\">try</span></div><div class=\"line\">            &#123;</div><div class=\"line\">                delete(tarPath);</div><div class=\"line\">            &#125;</div><div class=\"line\">            <span class=\"keyword\">catch</span> (Exception e2)</div><div class=\"line\">            &#123;</div><div class=\"line\">                <span class=\"comment\">// <span class=\"doctag\">TODO:</span> handle exception</span></div><div class=\"line\">            &#125;</div><div class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">finally</span></div><div class=\"line\">        &#123;</div><div class=\"line\">            <span class=\"keyword\">try</span></div><div class=\"line\">            &#123;</div><div class=\"line\">                <span class=\"keyword\">if</span>(tarFileOutputStream != <span class=\"keyword\">null</span>)</div><div class=\"line\">                &#123;</div><div class=\"line\">                    tarFileOutputStream.flush();</div><div class=\"line\">                    tarFileOutputStream.close();</div><div class=\"line\">                    srcFileInputStream.close();</div><div class=\"line\">                &#125;</div><div class=\"line\">            &#125;</div><div class=\"line\">            <span class=\"keyword\">catch</span> (Exception e2)</div><div class=\"line\">            &#123;</div><div class=\"line\">                <span class=\"comment\">// <span class=\"doctag\">TODO:</span> handle exception</span></div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">\t<span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\">\t * 将一个字符串写入某个路径</span></div><div class=\"line\"><span class=\"comment\">\t *</span></div><div class=\"line\"><span class=\"comment\">\t * <span class=\"doctag\">@param</span> text 要保存的字符串</span></div><div class=\"line\"><span class=\"comment\">\t * <span class=\"doctag\">@param</span> path 要保存的路径</span></div><div class=\"line\"><span class=\"comment\">\t */</span></div><div class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">writerString</span><span class=\"params\">(String text,String path)</span></span>&#123;</div><div class=\"line\"></div><div class=\"line\">\t\t<span class=\"keyword\">try</span></div><div class=\"line\">\t\t&#123;</div><div class=\"line\">\t\t\tPath f = <span class=\"keyword\">new</span> Path(path);</div><div class=\"line\">\t\t\tFSDataOutputStream os = fs.create(f, <span class=\"keyword\">true</span>);</div><div class=\"line\">\t\t\tBufferedWriter  writer = <span class=\"keyword\">new</span> BufferedWriter(<span class=\"keyword\">new</span> OutputStreamWriter(os, <span class=\"string\">\"utf-8\"</span>));<span class=\"comment\">// 以UTF-8格式写入文件，不乱码</span></div><div class=\"line\">\t\t\twriter.write(text);</div><div class=\"line\">\t\t\twriter.close();</div><div class=\"line\">\t\t\tos.close();</div><div class=\"line\">\t\t&#125;</div><div class=\"line\">\t\t<span class=\"keyword\">catch</span> (Exception e)</div><div class=\"line\">\t\t&#123;</div><div class=\"line\">\t\t\te.printStackTrace();</div><div class=\"line\"></div><div class=\"line\">\t\t&#125;</div><div class=\"line\"></div><div class=\"line\">\t&#125;</div><div class=\"line\"></div><div class=\"line\">\t<span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\">\t * 按行读取文件内容，并且防止乱码</span></div><div class=\"line\"><span class=\"comment\">\t * <span class=\"doctag\">@param</span> hdfsFilename</span></div><div class=\"line\"><span class=\"comment\">\t * <span class=\"doctag\">@return</span></span></div><div class=\"line\"><span class=\"comment\">\t */</span></div><div class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">readByLine</span><span class=\"params\">(String hdfsFilename)</span></span></div><div class=\"line\"><span class=\"function\">\t</span>&#123;</div><div class=\"line\">\t\t<span class=\"keyword\">try</span></div><div class=\"line\">\t\t&#123;</div><div class=\"line\">\t\t\tPath f = <span class=\"keyword\">new</span> Path(hdfsFilename);</div><div class=\"line\"></div><div class=\"line\">\t\t\tFSDataInputStream dis = hdfs.open(f);</div><div class=\"line\"></div><div class=\"line\">\t\t\tBufferedReader bf=<span class=\"keyword\">new</span> BufferedReader(<span class=\"keyword\">new</span> InputStreamReader(dis));<span class=\"comment\">//防止中文乱码</span></div><div class=\"line\">\t\t\tString line = <span class=\"keyword\">null</span>;</div><div class=\"line\">\t\t\t<span class=\"keyword\">while</span> ((line=bf.readLine())!=<span class=\"keyword\">null</span>)</div><div class=\"line\">\t\t\t&#123;</div><div class=\"line\">\t\t\t\tSystem.out.println(<span class=\"keyword\">new</span> String(line.getBytes(),<span class=\"string\">\"utf-8\"</span>));</div><div class=\"line\">\t\t\t&#125;</div><div class=\"line\"></div><div class=\"line\">\t\t\tdis.close();</div><div class=\"line\">\t\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</div><div class=\"line\">\t\t&#125;</div><div class=\"line\">\t\t<span class=\"keyword\">catch</span> (Exception e)</div><div class=\"line\">\t\t&#123;</div><div class=\"line\">\t\t\te.printStackTrace();</div><div class=\"line\">\t\t&#125;</div><div class=\"line\">\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</div><div class=\"line\">\t&#125;</div><div class=\"line\"></div><div class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">reNameExistsPath</span><span class=\"params\">(String srcPath, String tarPath)</span> <span class=\"keyword\">throws</span> Exception</span></div><div class=\"line\"><span class=\"function\">\t</span>&#123;</div><div class=\"line\">\t    <span class=\"comment\">//检测输出目录是否存在，存在就改名</span></div><div class=\"line\">\t    <span class=\"keyword\">if</span>(checkFileExist(srcPath))</div><div class=\"line\">\t    &#123;</div><div class=\"line\">\t    \ttarPath = srcPath.trim();</div><div class=\"line\">\t    \t<span class=\"keyword\">while</span>(tarPath.charAt(tarPath.length()-<span class=\"number\">1</span>) == <span class=\"string\">'/'</span>)</div><div class=\"line\">\t    \t&#123;</div><div class=\"line\">\t    \t\ttarPath = tarPath.substring(<span class=\"number\">0</span>, tarPath.length()-<span class=\"number\">1</span>);</div><div class=\"line\">\t    \t&#125;</div><div class=\"line\">\t    \tDate now = <span class=\"keyword\">new</span> Date(); </div><div class=\"line\">\t    \tSimpleDateFormat dateFormat = <span class=\"keyword\">new</span> SimpleDateFormat(<span class=\"string\">\"yyMMddHHmmss\"</span>);</div><div class=\"line\">\t    \tString nowStr = dateFormat.format(now); </div><div class=\"line\">\t    \ttarPath += <span class=\"string\">\"_\"</span> + nowStr;</div><div class=\"line\">\t    \tmovefile(srcPath, tarPath);</div><div class=\"line\">\t    &#125;</div><div class=\"line\">\t    <span class=\"keyword\">else</span> </div><div class=\"line\">\t    &#123;</div><div class=\"line\">\t    \ttarPath = srcPath;</div><div class=\"line\">\t    &#125;</div><div class=\"line\">\t&#125;</div></pre></td></tr></table></figure>"},{"title":"HDFS java操作（二）FileStatus 获取文件属性，globStatus 进行路径过滤","date":"2016-06-28T13:20:21.000Z","author":"kaishun","id":"11","blogexcerpt":"前言，本章主要记录了如何使用fileStatus来获取hdfs文件的一些属性，以及如何使用globStatus对路径进行过滤, filestatus 获取文件状态.globStatus 路径过滤等等","_content":"\n# 前言：  \n本章主要记录了如何使用fileStatus来获取hdfs文件的一些属性，以及如何使用globStatus对路径进行过滤, 列出某个目录下所有文件的信息\n\n![hdfs-commands](http://or49tneld.bkt.clouddn.com/18-1-23/63075597.jpg)\n\n# filestatus 获取文件状态\n\n```java\npackage hdfs.tutorial;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.FileStatus;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.Path;\n\nimport java.io.IOException;\nimport java.net.URI;\nimport java.net.URISyntaxException;\nimport java.sql.Timestamp;\n\n/**\n * Created by Administrator on 2017/5/25.\n */\npublic class ReadHdfs {\n    public static void main(String[] args) {\n\n        Path path = new Path(\"hdfs://192.xxx.xxx.xxx:9000/hdfstest\");\n        Path filePath = new Path(\"hdfs://192.xxx.xxx.xxx:9000/hdfstest/people.txt\");\n        try {\n\n            //得到 FileSystem 类\n            FileSystem hdfs = getFileSystem();\n\n            //列出某个目录下所有文件的信息\n            listFilesStatus(path, hdfs);\n\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n    }\n\n    /**\n     *\n     * @return 得到hdfs的连接 FileSystem类\n     * @throws URISyntaxException\n     * @throws IOException\n     * @throws InterruptedException\n     */\n    public static FileSystem getFileSystem() throws URISyntaxException, IOException, InterruptedException {\n      //获取FileSystem类的方法有很多种，这里只写一种\n        Configuration config = new Configuration();\n        URI uri = new URI(\"hdfs://192.xxx.xxx.xxx:9000\");\n        return FileSystem.get(uri,config,\"your_user\");// 第一位为uri，第二位为config，第三位是登录的用户\n    }\n\n    /**\n     * 列出某个目录下所有文件的信息\n     * @param path\n     * @param hdfs\n     * @throws IOException\n     */\n    public static void listFilesStatus(Path path, FileSystem hdfs) throws IOException {\n        // 列出目录下的所有文件\n        FileStatus[] files = hdfs.listStatus(path);\n\n        for (int i = 0; i <files.length ; i++) {\n            FileStatus file = files[i];\n            if(file.isFile()){\n                System.out.println(\"这是文件\");\n                long len = file.getLen();                   //文件长度\n                String pathSource = file.getPath().toString();//文件路径\n                String fileName = file.getPath().getName();   // 文件名称\n                String parentPath = file.getPath().getParent().toString();//文件父路径\n                Timestamp timestamp = new Timestamp(file.getModificationTime());//文件最后修改时间\n                long blockSize = file.getBlockSize();   //文件块大小\n                String group = file.getGroup();         //文件所属组\n                String owner = file.getOwner();          // 文件拥有者\n                long accessTime = file.getAccessTime();  //该文件上次访问时间\n                short replication = file.getReplication(); //文件副本数\n                System.out.println(\"文件长度: \"+len+\"\\n\"+\n                        \"文件路径: \"+pathSource+\"\\n\"+\n                        \"文件名称: \"+fileName+\"\\n\"+\n                        \"文件父路径: \"+parentPath+\"\\n\"+\n                        \"文件最后修改时间: \"+timestamp+\"\\n\"+\n                        \"文件块大小: \"+blockSize+\"\\n\"+\n                        \"文件所属组: \"+group+\"\\n\"+\n                        \"文件拥有者: \"+owner+\"\\n\"+\n                        \"该文件上次访问时间: \"+accessTime+\"\\n\"+\n                        \"文件副本数: \"+replication+\"\\n\"+\n                        \"==============================\");\n\n            }else if(file.isDirectory()){\n                System.out.println(\"这是文件夹\");\n                System.out.println(\"文件父路径: \"+file.getPath().toString());\n\n                //递归调用\n                listFilesStatus(file.getPath(),hdfs);\n\n            }else if(file.isSymlink()){\n                System.out.println(\"这是链接文件\");\n            }\n        }\n    }\n\n}\n\n\n```\n\n\n##  globStatus 路径过滤  \n例如下面的例子，我需要得到/filtertest/*/* 路径中 带有abc的路径\nglobStatus 很灵活，内部甚至可以写一些正则表达式，有时候在处理大数据的预处理的时候可能很有效\n\n```\npublic class FileListFilter {\n\n    public static void main(String[] args) {\n        FileSystem hdfs = null;\n        try{\n            Configuration config = new Configuration();\n\n            hdfs = FileSystem.get(new URI(\"hdfs://192.xxx.xxx.xxx:9000\"),\n                    config, \"hmaster\");\n            Path allPath = new Path(\"/filtertest/*/*\");\n\n            FileStatus[] fileGlobStatuses = hdfs.globStatus(allPath, new PathFilter() {\n                @Override\n                public boolean accept(Path path) {\n                    String contidion = \"abc\";\n                    // 过滤出路径中包含 abc字符串 的路径\n                    return  path.toString().contains(contidion);\n                }\n            });\n\n            Path[] globPaths = FileUtil.stat2Paths(fileGlobStatuses);\n            for (Path p :globPaths){\n                System.out.println(\"globe过滤后的路径\"+p);\n            }\n\n        }catch(Exception e){\n            e.printStackTrace();\n        }\n    }\n}\n\n```","source":"_posts/HDFS java操作（二）FileStatus 获取文件属性，globStatus 进行路径过滤.md","raw":"---\ntitle: HDFS java操作（二）FileStatus 获取文件属性，globStatus 进行路径过滤\ndate: 2016-06-28 21:20:21\ntags: [hdfs]\ncategories: [大数据,hdfs]\nauthor: kaishun\nid: 11\npermalink: hdfs-filesystem-2\nblogexcerpt: 前言，本章主要记录了如何使用fileStatus来获取hdfs文件的一些属性，以及如何使用globStatus对路径进行过滤, filestatus 获取文件状态.globStatus 路径过滤等等 \n---\n\n# 前言：  \n本章主要记录了如何使用fileStatus来获取hdfs文件的一些属性，以及如何使用globStatus对路径进行过滤, 列出某个目录下所有文件的信息\n\n![hdfs-commands](http://or49tneld.bkt.clouddn.com/18-1-23/63075597.jpg)\n\n# filestatus 获取文件状态\n\n```java\npackage hdfs.tutorial;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.FileStatus;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.Path;\n\nimport java.io.IOException;\nimport java.net.URI;\nimport java.net.URISyntaxException;\nimport java.sql.Timestamp;\n\n/**\n * Created by Administrator on 2017/5/25.\n */\npublic class ReadHdfs {\n    public static void main(String[] args) {\n\n        Path path = new Path(\"hdfs://192.xxx.xxx.xxx:9000/hdfstest\");\n        Path filePath = new Path(\"hdfs://192.xxx.xxx.xxx:9000/hdfstest/people.txt\");\n        try {\n\n            //得到 FileSystem 类\n            FileSystem hdfs = getFileSystem();\n\n            //列出某个目录下所有文件的信息\n            listFilesStatus(path, hdfs);\n\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n    }\n\n    /**\n     *\n     * @return 得到hdfs的连接 FileSystem类\n     * @throws URISyntaxException\n     * @throws IOException\n     * @throws InterruptedException\n     */\n    public static FileSystem getFileSystem() throws URISyntaxException, IOException, InterruptedException {\n      //获取FileSystem类的方法有很多种，这里只写一种\n        Configuration config = new Configuration();\n        URI uri = new URI(\"hdfs://192.xxx.xxx.xxx:9000\");\n        return FileSystem.get(uri,config,\"your_user\");// 第一位为uri，第二位为config，第三位是登录的用户\n    }\n\n    /**\n     * 列出某个目录下所有文件的信息\n     * @param path\n     * @param hdfs\n     * @throws IOException\n     */\n    public static void listFilesStatus(Path path, FileSystem hdfs) throws IOException {\n        // 列出目录下的所有文件\n        FileStatus[] files = hdfs.listStatus(path);\n\n        for (int i = 0; i <files.length ; i++) {\n            FileStatus file = files[i];\n            if(file.isFile()){\n                System.out.println(\"这是文件\");\n                long len = file.getLen();                   //文件长度\n                String pathSource = file.getPath().toString();//文件路径\n                String fileName = file.getPath().getName();   // 文件名称\n                String parentPath = file.getPath().getParent().toString();//文件父路径\n                Timestamp timestamp = new Timestamp(file.getModificationTime());//文件最后修改时间\n                long blockSize = file.getBlockSize();   //文件块大小\n                String group = file.getGroup();         //文件所属组\n                String owner = file.getOwner();          // 文件拥有者\n                long accessTime = file.getAccessTime();  //该文件上次访问时间\n                short replication = file.getReplication(); //文件副本数\n                System.out.println(\"文件长度: \"+len+\"\\n\"+\n                        \"文件路径: \"+pathSource+\"\\n\"+\n                        \"文件名称: \"+fileName+\"\\n\"+\n                        \"文件父路径: \"+parentPath+\"\\n\"+\n                        \"文件最后修改时间: \"+timestamp+\"\\n\"+\n                        \"文件块大小: \"+blockSize+\"\\n\"+\n                        \"文件所属组: \"+group+\"\\n\"+\n                        \"文件拥有者: \"+owner+\"\\n\"+\n                        \"该文件上次访问时间: \"+accessTime+\"\\n\"+\n                        \"文件副本数: \"+replication+\"\\n\"+\n                        \"==============================\");\n\n            }else if(file.isDirectory()){\n                System.out.println(\"这是文件夹\");\n                System.out.println(\"文件父路径: \"+file.getPath().toString());\n\n                //递归调用\n                listFilesStatus(file.getPath(),hdfs);\n\n            }else if(file.isSymlink()){\n                System.out.println(\"这是链接文件\");\n            }\n        }\n    }\n\n}\n\n\n```\n\n\n##  globStatus 路径过滤  \n例如下面的例子，我需要得到/filtertest/*/* 路径中 带有abc的路径\nglobStatus 很灵活，内部甚至可以写一些正则表达式，有时候在处理大数据的预处理的时候可能很有效\n\n```\npublic class FileListFilter {\n\n    public static void main(String[] args) {\n        FileSystem hdfs = null;\n        try{\n            Configuration config = new Configuration();\n\n            hdfs = FileSystem.get(new URI(\"hdfs://192.xxx.xxx.xxx:9000\"),\n                    config, \"hmaster\");\n            Path allPath = new Path(\"/filtertest/*/*\");\n\n            FileStatus[] fileGlobStatuses = hdfs.globStatus(allPath, new PathFilter() {\n                @Override\n                public boolean accept(Path path) {\n                    String contidion = \"abc\";\n                    // 过滤出路径中包含 abc字符串 的路径\n                    return  path.toString().contains(contidion);\n                }\n            });\n\n            Path[] globPaths = FileUtil.stat2Paths(fileGlobStatuses);\n            for (Path p :globPaths){\n                System.out.println(\"globe过滤后的路径\"+p);\n            }\n\n        }catch(Exception e){\n            e.printStackTrace();\n        }\n    }\n}\n\n```","slug":"hdfs-filesystem-2","published":1,"updated":"2018-01-23T14:36:15.600Z","_id":"cjcrpnzn800022wv3iyxu0l7v","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"前言：\"><a href=\"#前言：\" class=\"headerlink\" title=\"前言：\"></a>前言：</h1><p>本章主要记录了如何使用fileStatus来获取hdfs文件的一些属性，以及如何使用globStatus对路径进行过滤, 列出某个目录下所有文件的信息</p>\n<p><img src=\"http://or49tneld.bkt.clouddn.com/18-1-23/63075597.jpg\" alt=\"hdfs-commands\"></p>\n<h1 id=\"filestatus-获取文件状态\"><a href=\"#filestatus-获取文件状态\" class=\"headerlink\" title=\"filestatus 获取文件状态\"></a>filestatus 获取文件状态</h1><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">package</span> hdfs.tutorial;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.conf.Configuration;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.fs.FileStatus;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.fs.FileSystem;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.fs.Path;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">import</span> java.io.IOException;</div><div class=\"line\"><span class=\"keyword\">import</span> java.net.URI;</div><div class=\"line\"><span class=\"keyword\">import</span> java.net.URISyntaxException;</div><div class=\"line\"><span class=\"keyword\">import</span> java.sql.Timestamp;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\"> * Created by Administrator on 2017/5/25.</span></div><div class=\"line\"><span class=\"comment\"> */</span></div><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReadHdfs</span> </span>&#123;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> </span>&#123;</div><div class=\"line\"></div><div class=\"line\">        Path path = <span class=\"keyword\">new</span> Path(<span class=\"string\">\"hdfs://192.xxx.xxx.xxx:9000/hdfstest\"</span>);</div><div class=\"line\">        Path filePath = <span class=\"keyword\">new</span> Path(<span class=\"string\">\"hdfs://192.xxx.xxx.xxx:9000/hdfstest/people.txt\"</span>);</div><div class=\"line\">        <span class=\"keyword\">try</span> &#123;</div><div class=\"line\"></div><div class=\"line\">            <span class=\"comment\">//得到 FileSystem 类</span></div><div class=\"line\">            FileSystem hdfs = getFileSystem();</div><div class=\"line\"></div><div class=\"line\">            <span class=\"comment\">//列出某个目录下所有文件的信息</span></div><div class=\"line\">            listFilesStatus(path, hdfs);</div><div class=\"line\"></div><div class=\"line\">        &#125; <span class=\"keyword\">catch</span> (Exception e) &#123;</div><div class=\"line\">            e.printStackTrace();</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\">     *</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span> 得到hdfs的连接 FileSystem类</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@throws</span> URISyntaxException</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@throws</span> IOException</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@throws</span> InterruptedException</span></div><div class=\"line\"><span class=\"comment\">     */</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> FileSystem <span class=\"title\">getFileSystem</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> URISyntaxException, IOException, InterruptedException </span>&#123;</div><div class=\"line\">      <span class=\"comment\">//获取FileSystem类的方法有很多种，这里只写一种</span></div><div class=\"line\">        Configuration config = <span class=\"keyword\">new</span> Configuration();</div><div class=\"line\">        URI uri = <span class=\"keyword\">new</span> URI(<span class=\"string\">\"hdfs://192.xxx.xxx.xxx:9000\"</span>);</div><div class=\"line\">        <span class=\"keyword\">return</span> FileSystem.get(uri,config,<span class=\"string\">\"your_user\"</span>);<span class=\"comment\">// 第一位为uri，第二位为config，第三位是登录的用户</span></div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\">     * 列出某个目录下所有文件的信息</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> path</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> hdfs</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@throws</span> IOException</span></div><div class=\"line\"><span class=\"comment\">     */</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">listFilesStatus</span><span class=\"params\">(Path path, FileSystem hdfs)</span> <span class=\"keyword\">throws</span> IOException </span>&#123;</div><div class=\"line\">        <span class=\"comment\">// 列出目录下的所有文件</span></div><div class=\"line\">        FileStatus[] files = hdfs.listStatus(path);</div><div class=\"line\"></div><div class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt;files.length ; i++) &#123;</div><div class=\"line\">            FileStatus file = files[i];</div><div class=\"line\">            <span class=\"keyword\">if</span>(file.isFile())&#123;</div><div class=\"line\">                System.out.println(<span class=\"string\">\"这是文件\"</span>);</div><div class=\"line\">                <span class=\"keyword\">long</span> len = file.getLen();                   <span class=\"comment\">//文件长度</span></div><div class=\"line\">                String pathSource = file.getPath().toString();<span class=\"comment\">//文件路径</span></div><div class=\"line\">                String fileName = file.getPath().getName();   <span class=\"comment\">// 文件名称</span></div><div class=\"line\">                String parentPath = file.getPath().getParent().toString();<span class=\"comment\">//文件父路径</span></div><div class=\"line\">                Timestamp timestamp = <span class=\"keyword\">new</span> Timestamp(file.getModificationTime());<span class=\"comment\">//文件最后修改时间</span></div><div class=\"line\">                <span class=\"keyword\">long</span> blockSize = file.getBlockSize();   <span class=\"comment\">//文件块大小</span></div><div class=\"line\">                String group = file.getGroup();         <span class=\"comment\">//文件所属组</span></div><div class=\"line\">                String owner = file.getOwner();          <span class=\"comment\">// 文件拥有者</span></div><div class=\"line\">                <span class=\"keyword\">long</span> accessTime = file.getAccessTime();  <span class=\"comment\">//该文件上次访问时间</span></div><div class=\"line\">                <span class=\"keyword\">short</span> replication = file.getReplication(); <span class=\"comment\">//文件副本数</span></div><div class=\"line\">                System.out.println(<span class=\"string\">\"文件长度: \"</span>+len+<span class=\"string\">\"\\n\"</span>+</div><div class=\"line\">                        <span class=\"string\">\"文件路径: \"</span>+pathSource+<span class=\"string\">\"\\n\"</span>+</div><div class=\"line\">                        <span class=\"string\">\"文件名称: \"</span>+fileName+<span class=\"string\">\"\\n\"</span>+</div><div class=\"line\">                        <span class=\"string\">\"文件父路径: \"</span>+parentPath+<span class=\"string\">\"\\n\"</span>+</div><div class=\"line\">                        <span class=\"string\">\"文件最后修改时间: \"</span>+timestamp+<span class=\"string\">\"\\n\"</span>+</div><div class=\"line\">                        <span class=\"string\">\"文件块大小: \"</span>+blockSize+<span class=\"string\">\"\\n\"</span>+</div><div class=\"line\">                        <span class=\"string\">\"文件所属组: \"</span>+group+<span class=\"string\">\"\\n\"</span>+</div><div class=\"line\">                        <span class=\"string\">\"文件拥有者: \"</span>+owner+<span class=\"string\">\"\\n\"</span>+</div><div class=\"line\">                        <span class=\"string\">\"该文件上次访问时间: \"</span>+accessTime+<span class=\"string\">\"\\n\"</span>+</div><div class=\"line\">                        <span class=\"string\">\"文件副本数: \"</span>+replication+<span class=\"string\">\"\\n\"</span>+</div><div class=\"line\">                        <span class=\"string\">\"==============================\"</span>);</div><div class=\"line\"></div><div class=\"line\">            &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(file.isDirectory())&#123;</div><div class=\"line\">                System.out.println(<span class=\"string\">\"这是文件夹\"</span>);</div><div class=\"line\">                System.out.println(<span class=\"string\">\"文件父路径: \"</span>+file.getPath().toString());</div><div class=\"line\"></div><div class=\"line\">                <span class=\"comment\">//递归调用</span></div><div class=\"line\">                listFilesStatus(file.getPath(),hdfs);</div><div class=\"line\"></div><div class=\"line\">            &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(file.isSymlink())&#123;</div><div class=\"line\">                System.out.println(<span class=\"string\">\"这是链接文件\"</span>);</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h2 id=\"globStatus-路径过滤\"><a href=\"#globStatus-路径过滤\" class=\"headerlink\" title=\"globStatus 路径过滤\"></a>globStatus 路径过滤</h2><p>例如下面的例子，我需要得到/filtertest/<em>/</em> 路径中 带有abc的路径<br>globStatus 很灵活，内部甚至可以写一些正则表达式，有时候在处理大数据的预处理的时候可能很有效</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div></pre></td><td class=\"code\"><pre><div class=\"line\">public class FileListFilter &#123;</div><div class=\"line\"></div><div class=\"line\">    public static void main(String[] args) &#123;</div><div class=\"line\">        FileSystem hdfs = null;</div><div class=\"line\">        try&#123;</div><div class=\"line\">            Configuration config = new Configuration();</div><div class=\"line\"></div><div class=\"line\">            hdfs = FileSystem.get(new URI(&quot;hdfs://192.xxx.xxx.xxx:9000&quot;),</div><div class=\"line\">                    config, &quot;hmaster&quot;);</div><div class=\"line\">            Path allPath = new Path(&quot;/filtertest/*/*&quot;);</div><div class=\"line\"></div><div class=\"line\">            FileStatus[] fileGlobStatuses = hdfs.globStatus(allPath, new PathFilter() &#123;</div><div class=\"line\">                @Override</div><div class=\"line\">                public boolean accept(Path path) &#123;</div><div class=\"line\">                    String contidion = &quot;abc&quot;;</div><div class=\"line\">                    // 过滤出路径中包含 abc字符串 的路径</div><div class=\"line\">                    return  path.toString().contains(contidion);</div><div class=\"line\">                &#125;</div><div class=\"line\">            &#125;);</div><div class=\"line\"></div><div class=\"line\">            Path[] globPaths = FileUtil.stat2Paths(fileGlobStatuses);</div><div class=\"line\">            for (Path p :globPaths)&#123;</div><div class=\"line\">                System.out.println(&quot;globe过滤后的路径&quot;+p);</div><div class=\"line\">            &#125;</div><div class=\"line\"></div><div class=\"line\">        &#125;catch(Exception e)&#123;</div><div class=\"line\">            e.printStackTrace();</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"前言：\"><a href=\"#前言：\" class=\"headerlink\" title=\"前言：\"></a>前言：</h1><p>本章主要记录了如何使用fileStatus来获取hdfs文件的一些属性，以及如何使用globStatus对路径进行过滤, 列出某个目录下所有文件的信息</p>\n<p><img src=\"http://or49tneld.bkt.clouddn.com/18-1-23/63075597.jpg\" alt=\"hdfs-commands\"></p>\n<h1 id=\"filestatus-获取文件状态\"><a href=\"#filestatus-获取文件状态\" class=\"headerlink\" title=\"filestatus 获取文件状态\"></a>filestatus 获取文件状态</h1><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">package</span> hdfs.tutorial;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.conf.Configuration;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.fs.FileStatus;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.fs.FileSystem;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.fs.Path;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">import</span> java.io.IOException;</div><div class=\"line\"><span class=\"keyword\">import</span> java.net.URI;</div><div class=\"line\"><span class=\"keyword\">import</span> java.net.URISyntaxException;</div><div class=\"line\"><span class=\"keyword\">import</span> java.sql.Timestamp;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\"> * Created by Administrator on 2017/5/25.</span></div><div class=\"line\"><span class=\"comment\"> */</span></div><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ReadHdfs</span> </span>&#123;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> </span>&#123;</div><div class=\"line\"></div><div class=\"line\">        Path path = <span class=\"keyword\">new</span> Path(<span class=\"string\">\"hdfs://192.xxx.xxx.xxx:9000/hdfstest\"</span>);</div><div class=\"line\">        Path filePath = <span class=\"keyword\">new</span> Path(<span class=\"string\">\"hdfs://192.xxx.xxx.xxx:9000/hdfstest/people.txt\"</span>);</div><div class=\"line\">        <span class=\"keyword\">try</span> &#123;</div><div class=\"line\"></div><div class=\"line\">            <span class=\"comment\">//得到 FileSystem 类</span></div><div class=\"line\">            FileSystem hdfs = getFileSystem();</div><div class=\"line\"></div><div class=\"line\">            <span class=\"comment\">//列出某个目录下所有文件的信息</span></div><div class=\"line\">            listFilesStatus(path, hdfs);</div><div class=\"line\"></div><div class=\"line\">        &#125; <span class=\"keyword\">catch</span> (Exception e) &#123;</div><div class=\"line\">            e.printStackTrace();</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\">     *</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@return</span> 得到hdfs的连接 FileSystem类</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@throws</span> URISyntaxException</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@throws</span> IOException</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@throws</span> InterruptedException</span></div><div class=\"line\"><span class=\"comment\">     */</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> FileSystem <span class=\"title\">getFileSystem</span><span class=\"params\">()</span> <span class=\"keyword\">throws</span> URISyntaxException, IOException, InterruptedException </span>&#123;</div><div class=\"line\">      <span class=\"comment\">//获取FileSystem类的方法有很多种，这里只写一种</span></div><div class=\"line\">        Configuration config = <span class=\"keyword\">new</span> Configuration();</div><div class=\"line\">        URI uri = <span class=\"keyword\">new</span> URI(<span class=\"string\">\"hdfs://192.xxx.xxx.xxx:9000\"</span>);</div><div class=\"line\">        <span class=\"keyword\">return</span> FileSystem.get(uri,config,<span class=\"string\">\"your_user\"</span>);<span class=\"comment\">// 第一位为uri，第二位为config，第三位是登录的用户</span></div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\">     * 列出某个目录下所有文件的信息</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> path</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> hdfs</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@throws</span> IOException</span></div><div class=\"line\"><span class=\"comment\">     */</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">listFilesStatus</span><span class=\"params\">(Path path, FileSystem hdfs)</span> <span class=\"keyword\">throws</span> IOException </span>&#123;</div><div class=\"line\">        <span class=\"comment\">// 列出目录下的所有文件</span></div><div class=\"line\">        FileStatus[] files = hdfs.listStatus(path);</div><div class=\"line\"></div><div class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt;files.length ; i++) &#123;</div><div class=\"line\">            FileStatus file = files[i];</div><div class=\"line\">            <span class=\"keyword\">if</span>(file.isFile())&#123;</div><div class=\"line\">                System.out.println(<span class=\"string\">\"这是文件\"</span>);</div><div class=\"line\">                <span class=\"keyword\">long</span> len = file.getLen();                   <span class=\"comment\">//文件长度</span></div><div class=\"line\">                String pathSource = file.getPath().toString();<span class=\"comment\">//文件路径</span></div><div class=\"line\">                String fileName = file.getPath().getName();   <span class=\"comment\">// 文件名称</span></div><div class=\"line\">                String parentPath = file.getPath().getParent().toString();<span class=\"comment\">//文件父路径</span></div><div class=\"line\">                Timestamp timestamp = <span class=\"keyword\">new</span> Timestamp(file.getModificationTime());<span class=\"comment\">//文件最后修改时间</span></div><div class=\"line\">                <span class=\"keyword\">long</span> blockSize = file.getBlockSize();   <span class=\"comment\">//文件块大小</span></div><div class=\"line\">                String group = file.getGroup();         <span class=\"comment\">//文件所属组</span></div><div class=\"line\">                String owner = file.getOwner();          <span class=\"comment\">// 文件拥有者</span></div><div class=\"line\">                <span class=\"keyword\">long</span> accessTime = file.getAccessTime();  <span class=\"comment\">//该文件上次访问时间</span></div><div class=\"line\">                <span class=\"keyword\">short</span> replication = file.getReplication(); <span class=\"comment\">//文件副本数</span></div><div class=\"line\">                System.out.println(<span class=\"string\">\"文件长度: \"</span>+len+<span class=\"string\">\"\\n\"</span>+</div><div class=\"line\">                        <span class=\"string\">\"文件路径: \"</span>+pathSource+<span class=\"string\">\"\\n\"</span>+</div><div class=\"line\">                        <span class=\"string\">\"文件名称: \"</span>+fileName+<span class=\"string\">\"\\n\"</span>+</div><div class=\"line\">                        <span class=\"string\">\"文件父路径: \"</span>+parentPath+<span class=\"string\">\"\\n\"</span>+</div><div class=\"line\">                        <span class=\"string\">\"文件最后修改时间: \"</span>+timestamp+<span class=\"string\">\"\\n\"</span>+</div><div class=\"line\">                        <span class=\"string\">\"文件块大小: \"</span>+blockSize+<span class=\"string\">\"\\n\"</span>+</div><div class=\"line\">                        <span class=\"string\">\"文件所属组: \"</span>+group+<span class=\"string\">\"\\n\"</span>+</div><div class=\"line\">                        <span class=\"string\">\"文件拥有者: \"</span>+owner+<span class=\"string\">\"\\n\"</span>+</div><div class=\"line\">                        <span class=\"string\">\"该文件上次访问时间: \"</span>+accessTime+<span class=\"string\">\"\\n\"</span>+</div><div class=\"line\">                        <span class=\"string\">\"文件副本数: \"</span>+replication+<span class=\"string\">\"\\n\"</span>+</div><div class=\"line\">                        <span class=\"string\">\"==============================\"</span>);</div><div class=\"line\"></div><div class=\"line\">            &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(file.isDirectory())&#123;</div><div class=\"line\">                System.out.println(<span class=\"string\">\"这是文件夹\"</span>);</div><div class=\"line\">                System.out.println(<span class=\"string\">\"文件父路径: \"</span>+file.getPath().toString());</div><div class=\"line\"></div><div class=\"line\">                <span class=\"comment\">//递归调用</span></div><div class=\"line\">                listFilesStatus(file.getPath(),hdfs);</div><div class=\"line\"></div><div class=\"line\">            &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(file.isSymlink())&#123;</div><div class=\"line\">                System.out.println(<span class=\"string\">\"这是链接文件\"</span>);</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h2 id=\"globStatus-路径过滤\"><a href=\"#globStatus-路径过滤\" class=\"headerlink\" title=\"globStatus 路径过滤\"></a>globStatus 路径过滤</h2><p>例如下面的例子，我需要得到/filtertest/<em>/</em> 路径中 带有abc的路径<br>globStatus 很灵活，内部甚至可以写一些正则表达式，有时候在处理大数据的预处理的时候可能很有效</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div></pre></td><td class=\"code\"><pre><div class=\"line\">public class FileListFilter &#123;</div><div class=\"line\"></div><div class=\"line\">    public static void main(String[] args) &#123;</div><div class=\"line\">        FileSystem hdfs = null;</div><div class=\"line\">        try&#123;</div><div class=\"line\">            Configuration config = new Configuration();</div><div class=\"line\"></div><div class=\"line\">            hdfs = FileSystem.get(new URI(&quot;hdfs://192.xxx.xxx.xxx:9000&quot;),</div><div class=\"line\">                    config, &quot;hmaster&quot;);</div><div class=\"line\">            Path allPath = new Path(&quot;/filtertest/*/*&quot;);</div><div class=\"line\"></div><div class=\"line\">            FileStatus[] fileGlobStatuses = hdfs.globStatus(allPath, new PathFilter() &#123;</div><div class=\"line\">                @Override</div><div class=\"line\">                public boolean accept(Path path) &#123;</div><div class=\"line\">                    String contidion = &quot;abc&quot;;</div><div class=\"line\">                    // 过滤出路径中包含 abc字符串 的路径</div><div class=\"line\">                    return  path.toString().contains(contidion);</div><div class=\"line\">                &#125;</div><div class=\"line\">            &#125;);</div><div class=\"line\"></div><div class=\"line\">            Path[] globPaths = FileUtil.stat2Paths(fileGlobStatuses);</div><div class=\"line\">            for (Path p :globPaths)&#123;</div><div class=\"line\">                System.out.println(&quot;globe过滤后的路径&quot;+p);</div><div class=\"line\">            &#125;</div><div class=\"line\"></div><div class=\"line\">        &#125;catch(Exception e)&#123;</div><div class=\"line\">            e.printStackTrace();</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>"},{"title":"HDFS 指令（四）find,help,setfatter,truncate,usage","date":"2016-05-19T15:20:21.000Z","author":"kaishun","id":"9","blogexcerpt":"关键字：hdfs命令,find,help,setfatter,truncate,usage 的用法,前言,在这个HDFS教程中，主要介绍了如何查看命令帮助，如何设置文件扩展属性，文件的截断 . 1. find, 找出能匹配上的所有文件, 不区分大小写，对大小写不敏感","_content":"\n关键字：hdfs命令,find,help,setfatter,truncate,usage 的用法\n# **前言**\n在这个HDFS教程中，主要介绍了 如何查看命令帮助，如何设置文件扩展属性，文件的截断\n![hdfs-commands](http://or49tneld.bkt.clouddn.com/18-1-23/63075597.jpg)\n\n# **1. find**\n找出能匹配上的所有文件  \n-name pattern 不区分大小写，对大小写不敏感\n-iname pattern 对大小写敏感。\n-print 打印。\n-print0 打印在一行\n**命令用法**\n```\nhadoop fs -find <path> ... <expression> ...\n```\n**例子**\n```\nhadoop fs -find /user/dataflair/dir1/ -name sample -print\n```\n\n# **2. help**\n查看帮助文档\n**命令用法**\n```\nhadoop fs -help\n```\n**例子**\n```\nhadoop fs -help\n```\n\n# **3. setfattr**\n为一个文件或者文件夹设置一个扩展属性和值  \n操作  \n-b: 除了ACL的文件不移除，他移除所有的扩展属性。所有文件信息都保留给用户，组和其他用户\n-n name: 显示扩展的属性的名.\n-v value: 显示扩展的属性的值。\n-x name: 移除所有的扩展属性\npath: 文件或文件夹.\n**命令用法**\n```\nhadoop fs -setfattr -n name [-v value] | -x name <path>\n```\n**例子**\n```\nhdfs dfs -setfattr -n user.myAttr -v myValue /user/dataflair/dir2/purchases.txt\nhdfs dfs -setfattr -n user.noValue /user/dataflair/dir2/purchases.txt\nhdfs dfs -setfattr -x user.myAttr /user/dataflair/dir2/purchases.txt\n```\n\n# **4. truncate ** \n参考文章：http://blog.csdn.net/androidlushangderen/article/details/52651995\n类似于linux的truncate\n**命令用法**\n```\nhadoop fs -truncate [-w] <length> <paths>\n```\n**例子**\n```\nhadoop fs -truncate 55  /user/dataflair/dir2/purchases.txt  /user/dataflair/dir1/purchases.txt\nhadoop fs -truncate -w 127 /user/dataflair/dir2/purchases.txt\n```\n\n# **5. usage**\n返回某个命令的帮助\n**命令用法**\n```\nhadoop fs -usage command\n```\n**例子**\n```\nhadoop fs -usage mkdir\n```\n","source":"_posts/HDFS 指令（四）find,help,setfatter,truncate,usage.md","raw":"---\ntitle: HDFS 指令（四）find,help,setfatter,truncate,usage\ndate: 2016-05-19 23:20:21\ntags: [hdfs]\ncategories: [大数据,hdfs]\nauthor: kaishun\nid: 9\npermalink: hdfs-operator-4\nblogexcerpt: 关键字：hdfs命令,find,help,setfatter,truncate,usage 的用法,前言,在这个HDFS教程中，主要介绍了如何查看命令帮助，如何设置文件扩展属性，文件的截断 . 1. find, 找出能匹配上的所有文件, 不区分大小写，对大小写不敏感\n---\n\n关键字：hdfs命令,find,help,setfatter,truncate,usage 的用法\n# **前言**\n在这个HDFS教程中，主要介绍了 如何查看命令帮助，如何设置文件扩展属性，文件的截断\n![hdfs-commands](http://or49tneld.bkt.clouddn.com/18-1-23/63075597.jpg)\n\n# **1. find**\n找出能匹配上的所有文件  \n-name pattern 不区分大小写，对大小写不敏感\n-iname pattern 对大小写敏感。\n-print 打印。\n-print0 打印在一行\n**命令用法**\n```\nhadoop fs -find <path> ... <expression> ...\n```\n**例子**\n```\nhadoop fs -find /user/dataflair/dir1/ -name sample -print\n```\n\n# **2. help**\n查看帮助文档\n**命令用法**\n```\nhadoop fs -help\n```\n**例子**\n```\nhadoop fs -help\n```\n\n# **3. setfattr**\n为一个文件或者文件夹设置一个扩展属性和值  \n操作  \n-b: 除了ACL的文件不移除，他移除所有的扩展属性。所有文件信息都保留给用户，组和其他用户\n-n name: 显示扩展的属性的名.\n-v value: 显示扩展的属性的值。\n-x name: 移除所有的扩展属性\npath: 文件或文件夹.\n**命令用法**\n```\nhadoop fs -setfattr -n name [-v value] | -x name <path>\n```\n**例子**\n```\nhdfs dfs -setfattr -n user.myAttr -v myValue /user/dataflair/dir2/purchases.txt\nhdfs dfs -setfattr -n user.noValue /user/dataflair/dir2/purchases.txt\nhdfs dfs -setfattr -x user.myAttr /user/dataflair/dir2/purchases.txt\n```\n\n# **4. truncate ** \n参考文章：http://blog.csdn.net/androidlushangderen/article/details/52651995\n类似于linux的truncate\n**命令用法**\n```\nhadoop fs -truncate [-w] <length> <paths>\n```\n**例子**\n```\nhadoop fs -truncate 55  /user/dataflair/dir2/purchases.txt  /user/dataflair/dir1/purchases.txt\nhadoop fs -truncate -w 127 /user/dataflair/dir2/purchases.txt\n```\n\n# **5. usage**\n返回某个命令的帮助\n**命令用法**\n```\nhadoop fs -usage command\n```\n**例子**\n```\nhadoop fs -usage mkdir\n```\n","slug":"hdfs-operator-4","published":1,"updated":"2018-01-23T14:36:24.217Z","_id":"cjcrpnzn800062wv3n98yv5ai","comments":1,"layout":"post","photos":[],"link":"","content":"<p>关键字：hdfs命令,find,help,setfatter,truncate,usage 的用法</p>\n<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a><strong>前言</strong></h1><p>在这个HDFS教程中，主要介绍了 如何查看命令帮助，如何设置文件扩展属性，文件的截断<br><img src=\"http://or49tneld.bkt.clouddn.com/18-1-23/63075597.jpg\" alt=\"hdfs-commands\"></p>\n<h1 id=\"1-find\"><a href=\"#1-find\" class=\"headerlink\" title=\"1. find\"></a><strong>1. find</strong></h1><p>找出能匹配上的所有文件<br>-name pattern 不区分大小写，对大小写不敏感<br>-iname pattern 对大小写敏感。<br>-print 打印。<br>-print0 打印在一行<br><strong>命令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hadoop fs -find &lt;path&gt; ... &lt;expression&gt; ...</div></pre></td></tr></table></figure></p>\n<p><strong>例子</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hadoop fs -find /user/dataflair/dir1/ -name sample -print</div></pre></td></tr></table></figure></p>\n<h1 id=\"2-help\"><a href=\"#2-help\" class=\"headerlink\" title=\"2. help\"></a><strong>2. help</strong></h1><p>查看帮助文档<br><strong>命令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hadoop fs -help</div></pre></td></tr></table></figure></p>\n<p><strong>例子</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hadoop fs -help</div></pre></td></tr></table></figure></p>\n<h1 id=\"3-setfattr\"><a href=\"#3-setfattr\" class=\"headerlink\" title=\"3. setfattr\"></a><strong>3. setfattr</strong></h1><p>为一个文件或者文件夹设置一个扩展属性和值<br>操作<br>-b: 除了ACL的文件不移除，他移除所有的扩展属性。所有文件信息都保留给用户，组和其他用户<br>-n name: 显示扩展的属性的名.<br>-v value: 显示扩展的属性的值。<br>-x name: 移除所有的扩展属性<br>path: 文件或文件夹.<br><strong>命令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hadoop fs -setfattr -n name [-v value] | -x name &lt;path&gt;</div></pre></td></tr></table></figure></p>\n<p><strong>例子</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -setfattr -n user.myAttr -v myValue /user/dataflair/dir2/purchases.txt</div><div class=\"line\">hdfs dfs -setfattr -n user.noValue /user/dataflair/dir2/purchases.txt</div><div class=\"line\">hdfs dfs -setfattr -x user.myAttr /user/dataflair/dir2/purchases.txt</div></pre></td></tr></table></figure></p>\n<h1 id=\"4-truncate\"><a href=\"#4-truncate\" class=\"headerlink\" title=\"4. truncate \"></a><strong>4. truncate </strong></h1><p>参考文章：<a href=\"http://blog.csdn.net/androidlushangderen/article/details/52651995\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/androidlushangderen/article/details/52651995</a><br>类似于linux的truncate<br><strong>命令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hadoop fs -truncate [-w] &lt;length&gt; &lt;paths&gt;</div></pre></td></tr></table></figure></p>\n<p><strong>例子</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">hadoop fs -truncate 55  /user/dataflair/dir2/purchases.txt  /user/dataflair/dir1/purchases.txt</div><div class=\"line\">hadoop fs -truncate -w 127 /user/dataflair/dir2/purchases.txt</div></pre></td></tr></table></figure></p>\n<h1 id=\"5-usage\"><a href=\"#5-usage\" class=\"headerlink\" title=\"5. usage\"></a><strong>5. usage</strong></h1><p>返回某个命令的帮助<br><strong>命令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hadoop fs -usage command</div></pre></td></tr></table></figure></p>\n<p><strong>例子</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hadoop fs -usage mkdir</div></pre></td></tr></table></figure></p>\n","site":{"data":{}},"excerpt":"","more":"<p>关键字：hdfs命令,find,help,setfatter,truncate,usage 的用法</p>\n<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a><strong>前言</strong></h1><p>在这个HDFS教程中，主要介绍了 如何查看命令帮助，如何设置文件扩展属性，文件的截断<br><img src=\"http://or49tneld.bkt.clouddn.com/18-1-23/63075597.jpg\" alt=\"hdfs-commands\"></p>\n<h1 id=\"1-find\"><a href=\"#1-find\" class=\"headerlink\" title=\"1. find\"></a><strong>1. find</strong></h1><p>找出能匹配上的所有文件<br>-name pattern 不区分大小写，对大小写不敏感<br>-iname pattern 对大小写敏感。<br>-print 打印。<br>-print0 打印在一行<br><strong>命令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hadoop fs -find &lt;path&gt; ... &lt;expression&gt; ...</div></pre></td></tr></table></figure></p>\n<p><strong>例子</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hadoop fs -find /user/dataflair/dir1/ -name sample -print</div></pre></td></tr></table></figure></p>\n<h1 id=\"2-help\"><a href=\"#2-help\" class=\"headerlink\" title=\"2. help\"></a><strong>2. help</strong></h1><p>查看帮助文档<br><strong>命令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hadoop fs -help</div></pre></td></tr></table></figure></p>\n<p><strong>例子</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hadoop fs -help</div></pre></td></tr></table></figure></p>\n<h1 id=\"3-setfattr\"><a href=\"#3-setfattr\" class=\"headerlink\" title=\"3. setfattr\"></a><strong>3. setfattr</strong></h1><p>为一个文件或者文件夹设置一个扩展属性和值<br>操作<br>-b: 除了ACL的文件不移除，他移除所有的扩展属性。所有文件信息都保留给用户，组和其他用户<br>-n name: 显示扩展的属性的名.<br>-v value: 显示扩展的属性的值。<br>-x name: 移除所有的扩展属性<br>path: 文件或文件夹.<br><strong>命令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hadoop fs -setfattr -n name [-v value] | -x name &lt;path&gt;</div></pre></td></tr></table></figure></p>\n<p><strong>例子</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -setfattr -n user.myAttr -v myValue /user/dataflair/dir2/purchases.txt</div><div class=\"line\">hdfs dfs -setfattr -n user.noValue /user/dataflair/dir2/purchases.txt</div><div class=\"line\">hdfs dfs -setfattr -x user.myAttr /user/dataflair/dir2/purchases.txt</div></pre></td></tr></table></figure></p>\n<h1 id=\"4-truncate\"><a href=\"#4-truncate\" class=\"headerlink\" title=\"4. truncate \"></a><strong>4. truncate </strong></h1><p>参考文章：<a href=\"http://blog.csdn.net/androidlushangderen/article/details/52651995\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/androidlushangderen/article/details/52651995</a><br>类似于linux的truncate<br><strong>命令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hadoop fs -truncate [-w] &lt;length&gt; &lt;paths&gt;</div></pre></td></tr></table></figure></p>\n<p><strong>例子</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">hadoop fs -truncate 55  /user/dataflair/dir2/purchases.txt  /user/dataflair/dir1/purchases.txt</div><div class=\"line\">hadoop fs -truncate -w 127 /user/dataflair/dir2/purchases.txt</div></pre></td></tr></table></figure></p>\n<h1 id=\"5-usage\"><a href=\"#5-usage\" class=\"headerlink\" title=\"5. usage\"></a><strong>5. usage</strong></h1><p>返回某个命令的帮助<br><strong>命令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hadoop fs -usage command</div></pre></td></tr></table></figure></p>\n<p><strong>例子</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hadoop fs -usage mkdir</div></pre></td></tr></table></figure></p>\n"},{"title":"HDFS 指令（三）touchz，test，text，stat，appendToFile，checksum，count，chmod","date":"2016-05-19T15:13:21.000Z","author":"kaishun","id":"7","blogexcerpt":"关键词：hdfs命令，touchz，test，text，stat，appendToFile，checksum，count，chmod. 本章目的. 在这个Hadoop HDFS命令教程中，我们将学习剩下一些重要并且经常使用的HDFS命令，借助这些命令，我们将能够执行HDFS文件操作，如复制文件，更改文件权限，查看文件内容，更改文件所有权，创建目录等。要了解有关世界上最可靠的存储层的更多信息，请参阅HDFS入门指南。","_content":"\n关键词：hdfs命令，touchz，test，text，stat，appendToFile，checksum，count，chmod\n# **本章目的**\n在这个Hadoop HDFS命令教程中，我们将学习剩下一些重要并且经常使用的HDFS命令，借助这些命令，我们将能够执行HDFS文件操作，如复制文件，更改文件权限，查看文件内容，更改文件所有权，创建目录等。要了解有关世界上最可靠的存储层的更多信息，请参阅HDFS入门指南。  \n![hdfs-commands](http://or49tneld.bkt.clouddn.com/18-1-23/63075597.jpg)\n\n# **1. touchz**  \n在指定目录创建一个新文件，如果文件存在，则创建失败\n**命令用法**\n```\ntouchz <path>\n```\n**例子**\n```\nhdfs dfs -touchz /user/dataflair/dir2\n```\n\n#  **2. test**  \n测试hdfs中的某个文件或者目录是否存在  \n-d: 如果测试的路径是一个文件夹, 则返回0，否则返回1。    \n-e: 如果测试的路径存在, 则返回0，否则返回1。   \n-f: 如果测试的路径是一个文件, 则返回0，否则返回1。    \n-s: if 如果测试的路径不是空(文件夹下有文件或者文件夹), 则返回0，否则返回1。    \n-z: 如果测试的是一个文件，并且这个文件不为空, 则返回0，否则返回1。\n**命令用法**\n```\nhdfs dfs -test -[ezd] URI\n```\n**例子**\n```\n\"hdfs dfs -test -e /test/file/sample\nhdfs dfs -test -z /test/file/sample\nhdfs dfs -test -d /test/file/sample\n```\n\n# **3. text**  \n格式化输出文件的内容，允许的格式化包括zip,和 TextRecordInputStream  \n**命令用法**\n```\nhdfs dfs -text <source>\n```\n**例子**\n```\nhdfs dfs -text /user/dataflair/dir1/sample\n```\n\n#  **4. stat**  \n打印有关路径的信息，可以加下面的格式化输出      \n%b: 文件大小  \n\n%n: 文件名  \n\n%o: 块大小  \n\n%r: 副本个数  \n\n%y, %Y: 修改日期.  \n**命令用法**\n```\nhdfs dfs -stat [format] path\n\n```\n**例子**\n```\nhdfs dfs -stat /user/dataflair/dir1\nhdfs fs -stat \"%o %r\" /user/dataflair/dir1\n```\n\n\n\n# **5. appendToFile**  \nappendToFile命令是将一个或者多个文件添加到HDFS系统中，他也是从标准输入中读取，然后添加到目标文件系统汇总  \n**命令用法**\n```\nhadoop fs -appendToFile <localsource> ... <dst>\n\n```\n**例子**\n```\nhadoop fs -appendToFile /home/dataflair/Desktop/sample /user/dataflair/dir1\n```\n\n# **6. checksum**  \nDatanode在把数据实际存储之前会验证数据的校验和（checksum的初始值？）如果某个client在读取数据时检测到数据错误, 在抛出ChecksumException  \n参考： http://blog.csdn.net/oh_mourinho/article/details/52524442\n**命令用法**\n```\nhadoop fs -checksum URI\n```\n**例子**\n```\nhadoop fs -checksum /user/dataflair/dir1/sample\n```\n\n# **7. count**  \n统计hdfs对应路径下的目录个数，文件个数，文件总计大小   \n显示为目录个数，文件个数，文件总计大小，输入路径\n**命令用法**\n```\nhdfs dfs -count [-q] <paths>\n```\n**例子**\n```\nhdfs dfs -count /user/dataflair\n```\n\n# **8. chmod**  \n类似于linux，用来修改权限\n**命令用法**\n```\nchmod [-R] mode,mode,... <path>...\n```\n**例子**\n```\nhdfs dfs -chmod 777 /user/dataflair/dir1/sample\n```\n翻译原文：http://data-flair.training/blogs/hadoop-hdfs-commands-tutorial/","source":"_posts/HDFS 指令（三）touchz，test，text，stat，appendToFile，checksum，count，chmod.md","raw":"---\ntitle: HDFS 指令（三）touchz，test，text，stat，appendToFile，checksum，count，chmod\ndate: 2016-05-19 23:13:21\ntags: [hdfs]\ncategories: [大数据,hdfs]\nauthor: kaishun\nid: 7\npermalink: hdfs-operator-3\nblogexcerpt: 关键词：hdfs命令，touchz，test，text，stat，appendToFile，checksum，count，chmod. 本章目的. 在这个Hadoop HDFS命令教程中，我们将学习剩下一些重要并且经常使用的HDFS命令，借助这些命令，我们将能够执行HDFS文件操作，如复制文件，更改文件权限，查看文件内容，更改文件所有权，创建目录等。要了解有关世界上最可靠的存储层的更多信息，请参阅HDFS入门指南。  \n---\n\n关键词：hdfs命令，touchz，test，text，stat，appendToFile，checksum，count，chmod\n# **本章目的**\n在这个Hadoop HDFS命令教程中，我们将学习剩下一些重要并且经常使用的HDFS命令，借助这些命令，我们将能够执行HDFS文件操作，如复制文件，更改文件权限，查看文件内容，更改文件所有权，创建目录等。要了解有关世界上最可靠的存储层的更多信息，请参阅HDFS入门指南。  \n![hdfs-commands](http://or49tneld.bkt.clouddn.com/18-1-23/63075597.jpg)\n\n# **1. touchz**  \n在指定目录创建一个新文件，如果文件存在，则创建失败\n**命令用法**\n```\ntouchz <path>\n```\n**例子**\n```\nhdfs dfs -touchz /user/dataflair/dir2\n```\n\n#  **2. test**  \n测试hdfs中的某个文件或者目录是否存在  \n-d: 如果测试的路径是一个文件夹, 则返回0，否则返回1。    \n-e: 如果测试的路径存在, 则返回0，否则返回1。   \n-f: 如果测试的路径是一个文件, 则返回0，否则返回1。    \n-s: if 如果测试的路径不是空(文件夹下有文件或者文件夹), 则返回0，否则返回1。    \n-z: 如果测试的是一个文件，并且这个文件不为空, 则返回0，否则返回1。\n**命令用法**\n```\nhdfs dfs -test -[ezd] URI\n```\n**例子**\n```\n\"hdfs dfs -test -e /test/file/sample\nhdfs dfs -test -z /test/file/sample\nhdfs dfs -test -d /test/file/sample\n```\n\n# **3. text**  \n格式化输出文件的内容，允许的格式化包括zip,和 TextRecordInputStream  \n**命令用法**\n```\nhdfs dfs -text <source>\n```\n**例子**\n```\nhdfs dfs -text /user/dataflair/dir1/sample\n```\n\n#  **4. stat**  \n打印有关路径的信息，可以加下面的格式化输出      \n%b: 文件大小  \n\n%n: 文件名  \n\n%o: 块大小  \n\n%r: 副本个数  \n\n%y, %Y: 修改日期.  \n**命令用法**\n```\nhdfs dfs -stat [format] path\n\n```\n**例子**\n```\nhdfs dfs -stat /user/dataflair/dir1\nhdfs fs -stat \"%o %r\" /user/dataflair/dir1\n```\n\n\n\n# **5. appendToFile**  \nappendToFile命令是将一个或者多个文件添加到HDFS系统中，他也是从标准输入中读取，然后添加到目标文件系统汇总  \n**命令用法**\n```\nhadoop fs -appendToFile <localsource> ... <dst>\n\n```\n**例子**\n```\nhadoop fs -appendToFile /home/dataflair/Desktop/sample /user/dataflair/dir1\n```\n\n# **6. checksum**  \nDatanode在把数据实际存储之前会验证数据的校验和（checksum的初始值？）如果某个client在读取数据时检测到数据错误, 在抛出ChecksumException  \n参考： http://blog.csdn.net/oh_mourinho/article/details/52524442\n**命令用法**\n```\nhadoop fs -checksum URI\n```\n**例子**\n```\nhadoop fs -checksum /user/dataflair/dir1/sample\n```\n\n# **7. count**  \n统计hdfs对应路径下的目录个数，文件个数，文件总计大小   \n显示为目录个数，文件个数，文件总计大小，输入路径\n**命令用法**\n```\nhdfs dfs -count [-q] <paths>\n```\n**例子**\n```\nhdfs dfs -count /user/dataflair\n```\n\n# **8. chmod**  \n类似于linux，用来修改权限\n**命令用法**\n```\nchmod [-R] mode,mode,... <path>...\n```\n**例子**\n```\nhdfs dfs -chmod 777 /user/dataflair/dir1/sample\n```\n翻译原文：http://data-flair.training/blogs/hadoop-hdfs-commands-tutorial/","slug":"hdfs-operator-3","published":1,"updated":"2018-01-23T14:35:51.723Z","_id":"cjcrpnznn00082wv354a8qrsu","comments":1,"layout":"post","photos":[],"link":"","content":"<p>关键词：hdfs命令，touchz，test，text，stat，appendToFile，checksum，count，chmod</p>\n<h1 id=\"本章目的\"><a href=\"#本章目的\" class=\"headerlink\" title=\"本章目的\"></a><strong>本章目的</strong></h1><p>在这个Hadoop HDFS命令教程中，我们将学习剩下一些重要并且经常使用的HDFS命令，借助这些命令，我们将能够执行HDFS文件操作，如复制文件，更改文件权限，查看文件内容，更改文件所有权，创建目录等。要了解有关世界上最可靠的存储层的更多信息，请参阅HDFS入门指南。<br><img src=\"http://or49tneld.bkt.clouddn.com/18-1-23/63075597.jpg\" alt=\"hdfs-commands\"></p>\n<h1 id=\"1-touchz\"><a href=\"#1-touchz\" class=\"headerlink\" title=\"1. touchz\"></a><strong>1. touchz</strong></h1><p>在指定目录创建一个新文件，如果文件存在，则创建失败<br><strong>命令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">touchz &lt;path&gt;</div></pre></td></tr></table></figure></p>\n<p><strong>例子</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -touchz /user/dataflair/dir2</div></pre></td></tr></table></figure></p>\n<h1 id=\"2-test\"><a href=\"#2-test\" class=\"headerlink\" title=\"2. test\"></a><strong>2. test</strong></h1><p>测试hdfs中的某个文件或者目录是否存在<br>-d: 如果测试的路径是一个文件夹, 则返回0，否则返回1。<br>-e: 如果测试的路径存在, 则返回0，否则返回1。<br>-f: 如果测试的路径是一个文件, 则返回0，否则返回1。<br>-s: if 如果测试的路径不是空(文件夹下有文件或者文件夹), 则返回0，否则返回1。<br>-z: 如果测试的是一个文件，并且这个文件不为空, 则返回0，否则返回1。<br><strong>命令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -test -[ezd] URI</div></pre></td></tr></table></figure></p>\n<p><strong>例子</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">&quot;hdfs dfs -test -e /test/file/sample</div><div class=\"line\">hdfs dfs -test -z /test/file/sample</div><div class=\"line\">hdfs dfs -test -d /test/file/sample</div></pre></td></tr></table></figure></p>\n<h1 id=\"3-text\"><a href=\"#3-text\" class=\"headerlink\" title=\"3. text\"></a><strong>3. text</strong></h1><p>格式化输出文件的内容，允许的格式化包括zip,和 TextRecordInputStream<br><strong>命令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -text &lt;source&gt;</div></pre></td></tr></table></figure></p>\n<p><strong>例子</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -text /user/dataflair/dir1/sample</div></pre></td></tr></table></figure></p>\n<h1 id=\"4-stat\"><a href=\"#4-stat\" class=\"headerlink\" title=\"4. stat\"></a><strong>4. stat</strong></h1><p>打印有关路径的信息，可以加下面的格式化输出<br>%b: 文件大小  </p>\n<p>%n: 文件名  </p>\n<p>%o: 块大小  </p>\n<p>%r: 副本个数  </p>\n<p>%y, %Y: 修改日期.<br><strong>命令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -stat [format] path</div></pre></td></tr></table></figure></p>\n<p><strong>例子</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -stat /user/dataflair/dir1</div><div class=\"line\">hdfs fs -stat &quot;%o %r&quot; /user/dataflair/dir1</div></pre></td></tr></table></figure></p>\n<h1 id=\"5-appendToFile\"><a href=\"#5-appendToFile\" class=\"headerlink\" title=\"5. appendToFile\"></a><strong>5. appendToFile</strong></h1><p>appendToFile命令是将一个或者多个文件添加到HDFS系统中，他也是从标准输入中读取，然后添加到目标文件系统汇总<br><strong>命令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hadoop fs -appendToFile &lt;localsource&gt; ... &lt;dst&gt;</div></pre></td></tr></table></figure></p>\n<p><strong>例子</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hadoop fs -appendToFile /home/dataflair/Desktop/sample /user/dataflair/dir1</div></pre></td></tr></table></figure></p>\n<h1 id=\"6-checksum\"><a href=\"#6-checksum\" class=\"headerlink\" title=\"6. checksum\"></a><strong>6. checksum</strong></h1><p>Datanode在把数据实际存储之前会验证数据的校验和（checksum的初始值？）如果某个client在读取数据时检测到数据错误, 在抛出ChecksumException<br>参考： <a href=\"http://blog.csdn.net/oh_mourinho/article/details/52524442\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/oh_mourinho/article/details/52524442</a><br><strong>命令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hadoop fs -checksum URI</div></pre></td></tr></table></figure></p>\n<p><strong>例子</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hadoop fs -checksum /user/dataflair/dir1/sample</div></pre></td></tr></table></figure></p>\n<h1 id=\"7-count\"><a href=\"#7-count\" class=\"headerlink\" title=\"7. count\"></a><strong>7. count</strong></h1><p>统计hdfs对应路径下的目录个数，文件个数，文件总计大小<br>显示为目录个数，文件个数，文件总计大小，输入路径<br><strong>命令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -count [-q] &lt;paths&gt;</div></pre></td></tr></table></figure></p>\n<p><strong>例子</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -count /user/dataflair</div></pre></td></tr></table></figure></p>\n<h1 id=\"8-chmod\"><a href=\"#8-chmod\" class=\"headerlink\" title=\"8. chmod\"></a><strong>8. chmod</strong></h1><p>类似于linux，用来修改权限<br><strong>命令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">chmod [-R] mode,mode,... &lt;path&gt;...</div></pre></td></tr></table></figure></p>\n<p><strong>例子</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -chmod 777 /user/dataflair/dir1/sample</div></pre></td></tr></table></figure></p>\n<p>翻译原文：<a href=\"http://data-flair.training/blogs/hadoop-hdfs-commands-tutorial/\" target=\"_blank\" rel=\"external\">http://data-flair.training/blogs/hadoop-hdfs-commands-tutorial/</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>关键词：hdfs命令，touchz，test，text，stat，appendToFile，checksum，count，chmod</p>\n<h1 id=\"本章目的\"><a href=\"#本章目的\" class=\"headerlink\" title=\"本章目的\"></a><strong>本章目的</strong></h1><p>在这个Hadoop HDFS命令教程中，我们将学习剩下一些重要并且经常使用的HDFS命令，借助这些命令，我们将能够执行HDFS文件操作，如复制文件，更改文件权限，查看文件内容，更改文件所有权，创建目录等。要了解有关世界上最可靠的存储层的更多信息，请参阅HDFS入门指南。<br><img src=\"http://or49tneld.bkt.clouddn.com/18-1-23/63075597.jpg\" alt=\"hdfs-commands\"></p>\n<h1 id=\"1-touchz\"><a href=\"#1-touchz\" class=\"headerlink\" title=\"1. touchz\"></a><strong>1. touchz</strong></h1><p>在指定目录创建一个新文件，如果文件存在，则创建失败<br><strong>命令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">touchz &lt;path&gt;</div></pre></td></tr></table></figure></p>\n<p><strong>例子</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -touchz /user/dataflair/dir2</div></pre></td></tr></table></figure></p>\n<h1 id=\"2-test\"><a href=\"#2-test\" class=\"headerlink\" title=\"2. test\"></a><strong>2. test</strong></h1><p>测试hdfs中的某个文件或者目录是否存在<br>-d: 如果测试的路径是一个文件夹, 则返回0，否则返回1。<br>-e: 如果测试的路径存在, 则返回0，否则返回1。<br>-f: 如果测试的路径是一个文件, 则返回0，否则返回1。<br>-s: if 如果测试的路径不是空(文件夹下有文件或者文件夹), 则返回0，否则返回1。<br>-z: 如果测试的是一个文件，并且这个文件不为空, 则返回0，否则返回1。<br><strong>命令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -test -[ezd] URI</div></pre></td></tr></table></figure></p>\n<p><strong>例子</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">&quot;hdfs dfs -test -e /test/file/sample</div><div class=\"line\">hdfs dfs -test -z /test/file/sample</div><div class=\"line\">hdfs dfs -test -d /test/file/sample</div></pre></td></tr></table></figure></p>\n<h1 id=\"3-text\"><a href=\"#3-text\" class=\"headerlink\" title=\"3. text\"></a><strong>3. text</strong></h1><p>格式化输出文件的内容，允许的格式化包括zip,和 TextRecordInputStream<br><strong>命令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -text &lt;source&gt;</div></pre></td></tr></table></figure></p>\n<p><strong>例子</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -text /user/dataflair/dir1/sample</div></pre></td></tr></table></figure></p>\n<h1 id=\"4-stat\"><a href=\"#4-stat\" class=\"headerlink\" title=\"4. stat\"></a><strong>4. stat</strong></h1><p>打印有关路径的信息，可以加下面的格式化输出<br>%b: 文件大小  </p>\n<p>%n: 文件名  </p>\n<p>%o: 块大小  </p>\n<p>%r: 副本个数  </p>\n<p>%y, %Y: 修改日期.<br><strong>命令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -stat [format] path</div></pre></td></tr></table></figure></p>\n<p><strong>例子</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -stat /user/dataflair/dir1</div><div class=\"line\">hdfs fs -stat &quot;%o %r&quot; /user/dataflair/dir1</div></pre></td></tr></table></figure></p>\n<h1 id=\"5-appendToFile\"><a href=\"#5-appendToFile\" class=\"headerlink\" title=\"5. appendToFile\"></a><strong>5. appendToFile</strong></h1><p>appendToFile命令是将一个或者多个文件添加到HDFS系统中，他也是从标准输入中读取，然后添加到目标文件系统汇总<br><strong>命令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hadoop fs -appendToFile &lt;localsource&gt; ... &lt;dst&gt;</div></pre></td></tr></table></figure></p>\n<p><strong>例子</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hadoop fs -appendToFile /home/dataflair/Desktop/sample /user/dataflair/dir1</div></pre></td></tr></table></figure></p>\n<h1 id=\"6-checksum\"><a href=\"#6-checksum\" class=\"headerlink\" title=\"6. checksum\"></a><strong>6. checksum</strong></h1><p>Datanode在把数据实际存储之前会验证数据的校验和（checksum的初始值？）如果某个client在读取数据时检测到数据错误, 在抛出ChecksumException<br>参考： <a href=\"http://blog.csdn.net/oh_mourinho/article/details/52524442\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/oh_mourinho/article/details/52524442</a><br><strong>命令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hadoop fs -checksum URI</div></pre></td></tr></table></figure></p>\n<p><strong>例子</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hadoop fs -checksum /user/dataflair/dir1/sample</div></pre></td></tr></table></figure></p>\n<h1 id=\"7-count\"><a href=\"#7-count\" class=\"headerlink\" title=\"7. count\"></a><strong>7. count</strong></h1><p>统计hdfs对应路径下的目录个数，文件个数，文件总计大小<br>显示为目录个数，文件个数，文件总计大小，输入路径<br><strong>命令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -count [-q] &lt;paths&gt;</div></pre></td></tr></table></figure></p>\n<p><strong>例子</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -count /user/dataflair</div></pre></td></tr></table></figure></p>\n<h1 id=\"8-chmod\"><a href=\"#8-chmod\" class=\"headerlink\" title=\"8. chmod\"></a><strong>8. chmod</strong></h1><p>类似于linux，用来修改权限<br><strong>命令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">chmod [-R] mode,mode,... &lt;path&gt;...</div></pre></td></tr></table></figure></p>\n<p><strong>例子</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -chmod 777 /user/dataflair/dir1/sample</div></pre></td></tr></table></figure></p>\n<p>翻译原文：<a href=\"http://data-flair.training/blogs/hadoop-hdfs-commands-tutorial/\" target=\"_blank\" rel=\"external\">http://data-flair.training/blogs/hadoop-hdfs-commands-tutorial/</a></p>\n"},{"title":"HDFS获取目录大小API","date":"2016-05-16T15:13:21.000Z","author":"kaishun","id":"2","blogexcerpt":"获取文件大小，在命令行上，使用hadoop fs -du 命令可以，但是通过javaAPI怎么获取呢，最开始我想到的是递归的方法，这个方法很慢，后来发现FileSystem.getContentSummary的方法,最慢的一个方法--递归,使用FileSystem.getContentSummary方法,","_content":"\n获取文件大小，在命令行上，使用hadoop fs -du 命令可以，但是通过javaAPI怎么获取呢，\n最开始我想到的是递归的方法，这个方法很慢，后来发现FileSystem.getContentSummary的方法\n\n# **最慢的一个方法--递归**\n\n```java\n网上很多类似的方法，不建议\n```\n\n# **使用FileSystem.getContentSummary方法**\n下面是api的一句话： \n\nThe getSpaceConsumed() function in the ContentSummary class will return the actual space the file/directory occupies in the cluster i.e. it takes into account the replication factor set for the cluster.\n\nFor instance, if the replication factor in the hadoop cluster is set to 3 and the directory size is 1.5GB, the getSpaceConsumed() function will return the value as 4.5GB.\n\nUsing getLength() function in the ContentSummary class will return you the actual file/directory size.  \n示例代码如下\n\n```java\n public static void main(String[] args) {\n        FileSystem hdfs = null;\n\n        Configuration conf = new Configuration();\n\n        try {\n            hdfs = FileSystem.get(new URI(\"hdfs://192.xxx.xx.xx:9000\"),conf,\"username\");\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n\n        Path filenamePath = new Path(\"/test/input\");\n        try {\n            //会根据集群的配置输出，例如我这里输出3G\n            System.out.println(\"SIZE OF THE HDFS DIRECTORY : \" + hdfs.getContentSummary(filenamePath).getSpaceConsumed());\n           // 显示实际的输出，例如这里显示 1G\n            System.out.println(\"SIZE OF THE HDFS DIRECTORY : \" + hdfs.getContentSummary(filenamePath).getLength());\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n\n    }\n\n```","source":"_posts/HDFS获取目录大小API.md","raw":"---\ntitle: HDFS获取目录大小API\ndate: 2016-05-16 23:13:21\ntags: [hdfs]\ncategories: [大数据,hdfs]\nauthor: kaishun\nid: 2\npermalink: hdfs-ContentSummary-directory\nblogexcerpt: 获取文件大小，在命令行上，使用hadoop fs -du 命令可以，但是通过javaAPI怎么获取呢，最开始我想到的是递归的方法，这个方法很慢，后来发现FileSystem.getContentSummary的方法,最慢的一个方法--递归,使用FileSystem.getContentSummary方法,\n---\n\n获取文件大小，在命令行上，使用hadoop fs -du 命令可以，但是通过javaAPI怎么获取呢，\n最开始我想到的是递归的方法，这个方法很慢，后来发现FileSystem.getContentSummary的方法\n\n# **最慢的一个方法--递归**\n\n```java\n网上很多类似的方法，不建议\n```\n\n# **使用FileSystem.getContentSummary方法**\n下面是api的一句话： \n\nThe getSpaceConsumed() function in the ContentSummary class will return the actual space the file/directory occupies in the cluster i.e. it takes into account the replication factor set for the cluster.\n\nFor instance, if the replication factor in the hadoop cluster is set to 3 and the directory size is 1.5GB, the getSpaceConsumed() function will return the value as 4.5GB.\n\nUsing getLength() function in the ContentSummary class will return you the actual file/directory size.  \n示例代码如下\n\n```java\n public static void main(String[] args) {\n        FileSystem hdfs = null;\n\n        Configuration conf = new Configuration();\n\n        try {\n            hdfs = FileSystem.get(new URI(\"hdfs://192.xxx.xx.xx:9000\"),conf,\"username\");\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n\n        Path filenamePath = new Path(\"/test/input\");\n        try {\n            //会根据集群的配置输出，例如我这里输出3G\n            System.out.println(\"SIZE OF THE HDFS DIRECTORY : \" + hdfs.getContentSummary(filenamePath).getSpaceConsumed());\n           // 显示实际的输出，例如这里显示 1G\n            System.out.println(\"SIZE OF THE HDFS DIRECTORY : \" + hdfs.getContentSummary(filenamePath).getLength());\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n\n    }\n\n```","slug":"hdfs-ContentSummary-directory","published":1,"updated":"2018-01-23T14:21:03.234Z","_id":"cjcrpnznn000a2wv33us3gtup","comments":1,"layout":"post","photos":[],"link":"","content":"<p>获取文件大小，在命令行上，使用hadoop fs -du 命令可以，但是通过javaAPI怎么获取呢，<br>最开始我想到的是递归的方法，这个方法很慢，后来发现FileSystem.getContentSummary的方法</p>\n<h1 id=\"最慢的一个方法–递归\"><a href=\"#最慢的一个方法–递归\" class=\"headerlink\" title=\"最慢的一个方法–递归\"></a><strong>最慢的一个方法–递归</strong></h1><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">网上很多类似的方法，不建议</div></pre></td></tr></table></figure>\n<h1 id=\"使用FileSystem-getContentSummary方法\"><a href=\"#使用FileSystem-getContentSummary方法\" class=\"headerlink\" title=\"使用FileSystem.getContentSummary方法\"></a><strong>使用FileSystem.getContentSummary方法</strong></h1><p>下面是api的一句话： </p>\n<p>The getSpaceConsumed() function in the ContentSummary class will return the actual space the file/directory occupies in the cluster i.e. it takes into account the replication factor set for the cluster.</p>\n<p>For instance, if the replication factor in the hadoop cluster is set to 3 and the directory size is 1.5GB, the getSpaceConsumed() function will return the value as 4.5GB.</p>\n<p>Using getLength() function in the ContentSummary class will return you the actual file/directory size.<br>示例代码如下</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> </span>&#123;</div><div class=\"line\">       FileSystem hdfs = <span class=\"keyword\">null</span>;</div><div class=\"line\"></div><div class=\"line\">       Configuration conf = <span class=\"keyword\">new</span> Configuration();</div><div class=\"line\"></div><div class=\"line\">       <span class=\"keyword\">try</span> &#123;</div><div class=\"line\">           hdfs = FileSystem.get(<span class=\"keyword\">new</span> URI(<span class=\"string\">\"hdfs://192.xxx.xx.xx:9000\"</span>),conf,<span class=\"string\">\"username\"</span>);</div><div class=\"line\">       &#125; <span class=\"keyword\">catch</span> (Exception e) &#123;</div><div class=\"line\">           e.printStackTrace();</div><div class=\"line\">       &#125;</div><div class=\"line\"></div><div class=\"line\">       Path filenamePath = <span class=\"keyword\">new</span> Path(<span class=\"string\">\"/test/input\"</span>);</div><div class=\"line\">       <span class=\"keyword\">try</span> &#123;</div><div class=\"line\">           <span class=\"comment\">//会根据集群的配置输出，例如我这里输出3G</span></div><div class=\"line\">           System.out.println(<span class=\"string\">\"SIZE OF THE HDFS DIRECTORY : \"</span> + hdfs.getContentSummary(filenamePath).getSpaceConsumed());</div><div class=\"line\">          <span class=\"comment\">// 显示实际的输出，例如这里显示 1G</span></div><div class=\"line\">           System.out.println(<span class=\"string\">\"SIZE OF THE HDFS DIRECTORY : \"</span> + hdfs.getContentSummary(filenamePath).getLength());</div><div class=\"line\">       &#125; <span class=\"keyword\">catch</span> (IOException e) &#123;</div><div class=\"line\">           e.printStackTrace();</div><div class=\"line\">       &#125;</div><div class=\"line\"></div><div class=\"line\">   &#125;</div></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<p>获取文件大小，在命令行上，使用hadoop fs -du 命令可以，但是通过javaAPI怎么获取呢，<br>最开始我想到的是递归的方法，这个方法很慢，后来发现FileSystem.getContentSummary的方法</p>\n<h1 id=\"最慢的一个方法–递归\"><a href=\"#最慢的一个方法–递归\" class=\"headerlink\" title=\"最慢的一个方法–递归\"></a><strong>最慢的一个方法–递归</strong></h1><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">网上很多类似的方法，不建议</div></pre></td></tr></table></figure>\n<h1 id=\"使用FileSystem-getContentSummary方法\"><a href=\"#使用FileSystem-getContentSummary方法\" class=\"headerlink\" title=\"使用FileSystem.getContentSummary方法\"></a><strong>使用FileSystem.getContentSummary方法</strong></h1><p>下面是api的一句话： </p>\n<p>The getSpaceConsumed() function in the ContentSummary class will return the actual space the file/directory occupies in the cluster i.e. it takes into account the replication factor set for the cluster.</p>\n<p>For instance, if the replication factor in the hadoop cluster is set to 3 and the directory size is 1.5GB, the getSpaceConsumed() function will return the value as 4.5GB.</p>\n<p>Using getLength() function in the ContentSummary class will return you the actual file/directory size.<br>示例代码如下</p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> </span>&#123;</div><div class=\"line\">       FileSystem hdfs = <span class=\"keyword\">null</span>;</div><div class=\"line\"></div><div class=\"line\">       Configuration conf = <span class=\"keyword\">new</span> Configuration();</div><div class=\"line\"></div><div class=\"line\">       <span class=\"keyword\">try</span> &#123;</div><div class=\"line\">           hdfs = FileSystem.get(<span class=\"keyword\">new</span> URI(<span class=\"string\">\"hdfs://192.xxx.xx.xx:9000\"</span>),conf,<span class=\"string\">\"username\"</span>);</div><div class=\"line\">       &#125; <span class=\"keyword\">catch</span> (Exception e) &#123;</div><div class=\"line\">           e.printStackTrace();</div><div class=\"line\">       &#125;</div><div class=\"line\"></div><div class=\"line\">       Path filenamePath = <span class=\"keyword\">new</span> Path(<span class=\"string\">\"/test/input\"</span>);</div><div class=\"line\">       <span class=\"keyword\">try</span> &#123;</div><div class=\"line\">           <span class=\"comment\">//会根据集群的配置输出，例如我这里输出3G</span></div><div class=\"line\">           System.out.println(<span class=\"string\">\"SIZE OF THE HDFS DIRECTORY : \"</span> + hdfs.getContentSummary(filenamePath).getSpaceConsumed());</div><div class=\"line\">          <span class=\"comment\">// 显示实际的输出，例如这里显示 1G</span></div><div class=\"line\">           System.out.println(<span class=\"string\">\"SIZE OF THE HDFS DIRECTORY : \"</span> + hdfs.getContentSummary(filenamePath).getLength());</div><div class=\"line\">       &#125; <span class=\"keyword\">catch</span> (IOException e) &#123;</div><div class=\"line\">           e.printStackTrace();</div><div class=\"line\">       &#125;</div><div class=\"line\"></div><div class=\"line\">   &#125;</div></pre></td></tr></table></figure>"},{"title":"HDFS 指令（一）version，mkdir，ls，put，copyFromLocal，get，copyToLocal，cat，mv，cp","date":"2016-05-17T15:13:21.000Z","author":"kaishun","id":"3","_content":"# **1. 简介**  \n本文主要简单介绍一些常用的hdfs，包括复制文件，修改文件权限，查看文件内容，改变文件的所有权，创建目录，等HDFS文件操作  \n所有的Hadoop文件系统shell命令是由在bin / HDFS脚本调用  \n![hdfs-commands](http://or49tneld.bkt.clouddn.com/18-1-23/63075597.jpg)\n<!-- more -->\n# **2. version  查看版本**\n**指令用法**\n```\nhdfs version\n```\n例子\n```\n[hadoop@master ~]$ hdfs version\n```\n输出hadoop版本号  Hadoop 2.7.1\n\n# **3. mkdir 创建目录**\n**指令用法**\n```\nmkdir <path>\n```\n例子 , 创建testdir目录\n```\n[hadoop@master ~]$ hdfs dfs -mkdir /testdir\n```\n\n\n# **4. ls  查看目录文件**\n能显示指定路径的文件或文件夹列表，显示每个条目 的名称，权限，拥有者，大小和修改日期，路径。  \n\n**指令用法**\n```\nls <path>\n\n```\n例子，查看 /test目录 (我之前创建了test目录，并且放了一个文件在里面)\n```\n[hadoop@master ~]$ hdfs dfs -ls  /test\n```\n输出\n```\nFound 1 items\n-rw-r--r--   2 hadoop supergroup         14 2017-04-03 14:45 /test/slaves\n```\n# **5. put 复制文件到hdfs**  \n可以复制本地文件到hdfs，也可以复制hdfs上的文件，到hdfs的另外一个地方\n**指令用法**\n```\nput <localSrc> <dest>\n\n```\n例子，把hdfs.site文件放到 /test目录下\n```\n[hadoop@master hadoop]$ hdfs dfs -put /home/hadoop/MyCloudera/APP/hadoop/hadoop/etc/hadoop/hdfs-site.xml /test\n```\n\n\n# **6. copyFromLocal**  \n和put很像，但是指定了只能是从本地复制到hdfs  \n**指令用法**\n```\ncopyFromLocal <localSrc> <dest>\n```\n例子，把hadoop-env.sh复制到 /test中\n```\n[hadoop@master hadoop]$ hdfs dfs -copyFromLocal /home/hadoop/MyCloudera/APP/hadoop/hadoop/etc/hadoop/hadoop-env.sh  /test\n```\n\n# **7. get 复制hdfs文件到本地**  \n指令用法\n```\nget [-crc] <src> <localDest>\n\n```\n例如我将hdfs 上的文件拷贝到本地\n```\n[hadoop@master Downloads]$ hdfs dfs -get /test/slaves /home/hadoop/Downloads/\n```\n\n# **8. copyToLocal  复制hdfs文件到本地**\n类似于get命令，唯一的区别是，在这个目的路径被限制到本地\n\n**指令用法**\n```\ncopyToLocal <src> <localDest>\n\n```\n例子, 复制slaves到本地,Downloads目录\n```\n[hadoop@master Downloads]$ hdfs dfs -copyToLocal /test/slaves /home/hadoop/Downloads/\n```\n\n# **9. cat 查看文件内容**\n将文件的内容输出到console或者stdout控制台\n**指令用法**\n```\ncat <file-name>\n\n```\n例如，查看/test/slaves文件\n```\n[hadoop@master Downloads]$ hdfs dfs -cat /test/slaves\n```\n控制台打印\n```\nmaster\nnode01\n```\n\n# **10. mv 移动**  \n类似于linux的mv命令，移动hdfs中的一个地方到另外一个地方\n**指定用法**\n```\nmv <src> <dest>\n\n```\n把/test/slaves 复制到 /testdir \n```\n[hadoop@master Downloads]$ hdfs dfs -mv /test/slaves /testdir \n```\n\n# **11. copy复制**\n类似于linux中的cp方法，复制hdfs中的一个文件到另外一个文件目录\n\n**指令用法**\n```\ncp <src> <dest>\n```\n例子\n```\n[hadoop@master Downloads]$ hdfs dfs -cp /testdir/slaves /test\n```  \n翻译原文：http://data-flair.training/blogs/top-hadoop-hdfs-commands-tutorial/\n","source":"_posts/HDFS 指令（一） version，mkdir，ls，put，copyFromLocal，get，copyToLocal，cat，mv，cp.md","raw":"---\ntitle: HDFS 指令（一）version，mkdir，ls，put，copyFromLocal，get，copyToLocal，cat，mv，cp\ndate: 2016-05-17 23:13:21\ntags: [hdfs]\ncategories: [大数据,hdfs]\nauthor: kaishun\nid: 3\npermalink: hdfs-operator-1\n---\n# **1. 简介**  \n本文主要简单介绍一些常用的hdfs，包括复制文件，修改文件权限，查看文件内容，改变文件的所有权，创建目录，等HDFS文件操作  \n所有的Hadoop文件系统shell命令是由在bin / HDFS脚本调用  \n![hdfs-commands](http://or49tneld.bkt.clouddn.com/18-1-23/63075597.jpg)\n<!-- more -->\n# **2. version  查看版本**\n**指令用法**\n```\nhdfs version\n```\n例子\n```\n[hadoop@master ~]$ hdfs version\n```\n输出hadoop版本号  Hadoop 2.7.1\n\n# **3. mkdir 创建目录**\n**指令用法**\n```\nmkdir <path>\n```\n例子 , 创建testdir目录\n```\n[hadoop@master ~]$ hdfs dfs -mkdir /testdir\n```\n\n\n# **4. ls  查看目录文件**\n能显示指定路径的文件或文件夹列表，显示每个条目 的名称，权限，拥有者，大小和修改日期，路径。  \n\n**指令用法**\n```\nls <path>\n\n```\n例子，查看 /test目录 (我之前创建了test目录，并且放了一个文件在里面)\n```\n[hadoop@master ~]$ hdfs dfs -ls  /test\n```\n输出\n```\nFound 1 items\n-rw-r--r--   2 hadoop supergroup         14 2017-04-03 14:45 /test/slaves\n```\n# **5. put 复制文件到hdfs**  \n可以复制本地文件到hdfs，也可以复制hdfs上的文件，到hdfs的另外一个地方\n**指令用法**\n```\nput <localSrc> <dest>\n\n```\n例子，把hdfs.site文件放到 /test目录下\n```\n[hadoop@master hadoop]$ hdfs dfs -put /home/hadoop/MyCloudera/APP/hadoop/hadoop/etc/hadoop/hdfs-site.xml /test\n```\n\n\n# **6. copyFromLocal**  \n和put很像，但是指定了只能是从本地复制到hdfs  \n**指令用法**\n```\ncopyFromLocal <localSrc> <dest>\n```\n例子，把hadoop-env.sh复制到 /test中\n```\n[hadoop@master hadoop]$ hdfs dfs -copyFromLocal /home/hadoop/MyCloudera/APP/hadoop/hadoop/etc/hadoop/hadoop-env.sh  /test\n```\n\n# **7. get 复制hdfs文件到本地**  \n指令用法\n```\nget [-crc] <src> <localDest>\n\n```\n例如我将hdfs 上的文件拷贝到本地\n```\n[hadoop@master Downloads]$ hdfs dfs -get /test/slaves /home/hadoop/Downloads/\n```\n\n# **8. copyToLocal  复制hdfs文件到本地**\n类似于get命令，唯一的区别是，在这个目的路径被限制到本地\n\n**指令用法**\n```\ncopyToLocal <src> <localDest>\n\n```\n例子, 复制slaves到本地,Downloads目录\n```\n[hadoop@master Downloads]$ hdfs dfs -copyToLocal /test/slaves /home/hadoop/Downloads/\n```\n\n# **9. cat 查看文件内容**\n将文件的内容输出到console或者stdout控制台\n**指令用法**\n```\ncat <file-name>\n\n```\n例如，查看/test/slaves文件\n```\n[hadoop@master Downloads]$ hdfs dfs -cat /test/slaves\n```\n控制台打印\n```\nmaster\nnode01\n```\n\n# **10. mv 移动**  \n类似于linux的mv命令，移动hdfs中的一个地方到另外一个地方\n**指定用法**\n```\nmv <src> <dest>\n\n```\n把/test/slaves 复制到 /testdir \n```\n[hadoop@master Downloads]$ hdfs dfs -mv /test/slaves /testdir \n```\n\n# **11. copy复制**\n类似于linux中的cp方法，复制hdfs中的一个文件到另外一个文件目录\n\n**指令用法**\n```\ncp <src> <dest>\n```\n例子\n```\n[hadoop@master Downloads]$ hdfs dfs -cp /testdir/slaves /test\n```  \n翻译原文：http://data-flair.training/blogs/top-hadoop-hdfs-commands-tutorial/\n","slug":"hdfs-operator-1","published":1,"updated":"2018-01-23T14:35:45.119Z","_id":"cjcrpnzo3000e2wv3uwcepjae","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"1-简介\"><a href=\"#1-简介\" class=\"headerlink\" title=\"1. 简介\"></a><strong>1. 简介</strong></h1><p>本文主要简单介绍一些常用的hdfs，包括复制文件，修改文件权限，查看文件内容，改变文件的所有权，创建目录，等HDFS文件操作<br>所有的Hadoop文件系统shell命令是由在bin / HDFS脚本调用<br><img src=\"http://or49tneld.bkt.clouddn.com/18-1-23/63075597.jpg\" alt=\"hdfs-commands\"><br><a id=\"more\"></a></p>\n<h1 id=\"2-version-查看版本\"><a href=\"#2-version-查看版本\" class=\"headerlink\" title=\"2. version  查看版本\"></a><strong>2. version  查看版本</strong></h1><p><strong>指令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs version</div></pre></td></tr></table></figure></p>\n<p>例子<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">[hadoop@master ~]$ hdfs version</div></pre></td></tr></table></figure></p>\n<p>输出hadoop版本号  Hadoop 2.7.1</p>\n<h1 id=\"3-mkdir-创建目录\"><a href=\"#3-mkdir-创建目录\" class=\"headerlink\" title=\"3. mkdir 创建目录\"></a><strong>3. mkdir 创建目录</strong></h1><p><strong>指令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">mkdir &lt;path&gt;</div></pre></td></tr></table></figure></p>\n<p>例子 , 创建testdir目录<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">[hadoop@master ~]$ hdfs dfs -mkdir /testdir</div></pre></td></tr></table></figure></p>\n<h1 id=\"4-ls-查看目录文件\"><a href=\"#4-ls-查看目录文件\" class=\"headerlink\" title=\"4. ls  查看目录文件\"></a><strong>4. ls  查看目录文件</strong></h1><p>能显示指定路径的文件或文件夹列表，显示每个条目 的名称，权限，拥有者，大小和修改日期，路径。  </p>\n<p><strong>指令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">ls &lt;path&gt;</div></pre></td></tr></table></figure></p>\n<p>例子，查看 /test目录 (我之前创建了test目录，并且放了一个文件在里面)<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">[hadoop@master ~]$ hdfs dfs -ls  /test</div></pre></td></tr></table></figure></p>\n<p>输出<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">Found 1 items</div><div class=\"line\">-rw-r--r--   2 hadoop supergroup         14 2017-04-03 14:45 /test/slaves</div></pre></td></tr></table></figure></p>\n<h1 id=\"5-put-复制文件到hdfs\"><a href=\"#5-put-复制文件到hdfs\" class=\"headerlink\" title=\"5. put 复制文件到hdfs\"></a><strong>5. put 复制文件到hdfs</strong></h1><p>可以复制本地文件到hdfs，也可以复制hdfs上的文件，到hdfs的另外一个地方<br><strong>指令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">put &lt;localSrc&gt; &lt;dest&gt;</div></pre></td></tr></table></figure></p>\n<p>例子，把hdfs.site文件放到 /test目录下<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">[hadoop@master hadoop]$ hdfs dfs -put /home/hadoop/MyCloudera/APP/hadoop/hadoop/etc/hadoop/hdfs-site.xml /test</div></pre></td></tr></table></figure></p>\n<h1 id=\"6-copyFromLocal\"><a href=\"#6-copyFromLocal\" class=\"headerlink\" title=\"6. copyFromLocal\"></a><strong>6. copyFromLocal</strong></h1><p>和put很像，但是指定了只能是从本地复制到hdfs<br><strong>指令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">copyFromLocal &lt;localSrc&gt; &lt;dest&gt;</div></pre></td></tr></table></figure></p>\n<p>例子，把hadoop-env.sh复制到 /test中<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">[hadoop@master hadoop]$ hdfs dfs -copyFromLocal /home/hadoop/MyCloudera/APP/hadoop/hadoop/etc/hadoop/hadoop-env.sh  /test</div></pre></td></tr></table></figure></p>\n<h1 id=\"7-get-复制hdfs文件到本地\"><a href=\"#7-get-复制hdfs文件到本地\" class=\"headerlink\" title=\"7. get 复制hdfs文件到本地\"></a><strong>7. get 复制hdfs文件到本地</strong></h1><p>指令用法<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">get [-crc] &lt;src&gt; &lt;localDest&gt;</div></pre></td></tr></table></figure></p>\n<p>例如我将hdfs 上的文件拷贝到本地<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">[hadoop@master Downloads]$ hdfs dfs -get /test/slaves /home/hadoop/Downloads/</div></pre></td></tr></table></figure></p>\n<h1 id=\"8-copyToLocal-复制hdfs文件到本地\"><a href=\"#8-copyToLocal-复制hdfs文件到本地\" class=\"headerlink\" title=\"8. copyToLocal  复制hdfs文件到本地\"></a><strong>8. copyToLocal  复制hdfs文件到本地</strong></h1><p>类似于get命令，唯一的区别是，在这个目的路径被限制到本地</p>\n<p><strong>指令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">copyToLocal &lt;src&gt; &lt;localDest&gt;</div></pre></td></tr></table></figure></p>\n<p>例子, 复制slaves到本地,Downloads目录<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">[hadoop@master Downloads]$ hdfs dfs -copyToLocal /test/slaves /home/hadoop/Downloads/</div></pre></td></tr></table></figure></p>\n<h1 id=\"9-cat-查看文件内容\"><a href=\"#9-cat-查看文件内容\" class=\"headerlink\" title=\"9. cat 查看文件内容\"></a><strong>9. cat 查看文件内容</strong></h1><p>将文件的内容输出到console或者stdout控制台<br><strong>指令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">cat &lt;file-name&gt;</div></pre></td></tr></table></figure></p>\n<p>例如，查看/test/slaves文件<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">[hadoop@master Downloads]$ hdfs dfs -cat /test/slaves</div></pre></td></tr></table></figure></p>\n<p>控制台打印<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">master</div><div class=\"line\">node01</div></pre></td></tr></table></figure></p>\n<h1 id=\"10-mv-移动\"><a href=\"#10-mv-移动\" class=\"headerlink\" title=\"10. mv 移动\"></a><strong>10. mv 移动</strong></h1><p>类似于linux的mv命令，移动hdfs中的一个地方到另外一个地方<br><strong>指定用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">mv &lt;src&gt; &lt;dest&gt;</div></pre></td></tr></table></figure></p>\n<p>把/test/slaves 复制到 /testdir<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">[hadoop@master Downloads]$ hdfs dfs -mv /test/slaves /testdir</div></pre></td></tr></table></figure></p>\n<h1 id=\"11-copy复制\"><a href=\"#11-copy复制\" class=\"headerlink\" title=\"11. copy复制\"></a><strong>11. copy复制</strong></h1><p>类似于linux中的cp方法，复制hdfs中的一个文件到另外一个文件目录</p>\n<p><strong>指令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">cp &lt;src&gt; &lt;dest&gt;</div></pre></td></tr></table></figure></p>\n<p>例子</p>\n<pre><code>[hadoop@master Downloads]$ hdfs dfs -cp /testdir/slaves /test\n</code></pre><p>翻译原文：<a href=\"http://data-flair.training/blogs/top-hadoop-hdfs-commands-tutorial/\" target=\"_blank\" rel=\"external\">http://data-flair.training/blogs/top-hadoop-hdfs-commands-tutorial/</a></p>\n","site":{"data":{}},"excerpt":"<h1 id=\"1-简介\"><a href=\"#1-简介\" class=\"headerlink\" title=\"1. 简介\"></a><strong>1. 简介</strong></h1><p>本文主要简单介绍一些常用的hdfs，包括复制文件，修改文件权限，查看文件内容，改变文件的所有权，创建目录，等HDFS文件操作<br>所有的Hadoop文件系统shell命令是由在bin / HDFS脚本调用<br><img src=\"http://or49tneld.bkt.clouddn.com/18-1-23/63075597.jpg\" alt=\"hdfs-commands\"><br>","more":"</p>\n<h1 id=\"2-version-查看版本\"><a href=\"#2-version-查看版本\" class=\"headerlink\" title=\"2. version  查看版本\"></a><strong>2. version  查看版本</strong></h1><p><strong>指令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs version</div></pre></td></tr></table></figure></p>\n<p>例子<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">[hadoop@master ~]$ hdfs version</div></pre></td></tr></table></figure></p>\n<p>输出hadoop版本号  Hadoop 2.7.1</p>\n<h1 id=\"3-mkdir-创建目录\"><a href=\"#3-mkdir-创建目录\" class=\"headerlink\" title=\"3. mkdir 创建目录\"></a><strong>3. mkdir 创建目录</strong></h1><p><strong>指令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">mkdir &lt;path&gt;</div></pre></td></tr></table></figure></p>\n<p>例子 , 创建testdir目录<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">[hadoop@master ~]$ hdfs dfs -mkdir /testdir</div></pre></td></tr></table></figure></p>\n<h1 id=\"4-ls-查看目录文件\"><a href=\"#4-ls-查看目录文件\" class=\"headerlink\" title=\"4. ls  查看目录文件\"></a><strong>4. ls  查看目录文件</strong></h1><p>能显示指定路径的文件或文件夹列表，显示每个条目 的名称，权限，拥有者，大小和修改日期，路径。  </p>\n<p><strong>指令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">ls &lt;path&gt;</div></pre></td></tr></table></figure></p>\n<p>例子，查看 /test目录 (我之前创建了test目录，并且放了一个文件在里面)<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">[hadoop@master ~]$ hdfs dfs -ls  /test</div></pre></td></tr></table></figure></p>\n<p>输出<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">Found 1 items</div><div class=\"line\">-rw-r--r--   2 hadoop supergroup         14 2017-04-03 14:45 /test/slaves</div></pre></td></tr></table></figure></p>\n<h1 id=\"5-put-复制文件到hdfs\"><a href=\"#5-put-复制文件到hdfs\" class=\"headerlink\" title=\"5. put 复制文件到hdfs\"></a><strong>5. put 复制文件到hdfs</strong></h1><p>可以复制本地文件到hdfs，也可以复制hdfs上的文件，到hdfs的另外一个地方<br><strong>指令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">put &lt;localSrc&gt; &lt;dest&gt;</div></pre></td></tr></table></figure></p>\n<p>例子，把hdfs.site文件放到 /test目录下<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">[hadoop@master hadoop]$ hdfs dfs -put /home/hadoop/MyCloudera/APP/hadoop/hadoop/etc/hadoop/hdfs-site.xml /test</div></pre></td></tr></table></figure></p>\n<h1 id=\"6-copyFromLocal\"><a href=\"#6-copyFromLocal\" class=\"headerlink\" title=\"6. copyFromLocal\"></a><strong>6. copyFromLocal</strong></h1><p>和put很像，但是指定了只能是从本地复制到hdfs<br><strong>指令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">copyFromLocal &lt;localSrc&gt; &lt;dest&gt;</div></pre></td></tr></table></figure></p>\n<p>例子，把hadoop-env.sh复制到 /test中<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">[hadoop@master hadoop]$ hdfs dfs -copyFromLocal /home/hadoop/MyCloudera/APP/hadoop/hadoop/etc/hadoop/hadoop-env.sh  /test</div></pre></td></tr></table></figure></p>\n<h1 id=\"7-get-复制hdfs文件到本地\"><a href=\"#7-get-复制hdfs文件到本地\" class=\"headerlink\" title=\"7. get 复制hdfs文件到本地\"></a><strong>7. get 复制hdfs文件到本地</strong></h1><p>指令用法<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">get [-crc] &lt;src&gt; &lt;localDest&gt;</div></pre></td></tr></table></figure></p>\n<p>例如我将hdfs 上的文件拷贝到本地<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">[hadoop@master Downloads]$ hdfs dfs -get /test/slaves /home/hadoop/Downloads/</div></pre></td></tr></table></figure></p>\n<h1 id=\"8-copyToLocal-复制hdfs文件到本地\"><a href=\"#8-copyToLocal-复制hdfs文件到本地\" class=\"headerlink\" title=\"8. copyToLocal  复制hdfs文件到本地\"></a><strong>8. copyToLocal  复制hdfs文件到本地</strong></h1><p>类似于get命令，唯一的区别是，在这个目的路径被限制到本地</p>\n<p><strong>指令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">copyToLocal &lt;src&gt; &lt;localDest&gt;</div></pre></td></tr></table></figure></p>\n<p>例子, 复制slaves到本地,Downloads目录<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">[hadoop@master Downloads]$ hdfs dfs -copyToLocal /test/slaves /home/hadoop/Downloads/</div></pre></td></tr></table></figure></p>\n<h1 id=\"9-cat-查看文件内容\"><a href=\"#9-cat-查看文件内容\" class=\"headerlink\" title=\"9. cat 查看文件内容\"></a><strong>9. cat 查看文件内容</strong></h1><p>将文件的内容输出到console或者stdout控制台<br><strong>指令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">cat &lt;file-name&gt;</div></pre></td></tr></table></figure></p>\n<p>例如，查看/test/slaves文件<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">[hadoop@master Downloads]$ hdfs dfs -cat /test/slaves</div></pre></td></tr></table></figure></p>\n<p>控制台打印<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">master</div><div class=\"line\">node01</div></pre></td></tr></table></figure></p>\n<h1 id=\"10-mv-移动\"><a href=\"#10-mv-移动\" class=\"headerlink\" title=\"10. mv 移动\"></a><strong>10. mv 移动</strong></h1><p>类似于linux的mv命令，移动hdfs中的一个地方到另外一个地方<br><strong>指定用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">mv &lt;src&gt; &lt;dest&gt;</div></pre></td></tr></table></figure></p>\n<p>把/test/slaves 复制到 /testdir<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">[hadoop@master Downloads]$ hdfs dfs -mv /test/slaves /testdir</div></pre></td></tr></table></figure></p>\n<h1 id=\"11-copy复制\"><a href=\"#11-copy复制\" class=\"headerlink\" title=\"11. copy复制\"></a><strong>11. copy复制</strong></h1><p>类似于linux中的cp方法，复制hdfs中的一个文件到另外一个文件目录</p>\n<p><strong>指令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">cp &lt;src&gt; &lt;dest&gt;</div></pre></td></tr></table></figure></p>\n<p>例子</p>\n<pre><code>[hadoop@master Downloads]$ hdfs dfs -cp /testdir/slaves /test\n</code></pre><p>翻译原文：<a href=\"http://data-flair.training/blogs/top-hadoop-hdfs-commands-tutorial/\" target=\"_blank\" rel=\"external\">http://data-flair.training/blogs/top-hadoop-hdfs-commands-tutorial/</a></p>"},{"title":"HDFS 指令（二）moveFromLocal，moveToLocal，tail，rm，expunge，chown，chgrp，setrep，du，df","date":"2016-05-17T15:13:21.000Z","author":"kaishun","id":"6","blogexcerpt":"本文主要学习hadoop hdfs, 从hdfs移动到本地，从本地移动到hdfs，tail查看最后，rm删除文件，expunge清空 trash,chown, 改变拥有者，setrep 改变文件副本数，chgrp改变所属组，，du,df磁盘占用情况","_content":"\n# **目的**  \n本文主要学习hadoop hdfs 从hdfs移动到本地，从本地移动到hdfs，tail查看最后，rm删除文件，expunge清空 trash,chown 改变拥有者，setrep 改变文件副本数，chgrp改变所属组，，du,df磁盘占用情况\n\n![hdfs-commands](http://or49tneld.bkt.clouddn.com/18-1-23/63075597.jpg)\n\n# **moveFromLocal**  \n复制一份本地文件到hdfs，当成功后，删除本地文件\n**指令用法**\n```\nmoveFromLocal <localSrc> <dest>\n```\n**例子**\n```\nhdfs dfs -moveFromLocal /home/dataflair/Desktop/sample /user/dataflair/dir1\n```\n\n# **moveToLocal**  \n类似于-get，但是当复制完成后，会删除hdfs上的文件\n**指令用法**\n```\nmoveToLocal <src> <localDest>\n```\n**例子**\n```\nhdfs dfs -moveToLocal /user/dataflair/dir2/sample /user/dataflair/Desktop\n```\n\n# **tail**  \n类似于linux中的tail，把文件最后1kb给打印到console 或者 stdout.\n**指令用法**\n```\nhdfs dfs -tail [-f] <filename>\n```\n**例子**\n```\nhdfs dfs -tail /user/dataflair/dir2/purchases.txt\nhdfs dfs -tail -f /user/dataflair/dir2/purchases.txt\n```\n\n#  **rm**  \n移除文件或者清空路径下的数据\n**指令用法**  \n\n```\nrm <path>\n```\n**例子**\n\n```java\nhdfs dfs -rm /user/dataflair/dir2/sample\n```\n**递归删除**\n\n```\nhdfs dfs -rm -r /user/dataflair/dir2\n\n```\n\n#   **expunge**  \n清空 trash  \nHDFS中的数据删除也是比较有特点的，并不是直接删除，而是先放在一个类似回收站的地方（/trash），可供恢复  \n对于用户或者应用程序想要删除的文件，HDFS会将它重命名并移动到/trash中  ，当被覆盖或者到了一定的生命周期（现在默认为6小时），hdfs才会从文件中删除，并且只有这个时候，Datanode上相关的磁盘空间才能节省出  \n而 expunge 就类会清空/trash，类似于清空回收站\n**指令用法**\n```\nhdfs dfs -expunge\n```\n**例子**\n```\nhdfs dfs -expunge\n\n```\n\n#   **chown**  \n类似于linux中的chown，用户应该是超级用户才能做次操作  \n改变文件拥有者\n**指令用法**\n```\nhdfs dfs -chown [-R] [OWNER][:[GROUP]] URI [URI ]\n```\n**例子**\n```\nhdfs dfs -chown -R dataflair /opt/hadoop/logs\n```\n\n#   **chgrp**  \n类似于linux中的chgrp，用户应该是超级用户才能做次操作  \n改变文件所有组\n**指令用法**\n```\nhdfs dfs -chgrp [-R] <NewGroupName> <file or directory name>\n```\n**例子**\n```\nhdfs dfs -chgrp [-R] New Group sample\n```\n\n#  **setrep** \n**指令用法** \n用来改变文件的副本数，如果是文件夹，那么次命令会针对该文件夹下的所有文件都会改变副本数  \n-w 表示副本数改为多少\n使用-R选项可以对一个目录下的所有目录+文件递归执行改变副本个数的操作  \n```\nsetrep [-R] [-w] rep <path>\n```\n**例子**\n```\nhdfs dfs -setrep -w 3 /user/dataflair/dir1\n\n```\n\n#  **du** \n类似于linux中的du ， 统计个目录下各个文件大小，可以加-h 提高文件可读性\n**指令用法**\n```\ndu <path>\n```\n**例子**\n```\nhdfs dfs -du /user/dataflair/dir1/sample\n```\n\n#  **df** \n类似于linux中的du ，查询某个目录的空间大小，可以加-h 提高文件可读性\n**指令用法**\n```\nhdfs dfs -df [-h] URI [URI ...]\n```\n**例子**\n```\nhdfs dfs -df -h\n```\n\n翻译原文：http://data-flair.training/blogs/most-used-hdfs-commands-tutorial-examples/","source":"_posts/HDFS 指令（二）moveFromLocal，moveToLocal，tail，rm，expunge，chown，chgrp，setrep，du，df.md","raw":"---\ntitle: HDFS 指令（二）moveFromLocal，moveToLocal，tail，rm，expunge，chown，chgrp，setrep，du，df\ndate: 2016-05-17 23:13:21\ntags: [hdfs]\ncategories: [大数据,hdfs]\nauthor: kaishun\nid: 6\npermalink: hdfs-operator-2\nblogexcerpt: 本文主要学习hadoop hdfs, 从hdfs移动到本地，从本地移动到hdfs，tail查看最后，rm删除文件，expunge清空 trash,chown, 改变拥有者，setrep 改变文件副本数，chgrp改变所属组，，du,df磁盘占用情况\n---\n\n# **目的**  \n本文主要学习hadoop hdfs 从hdfs移动到本地，从本地移动到hdfs，tail查看最后，rm删除文件，expunge清空 trash,chown 改变拥有者，setrep 改变文件副本数，chgrp改变所属组，，du,df磁盘占用情况\n\n![hdfs-commands](http://or49tneld.bkt.clouddn.com/18-1-23/63075597.jpg)\n\n# **moveFromLocal**  \n复制一份本地文件到hdfs，当成功后，删除本地文件\n**指令用法**\n```\nmoveFromLocal <localSrc> <dest>\n```\n**例子**\n```\nhdfs dfs -moveFromLocal /home/dataflair/Desktop/sample /user/dataflair/dir1\n```\n\n# **moveToLocal**  \n类似于-get，但是当复制完成后，会删除hdfs上的文件\n**指令用法**\n```\nmoveToLocal <src> <localDest>\n```\n**例子**\n```\nhdfs dfs -moveToLocal /user/dataflair/dir2/sample /user/dataflair/Desktop\n```\n\n# **tail**  \n类似于linux中的tail，把文件最后1kb给打印到console 或者 stdout.\n**指令用法**\n```\nhdfs dfs -tail [-f] <filename>\n```\n**例子**\n```\nhdfs dfs -tail /user/dataflair/dir2/purchases.txt\nhdfs dfs -tail -f /user/dataflair/dir2/purchases.txt\n```\n\n#  **rm**  \n移除文件或者清空路径下的数据\n**指令用法**  \n\n```\nrm <path>\n```\n**例子**\n\n```java\nhdfs dfs -rm /user/dataflair/dir2/sample\n```\n**递归删除**\n\n```\nhdfs dfs -rm -r /user/dataflair/dir2\n\n```\n\n#   **expunge**  \n清空 trash  \nHDFS中的数据删除也是比较有特点的，并不是直接删除，而是先放在一个类似回收站的地方（/trash），可供恢复  \n对于用户或者应用程序想要删除的文件，HDFS会将它重命名并移动到/trash中  ，当被覆盖或者到了一定的生命周期（现在默认为6小时），hdfs才会从文件中删除，并且只有这个时候，Datanode上相关的磁盘空间才能节省出  \n而 expunge 就类会清空/trash，类似于清空回收站\n**指令用法**\n```\nhdfs dfs -expunge\n```\n**例子**\n```\nhdfs dfs -expunge\n\n```\n\n#   **chown**  \n类似于linux中的chown，用户应该是超级用户才能做次操作  \n改变文件拥有者\n**指令用法**\n```\nhdfs dfs -chown [-R] [OWNER][:[GROUP]] URI [URI ]\n```\n**例子**\n```\nhdfs dfs -chown -R dataflair /opt/hadoop/logs\n```\n\n#   **chgrp**  \n类似于linux中的chgrp，用户应该是超级用户才能做次操作  \n改变文件所有组\n**指令用法**\n```\nhdfs dfs -chgrp [-R] <NewGroupName> <file or directory name>\n```\n**例子**\n```\nhdfs dfs -chgrp [-R] New Group sample\n```\n\n#  **setrep** \n**指令用法** \n用来改变文件的副本数，如果是文件夹，那么次命令会针对该文件夹下的所有文件都会改变副本数  \n-w 表示副本数改为多少\n使用-R选项可以对一个目录下的所有目录+文件递归执行改变副本个数的操作  \n```\nsetrep [-R] [-w] rep <path>\n```\n**例子**\n```\nhdfs dfs -setrep -w 3 /user/dataflair/dir1\n\n```\n\n#  **du** \n类似于linux中的du ， 统计个目录下各个文件大小，可以加-h 提高文件可读性\n**指令用法**\n```\ndu <path>\n```\n**例子**\n```\nhdfs dfs -du /user/dataflair/dir1/sample\n```\n\n#  **df** \n类似于linux中的du ，查询某个目录的空间大小，可以加-h 提高文件可读性\n**指令用法**\n```\nhdfs dfs -df [-h] URI [URI ...]\n```\n**例子**\n```\nhdfs dfs -df -h\n```\n\n翻译原文：http://data-flair.training/blogs/most-used-hdfs-commands-tutorial-examples/","slug":"hdfs-operator-2","published":1,"updated":"2018-01-23T14:36:03.384Z","_id":"cjcrpnzo3000g2wv3h7fg1qlx","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"目的\"><a href=\"#目的\" class=\"headerlink\" title=\"目的\"></a><strong>目的</strong></h1><p>本文主要学习hadoop hdfs 从hdfs移动到本地，从本地移动到hdfs，tail查看最后，rm删除文件，expunge清空 trash,chown 改变拥有者，setrep 改变文件副本数，chgrp改变所属组，，du,df磁盘占用情况</p>\n<p><img src=\"http://or49tneld.bkt.clouddn.com/18-1-23/63075597.jpg\" alt=\"hdfs-commands\"></p>\n<h1 id=\"moveFromLocal\"><a href=\"#moveFromLocal\" class=\"headerlink\" title=\"moveFromLocal\"></a><strong>moveFromLocal</strong></h1><p>复制一份本地文件到hdfs，当成功后，删除本地文件<br><strong>指令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">moveFromLocal &lt;localSrc&gt; &lt;dest&gt;</div></pre></td></tr></table></figure></p>\n<p><strong>例子</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -moveFromLocal /home/dataflair/Desktop/sample /user/dataflair/dir1</div></pre></td></tr></table></figure></p>\n<h1 id=\"moveToLocal\"><a href=\"#moveToLocal\" class=\"headerlink\" title=\"moveToLocal\"></a><strong>moveToLocal</strong></h1><p>类似于-get，但是当复制完成后，会删除hdfs上的文件<br><strong>指令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">moveToLocal &lt;src&gt; &lt;localDest&gt;</div></pre></td></tr></table></figure></p>\n<p><strong>例子</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -moveToLocal /user/dataflair/dir2/sample /user/dataflair/Desktop</div></pre></td></tr></table></figure></p>\n<h1 id=\"tail\"><a href=\"#tail\" class=\"headerlink\" title=\"tail\"></a><strong>tail</strong></h1><p>类似于linux中的tail，把文件最后1kb给打印到console 或者 stdout.<br><strong>指令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -tail [-f] &lt;filename&gt;</div></pre></td></tr></table></figure></p>\n<p><strong>例子</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -tail /user/dataflair/dir2/purchases.txt</div><div class=\"line\">hdfs dfs -tail -f /user/dataflair/dir2/purchases.txt</div></pre></td></tr></table></figure></p>\n<h1 id=\"rm\"><a href=\"#rm\" class=\"headerlink\" title=\"rm\"></a><strong>rm</strong></h1><p>移除文件或者清空路径下的数据<br><strong>指令用法</strong>  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">rm &lt;path&gt;</div></pre></td></tr></table></figure>\n<p><strong>例子</strong></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -rm /user/dataflair/dir2/sample</div></pre></td></tr></table></figure>\n<p><strong>递归删除</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -rm -r /user/dataflair/dir2</div></pre></td></tr></table></figure>\n<h1 id=\"expunge\"><a href=\"#expunge\" class=\"headerlink\" title=\"expunge\"></a><strong>expunge</strong></h1><p>清空 trash<br>HDFS中的数据删除也是比较有特点的，并不是直接删除，而是先放在一个类似回收站的地方（/trash），可供恢复<br>对于用户或者应用程序想要删除的文件，HDFS会将它重命名并移动到/trash中  ，当被覆盖或者到了一定的生命周期（现在默认为6小时），hdfs才会从文件中删除，并且只有这个时候，Datanode上相关的磁盘空间才能节省出<br>而 expunge 就类会清空/trash，类似于清空回收站<br><strong>指令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -expunge</div></pre></td></tr></table></figure></p>\n<p><strong>例子</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -expunge</div></pre></td></tr></table></figure></p>\n<h1 id=\"chown\"><a href=\"#chown\" class=\"headerlink\" title=\"chown\"></a><strong>chown</strong></h1><p>类似于linux中的chown，用户应该是超级用户才能做次操作<br>改变文件拥有者<br><strong>指令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -chown [-R] [OWNER][:[GROUP]] URI [URI ]</div></pre></td></tr></table></figure></p>\n<p><strong>例子</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -chown -R dataflair /opt/hadoop/logs</div></pre></td></tr></table></figure></p>\n<h1 id=\"chgrp\"><a href=\"#chgrp\" class=\"headerlink\" title=\"chgrp\"></a><strong>chgrp</strong></h1><p>类似于linux中的chgrp，用户应该是超级用户才能做次操作<br>改变文件所有组<br><strong>指令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -chgrp [-R] &lt;NewGroupName&gt; &lt;file or directory name&gt;</div></pre></td></tr></table></figure></p>\n<p><strong>例子</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -chgrp [-R] New Group sample</div></pre></td></tr></table></figure></p>\n<h1 id=\"setrep\"><a href=\"#setrep\" class=\"headerlink\" title=\"setrep\"></a><strong>setrep</strong></h1><p><strong>指令用法</strong><br>用来改变文件的副本数，如果是文件夹，那么次命令会针对该文件夹下的所有文件都会改变副本数<br>-w 表示副本数改为多少<br>使用-R选项可以对一个目录下的所有目录+文件递归执行改变副本个数的操作<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">setrep [-R] [-w] rep &lt;path&gt;</div></pre></td></tr></table></figure></p>\n<p><strong>例子</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -setrep -w 3 /user/dataflair/dir1</div></pre></td></tr></table></figure></p>\n<h1 id=\"du\"><a href=\"#du\" class=\"headerlink\" title=\"du\"></a><strong>du</strong></h1><p>类似于linux中的du ， 统计个目录下各个文件大小，可以加-h 提高文件可读性<br><strong>指令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">du &lt;path&gt;</div></pre></td></tr></table></figure></p>\n<p><strong>例子</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -du /user/dataflair/dir1/sample</div></pre></td></tr></table></figure></p>\n<h1 id=\"df\"><a href=\"#df\" class=\"headerlink\" title=\"df\"></a><strong>df</strong></h1><p>类似于linux中的du ，查询某个目录的空间大小，可以加-h 提高文件可读性<br><strong>指令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -df [-h] URI [URI ...]</div></pre></td></tr></table></figure></p>\n<p><strong>例子</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -df -h</div></pre></td></tr></table></figure></p>\n<p>翻译原文：<a href=\"http://data-flair.training/blogs/most-used-hdfs-commands-tutorial-examples/\" target=\"_blank\" rel=\"external\">http://data-flair.training/blogs/most-used-hdfs-commands-tutorial-examples/</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"目的\"><a href=\"#目的\" class=\"headerlink\" title=\"目的\"></a><strong>目的</strong></h1><p>本文主要学习hadoop hdfs 从hdfs移动到本地，从本地移动到hdfs，tail查看最后，rm删除文件，expunge清空 trash,chown 改变拥有者，setrep 改变文件副本数，chgrp改变所属组，，du,df磁盘占用情况</p>\n<p><img src=\"http://or49tneld.bkt.clouddn.com/18-1-23/63075597.jpg\" alt=\"hdfs-commands\"></p>\n<h1 id=\"moveFromLocal\"><a href=\"#moveFromLocal\" class=\"headerlink\" title=\"moveFromLocal\"></a><strong>moveFromLocal</strong></h1><p>复制一份本地文件到hdfs，当成功后，删除本地文件<br><strong>指令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">moveFromLocal &lt;localSrc&gt; &lt;dest&gt;</div></pre></td></tr></table></figure></p>\n<p><strong>例子</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -moveFromLocal /home/dataflair/Desktop/sample /user/dataflair/dir1</div></pre></td></tr></table></figure></p>\n<h1 id=\"moveToLocal\"><a href=\"#moveToLocal\" class=\"headerlink\" title=\"moveToLocal\"></a><strong>moveToLocal</strong></h1><p>类似于-get，但是当复制完成后，会删除hdfs上的文件<br><strong>指令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">moveToLocal &lt;src&gt; &lt;localDest&gt;</div></pre></td></tr></table></figure></p>\n<p><strong>例子</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -moveToLocal /user/dataflair/dir2/sample /user/dataflair/Desktop</div></pre></td></tr></table></figure></p>\n<h1 id=\"tail\"><a href=\"#tail\" class=\"headerlink\" title=\"tail\"></a><strong>tail</strong></h1><p>类似于linux中的tail，把文件最后1kb给打印到console 或者 stdout.<br><strong>指令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -tail [-f] &lt;filename&gt;</div></pre></td></tr></table></figure></p>\n<p><strong>例子</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -tail /user/dataflair/dir2/purchases.txt</div><div class=\"line\">hdfs dfs -tail -f /user/dataflair/dir2/purchases.txt</div></pre></td></tr></table></figure></p>\n<h1 id=\"rm\"><a href=\"#rm\" class=\"headerlink\" title=\"rm\"></a><strong>rm</strong></h1><p>移除文件或者清空路径下的数据<br><strong>指令用法</strong>  </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">rm &lt;path&gt;</div></pre></td></tr></table></figure>\n<p><strong>例子</strong></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -rm /user/dataflair/dir2/sample</div></pre></td></tr></table></figure>\n<p><strong>递归删除</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -rm -r /user/dataflair/dir2</div></pre></td></tr></table></figure>\n<h1 id=\"expunge\"><a href=\"#expunge\" class=\"headerlink\" title=\"expunge\"></a><strong>expunge</strong></h1><p>清空 trash<br>HDFS中的数据删除也是比较有特点的，并不是直接删除，而是先放在一个类似回收站的地方（/trash），可供恢复<br>对于用户或者应用程序想要删除的文件，HDFS会将它重命名并移动到/trash中  ，当被覆盖或者到了一定的生命周期（现在默认为6小时），hdfs才会从文件中删除，并且只有这个时候，Datanode上相关的磁盘空间才能节省出<br>而 expunge 就类会清空/trash，类似于清空回收站<br><strong>指令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -expunge</div></pre></td></tr></table></figure></p>\n<p><strong>例子</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -expunge</div></pre></td></tr></table></figure></p>\n<h1 id=\"chown\"><a href=\"#chown\" class=\"headerlink\" title=\"chown\"></a><strong>chown</strong></h1><p>类似于linux中的chown，用户应该是超级用户才能做次操作<br>改变文件拥有者<br><strong>指令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -chown [-R] [OWNER][:[GROUP]] URI [URI ]</div></pre></td></tr></table></figure></p>\n<p><strong>例子</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -chown -R dataflair /opt/hadoop/logs</div></pre></td></tr></table></figure></p>\n<h1 id=\"chgrp\"><a href=\"#chgrp\" class=\"headerlink\" title=\"chgrp\"></a><strong>chgrp</strong></h1><p>类似于linux中的chgrp，用户应该是超级用户才能做次操作<br>改变文件所有组<br><strong>指令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -chgrp [-R] &lt;NewGroupName&gt; &lt;file or directory name&gt;</div></pre></td></tr></table></figure></p>\n<p><strong>例子</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -chgrp [-R] New Group sample</div></pre></td></tr></table></figure></p>\n<h1 id=\"setrep\"><a href=\"#setrep\" class=\"headerlink\" title=\"setrep\"></a><strong>setrep</strong></h1><p><strong>指令用法</strong><br>用来改变文件的副本数，如果是文件夹，那么次命令会针对该文件夹下的所有文件都会改变副本数<br>-w 表示副本数改为多少<br>使用-R选项可以对一个目录下的所有目录+文件递归执行改变副本个数的操作<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">setrep [-R] [-w] rep &lt;path&gt;</div></pre></td></tr></table></figure></p>\n<p><strong>例子</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -setrep -w 3 /user/dataflair/dir1</div></pre></td></tr></table></figure></p>\n<h1 id=\"du\"><a href=\"#du\" class=\"headerlink\" title=\"du\"></a><strong>du</strong></h1><p>类似于linux中的du ， 统计个目录下各个文件大小，可以加-h 提高文件可读性<br><strong>指令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">du &lt;path&gt;</div></pre></td></tr></table></figure></p>\n<p><strong>例子</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -du /user/dataflair/dir1/sample</div></pre></td></tr></table></figure></p>\n<h1 id=\"df\"><a href=\"#df\" class=\"headerlink\" title=\"df\"></a><strong>df</strong></h1><p>类似于linux中的du ，查询某个目录的空间大小，可以加-h 提高文件可读性<br><strong>指令用法</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -df [-h] URI [URI ...]</div></pre></td></tr></table></figure></p>\n<p><strong>例子</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs dfs -df -h</div></pre></td></tr></table></figure></p>\n<p>翻译原文：<a href=\"http://data-flair.training/blogs/most-used-hdfs-commands-tutorial-examples/\" target=\"_blank\" rel=\"external\">http://data-flair.training/blogs/most-used-hdfs-commands-tutorial-examples/</a></p>\n"},{"title":"Hadoop入门案例（三）全排序之自定义分区 数字排序","date":"2016-07-25T13:25:21.000Z","author":"kaishun","id":"16","blogexcerpt":"需求介绍：大量的文本中有大量数字，需要对数字进行全排序,按照升序排序。 原理分析 利用mapReduce中map到reduce端的shuffle进行排序，MapReduce只能保证各个分区内部有序，但不能保证全局有序，于是我还自定义了分区，在map后、shuffle之前，我先将小于50的放在0分区，50-100的放在1分区，100到150的放在2分区，其余的放在三分区，这样，首先保证了分区与分区之间是整体有序，然后各个分区进行各自的shuffle，使其分区内部有序","_content":"\n\n## **需求介绍**  \n大量的文本中有大量数字，需要对数字进行全排序,按照升序排序  \n## **测试的文本**\n```\n145 \n95 167 84 6 120 164 195 81 35 63 1 11 89 170 55 \n58 88 125 173 2 173 129 74 69 24 107 55 149 83 178 \n159 147 178 53 137 53 132 134 154 174 164 122 108 130 184 \n28 129 93 157 171 127 192 86 194 41 111 114 190 98 99 \n99 5 161 146 120 122 80 1 66 171 47 54 121 130 170 \n125 119 8 52 182 112 146 1 198 0 149 72 56 191 48 \n172 165 49 73 107 134 179 0 59 16 143 83 92 113 152 \n109 118 186 186 97 117 193 67 34 152 92 179 52 51 26 \n163 121 115 72 17 61 107 125 115 163 18 76 2 172 39 \n190 184 73 108 7 142 68 54 60 169 71 28 141 48 139 \n182 140 158 102 99 36 158 55 190 176 45 63 126 179 130 \n95 22 120 109 59 78 38 13 5 88 1 87 184 83 198 \n47 73 82 94 141 190 184 161 56 141 99 177 107 21 158 \n71 149 61 137 \n```  \n## **原理分析**  \n利用mapReduce中map到reduce端的shuffle进行排序，MapReduce只能保证各个分区内部有序，但不能保证全局有序，于是我还自定义了分区，在map后、shuffle之前，我先将小于50的放在0分区，50-100的放在1分区，100到150的放在2分区，其余的放在三分区，这样，首先保证了分区与分区之间是整体有序，然后各个分区进行各自的shuffle，使其分区内部有序\n## **代码**  \n``` java\npackage com.myhadoop.mapreduce.test;\n\nimport org.apache.hadoop.conf.Configured;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Partitioner;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.util.Tool;\nimport org.apache.hadoop.util.ToolRunner;\n\nimport java.io.IOException;\n\npublic class TotalSort extends Configured implements Tool{\n\n    public static class Map extends Mapper<LongWritable, Text, IntWritable, IntWritable>\n    {\n        public void map(LongWritable key,Text value,Context context) throws IOException,InterruptedException\n        {\n            String[] split = value.toString().split(\"\\\\s+\");\n            for (int i = 0; i <split.length ; i++) {\n                try{\n                    IntWritable intWritable = new IntWritable(Integer.parseInt(split[i]));\n                    context.write(intWritable, intWritable);\n                }catch (Exception e){\n                    e.printStackTrace();\n                }\n            }\n\n\n        }\n    }\n\n    public static class Reduce extends Reducer<IntWritable, IntWritable, IntWritable, Text>\n    {\n        public void reduce(IntWritable key,Iterable<IntWritable> values,Context context) throws IOException,InterruptedException\n        {\n            for (IntWritable value:values) {\n                context.write(value, new Text(\"\"));\n            }\n        }\n    }\n\n    /*\n    ·* 重写Partition的方法\n     */\n    public static class Partition extends Partitioner<IntWritable, IntWritable>{\n        @Override\n        public int getPartition(IntWritable key, IntWritable intWritable2, int i) {\n            int i1=key.get();\n            if(i1<50){\n                return 0;\n            }else if(i1<100){\n                return 1;\n            }else if(i1<150){\n                return 2;\n            }\n            return 3;\n\n        }\n    }\n\n    @Override\n    public int run(String[] args) throws Exception {\n        Job job = Job.getInstance(getConf());\n        job.setJarByClass(TotalSort.class);\n        job.setJobName(\"TotalSort\");\n\n        job.setOutputKeyClass(IntWritable.class);\n        job.setOutputValueClass(IntWritable.class);\n\n        job.setPartitionerClass(Partition.class);\n        job.setMapperClass(Map.class);\n        job.setReducerClass(Reduce.class);\n        job.setNumReduceTasks(4);\n\n        FileInputFormat.setInputPaths(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n        boolean success = job.waitForCompletion(true);\n        return success ? 0:1;\n    }\n\n    public static void main(String[] args) throws Exception {\n        int ret = ToolRunner.run(new TotalSort(), args);\n        System.exit(ret);\n    }\n}\n\n```  \n## **运行结果**\n生成了4个文件，part-r-00000，part-r-00001，part-r-00002，part-r-00003，这四个文件内部都有序\npart-r-00000内的元素都小于part-r00001的元素，其他的以此类推  \n\n## **总结**\n利用自定义分区，保证整体有序，利用mapreduce内部的shuffle，对key进行排序，保证了局部有序，从而实现了全排序","source":"_posts/Hadoop入门案例（三）全排序之自定义分区 数字排序.md","raw":"---\ntitle: Hadoop入门案例（三）全排序之自定义分区 数字排序\ndate: 2016-07-25 21:25:21\ntags: [hadoop]\ncategories: [大数据,hadoop]\nauthor: kaishun\nid: 16\npermalink: hadoop-example-3\nblogexcerpt: 需求介绍：大量的文本中有大量数字，需要对数字进行全排序,按照升序排序。 原理分析 利用mapReduce中map到reduce端的shuffle进行排序，MapReduce只能保证各个分区内部有序，但不能保证全局有序，于是我还自定义了分区，在map后、shuffle之前，我先将小于50的放在0分区，50-100的放在1分区，100到150的放在2分区，其余的放在三分区，这样，首先保证了分区与分区之间是整体有序，然后各个分区进行各自的shuffle，使其分区内部有序\n---\n\n\n## **需求介绍**  \n大量的文本中有大量数字，需要对数字进行全排序,按照升序排序  \n## **测试的文本**\n```\n145 \n95 167 84 6 120 164 195 81 35 63 1 11 89 170 55 \n58 88 125 173 2 173 129 74 69 24 107 55 149 83 178 \n159 147 178 53 137 53 132 134 154 174 164 122 108 130 184 \n28 129 93 157 171 127 192 86 194 41 111 114 190 98 99 \n99 5 161 146 120 122 80 1 66 171 47 54 121 130 170 \n125 119 8 52 182 112 146 1 198 0 149 72 56 191 48 \n172 165 49 73 107 134 179 0 59 16 143 83 92 113 152 \n109 118 186 186 97 117 193 67 34 152 92 179 52 51 26 \n163 121 115 72 17 61 107 125 115 163 18 76 2 172 39 \n190 184 73 108 7 142 68 54 60 169 71 28 141 48 139 \n182 140 158 102 99 36 158 55 190 176 45 63 126 179 130 \n95 22 120 109 59 78 38 13 5 88 1 87 184 83 198 \n47 73 82 94 141 190 184 161 56 141 99 177 107 21 158 \n71 149 61 137 \n```  \n## **原理分析**  \n利用mapReduce中map到reduce端的shuffle进行排序，MapReduce只能保证各个分区内部有序，但不能保证全局有序，于是我还自定义了分区，在map后、shuffle之前，我先将小于50的放在0分区，50-100的放在1分区，100到150的放在2分区，其余的放在三分区，这样，首先保证了分区与分区之间是整体有序，然后各个分区进行各自的shuffle，使其分区内部有序\n## **代码**  \n``` java\npackage com.myhadoop.mapreduce.test;\n\nimport org.apache.hadoop.conf.Configured;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Partitioner;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.util.Tool;\nimport org.apache.hadoop.util.ToolRunner;\n\nimport java.io.IOException;\n\npublic class TotalSort extends Configured implements Tool{\n\n    public static class Map extends Mapper<LongWritable, Text, IntWritable, IntWritable>\n    {\n        public void map(LongWritable key,Text value,Context context) throws IOException,InterruptedException\n        {\n            String[] split = value.toString().split(\"\\\\s+\");\n            for (int i = 0; i <split.length ; i++) {\n                try{\n                    IntWritable intWritable = new IntWritable(Integer.parseInt(split[i]));\n                    context.write(intWritable, intWritable);\n                }catch (Exception e){\n                    e.printStackTrace();\n                }\n            }\n\n\n        }\n    }\n\n    public static class Reduce extends Reducer<IntWritable, IntWritable, IntWritable, Text>\n    {\n        public void reduce(IntWritable key,Iterable<IntWritable> values,Context context) throws IOException,InterruptedException\n        {\n            for (IntWritable value:values) {\n                context.write(value, new Text(\"\"));\n            }\n        }\n    }\n\n    /*\n    ·* 重写Partition的方法\n     */\n    public static class Partition extends Partitioner<IntWritable, IntWritable>{\n        @Override\n        public int getPartition(IntWritable key, IntWritable intWritable2, int i) {\n            int i1=key.get();\n            if(i1<50){\n                return 0;\n            }else if(i1<100){\n                return 1;\n            }else if(i1<150){\n                return 2;\n            }\n            return 3;\n\n        }\n    }\n\n    @Override\n    public int run(String[] args) throws Exception {\n        Job job = Job.getInstance(getConf());\n        job.setJarByClass(TotalSort.class);\n        job.setJobName(\"TotalSort\");\n\n        job.setOutputKeyClass(IntWritable.class);\n        job.setOutputValueClass(IntWritable.class);\n\n        job.setPartitionerClass(Partition.class);\n        job.setMapperClass(Map.class);\n        job.setReducerClass(Reduce.class);\n        job.setNumReduceTasks(4);\n\n        FileInputFormat.setInputPaths(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n        boolean success = job.waitForCompletion(true);\n        return success ? 0:1;\n    }\n\n    public static void main(String[] args) throws Exception {\n        int ret = ToolRunner.run(new TotalSort(), args);\n        System.exit(ret);\n    }\n}\n\n```  \n## **运行结果**\n生成了4个文件，part-r-00000，part-r-00001，part-r-00002，part-r-00003，这四个文件内部都有序\npart-r-00000内的元素都小于part-r00001的元素，其他的以此类推  \n\n## **总结**\n利用自定义分区，保证整体有序，利用mapreduce内部的shuffle，对key进行排序，保证了局部有序，从而实现了全排序","slug":"hadoop-example-3","published":1,"updated":"2018-01-23T14:15:07.260Z","_id":"cjcrpnzo3000k2wv30sqlt08c","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"需求介绍\"><a href=\"#需求介绍\" class=\"headerlink\" title=\"需求介绍\"></a><strong>需求介绍</strong></h2><p>大量的文本中有大量数字，需要对数字进行全排序,按照升序排序  </p>\n<h2 id=\"测试的文本\"><a href=\"#测试的文本\" class=\"headerlink\" title=\"测试的文本\"></a><strong>测试的文本</strong></h2><pre><code>145 \n95 167 84 6 120 164 195 81 35 63 1 11 89 170 55 \n58 88 125 173 2 173 129 74 69 24 107 55 149 83 178 \n159 147 178 53 137 53 132 134 154 174 164 122 108 130 184 \n28 129 93 157 171 127 192 86 194 41 111 114 190 98 99 \n99 5 161 146 120 122 80 1 66 171 47 54 121 130 170 \n125 119 8 52 182 112 146 1 198 0 149 72 56 191 48 \n172 165 49 73 107 134 179 0 59 16 143 83 92 113 152 \n109 118 186 186 97 117 193 67 34 152 92 179 52 51 26 \n163 121 115 72 17 61 107 125 115 163 18 76 2 172 39 \n190 184 73 108 7 142 68 54 60 169 71 28 141 48 139 \n182 140 158 102 99 36 158 55 190 176 45 63 126 179 130 \n95 22 120 109 59 78 38 13 5 88 1 87 184 83 198 \n47 73 82 94 141 190 184 161 56 141 99 177 107 21 158 \n71 149 61 137\n</code></pre><h2 id=\"原理分析\"><a href=\"#原理分析\" class=\"headerlink\" title=\"原理分析\"></a><strong>原理分析</strong></h2><p>利用mapReduce中map到reduce端的shuffle进行排序，MapReduce只能保证各个分区内部有序，但不能保证全局有序，于是我还自定义了分区，在map后、shuffle之前，我先将小于50的放在0分区，50-100的放在1分区，100到150的放在2分区，其余的放在三分区，这样，首先保证了分区与分区之间是整体有序，然后各个分区进行各自的shuffle，使其分区内部有序</p>\n<h2 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a><strong>代码</strong></h2><pre><code class=\"java\"><span class=\"keyword\">package</span> com.myhadoop.mapreduce.test;\n\n<span class=\"keyword\">import</span> org.apache.hadoop.conf.Configured;\n<span class=\"keyword\">import</span> org.apache.hadoop.fs.Path;\n<span class=\"keyword\">import</span> org.apache.hadoop.io.IntWritable;\n<span class=\"keyword\">import</span> org.apache.hadoop.io.LongWritable;\n<span class=\"keyword\">import</span> org.apache.hadoop.io.Text;\n<span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Job;\n<span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Mapper;\n<span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Partitioner;\n<span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Reducer;\n<span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n<span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n<span class=\"keyword\">import</span> org.apache.hadoop.util.Tool;\n<span class=\"keyword\">import</span> org.apache.hadoop.util.ToolRunner;\n\n<span class=\"keyword\">import</span> java.io.IOException;\n\n<span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TotalSort</span> <span class=\"keyword\">extends</span> <span class=\"title\">Configured</span> <span class=\"keyword\">implements</span> <span class=\"title\">Tool</span></span>{\n\n    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Map</span> <span class=\"keyword\">extends</span> <span class=\"title\">Mapper</span>&lt;<span class=\"title\">LongWritable</span>, <span class=\"title\">Text</span>, <span class=\"title\">IntWritable</span>, <span class=\"title\">IntWritable</span>&gt;</span>\n<span class=\"class\">    </span>{\n        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">map</span><span class=\"params\">(LongWritable key,Text value,Context context)</span> <span class=\"keyword\">throws</span> IOException,InterruptedException</span>\n<span class=\"function\">        </span>{\n            String[] split = value.toString().split(<span class=\"string\">\"\\\\s+\"</span>);\n            <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt;split.length ; i++) {\n                <span class=\"keyword\">try</span>{\n                    IntWritable intWritable = <span class=\"keyword\">new</span> IntWritable(Integer.parseInt(split[i]));\n                    context.write(intWritable, intWritable);\n                }<span class=\"keyword\">catch</span> (Exception e){\n                    e.printStackTrace();\n                }\n            }\n\n\n        }\n    }\n\n    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Reduce</span> <span class=\"keyword\">extends</span> <span class=\"title\">Reducer</span>&lt;<span class=\"title\">IntWritable</span>, <span class=\"title\">IntWritable</span>, <span class=\"title\">IntWritable</span>, <span class=\"title\">Text</span>&gt;</span>\n<span class=\"class\">    </span>{\n        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">reduce</span><span class=\"params\">(IntWritable key,Iterable&lt;IntWritable&gt; values,Context context)</span> <span class=\"keyword\">throws</span> IOException,InterruptedException</span>\n<span class=\"function\">        </span>{\n            <span class=\"keyword\">for</span> (IntWritable value:values) {\n                context.write(value, <span class=\"keyword\">new</span> Text(<span class=\"string\">\"\"</span>));\n            }\n        }\n    }\n\n    <span class=\"comment\">/*</span>\n<span class=\"comment\">    ·* 重写Partition的方法</span>\n<span class=\"comment\">     */</span>\n    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Partition</span> <span class=\"keyword\">extends</span> <span class=\"title\">Partitioner</span>&lt;<span class=\"title\">IntWritable</span>, <span class=\"title\">IntWritable</span>&gt;</span>{\n        <span class=\"meta\">@Override</span>\n        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">getPartition</span><span class=\"params\">(IntWritable key, IntWritable intWritable2, <span class=\"keyword\">int</span> i)</span> </span>{\n            <span class=\"keyword\">int</span> i1=key.get();\n            <span class=\"keyword\">if</span>(i1&lt;<span class=\"number\">50</span>){\n                <span class=\"keyword\">return</span> <span class=\"number\">0</span>;\n            }<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(i1&lt;<span class=\"number\">100</span>){\n                <span class=\"keyword\">return</span> <span class=\"number\">1</span>;\n            }<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(i1&lt;<span class=\"number\">150</span>){\n                <span class=\"keyword\">return</span> <span class=\"number\">2</span>;\n            }\n            <span class=\"keyword\">return</span> <span class=\"number\">3</span>;\n\n        }\n    }\n\n    <span class=\"meta\">@Override</span>\n    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">run</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception </span>{\n        Job job = Job.getInstance(getConf());\n        job.setJarByClass(TotalSort.class);\n        job.setJobName(<span class=\"string\">\"TotalSort\"</span>);\n\n        job.setOutputKeyClass(IntWritable.class);\n        job.setOutputValueClass(IntWritable.class);\n\n        job.setPartitionerClass(Partition.class);\n        job.setMapperClass(Map.class);\n        job.setReducerClass(Reduce.class);\n        job.setNumReduceTasks(<span class=\"number\">4</span>);\n\n        FileInputFormat.setInputPaths(job, <span class=\"keyword\">new</span> Path(args[<span class=\"number\">0</span>]));\n        FileOutputFormat.setOutputPath(job, <span class=\"keyword\">new</span> Path(args[<span class=\"number\">1</span>]));\n\n        <span class=\"keyword\">boolean</span> success = job.waitForCompletion(<span class=\"keyword\">true</span>);\n        <span class=\"keyword\">return</span> success ? <span class=\"number\">0</span>:<span class=\"number\">1</span>;\n    }\n\n    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception </span>{\n        <span class=\"keyword\">int</span> ret = ToolRunner.run(<span class=\"keyword\">new</span> TotalSort(), args);\n        System.exit(ret);\n    }\n}\n</code></pre>\n<h2 id=\"运行结果\"><a href=\"#运行结果\" class=\"headerlink\" title=\"运行结果\"></a><strong>运行结果</strong></h2><p>生成了4个文件，part-r-00000，part-r-00001，part-r-00002，part-r-00003，这四个文件内部都有序<br>part-r-00000内的元素都小于part-r00001的元素，其他的以此类推  </p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a><strong>总结</strong></h2><p>利用自定义分区，保证整体有序，利用mapreduce内部的shuffle，对key进行排序，保证了局部有序，从而实现了全排序</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"需求介绍\"><a href=\"#需求介绍\" class=\"headerlink\" title=\"需求介绍\"></a><strong>需求介绍</strong></h2><p>大量的文本中有大量数字，需要对数字进行全排序,按照升序排序  </p>\n<h2 id=\"测试的文本\"><a href=\"#测试的文本\" class=\"headerlink\" title=\"测试的文本\"></a><strong>测试的文本</strong></h2><pre><code>145 \n95 167 84 6 120 164 195 81 35 63 1 11 89 170 55 \n58 88 125 173 2 173 129 74 69 24 107 55 149 83 178 \n159 147 178 53 137 53 132 134 154 174 164 122 108 130 184 \n28 129 93 157 171 127 192 86 194 41 111 114 190 98 99 \n99 5 161 146 120 122 80 1 66 171 47 54 121 130 170 \n125 119 8 52 182 112 146 1 198 0 149 72 56 191 48 \n172 165 49 73 107 134 179 0 59 16 143 83 92 113 152 \n109 118 186 186 97 117 193 67 34 152 92 179 52 51 26 \n163 121 115 72 17 61 107 125 115 163 18 76 2 172 39 \n190 184 73 108 7 142 68 54 60 169 71 28 141 48 139 \n182 140 158 102 99 36 158 55 190 176 45 63 126 179 130 \n95 22 120 109 59 78 38 13 5 88 1 87 184 83 198 \n47 73 82 94 141 190 184 161 56 141 99 177 107 21 158 \n71 149 61 137\n</code></pre><h2 id=\"原理分析\"><a href=\"#原理分析\" class=\"headerlink\" title=\"原理分析\"></a><strong>原理分析</strong></h2><p>利用mapReduce中map到reduce端的shuffle进行排序，MapReduce只能保证各个分区内部有序，但不能保证全局有序，于是我还自定义了分区，在map后、shuffle之前，我先将小于50的放在0分区，50-100的放在1分区，100到150的放在2分区，其余的放在三分区，这样，首先保证了分区与分区之间是整体有序，然后各个分区进行各自的shuffle，使其分区内部有序</p>\n<h2 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a><strong>代码</strong></h2><pre><code class=\"java\"><span class=\"keyword\">package</span> com.myhadoop.mapreduce.test;\n\n<span class=\"keyword\">import</span> org.apache.hadoop.conf.Configured;\n<span class=\"keyword\">import</span> org.apache.hadoop.fs.Path;\n<span class=\"keyword\">import</span> org.apache.hadoop.io.IntWritable;\n<span class=\"keyword\">import</span> org.apache.hadoop.io.LongWritable;\n<span class=\"keyword\">import</span> org.apache.hadoop.io.Text;\n<span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Job;\n<span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Mapper;\n<span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Partitioner;\n<span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Reducer;\n<span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n<span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n<span class=\"keyword\">import</span> org.apache.hadoop.util.Tool;\n<span class=\"keyword\">import</span> org.apache.hadoop.util.ToolRunner;\n\n<span class=\"keyword\">import</span> java.io.IOException;\n\n<span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TotalSort</span> <span class=\"keyword\">extends</span> <span class=\"title\">Configured</span> <span class=\"keyword\">implements</span> <span class=\"title\">Tool</span></span>{\n\n    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Map</span> <span class=\"keyword\">extends</span> <span class=\"title\">Mapper</span>&lt;<span class=\"title\">LongWritable</span>, <span class=\"title\">Text</span>, <span class=\"title\">IntWritable</span>, <span class=\"title\">IntWritable</span>&gt;</span>\n<span class=\"class\">    </span>{\n        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">map</span><span class=\"params\">(LongWritable key,Text value,Context context)</span> <span class=\"keyword\">throws</span> IOException,InterruptedException</span>\n<span class=\"function\">        </span>{\n            String[] split = value.toString().split(<span class=\"string\">\"\\\\s+\"</span>);\n            <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt;split.length ; i++) {\n                <span class=\"keyword\">try</span>{\n                    IntWritable intWritable = <span class=\"keyword\">new</span> IntWritable(Integer.parseInt(split[i]));\n                    context.write(intWritable, intWritable);\n                }<span class=\"keyword\">catch</span> (Exception e){\n                    e.printStackTrace();\n                }\n            }\n\n\n        }\n    }\n\n    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Reduce</span> <span class=\"keyword\">extends</span> <span class=\"title\">Reducer</span>&lt;<span class=\"title\">IntWritable</span>, <span class=\"title\">IntWritable</span>, <span class=\"title\">IntWritable</span>, <span class=\"title\">Text</span>&gt;</span>\n<span class=\"class\">    </span>{\n        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">reduce</span><span class=\"params\">(IntWritable key,Iterable&lt;IntWritable&gt; values,Context context)</span> <span class=\"keyword\">throws</span> IOException,InterruptedException</span>\n<span class=\"function\">        </span>{\n            <span class=\"keyword\">for</span> (IntWritable value:values) {\n                context.write(value, <span class=\"keyword\">new</span> Text(<span class=\"string\">\"\"</span>));\n            }\n        }\n    }\n\n    <span class=\"comment\">/*</span>\n<span class=\"comment\">    ·* 重写Partition的方法</span>\n<span class=\"comment\">     */</span>\n    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Partition</span> <span class=\"keyword\">extends</span> <span class=\"title\">Partitioner</span>&lt;<span class=\"title\">IntWritable</span>, <span class=\"title\">IntWritable</span>&gt;</span>{\n        <span class=\"meta\">@Override</span>\n        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">getPartition</span><span class=\"params\">(IntWritable key, IntWritable intWritable2, <span class=\"keyword\">int</span> i)</span> </span>{\n            <span class=\"keyword\">int</span> i1=key.get();\n            <span class=\"keyword\">if</span>(i1&lt;<span class=\"number\">50</span>){\n                <span class=\"keyword\">return</span> <span class=\"number\">0</span>;\n            }<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(i1&lt;<span class=\"number\">100</span>){\n                <span class=\"keyword\">return</span> <span class=\"number\">1</span>;\n            }<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(i1&lt;<span class=\"number\">150</span>){\n                <span class=\"keyword\">return</span> <span class=\"number\">2</span>;\n            }\n            <span class=\"keyword\">return</span> <span class=\"number\">3</span>;\n\n        }\n    }\n\n    <span class=\"meta\">@Override</span>\n    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">run</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception </span>{\n        Job job = Job.getInstance(getConf());\n        job.setJarByClass(TotalSort.class);\n        job.setJobName(<span class=\"string\">\"TotalSort\"</span>);\n\n        job.setOutputKeyClass(IntWritable.class);\n        job.setOutputValueClass(IntWritable.class);\n\n        job.setPartitionerClass(Partition.class);\n        job.setMapperClass(Map.class);\n        job.setReducerClass(Reduce.class);\n        job.setNumReduceTasks(<span class=\"number\">4</span>);\n\n        FileInputFormat.setInputPaths(job, <span class=\"keyword\">new</span> Path(args[<span class=\"number\">0</span>]));\n        FileOutputFormat.setOutputPath(job, <span class=\"keyword\">new</span> Path(args[<span class=\"number\">1</span>]));\n\n        <span class=\"keyword\">boolean</span> success = job.waitForCompletion(<span class=\"keyword\">true</span>);\n        <span class=\"keyword\">return</span> success ? <span class=\"number\">0</span>:<span class=\"number\">1</span>;\n    }\n\n    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception </span>{\n        <span class=\"keyword\">int</span> ret = ToolRunner.run(<span class=\"keyword\">new</span> TotalSort(), args);\n        System.exit(ret);\n    }\n}\n</code></pre>\n<h2 id=\"运行结果\"><a href=\"#运行结果\" class=\"headerlink\" title=\"运行结果\"></a><strong>运行结果</strong></h2><p>生成了4个文件，part-r-00000，part-r-00001，part-r-00002，part-r-00003，这四个文件内部都有序<br>part-r-00000内的元素都小于part-r00001的元素，其他的以此类推  </p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a><strong>总结</strong></h2><p>利用自定义分区，保证整体有序，利用mapreduce内部的shuffle，对key进行排序，保证了局部有序，从而实现了全排序</p>\n"},{"title":"HDFS合并文件","date":"2016-05-15T15:13:21.000Z","author":"kaishun","id":"1","blogexcerpt":"hdfs提供了一种FileUtil.copyMerge（）的方法， 注意下面的 false。 这个，如果改为true，就会删除这个目录","toc":true,"_content":"![hdfs-commands](http://or49tneld.bkt.clouddn.com/18-1-23/63075597.jpg)  \n\n# **HDFS到HDFS的合并**\nhdfs提供了一种FileUtil.copyMerge（）的方法， 注意下面的 false 这个，如果改为true，就会删除这个目录，public void copyMerge(String folder, String file) {Path src = new Path(folder).......;\n```java\n\tpublic void copyMerge(String folder, String file) {\n\n\t\tPath src = new Path(folder);\n\t\tPath dst = new Path(file);\n\n\t\ttry {\n\t\t\tFileUtil.copyMerge(src.getFileSystem(conf), src,\n\t\t\t\t\tdst.getFileSystem(conf), dst, false, conf, null);\n\t\t} catch (IOException e) {\n\t\t\t// TODO Auto-generated catch block\n\t\t\te.printStackTrace();\n\t\t}\n\t}\n\n\n```\n\n# **上传文件到HDFS并且合并**\n```java\n\n\t/**\n\t * 读取本地文件到HDFS系统<br>\n\t * 请保证文件格式一直是UTF-8，从本地->HDFS\n\t * \n\t */\n\t/**\n\t * \n\t * @param localDirname 源文件所在位置\n\t * @param hdfsPath 要放在服务器的位置\n\t * @param destFileName 要合并成的文件名称\n\t * @param filter\n\t * @return\n\t */\n\tpublic boolean putMerge(String localDirname, String hdfsPath,String destFileName, String filter )\n\t{\n\t\ttry\n\t\t{\n\t\t\tFile dir = new File(localDirname);\n\t\t\tif(!dir.isDirectory())\n\t\t\t{\n\t\t\t\tSystem.out.println(localDirname+\"不是目录 \");\n\t\t\t\treturn false;\n\t\t\t}\n\n\t\t\tFile[] files = dir.listFiles();\n\t\t\tif(files.length ==0)\n\t\t\t\treturn false;\n\t\t\t\n\t\t\tSystem.out.println(\"Begin move \" + localDirname + \" to \" + hdfsPath);\n\t\t\t\n\t\t\twhile(checkFileExist(hdfsPath+\"/\"+destFileName))\n\t\t\t{\n\t\t\t\tif(destFileName.contains(\".x\"))\n\t\t\t\t\tdestFileName += \"x\";\n\t\t\t\telse\n\t\t\t\t\tdestFileName += \".x\";\n\t\t\t}\n\t\t\t\n\t\t\tmkdir(hdfsPath);\n\t\t\t\n\t\t\tPath f = new Path(hdfsPath+\"/\"+destFileName);\n\t\t\tFSDataOutputStream os = fs.create(f, true);\n\t\t\tbyte[] buffer = new byte[10240000];\n \t\t\t\n\t\t\tfor(int i=0; i<files.length; i++)\n\t\t\t{\n\t\t\t\tif(MainSvr.bExitFlag == true)\n\t\t\t\t\tbreak;\t\n\t\t\t\t \n\t\t\t\tFile file = files[i];\n\t\t\t\tif(!file.getName().toLowerCase().contains(filter.toLowerCase()))\n\t\t\t\t\tcontinue;\n\t\t\t\tFileInputStream is = new FileInputStream(file);\t\t\t\t\t\n\t\t\t\tGZIPInputStream gis = null;\n\t\t\t\tif(file.getName().toLowerCase().endsWith(\"gz\"))\n\t\t\t\t\tgis=new GZIPInputStream(is);\n\t\t\t\t\n\t\t\t\twhile(MainSvr.bExitFlag != true)\n\t\t\t\t{\n\t\t            int bytesRead =0;\n\t\t            if(gis == null)\n\t\t            \tbytesRead= is.read(buffer);\n\t\t            else\n\t\t            \tbytesRead= gis.read(buffer,0,buffer.length);\n\t\t            if (bytesRead >= 0) \n\t\t            {\n\t\t                os.write(buffer, 0, bytesRead);\n\t\t            }\n\t\t            else\n\t\t            {\n\t\t            \tbreak;\n\t\t            }\n\t\t\t\t}\n\t            if(gis != null)\n\t            \tgis.close();\n\t\t\t\tis.close();\n\t\t\t}\n\t\t\tos.close();\n\t\t\tif(MainSvr.bExitFlag)\n\t\t\t\treturn false;\n\t\t\tSystem.out.println(\"Success move \" + localDirname + \" to \" + hdfsPath);\n\t\t\treturn true;\n\t\t}\n\t\tcatch (Exception e)\n\t\t{\n\t\t\te.printStackTrace();\n\t\t\tlog.info(\"putMerge error:\" + e.getMessage());\n\t\t}\n\t\treturn false;\n\t}\n\t\n\t\n\tpublic boolean mkdir(String dirName)\n\t{\t\n\t\ttry\n\t\t{\n\t\t\tif (checkFileExist(dirName))\n\t\t\treturn true;\t\t\t\n\t\t\tPath f = new Path(dirName);\n\t\t\tSystem.out.println(\"Create and Write :\" + dirName + \" to hdfs\");\n\t\t\treturn hdfs.mkdirs(f);\n\t\t}\n\t\tcatch (Exception e)\n\t\t{\n\t\t\tSystem.out.println(\"mkdir Fail:\" + e.getMessage());\n\t\t\te.printStackTrace();\t\t\n\t\t}\n\n\t\treturn false;\n\t}\n```","source":"_posts/HDFS合并文件.md","raw":"---\ntitle: HDFS合并文件\ndate: 2016-05-15 23:13:21\ntags: [hdfs]\ncategories: [大数据,hdfs]\nauthor: kaishun\nid: 1\npermalink: hdfs-merge-file\nblogexcerpt: hdfs提供了一种FileUtil.copyMerge（）的方法， 注意下面的 false。 这个，如果改为true，就会删除这个目录\ntoc: true\n---\n![hdfs-commands](http://or49tneld.bkt.clouddn.com/18-1-23/63075597.jpg)  \n\n# **HDFS到HDFS的合并**\nhdfs提供了一种FileUtil.copyMerge（）的方法， 注意下面的 false 这个，如果改为true，就会删除这个目录，public void copyMerge(String folder, String file) {Path src = new Path(folder).......;\n```java\n\tpublic void copyMerge(String folder, String file) {\n\n\t\tPath src = new Path(folder);\n\t\tPath dst = new Path(file);\n\n\t\ttry {\n\t\t\tFileUtil.copyMerge(src.getFileSystem(conf), src,\n\t\t\t\t\tdst.getFileSystem(conf), dst, false, conf, null);\n\t\t} catch (IOException e) {\n\t\t\t// TODO Auto-generated catch block\n\t\t\te.printStackTrace();\n\t\t}\n\t}\n\n\n```\n\n# **上传文件到HDFS并且合并**\n```java\n\n\t/**\n\t * 读取本地文件到HDFS系统<br>\n\t * 请保证文件格式一直是UTF-8，从本地->HDFS\n\t * \n\t */\n\t/**\n\t * \n\t * @param localDirname 源文件所在位置\n\t * @param hdfsPath 要放在服务器的位置\n\t * @param destFileName 要合并成的文件名称\n\t * @param filter\n\t * @return\n\t */\n\tpublic boolean putMerge(String localDirname, String hdfsPath,String destFileName, String filter )\n\t{\n\t\ttry\n\t\t{\n\t\t\tFile dir = new File(localDirname);\n\t\t\tif(!dir.isDirectory())\n\t\t\t{\n\t\t\t\tSystem.out.println(localDirname+\"不是目录 \");\n\t\t\t\treturn false;\n\t\t\t}\n\n\t\t\tFile[] files = dir.listFiles();\n\t\t\tif(files.length ==0)\n\t\t\t\treturn false;\n\t\t\t\n\t\t\tSystem.out.println(\"Begin move \" + localDirname + \" to \" + hdfsPath);\n\t\t\t\n\t\t\twhile(checkFileExist(hdfsPath+\"/\"+destFileName))\n\t\t\t{\n\t\t\t\tif(destFileName.contains(\".x\"))\n\t\t\t\t\tdestFileName += \"x\";\n\t\t\t\telse\n\t\t\t\t\tdestFileName += \".x\";\n\t\t\t}\n\t\t\t\n\t\t\tmkdir(hdfsPath);\n\t\t\t\n\t\t\tPath f = new Path(hdfsPath+\"/\"+destFileName);\n\t\t\tFSDataOutputStream os = fs.create(f, true);\n\t\t\tbyte[] buffer = new byte[10240000];\n \t\t\t\n\t\t\tfor(int i=0; i<files.length; i++)\n\t\t\t{\n\t\t\t\tif(MainSvr.bExitFlag == true)\n\t\t\t\t\tbreak;\t\n\t\t\t\t \n\t\t\t\tFile file = files[i];\n\t\t\t\tif(!file.getName().toLowerCase().contains(filter.toLowerCase()))\n\t\t\t\t\tcontinue;\n\t\t\t\tFileInputStream is = new FileInputStream(file);\t\t\t\t\t\n\t\t\t\tGZIPInputStream gis = null;\n\t\t\t\tif(file.getName().toLowerCase().endsWith(\"gz\"))\n\t\t\t\t\tgis=new GZIPInputStream(is);\n\t\t\t\t\n\t\t\t\twhile(MainSvr.bExitFlag != true)\n\t\t\t\t{\n\t\t            int bytesRead =0;\n\t\t            if(gis == null)\n\t\t            \tbytesRead= is.read(buffer);\n\t\t            else\n\t\t            \tbytesRead= gis.read(buffer,0,buffer.length);\n\t\t            if (bytesRead >= 0) \n\t\t            {\n\t\t                os.write(buffer, 0, bytesRead);\n\t\t            }\n\t\t            else\n\t\t            {\n\t\t            \tbreak;\n\t\t            }\n\t\t\t\t}\n\t            if(gis != null)\n\t            \tgis.close();\n\t\t\t\tis.close();\n\t\t\t}\n\t\t\tos.close();\n\t\t\tif(MainSvr.bExitFlag)\n\t\t\t\treturn false;\n\t\t\tSystem.out.println(\"Success move \" + localDirname + \" to \" + hdfsPath);\n\t\t\treturn true;\n\t\t}\n\t\tcatch (Exception e)\n\t\t{\n\t\t\te.printStackTrace();\n\t\t\tlog.info(\"putMerge error:\" + e.getMessage());\n\t\t}\n\t\treturn false;\n\t}\n\t\n\t\n\tpublic boolean mkdir(String dirName)\n\t{\t\n\t\ttry\n\t\t{\n\t\t\tif (checkFileExist(dirName))\n\t\t\treturn true;\t\t\t\n\t\t\tPath f = new Path(dirName);\n\t\t\tSystem.out.println(\"Create and Write :\" + dirName + \" to hdfs\");\n\t\t\treturn hdfs.mkdirs(f);\n\t\t}\n\t\tcatch (Exception e)\n\t\t{\n\t\t\tSystem.out.println(\"mkdir Fail:\" + e.getMessage());\n\t\t\te.printStackTrace();\t\t\n\t\t}\n\n\t\treturn false;\n\t}\n```","slug":"hdfs-merge-file","published":1,"updated":"2018-01-23T14:42:08.512Z","_id":"cjcrpnzoj000m2wv3db66k2bm","comments":1,"layout":"post","photos":[],"link":"","content":"<p><img src=\"http://or49tneld.bkt.clouddn.com/18-1-23/63075597.jpg\" alt=\"hdfs-commands\">  </p>\n<h1 id=\"HDFS到HDFS的合并\"><a href=\"#HDFS到HDFS的合并\" class=\"headerlink\" title=\"HDFS到HDFS的合并\"></a><strong>HDFS到HDFS的合并</strong></h1><p>hdfs提供了一种FileUtil.copyMerge（）的方法， 注意下面的 false 这个，如果改为true，就会删除这个目录，public void copyMerge(String folder, String file) {Path src = new Path(folder)…….;<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">copyMerge</span><span class=\"params\">(String folder, String file)</span> </span>&#123;</div><div class=\"line\"></div><div class=\"line\">\tPath src = <span class=\"keyword\">new</span> Path(folder);</div><div class=\"line\">\tPath dst = <span class=\"keyword\">new</span> Path(file);</div><div class=\"line\"></div><div class=\"line\">\t<span class=\"keyword\">try</span> &#123;</div><div class=\"line\">\t\tFileUtil.copyMerge(src.getFileSystem(conf), src,</div><div class=\"line\">\t\t\t\tdst.getFileSystem(conf), dst, <span class=\"keyword\">false</span>, conf, <span class=\"keyword\">null</span>);</div><div class=\"line\">\t&#125; <span class=\"keyword\">catch</span> (IOException e) &#123;</div><div class=\"line\">\t\t<span class=\"comment\">// TODO Auto-generated catch block</span></div><div class=\"line\">\t\te.printStackTrace();</div><div class=\"line\">\t&#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h1 id=\"上传文件到HDFS并且合并\"><a href=\"#上传文件到HDFS并且合并\" class=\"headerlink\" title=\"上传文件到HDFS并且合并\"></a><strong>上传文件到HDFS并且合并</strong></h1><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div><div class=\"line\">106</div><div class=\"line\">107</div><div class=\"line\">108</div><div class=\"line\">109</div><div class=\"line\">110</div><div class=\"line\">111</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\"> * 读取本地文件到HDFS系统&lt;br&gt;</span></div><div class=\"line\"><span class=\"comment\"> * 请保证文件格式一直是UTF-8，从本地-&gt;HDFS</span></div><div class=\"line\"><span class=\"comment\"> * </span></div><div class=\"line\"><span class=\"comment\"> */</span></div><div class=\"line\"><span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\"> * </span></div><div class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">@param</span> localDirname 源文件所在位置</span></div><div class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">@param</span> hdfsPath 要放在服务器的位置</span></div><div class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">@param</span> destFileName 要合并成的文件名称</span></div><div class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">@param</span> filter</span></div><div class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">@return</span></span></div><div class=\"line\"><span class=\"comment\"> */</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">putMerge</span><span class=\"params\">(String localDirname, String hdfsPath,String destFileName, String filter )</span></span></div><div class=\"line\"><span class=\"function\"></span>&#123;</div><div class=\"line\">\t<span class=\"keyword\">try</span></div><div class=\"line\">\t&#123;</div><div class=\"line\">\t\tFile dir = <span class=\"keyword\">new</span> File(localDirname);</div><div class=\"line\">\t\t<span class=\"keyword\">if</span>(!dir.isDirectory())</div><div class=\"line\">\t\t&#123;</div><div class=\"line\">\t\t\tSystem.out.println(localDirname+<span class=\"string\">\"不是目录 \"</span>);</div><div class=\"line\">\t\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</div><div class=\"line\">\t\t&#125;</div><div class=\"line\"></div><div class=\"line\">\t\tFile[] files = dir.listFiles();</div><div class=\"line\">\t\t<span class=\"keyword\">if</span>(files.length ==<span class=\"number\">0</span>)</div><div class=\"line\">\t\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</div><div class=\"line\">\t\t</div><div class=\"line\">\t\tSystem.out.println(<span class=\"string\">\"Begin move \"</span> + localDirname + <span class=\"string\">\" to \"</span> + hdfsPath);</div><div class=\"line\">\t\t</div><div class=\"line\">\t\t<span class=\"keyword\">while</span>(checkFileExist(hdfsPath+<span class=\"string\">\"/\"</span>+destFileName))</div><div class=\"line\">\t\t&#123;</div><div class=\"line\">\t\t\t<span class=\"keyword\">if</span>(destFileName.contains(<span class=\"string\">\".x\"</span>))</div><div class=\"line\">\t\t\t\tdestFileName += <span class=\"string\">\"x\"</span>;</div><div class=\"line\">\t\t\t<span class=\"keyword\">else</span></div><div class=\"line\">\t\t\t\tdestFileName += <span class=\"string\">\".x\"</span>;</div><div class=\"line\">\t\t&#125;</div><div class=\"line\">\t\t</div><div class=\"line\">\t\tmkdir(hdfsPath);</div><div class=\"line\">\t\t</div><div class=\"line\">\t\tPath f = <span class=\"keyword\">new</span> Path(hdfsPath+<span class=\"string\">\"/\"</span>+destFileName);</div><div class=\"line\">\t\tFSDataOutputStream os = fs.create(f, <span class=\"keyword\">true</span>);</div><div class=\"line\">\t\t<span class=\"keyword\">byte</span>[] buffer = <span class=\"keyword\">new</span> <span class=\"keyword\">byte</span>[<span class=\"number\">10240000</span>];</div><div class=\"line\">\t\t\t</div><div class=\"line\">\t\t<span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i=<span class=\"number\">0</span>; i&lt;files.length; i++)</div><div class=\"line\">\t\t&#123;</div><div class=\"line\">\t\t\t<span class=\"keyword\">if</span>(MainSvr.bExitFlag == <span class=\"keyword\">true</span>)</div><div class=\"line\">\t\t\t\t<span class=\"keyword\">break</span>;\t</div><div class=\"line\">\t\t\t </div><div class=\"line\">\t\t\tFile file = files[i];</div><div class=\"line\">\t\t\t<span class=\"keyword\">if</span>(!file.getName().toLowerCase().contains(filter.toLowerCase()))</div><div class=\"line\">\t\t\t\t<span class=\"keyword\">continue</span>;</div><div class=\"line\">\t\t\tFileInputStream is = <span class=\"keyword\">new</span> FileInputStream(file);\t\t\t\t\t</div><div class=\"line\">\t\t\tGZIPInputStream gis = <span class=\"keyword\">null</span>;</div><div class=\"line\">\t\t\t<span class=\"keyword\">if</span>(file.getName().toLowerCase().endsWith(<span class=\"string\">\"gz\"</span>))</div><div class=\"line\">\t\t\t\tgis=<span class=\"keyword\">new</span> GZIPInputStream(is);</div><div class=\"line\">\t\t\t</div><div class=\"line\">\t\t\t<span class=\"keyword\">while</span>(MainSvr.bExitFlag != <span class=\"keyword\">true</span>)</div><div class=\"line\">\t\t\t&#123;</div><div class=\"line\">\t            <span class=\"keyword\">int</span> bytesRead =<span class=\"number\">0</span>;</div><div class=\"line\">\t            <span class=\"keyword\">if</span>(gis == <span class=\"keyword\">null</span>)</div><div class=\"line\">\t            \tbytesRead= is.read(buffer);</div><div class=\"line\">\t            <span class=\"keyword\">else</span></div><div class=\"line\">\t            \tbytesRead= gis.read(buffer,<span class=\"number\">0</span>,buffer.length);</div><div class=\"line\">\t            <span class=\"keyword\">if</span> (bytesRead &gt;= <span class=\"number\">0</span>) </div><div class=\"line\">\t            &#123;</div><div class=\"line\">\t                os.write(buffer, <span class=\"number\">0</span>, bytesRead);</div><div class=\"line\">\t            &#125;</div><div class=\"line\">\t            <span class=\"keyword\">else</span></div><div class=\"line\">\t            &#123;</div><div class=\"line\">\t            \t<span class=\"keyword\">break</span>;</div><div class=\"line\">\t            &#125;</div><div class=\"line\">\t\t\t&#125;</div><div class=\"line\">            <span class=\"keyword\">if</span>(gis != <span class=\"keyword\">null</span>)</div><div class=\"line\">            \tgis.close();</div><div class=\"line\">\t\t\tis.close();</div><div class=\"line\">\t\t&#125;</div><div class=\"line\">\t\tos.close();</div><div class=\"line\">\t\t<span class=\"keyword\">if</span>(MainSvr.bExitFlag)</div><div class=\"line\">\t\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</div><div class=\"line\">\t\tSystem.out.println(<span class=\"string\">\"Success move \"</span> + localDirname + <span class=\"string\">\" to \"</span> + hdfsPath);</div><div class=\"line\">\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</div><div class=\"line\">\t&#125;</div><div class=\"line\">\t<span class=\"keyword\">catch</span> (Exception e)</div><div class=\"line\">\t&#123;</div><div class=\"line\">\t\te.printStackTrace();</div><div class=\"line\">\t\tlog.info(<span class=\"string\">\"putMerge error:\"</span> + e.getMessage());</div><div class=\"line\">\t&#125;</div><div class=\"line\">\t<span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">mkdir</span><span class=\"params\">(String dirName)</span></span></div><div class=\"line\"><span class=\"function\"></span>&#123;\t</div><div class=\"line\">\t<span class=\"keyword\">try</span></div><div class=\"line\">\t&#123;</div><div class=\"line\">\t\t<span class=\"keyword\">if</span> (checkFileExist(dirName))</div><div class=\"line\">\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;\t\t\t</div><div class=\"line\">\t\tPath f = <span class=\"keyword\">new</span> Path(dirName);</div><div class=\"line\">\t\tSystem.out.println(<span class=\"string\">\"Create and Write :\"</span> + dirName + <span class=\"string\">\" to hdfs\"</span>);</div><div class=\"line\">\t\t<span class=\"keyword\">return</span> hdfs.mkdirs(f);</div><div class=\"line\">\t&#125;</div><div class=\"line\">\t<span class=\"keyword\">catch</span> (Exception e)</div><div class=\"line\">\t&#123;</div><div class=\"line\">\t\tSystem.out.println(<span class=\"string\">\"mkdir Fail:\"</span> + e.getMessage());</div><div class=\"line\">\t\te.printStackTrace();\t\t</div><div class=\"line\">\t&#125;</div><div class=\"line\"></div><div class=\"line\">\t<span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<p><img src=\"http://or49tneld.bkt.clouddn.com/18-1-23/63075597.jpg\" alt=\"hdfs-commands\">  </p>\n<h1 id=\"HDFS到HDFS的合并\"><a href=\"#HDFS到HDFS的合并\" class=\"headerlink\" title=\"HDFS到HDFS的合并\"></a><strong>HDFS到HDFS的合并</strong></h1><p>hdfs提供了一种FileUtil.copyMerge（）的方法， 注意下面的 false 这个，如果改为true，就会删除这个目录，public void copyMerge(String folder, String file) {Path src = new Path(folder)…….;<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">copyMerge</span><span class=\"params\">(String folder, String file)</span> </span>&#123;</div><div class=\"line\"></div><div class=\"line\">\tPath src = <span class=\"keyword\">new</span> Path(folder);</div><div class=\"line\">\tPath dst = <span class=\"keyword\">new</span> Path(file);</div><div class=\"line\"></div><div class=\"line\">\t<span class=\"keyword\">try</span> &#123;</div><div class=\"line\">\t\tFileUtil.copyMerge(src.getFileSystem(conf), src,</div><div class=\"line\">\t\t\t\tdst.getFileSystem(conf), dst, <span class=\"keyword\">false</span>, conf, <span class=\"keyword\">null</span>);</div><div class=\"line\">\t&#125; <span class=\"keyword\">catch</span> (IOException e) &#123;</div><div class=\"line\">\t\t<span class=\"comment\">// TODO Auto-generated catch block</span></div><div class=\"line\">\t\te.printStackTrace();</div><div class=\"line\">\t&#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h1 id=\"上传文件到HDFS并且合并\"><a href=\"#上传文件到HDFS并且合并\" class=\"headerlink\" title=\"上传文件到HDFS并且合并\"></a><strong>上传文件到HDFS并且合并</strong></h1><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div><div class=\"line\">106</div><div class=\"line\">107</div><div class=\"line\">108</div><div class=\"line\">109</div><div class=\"line\">110</div><div class=\"line\">111</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\"> * 读取本地文件到HDFS系统&lt;br&gt;</span></div><div class=\"line\"><span class=\"comment\"> * 请保证文件格式一直是UTF-8，从本地-&gt;HDFS</span></div><div class=\"line\"><span class=\"comment\"> * </span></div><div class=\"line\"><span class=\"comment\"> */</span></div><div class=\"line\"><span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\"> * </span></div><div class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">@param</span> localDirname 源文件所在位置</span></div><div class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">@param</span> hdfsPath 要放在服务器的位置</span></div><div class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">@param</span> destFileName 要合并成的文件名称</span></div><div class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">@param</span> filter</span></div><div class=\"line\"><span class=\"comment\"> * <span class=\"doctag\">@return</span></span></div><div class=\"line\"><span class=\"comment\"> */</span></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">putMerge</span><span class=\"params\">(String localDirname, String hdfsPath,String destFileName, String filter )</span></span></div><div class=\"line\"><span class=\"function\"></span>&#123;</div><div class=\"line\">\t<span class=\"keyword\">try</span></div><div class=\"line\">\t&#123;</div><div class=\"line\">\t\tFile dir = <span class=\"keyword\">new</span> File(localDirname);</div><div class=\"line\">\t\t<span class=\"keyword\">if</span>(!dir.isDirectory())</div><div class=\"line\">\t\t&#123;</div><div class=\"line\">\t\t\tSystem.out.println(localDirname+<span class=\"string\">\"不是目录 \"</span>);</div><div class=\"line\">\t\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</div><div class=\"line\">\t\t&#125;</div><div class=\"line\"></div><div class=\"line\">\t\tFile[] files = dir.listFiles();</div><div class=\"line\">\t\t<span class=\"keyword\">if</span>(files.length ==<span class=\"number\">0</span>)</div><div class=\"line\">\t\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</div><div class=\"line\">\t\t</div><div class=\"line\">\t\tSystem.out.println(<span class=\"string\">\"Begin move \"</span> + localDirname + <span class=\"string\">\" to \"</span> + hdfsPath);</div><div class=\"line\">\t\t</div><div class=\"line\">\t\t<span class=\"keyword\">while</span>(checkFileExist(hdfsPath+<span class=\"string\">\"/\"</span>+destFileName))</div><div class=\"line\">\t\t&#123;</div><div class=\"line\">\t\t\t<span class=\"keyword\">if</span>(destFileName.contains(<span class=\"string\">\".x\"</span>))</div><div class=\"line\">\t\t\t\tdestFileName += <span class=\"string\">\"x\"</span>;</div><div class=\"line\">\t\t\t<span class=\"keyword\">else</span></div><div class=\"line\">\t\t\t\tdestFileName += <span class=\"string\">\".x\"</span>;</div><div class=\"line\">\t\t&#125;</div><div class=\"line\">\t\t</div><div class=\"line\">\t\tmkdir(hdfsPath);</div><div class=\"line\">\t\t</div><div class=\"line\">\t\tPath f = <span class=\"keyword\">new</span> Path(hdfsPath+<span class=\"string\">\"/\"</span>+destFileName);</div><div class=\"line\">\t\tFSDataOutputStream os = fs.create(f, <span class=\"keyword\">true</span>);</div><div class=\"line\">\t\t<span class=\"keyword\">byte</span>[] buffer = <span class=\"keyword\">new</span> <span class=\"keyword\">byte</span>[<span class=\"number\">10240000</span>];</div><div class=\"line\">\t\t\t</div><div class=\"line\">\t\t<span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i=<span class=\"number\">0</span>; i&lt;files.length; i++)</div><div class=\"line\">\t\t&#123;</div><div class=\"line\">\t\t\t<span class=\"keyword\">if</span>(MainSvr.bExitFlag == <span class=\"keyword\">true</span>)</div><div class=\"line\">\t\t\t\t<span class=\"keyword\">break</span>;\t</div><div class=\"line\">\t\t\t </div><div class=\"line\">\t\t\tFile file = files[i];</div><div class=\"line\">\t\t\t<span class=\"keyword\">if</span>(!file.getName().toLowerCase().contains(filter.toLowerCase()))</div><div class=\"line\">\t\t\t\t<span class=\"keyword\">continue</span>;</div><div class=\"line\">\t\t\tFileInputStream is = <span class=\"keyword\">new</span> FileInputStream(file);\t\t\t\t\t</div><div class=\"line\">\t\t\tGZIPInputStream gis = <span class=\"keyword\">null</span>;</div><div class=\"line\">\t\t\t<span class=\"keyword\">if</span>(file.getName().toLowerCase().endsWith(<span class=\"string\">\"gz\"</span>))</div><div class=\"line\">\t\t\t\tgis=<span class=\"keyword\">new</span> GZIPInputStream(is);</div><div class=\"line\">\t\t\t</div><div class=\"line\">\t\t\t<span class=\"keyword\">while</span>(MainSvr.bExitFlag != <span class=\"keyword\">true</span>)</div><div class=\"line\">\t\t\t&#123;</div><div class=\"line\">\t            <span class=\"keyword\">int</span> bytesRead =<span class=\"number\">0</span>;</div><div class=\"line\">\t            <span class=\"keyword\">if</span>(gis == <span class=\"keyword\">null</span>)</div><div class=\"line\">\t            \tbytesRead= is.read(buffer);</div><div class=\"line\">\t            <span class=\"keyword\">else</span></div><div class=\"line\">\t            \tbytesRead= gis.read(buffer,<span class=\"number\">0</span>,buffer.length);</div><div class=\"line\">\t            <span class=\"keyword\">if</span> (bytesRead &gt;= <span class=\"number\">0</span>) </div><div class=\"line\">\t            &#123;</div><div class=\"line\">\t                os.write(buffer, <span class=\"number\">0</span>, bytesRead);</div><div class=\"line\">\t            &#125;</div><div class=\"line\">\t            <span class=\"keyword\">else</span></div><div class=\"line\">\t            &#123;</div><div class=\"line\">\t            \t<span class=\"keyword\">break</span>;</div><div class=\"line\">\t            &#125;</div><div class=\"line\">\t\t\t&#125;</div><div class=\"line\">            <span class=\"keyword\">if</span>(gis != <span class=\"keyword\">null</span>)</div><div class=\"line\">            \tgis.close();</div><div class=\"line\">\t\t\tis.close();</div><div class=\"line\">\t\t&#125;</div><div class=\"line\">\t\tos.close();</div><div class=\"line\">\t\t<span class=\"keyword\">if</span>(MainSvr.bExitFlag)</div><div class=\"line\">\t\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</div><div class=\"line\">\t\tSystem.out.println(<span class=\"string\">\"Success move \"</span> + localDirname + <span class=\"string\">\" to \"</span> + hdfsPath);</div><div class=\"line\">\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</div><div class=\"line\">\t&#125;</div><div class=\"line\">\t<span class=\"keyword\">catch</span> (Exception e)</div><div class=\"line\">\t&#123;</div><div class=\"line\">\t\te.printStackTrace();</div><div class=\"line\">\t\tlog.info(<span class=\"string\">\"putMerge error:\"</span> + e.getMessage());</div><div class=\"line\">\t&#125;</div><div class=\"line\">\t<span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">mkdir</span><span class=\"params\">(String dirName)</span></span></div><div class=\"line\"><span class=\"function\"></span>&#123;\t</div><div class=\"line\">\t<span class=\"keyword\">try</span></div><div class=\"line\">\t&#123;</div><div class=\"line\">\t\t<span class=\"keyword\">if</span> (checkFileExist(dirName))</div><div class=\"line\">\t\t<span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;\t\t\t</div><div class=\"line\">\t\tPath f = <span class=\"keyword\">new</span> Path(dirName);</div><div class=\"line\">\t\tSystem.out.println(<span class=\"string\">\"Create and Write :\"</span> + dirName + <span class=\"string\">\" to hdfs\"</span>);</div><div class=\"line\">\t\t<span class=\"keyword\">return</span> hdfs.mkdirs(f);</div><div class=\"line\">\t&#125;</div><div class=\"line\">\t<span class=\"keyword\">catch</span> (Exception e)</div><div class=\"line\">\t&#123;</div><div class=\"line\">\t\tSystem.out.println(<span class=\"string\">\"mkdir Fail:\"</span> + e.getMessage());</div><div class=\"line\">\t\te.printStackTrace();\t\t</div><div class=\"line\">\t&#125;</div><div class=\"line\"></div><div class=\"line\">\t<span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>"},{"title":"Hadoop入门案例（七）之TOP K","date":"2016-08-05T13:25:21.000Z","author":"kaishun","id":"20","blogexcerpt":"目的：找出数据集中的top k，二. 思路1. 最开始是快速排序或者归并排序2. 其次就是wordcount，然后再进行一遍mapReduce 3. 先排序，再取前k个2.2 在mapper阶段，找出本地的top k, 然后所有的独立的top k集合在reduce中运算","_content":"\n\n## **一. 目的：**\n找出数据集中的top k，\n\n\n\n\n## **二. 思路**\n### 2.1 全排序，取前 k 个\n1. 最开始是快速排序或者归并排序\n2. 其次就是wordcount，然后再进行一遍mapReduce\n3. 先排序，再取前k个\n\n### 2.2 在mapper阶段，找出本地的top k, 然后所有的独立的top k集合在reduce中运算  \n因为k 一般比较小，所以我们只需要一个reduce来处理最后的运算\n          \n### 2.3 流程图如下  \n![topk](http://or49tneld.bkt.clouddn.com/17-6-25/82113418.jpg)\n\n### **三. 代码**\n代码中利用了 treemap 来获取前k个，Treemap 参考 http://blog.csdn.net/chenssy/article/details/26668941  \n其实也可以用其他的一些结构，例如参考这篇文章， 使用的是数组 https://my.oschina.net/u/1378204/blog/343666   \n\n```java\npackage com.myhadoop.mapreduce.test;\nimport java.io.IOException;\nimport java.util.StringTokenizer;\nimport java.util.TreeMap;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\npublic class TopN {\n    public static class TopTenMapper extends\n            Mapper<Object, Text, NullWritable, IntWritable> {\n        private TreeMap<Integer, String> repToRecordMap = new TreeMap<Integer, String>();\n\n        public void map(Object key, Text value, Context context) {\n            int N = 10; //默认为Top10\n            N = Integer.parseInt(context.getConfiguration().get(\"N\"));\n            StringTokenizer itr = new StringTokenizer(value.toString());\n            while (itr.hasMoreTokens()) {\n                repToRecordMap.put(Integer.parseInt(itr.nextToken()), \" \");\n                if (repToRecordMap.size() > N) {\n                    repToRecordMap.remove(repToRecordMap.firstKey());\n                }\n            }\n        }\n\n\n\n        protected void cleanup(Context context) {\n            for (Integer i : repToRecordMap.keySet()) {\n                try {\n                    context.write(NullWritable.get(), new IntWritable(i));\n                } catch (Exception e) {\n                    e.printStackTrace();\n                }\n            }\n        }\n    }\n\n    public static class TopTenReducer extends\n            Reducer<NullWritable, IntWritable, NullWritable, IntWritable> {\n        private TreeMap<Integer, String> repToRecordMap = new TreeMap<Integer, String>();\n\n        public void reduce(NullWritable key, Iterable<IntWritable> values,\n                           Context context) throws IOException, InterruptedException {\n            int N = 10; //默认为Top10\n            N = Integer.parseInt(context.getConfiguration().get(\"N\"));\n            for (IntWritable value : values) {\n                repToRecordMap.put(value.get(), \" \");\n                if (repToRecordMap.size() > N) {\n                    repToRecordMap.remove(repToRecordMap.firstKey());\n                }\n            }\n            for (Integer i : repToRecordMap.descendingMap().keySet()) {\n                context.write(NullWritable.get(), new IntWritable(i));\n            }\n        }\n\n    }\n\n    public static void main(String[] args) throws Exception {\n        if (args.length != 3) {\n            throw new IllegalArgumentException(\n                    \"!!!!!!!!!!!!!! Usage!!!!!!!!!!!!!!: hadoop jar <jar-name> \"\n                            + \"TopN.TopN \"\n                            + \"<the value of N>\"\n                            + \"<input-path> \"\n                            + \"<output-path>\");\n        }\n        Configuration conf = new Configuration();\n        conf.set(\"N\", args[0]);\n        Job job = Job.getInstance(conf, \"TopN\");\n        job.setJobName(\"TopN\");\n        Path inputPath = new Path(args[1]);\n        Path outputPath = new Path(args[2]);\n        FileInputFormat.setInputPaths(job, inputPath);\n        FileOutputFormat.setOutputPath(job, outputPath);\n        job.setJarByClass(TopN.class);\n        job.setMapperClass(TopTenMapper.class);\n        job.setReducerClass(TopTenReducer.class);\n        job.setNumReduceTasks(1);  //reduce Num 设置成1\n\n        job.setMapOutputKeyClass(NullWritable.class);// map阶段的输出的key\n        job.setMapOutputValueClass(IntWritable.class);// map阶段的输出的value\n\n        job.setOutputKeyClass(NullWritable.class);// reduce阶段的输出的key\n        job.setOutputValueClass(IntWritable.class);// reduce阶段的输出的value\n\n        System.exit(job.waitForCompletion(true) ? 0 : 1);\n    }\n\n}\n```\n\n\n## **四. 结果**\n上述代码经过测试，能返回Top k条记录\n\n## **五. 性能分析**\n\n上述代码使用的是第二种思路，避免了第一种思路的全排序，但是注意到，我们只能用一个reduce，如果数据量特别大，k也非常大，单一的reduce可能会出现一些问题    \n1. 这个reducer的主机需要通过网络获取大量数据，会造成单一节点工作负荷太大。  \n2. reducer的内存中可能会出现java虚拟机内存不足    \n3. 写文件不是并行的，当数据规模很大的时候，这种思路会导致效率变得很低\n\n\n  \n参考自 mappreduce 设计模式","source":"_posts/Hadoop入门案例（七）之TOP K.md","raw":"---\ntitle: Hadoop入门案例（七）之TOP K\ndate: 2016-08-05 21:25:21\ntags: [hadoop]\ncategories: [大数据,hadoop]\nauthor: kaishun\nid: 20\npermalink: hadoop-example-7\nblogexcerpt: 目的：找出数据集中的top k，二. 思路1. 最开始是快速排序或者归并排序2. 其次就是wordcount，然后再进行一遍mapReduce 3. 先排序，再取前k个2.2 在mapper阶段，找出本地的top k, 然后所有的独立的top k集合在reduce中运算\n---\n\n\n## **一. 目的：**\n找出数据集中的top k，\n\n\n\n\n## **二. 思路**\n### 2.1 全排序，取前 k 个\n1. 最开始是快速排序或者归并排序\n2. 其次就是wordcount，然后再进行一遍mapReduce\n3. 先排序，再取前k个\n\n### 2.2 在mapper阶段，找出本地的top k, 然后所有的独立的top k集合在reduce中运算  \n因为k 一般比较小，所以我们只需要一个reduce来处理最后的运算\n          \n### 2.3 流程图如下  \n![topk](http://or49tneld.bkt.clouddn.com/17-6-25/82113418.jpg)\n\n### **三. 代码**\n代码中利用了 treemap 来获取前k个，Treemap 参考 http://blog.csdn.net/chenssy/article/details/26668941  \n其实也可以用其他的一些结构，例如参考这篇文章， 使用的是数组 https://my.oschina.net/u/1378204/blog/343666   \n\n```java\npackage com.myhadoop.mapreduce.test;\nimport java.io.IOException;\nimport java.util.StringTokenizer;\nimport java.util.TreeMap;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\npublic class TopN {\n    public static class TopTenMapper extends\n            Mapper<Object, Text, NullWritable, IntWritable> {\n        private TreeMap<Integer, String> repToRecordMap = new TreeMap<Integer, String>();\n\n        public void map(Object key, Text value, Context context) {\n            int N = 10; //默认为Top10\n            N = Integer.parseInt(context.getConfiguration().get(\"N\"));\n            StringTokenizer itr = new StringTokenizer(value.toString());\n            while (itr.hasMoreTokens()) {\n                repToRecordMap.put(Integer.parseInt(itr.nextToken()), \" \");\n                if (repToRecordMap.size() > N) {\n                    repToRecordMap.remove(repToRecordMap.firstKey());\n                }\n            }\n        }\n\n\n\n        protected void cleanup(Context context) {\n            for (Integer i : repToRecordMap.keySet()) {\n                try {\n                    context.write(NullWritable.get(), new IntWritable(i));\n                } catch (Exception e) {\n                    e.printStackTrace();\n                }\n            }\n        }\n    }\n\n    public static class TopTenReducer extends\n            Reducer<NullWritable, IntWritable, NullWritable, IntWritable> {\n        private TreeMap<Integer, String> repToRecordMap = new TreeMap<Integer, String>();\n\n        public void reduce(NullWritable key, Iterable<IntWritable> values,\n                           Context context) throws IOException, InterruptedException {\n            int N = 10; //默认为Top10\n            N = Integer.parseInt(context.getConfiguration().get(\"N\"));\n            for (IntWritable value : values) {\n                repToRecordMap.put(value.get(), \" \");\n                if (repToRecordMap.size() > N) {\n                    repToRecordMap.remove(repToRecordMap.firstKey());\n                }\n            }\n            for (Integer i : repToRecordMap.descendingMap().keySet()) {\n                context.write(NullWritable.get(), new IntWritable(i));\n            }\n        }\n\n    }\n\n    public static void main(String[] args) throws Exception {\n        if (args.length != 3) {\n            throw new IllegalArgumentException(\n                    \"!!!!!!!!!!!!!! Usage!!!!!!!!!!!!!!: hadoop jar <jar-name> \"\n                            + \"TopN.TopN \"\n                            + \"<the value of N>\"\n                            + \"<input-path> \"\n                            + \"<output-path>\");\n        }\n        Configuration conf = new Configuration();\n        conf.set(\"N\", args[0]);\n        Job job = Job.getInstance(conf, \"TopN\");\n        job.setJobName(\"TopN\");\n        Path inputPath = new Path(args[1]);\n        Path outputPath = new Path(args[2]);\n        FileInputFormat.setInputPaths(job, inputPath);\n        FileOutputFormat.setOutputPath(job, outputPath);\n        job.setJarByClass(TopN.class);\n        job.setMapperClass(TopTenMapper.class);\n        job.setReducerClass(TopTenReducer.class);\n        job.setNumReduceTasks(1);  //reduce Num 设置成1\n\n        job.setMapOutputKeyClass(NullWritable.class);// map阶段的输出的key\n        job.setMapOutputValueClass(IntWritable.class);// map阶段的输出的value\n\n        job.setOutputKeyClass(NullWritable.class);// reduce阶段的输出的key\n        job.setOutputValueClass(IntWritable.class);// reduce阶段的输出的value\n\n        System.exit(job.waitForCompletion(true) ? 0 : 1);\n    }\n\n}\n```\n\n\n## **四. 结果**\n上述代码经过测试，能返回Top k条记录\n\n## **五. 性能分析**\n\n上述代码使用的是第二种思路，避免了第一种思路的全排序，但是注意到，我们只能用一个reduce，如果数据量特别大，k也非常大，单一的reduce可能会出现一些问题    \n1. 这个reducer的主机需要通过网络获取大量数据，会造成单一节点工作负荷太大。  \n2. reducer的内存中可能会出现java虚拟机内存不足    \n3. 写文件不是并行的，当数据规模很大的时候，这种思路会导致效率变得很低\n\n\n  \n参考自 mappreduce 设计模式","slug":"hadoop-example-7","published":1,"updated":"2018-01-23T14:14:13.441Z","_id":"cjcrpnzoj000q2wv3j1q8mhvm","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"一-目的：\"><a href=\"#一-目的：\" class=\"headerlink\" title=\"一. 目的：\"></a><strong>一. 目的：</strong></h2><p>找出数据集中的top k，</p>\n<h2 id=\"二-思路\"><a href=\"#二-思路\" class=\"headerlink\" title=\"二. 思路\"></a><strong>二. 思路</strong></h2><h3 id=\"2-1-全排序，取前-k-个\"><a href=\"#2-1-全排序，取前-k-个\" class=\"headerlink\" title=\"2.1 全排序，取前 k 个\"></a>2.1 全排序，取前 k 个</h3><ol>\n<li>最开始是快速排序或者归并排序</li>\n<li>其次就是wordcount，然后再进行一遍mapReduce</li>\n<li>先排序，再取前k个</li>\n</ol>\n<h3 id=\"2-2-在mapper阶段，找出本地的top-k-然后所有的独立的top-k集合在reduce中运算\"><a href=\"#2-2-在mapper阶段，找出本地的top-k-然后所有的独立的top-k集合在reduce中运算\" class=\"headerlink\" title=\"2.2 在mapper阶段，找出本地的top k, 然后所有的独立的top k集合在reduce中运算\"></a>2.2 在mapper阶段，找出本地的top k, 然后所有的独立的top k集合在reduce中运算</h3><p>因为k 一般比较小，所以我们只需要一个reduce来处理最后的运算</p>\n<h3 id=\"2-3-流程图如下\"><a href=\"#2-3-流程图如下\" class=\"headerlink\" title=\"2.3 流程图如下\"></a>2.3 流程图如下</h3><p><img src=\"http://or49tneld.bkt.clouddn.com/17-6-25/82113418.jpg\" alt=\"topk\"></p>\n<h3 id=\"三-代码\"><a href=\"#三-代码\" class=\"headerlink\" title=\"三. 代码\"></a><strong>三. 代码</strong></h3><p>代码中利用了 treemap 来获取前k个，Treemap 参考 <a href=\"http://blog.csdn.net/chenssy/article/details/26668941\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/chenssy/article/details/26668941</a><br>其实也可以用其他的一些结构，例如参考这篇文章， 使用的是数组 <a href=\"https://my.oschina.net/u/1378204/blog/343666\" target=\"_blank\" rel=\"external\">https://my.oschina.net/u/1378204/blog/343666</a>   </p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">package</span> com.myhadoop.mapreduce.test;</div><div class=\"line\"><span class=\"keyword\">import</span> java.io.IOException;</div><div class=\"line\"><span class=\"keyword\">import</span> java.util.StringTokenizer;</div><div class=\"line\"><span class=\"keyword\">import</span> java.util.TreeMap;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.conf.Configuration;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.fs.Path;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.IntWritable;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.NullWritable;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.Text;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Job;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Mapper;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Reducer;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TopN</span> </span>&#123;</div><div class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TopTenMapper</span> <span class=\"keyword\">extends</span></span></div><div class=\"line\"><span class=\"class\">            <span class=\"title\">Mapper</span>&lt;<span class=\"title\">Object</span>, <span class=\"title\">Text</span>, <span class=\"title\">NullWritable</span>, <span class=\"title\">IntWritable</span>&gt; </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">private</span> TreeMap&lt;Integer, String&gt; repToRecordMap = <span class=\"keyword\">new</span> TreeMap&lt;Integer, String&gt;();</div><div class=\"line\"></div><div class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">map</span><span class=\"params\">(Object key, Text value, Context context)</span> </span>&#123;</div><div class=\"line\">            <span class=\"keyword\">int</span> N = <span class=\"number\">10</span>; <span class=\"comment\">//默认为Top10</span></div><div class=\"line\">            N = Integer.parseInt(context.getConfiguration().get(<span class=\"string\">\"N\"</span>));</div><div class=\"line\">            StringTokenizer itr = <span class=\"keyword\">new</span> StringTokenizer(value.toString());</div><div class=\"line\">            <span class=\"keyword\">while</span> (itr.hasMoreTokens()) &#123;</div><div class=\"line\">                repToRecordMap.put(Integer.parseInt(itr.nextToken()), <span class=\"string\">\" \"</span>);</div><div class=\"line\">                <span class=\"keyword\">if</span> (repToRecordMap.size() &gt; N) &#123;</div><div class=\"line\">                    repToRecordMap.remove(repToRecordMap.firstKey());</div><div class=\"line\">                &#125;</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">        <span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title\">cleanup</span><span class=\"params\">(Context context)</span> </span>&#123;</div><div class=\"line\">            <span class=\"keyword\">for</span> (Integer i : repToRecordMap.keySet()) &#123;</div><div class=\"line\">                <span class=\"keyword\">try</span> &#123;</div><div class=\"line\">                    context.write(NullWritable.get(), <span class=\"keyword\">new</span> IntWritable(i));</div><div class=\"line\">                &#125; <span class=\"keyword\">catch</span> (Exception e) &#123;</div><div class=\"line\">                    e.printStackTrace();</div><div class=\"line\">                &#125;</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TopTenReducer</span> <span class=\"keyword\">extends</span></span></div><div class=\"line\"><span class=\"class\">            <span class=\"title\">Reducer</span>&lt;<span class=\"title\">NullWritable</span>, <span class=\"title\">IntWritable</span>, <span class=\"title\">NullWritable</span>, <span class=\"title\">IntWritable</span>&gt; </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">private</span> TreeMap&lt;Integer, String&gt; repToRecordMap = <span class=\"keyword\">new</span> TreeMap&lt;Integer, String&gt;();</div><div class=\"line\"></div><div class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">reduce</span><span class=\"params\">(NullWritable key, Iterable&lt;IntWritable&gt; values,</span></span></div><div class=\"line\"><span class=\"function\"><span class=\"params\">                           Context context)</span> <span class=\"keyword\">throws</span> IOException, InterruptedException </span>&#123;</div><div class=\"line\">            <span class=\"keyword\">int</span> N = <span class=\"number\">10</span>; <span class=\"comment\">//默认为Top10</span></div><div class=\"line\">            N = Integer.parseInt(context.getConfiguration().get(<span class=\"string\">\"N\"</span>));</div><div class=\"line\">            <span class=\"keyword\">for</span> (IntWritable value : values) &#123;</div><div class=\"line\">                repToRecordMap.put(value.get(), <span class=\"string\">\" \"</span>);</div><div class=\"line\">                <span class=\"keyword\">if</span> (repToRecordMap.size() &gt; N) &#123;</div><div class=\"line\">                    repToRecordMap.remove(repToRecordMap.firstKey());</div><div class=\"line\">                &#125;</div><div class=\"line\">            &#125;</div><div class=\"line\">            <span class=\"keyword\">for</span> (Integer i : repToRecordMap.descendingMap().keySet()) &#123;</div><div class=\"line\">                context.write(NullWritable.get(), <span class=\"keyword\">new</span> IntWritable(i));</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;</div><div class=\"line\"></div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">if</span> (args.length != <span class=\"number\">3</span>) &#123;</div><div class=\"line\">            <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> IllegalArgumentException(</div><div class=\"line\">                    <span class=\"string\">\"!!!!!!!!!!!!!! Usage!!!!!!!!!!!!!!: hadoop jar &lt;jar-name&gt; \"</span></div><div class=\"line\">                            + <span class=\"string\">\"TopN.TopN \"</span></div><div class=\"line\">                            + <span class=\"string\">\"&lt;the value of N&gt;\"</span></div><div class=\"line\">                            + <span class=\"string\">\"&lt;input-path&gt; \"</span></div><div class=\"line\">                            + <span class=\"string\">\"&lt;output-path&gt;\"</span>);</div><div class=\"line\">        &#125;</div><div class=\"line\">        Configuration conf = <span class=\"keyword\">new</span> Configuration();</div><div class=\"line\">        conf.set(<span class=\"string\">\"N\"</span>, args[<span class=\"number\">0</span>]);</div><div class=\"line\">        Job job = Job.getInstance(conf, <span class=\"string\">\"TopN\"</span>);</div><div class=\"line\">        job.setJobName(<span class=\"string\">\"TopN\"</span>);</div><div class=\"line\">        Path inputPath = <span class=\"keyword\">new</span> Path(args[<span class=\"number\">1</span>]);</div><div class=\"line\">        Path outputPath = <span class=\"keyword\">new</span> Path(args[<span class=\"number\">2</span>]);</div><div class=\"line\">        FileInputFormat.setInputPaths(job, inputPath);</div><div class=\"line\">        FileOutputFormat.setOutputPath(job, outputPath);</div><div class=\"line\">        job.setJarByClass(TopN.class);</div><div class=\"line\">        job.setMapperClass(TopTenMapper.class);</div><div class=\"line\">        job.setReducerClass(TopTenReducer.class);</div><div class=\"line\">        job.setNumReduceTasks(<span class=\"number\">1</span>);  <span class=\"comment\">//reduce Num 设置成1</span></div><div class=\"line\"></div><div class=\"line\">        job.setMapOutputKeyClass(NullWritable.class);<span class=\"comment\">// map阶段的输出的key</span></div><div class=\"line\">        job.setMapOutputValueClass(IntWritable.class);<span class=\"comment\">// map阶段的输出的value</span></div><div class=\"line\"></div><div class=\"line\">        job.setOutputKeyClass(NullWritable.class);<span class=\"comment\">// reduce阶段的输出的key</span></div><div class=\"line\">        job.setOutputValueClass(IntWritable.class);<span class=\"comment\">// reduce阶段的输出的value</span></div><div class=\"line\"></div><div class=\"line\">        System.exit(job.waitForCompletion(<span class=\"keyword\">true</span>) ? <span class=\"number\">0</span> : <span class=\"number\">1</span>);</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h2 id=\"四-结果\"><a href=\"#四-结果\" class=\"headerlink\" title=\"四. 结果\"></a><strong>四. 结果</strong></h2><p>上述代码经过测试，能返回Top k条记录</p>\n<h2 id=\"五-性能分析\"><a href=\"#五-性能分析\" class=\"headerlink\" title=\"五. 性能分析\"></a><strong>五. 性能分析</strong></h2><p>上述代码使用的是第二种思路，避免了第一种思路的全排序，但是注意到，我们只能用一个reduce，如果数据量特别大，k也非常大，单一的reduce可能会出现一些问题    </p>\n<ol>\n<li>这个reducer的主机需要通过网络获取大量数据，会造成单一节点工作负荷太大。  </li>\n<li>reducer的内存中可能会出现java虚拟机内存不足    </li>\n<li>写文件不是并行的，当数据规模很大的时候，这种思路会导致效率变得很低</li>\n</ol>\n<p>参考自 mappreduce 设计模式</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"一-目的：\"><a href=\"#一-目的：\" class=\"headerlink\" title=\"一. 目的：\"></a><strong>一. 目的：</strong></h2><p>找出数据集中的top k，</p>\n<h2 id=\"二-思路\"><a href=\"#二-思路\" class=\"headerlink\" title=\"二. 思路\"></a><strong>二. 思路</strong></h2><h3 id=\"2-1-全排序，取前-k-个\"><a href=\"#2-1-全排序，取前-k-个\" class=\"headerlink\" title=\"2.1 全排序，取前 k 个\"></a>2.1 全排序，取前 k 个</h3><ol>\n<li>最开始是快速排序或者归并排序</li>\n<li>其次就是wordcount，然后再进行一遍mapReduce</li>\n<li>先排序，再取前k个</li>\n</ol>\n<h3 id=\"2-2-在mapper阶段，找出本地的top-k-然后所有的独立的top-k集合在reduce中运算\"><a href=\"#2-2-在mapper阶段，找出本地的top-k-然后所有的独立的top-k集合在reduce中运算\" class=\"headerlink\" title=\"2.2 在mapper阶段，找出本地的top k, 然后所有的独立的top k集合在reduce中运算\"></a>2.2 在mapper阶段，找出本地的top k, 然后所有的独立的top k集合在reduce中运算</h3><p>因为k 一般比较小，所以我们只需要一个reduce来处理最后的运算</p>\n<h3 id=\"2-3-流程图如下\"><a href=\"#2-3-流程图如下\" class=\"headerlink\" title=\"2.3 流程图如下\"></a>2.3 流程图如下</h3><p><img src=\"http://or49tneld.bkt.clouddn.com/17-6-25/82113418.jpg\" alt=\"topk\"></p>\n<h3 id=\"三-代码\"><a href=\"#三-代码\" class=\"headerlink\" title=\"三. 代码\"></a><strong>三. 代码</strong></h3><p>代码中利用了 treemap 来获取前k个，Treemap 参考 <a href=\"http://blog.csdn.net/chenssy/article/details/26668941\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/chenssy/article/details/26668941</a><br>其实也可以用其他的一些结构，例如参考这篇文章， 使用的是数组 <a href=\"https://my.oschina.net/u/1378204/blog/343666\" target=\"_blank\" rel=\"external\">https://my.oschina.net/u/1378204/blog/343666</a>   </p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">package</span> com.myhadoop.mapreduce.test;</div><div class=\"line\"><span class=\"keyword\">import</span> java.io.IOException;</div><div class=\"line\"><span class=\"keyword\">import</span> java.util.StringTokenizer;</div><div class=\"line\"><span class=\"keyword\">import</span> java.util.TreeMap;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.conf.Configuration;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.fs.Path;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.IntWritable;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.NullWritable;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.Text;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Job;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Mapper;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Reducer;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TopN</span> </span>&#123;</div><div class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TopTenMapper</span> <span class=\"keyword\">extends</span></span></div><div class=\"line\"><span class=\"class\">            <span class=\"title\">Mapper</span>&lt;<span class=\"title\">Object</span>, <span class=\"title\">Text</span>, <span class=\"title\">NullWritable</span>, <span class=\"title\">IntWritable</span>&gt; </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">private</span> TreeMap&lt;Integer, String&gt; repToRecordMap = <span class=\"keyword\">new</span> TreeMap&lt;Integer, String&gt;();</div><div class=\"line\"></div><div class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">map</span><span class=\"params\">(Object key, Text value, Context context)</span> </span>&#123;</div><div class=\"line\">            <span class=\"keyword\">int</span> N = <span class=\"number\">10</span>; <span class=\"comment\">//默认为Top10</span></div><div class=\"line\">            N = Integer.parseInt(context.getConfiguration().get(<span class=\"string\">\"N\"</span>));</div><div class=\"line\">            StringTokenizer itr = <span class=\"keyword\">new</span> StringTokenizer(value.toString());</div><div class=\"line\">            <span class=\"keyword\">while</span> (itr.hasMoreTokens()) &#123;</div><div class=\"line\">                repToRecordMap.put(Integer.parseInt(itr.nextToken()), <span class=\"string\">\" \"</span>);</div><div class=\"line\">                <span class=\"keyword\">if</span> (repToRecordMap.size() &gt; N) &#123;</div><div class=\"line\">                    repToRecordMap.remove(repToRecordMap.firstKey());</div><div class=\"line\">                &#125;</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">        <span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"keyword\">void</span> <span class=\"title\">cleanup</span><span class=\"params\">(Context context)</span> </span>&#123;</div><div class=\"line\">            <span class=\"keyword\">for</span> (Integer i : repToRecordMap.keySet()) &#123;</div><div class=\"line\">                <span class=\"keyword\">try</span> &#123;</div><div class=\"line\">                    context.write(NullWritable.get(), <span class=\"keyword\">new</span> IntWritable(i));</div><div class=\"line\">                &#125; <span class=\"keyword\">catch</span> (Exception e) &#123;</div><div class=\"line\">                    e.printStackTrace();</div><div class=\"line\">                &#125;</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TopTenReducer</span> <span class=\"keyword\">extends</span></span></div><div class=\"line\"><span class=\"class\">            <span class=\"title\">Reducer</span>&lt;<span class=\"title\">NullWritable</span>, <span class=\"title\">IntWritable</span>, <span class=\"title\">NullWritable</span>, <span class=\"title\">IntWritable</span>&gt; </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">private</span> TreeMap&lt;Integer, String&gt; repToRecordMap = <span class=\"keyword\">new</span> TreeMap&lt;Integer, String&gt;();</div><div class=\"line\"></div><div class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">reduce</span><span class=\"params\">(NullWritable key, Iterable&lt;IntWritable&gt; values,</span></span></div><div class=\"line\"><span class=\"function\"><span class=\"params\">                           Context context)</span> <span class=\"keyword\">throws</span> IOException, InterruptedException </span>&#123;</div><div class=\"line\">            <span class=\"keyword\">int</span> N = <span class=\"number\">10</span>; <span class=\"comment\">//默认为Top10</span></div><div class=\"line\">            N = Integer.parseInt(context.getConfiguration().get(<span class=\"string\">\"N\"</span>));</div><div class=\"line\">            <span class=\"keyword\">for</span> (IntWritable value : values) &#123;</div><div class=\"line\">                repToRecordMap.put(value.get(), <span class=\"string\">\" \"</span>);</div><div class=\"line\">                <span class=\"keyword\">if</span> (repToRecordMap.size() &gt; N) &#123;</div><div class=\"line\">                    repToRecordMap.remove(repToRecordMap.firstKey());</div><div class=\"line\">                &#125;</div><div class=\"line\">            &#125;</div><div class=\"line\">            <span class=\"keyword\">for</span> (Integer i : repToRecordMap.descendingMap().keySet()) &#123;</div><div class=\"line\">                context.write(NullWritable.get(), <span class=\"keyword\">new</span> IntWritable(i));</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;</div><div class=\"line\"></div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">if</span> (args.length != <span class=\"number\">3</span>) &#123;</div><div class=\"line\">            <span class=\"keyword\">throw</span> <span class=\"keyword\">new</span> IllegalArgumentException(</div><div class=\"line\">                    <span class=\"string\">\"!!!!!!!!!!!!!! Usage!!!!!!!!!!!!!!: hadoop jar &lt;jar-name&gt; \"</span></div><div class=\"line\">                            + <span class=\"string\">\"TopN.TopN \"</span></div><div class=\"line\">                            + <span class=\"string\">\"&lt;the value of N&gt;\"</span></div><div class=\"line\">                            + <span class=\"string\">\"&lt;input-path&gt; \"</span></div><div class=\"line\">                            + <span class=\"string\">\"&lt;output-path&gt;\"</span>);</div><div class=\"line\">        &#125;</div><div class=\"line\">        Configuration conf = <span class=\"keyword\">new</span> Configuration();</div><div class=\"line\">        conf.set(<span class=\"string\">\"N\"</span>, args[<span class=\"number\">0</span>]);</div><div class=\"line\">        Job job = Job.getInstance(conf, <span class=\"string\">\"TopN\"</span>);</div><div class=\"line\">        job.setJobName(<span class=\"string\">\"TopN\"</span>);</div><div class=\"line\">        Path inputPath = <span class=\"keyword\">new</span> Path(args[<span class=\"number\">1</span>]);</div><div class=\"line\">        Path outputPath = <span class=\"keyword\">new</span> Path(args[<span class=\"number\">2</span>]);</div><div class=\"line\">        FileInputFormat.setInputPaths(job, inputPath);</div><div class=\"line\">        FileOutputFormat.setOutputPath(job, outputPath);</div><div class=\"line\">        job.setJarByClass(TopN.class);</div><div class=\"line\">        job.setMapperClass(TopTenMapper.class);</div><div class=\"line\">        job.setReducerClass(TopTenReducer.class);</div><div class=\"line\">        job.setNumReduceTasks(<span class=\"number\">1</span>);  <span class=\"comment\">//reduce Num 设置成1</span></div><div class=\"line\"></div><div class=\"line\">        job.setMapOutputKeyClass(NullWritable.class);<span class=\"comment\">// map阶段的输出的key</span></div><div class=\"line\">        job.setMapOutputValueClass(IntWritable.class);<span class=\"comment\">// map阶段的输出的value</span></div><div class=\"line\"></div><div class=\"line\">        job.setOutputKeyClass(NullWritable.class);<span class=\"comment\">// reduce阶段的输出的key</span></div><div class=\"line\">        job.setOutputValueClass(IntWritable.class);<span class=\"comment\">// reduce阶段的输出的value</span></div><div class=\"line\"></div><div class=\"line\">        System.exit(job.waitForCompletion(<span class=\"keyword\">true</span>) ? <span class=\"number\">0</span> : <span class=\"number\">1</span>);</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h2 id=\"四-结果\"><a href=\"#四-结果\" class=\"headerlink\" title=\"四. 结果\"></a><strong>四. 结果</strong></h2><p>上述代码经过测试，能返回Top k条记录</p>\n<h2 id=\"五-性能分析\"><a href=\"#五-性能分析\" class=\"headerlink\" title=\"五. 性能分析\"></a><strong>五. 性能分析</strong></h2><p>上述代码使用的是第二种思路，避免了第一种思路的全排序，但是注意到，我们只能用一个reduce，如果数据量特别大，k也非常大，单一的reduce可能会出现一些问题    </p>\n<ol>\n<li>这个reducer的主机需要通过网络获取大量数据，会造成单一节点工作负荷太大。  </li>\n<li>reducer的内存中可能会出现java虚拟机内存不足    </li>\n<li>写文件不是并行的，当数据规模很大的时候，这种思路会导致效率变得很低</li>\n</ol>\n<p>参考自 mappreduce 设计模式</p>\n"},{"title":"Hadoop入门案例（五）全排序之TotalOrderPartitioner工具类+自动采样","date":"2016-08-02T13:25:21.000Z","author":"kaishun","id":"18","_content":"\n\n\n**为什么用这种方法** \n我们之前的是自定义分区的，但是如果我们不知道数据的分布，手动分区不太容易，稍有不慎，会导致数据倾斜较大，这时候，我们应该使用采样点进行排序，本文使用的是 Hadoop内置的名为 TotalOrderPartitioner 的全排序，采样器使用的是 InputSampler.Sampler，关键解释已经存在于代码之中。 文章参考了    \n国外  http://blog.ditullio.fr/2016/01/04/hadoop-basics-total-order-sorting-mapreduce/#The_TotalOrderPartitioner    \n国内  http://www.cnblogs.com/one--way/p/5931308.html\n<!-- more -->\n## **代码**\n```\npackage com.myhadoop.mapreduce.test;\nimport org.apache.hadoop.conf.Configured;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapred.lib.InputSampler;\nimport org.apache.hadoop.mapred.lib.TotalOrderPartitioner;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.util.Tool;\nimport org.apache.hadoop.util.ToolRunner;\nimport java.io.IOException;\n\n/**\n * Created by kaishun on 2017/6/10.\n */\npublic class TotalOrderSort extends Configured implements Tool{\n\n    public static class myMap extends org.apache.hadoop.mapreduce.Mapper<LongWritable, Text, Text, Text>{\n\n        public void map(LongWritable key,Text value,Context context) throws IOException,InterruptedException{\n            String[] split = value.toString().split(\"\\\\s+\");\n            for (int i = 0; i <split.length ; i++) {\n                Text word = new Text(split[i]);\n                context.write(word,new Text(\"\"));\n            }\n        }\n    }\n    public static class myReduce extends Reducer<Text,Text,Text,Text>{\n\n        public void reduce(Text key, Iterable<Text> values,Context context) throws IOException,InterruptedException\n        {\n            context.write(key, new Text(\"\"));\n\n        }\n    }\n\n    @Override\n    public int run(String[] args) throws Exception {\n        Job job = Job.getInstance(getConf());\n        job.setJarByClass(TotalSort.class);\n        job.setJobName(\"TotalSortTest\");\n\n\n        job.setInputFormatClass(KeyValueTextInputFormat.class);\n\n        job.setNumReduceTasks(3);\n\n        //因为map和reduce的输出是同样的类型，所以输出一个就可以了\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(Text.class);\n\n        job.setMapperClass(myMap.class);\n        job.setReducerClass(myReduce.class);\n\n        FileInputFormat.setInputPaths(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n        // 设置分区文件，即采样后放在的文件的文件名，不是完整路径\n        TotalOrderPartitioner.setPartitionFile(job.getConfiguration(), new Path(args[2]));\n         //采样器：三个参数\n        /* 第一个参数 freq: 表示来一个样本，将其作为采样点的概率。如果样本数目很大\n         *第二个参数 numSamples：表示采样点最大数目为，我这里设置10代表我的采样点最大为10，如果超过10，那么每次有新的采样点生成时\n         * ，会删除原有的一个采样点,此参数大数据的时候尽量设置多一些\n         * 第三个参数 maxSplitSampled：表示的是最大的分区数：我这里设置100不会起作用，因为我设置的分区只有4个而已\n         */\n\n        InputSampler.Sampler<Text, Text> sampler = new InputSampler.RandomSampler<>(0.01, 10, 100);\n\n        //把分区文件放在hdfs上，对程序没什么效果，方便我们查看而已\n        FileInputFormat.addInputPath(job, new Path(\"/test/sort\"));\n        //将采样点写入到分区文件中，这个必须要\n        InputSampler.writePartitionFile(job, sampler);\n        job.setPartitionerClass(TotalOrderPartitioner.class);\n\n        boolean success = job.waitForCompletion(true);\n        return success ? 0:1;\n    }\n    public static void main(String[] args) throws Exception {\n        int ret = ToolRunner.run(new TotalSortTest(), args);\n        System.exit(ret);\n    }\n}\n\n\n```\n## **注意的地方**  \n\n```\n InputSampler.Sampler<Text, Text> sampler = new InputSampler.RandomSampler<>(0.01, 10, 100);中三个参数要注意\n``` \n```\nInputSampler.Sampler<Text, Text> 只能是Text,Text的类型\n```\n```\n3. TotalOrderPartitioner.setPartitionFile(job.getConfiguration(), new Path(args[2]));用来给TotalOrderPartitioner初始化赋值，job.setPartitionerClass(TotalOrderPartitioner.class); 进行分区，就不需要自己写分区函数了  \n```\n```\n4. job.setInputFormatClass(KeyValueTextInputFormat.class); 注意里面是KeyValueTextInputFormat.class，而不是TextInputFormat.class。\n```\n```\n5. 在集群上，次程序才能体现出来\n```\n\n6.由于我这里，map的输入和输出都是用的(Text,Text),所以我只需要设置 \n```\njob.setOutputKeyClass(Text.class); \njob.setOutputValueClass(Text.class);\n```\n如果不一样，那么 应该设置4个,前两个为map的输出类型，后两个为reduce的输出类型\n```\njob.setMapOutputKeyClass(Text.class);\njob.setMapOutputValueClass(IntWritable.class);\njob.setOutputKeyClass(IntWritable.class);\njob.setOutputValueClass(NullWritable.class);\n```  \n\n\n更多文章：\n自定义分区全排序  \n  http://blog.csdn.net/t1dmzks/article/details/73032796    \nhttp://blog.csdn.net/t1dmzks/article/details/73028776\n","source":"_posts/Hadoop入门案例（五）全排序之TotalOrderPartitioner工具类+自动采样.md","raw":"---\ntitle: Hadoop入门案例（五）全排序之TotalOrderPartitioner工具类+自动采样\ndate: 2016-08-02 21:25:21\ntags: [hadoop]\ncategories: [大数据,hadoop]\nauthor: kaishun\nid: 18\npermalink: hadoop-example-5\n---\n\n\n\n**为什么用这种方法** \n我们之前的是自定义分区的，但是如果我们不知道数据的分布，手动分区不太容易，稍有不慎，会导致数据倾斜较大，这时候，我们应该使用采样点进行排序，本文使用的是 Hadoop内置的名为 TotalOrderPartitioner 的全排序，采样器使用的是 InputSampler.Sampler，关键解释已经存在于代码之中。 文章参考了    \n国外  http://blog.ditullio.fr/2016/01/04/hadoop-basics-total-order-sorting-mapreduce/#The_TotalOrderPartitioner    \n国内  http://www.cnblogs.com/one--way/p/5931308.html\n<!-- more -->\n## **代码**\n```\npackage com.myhadoop.mapreduce.test;\nimport org.apache.hadoop.conf.Configured;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapred.lib.InputSampler;\nimport org.apache.hadoop.mapred.lib.TotalOrderPartitioner;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.util.Tool;\nimport org.apache.hadoop.util.ToolRunner;\nimport java.io.IOException;\n\n/**\n * Created by kaishun on 2017/6/10.\n */\npublic class TotalOrderSort extends Configured implements Tool{\n\n    public static class myMap extends org.apache.hadoop.mapreduce.Mapper<LongWritable, Text, Text, Text>{\n\n        public void map(LongWritable key,Text value,Context context) throws IOException,InterruptedException{\n            String[] split = value.toString().split(\"\\\\s+\");\n            for (int i = 0; i <split.length ; i++) {\n                Text word = new Text(split[i]);\n                context.write(word,new Text(\"\"));\n            }\n        }\n    }\n    public static class myReduce extends Reducer<Text,Text,Text,Text>{\n\n        public void reduce(Text key, Iterable<Text> values,Context context) throws IOException,InterruptedException\n        {\n            context.write(key, new Text(\"\"));\n\n        }\n    }\n\n    @Override\n    public int run(String[] args) throws Exception {\n        Job job = Job.getInstance(getConf());\n        job.setJarByClass(TotalSort.class);\n        job.setJobName(\"TotalSortTest\");\n\n\n        job.setInputFormatClass(KeyValueTextInputFormat.class);\n\n        job.setNumReduceTasks(3);\n\n        //因为map和reduce的输出是同样的类型，所以输出一个就可以了\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(Text.class);\n\n        job.setMapperClass(myMap.class);\n        job.setReducerClass(myReduce.class);\n\n        FileInputFormat.setInputPaths(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n        // 设置分区文件，即采样后放在的文件的文件名，不是完整路径\n        TotalOrderPartitioner.setPartitionFile(job.getConfiguration(), new Path(args[2]));\n         //采样器：三个参数\n        /* 第一个参数 freq: 表示来一个样本，将其作为采样点的概率。如果样本数目很大\n         *第二个参数 numSamples：表示采样点最大数目为，我这里设置10代表我的采样点最大为10，如果超过10，那么每次有新的采样点生成时\n         * ，会删除原有的一个采样点,此参数大数据的时候尽量设置多一些\n         * 第三个参数 maxSplitSampled：表示的是最大的分区数：我这里设置100不会起作用，因为我设置的分区只有4个而已\n         */\n\n        InputSampler.Sampler<Text, Text> sampler = new InputSampler.RandomSampler<>(0.01, 10, 100);\n\n        //把分区文件放在hdfs上，对程序没什么效果，方便我们查看而已\n        FileInputFormat.addInputPath(job, new Path(\"/test/sort\"));\n        //将采样点写入到分区文件中，这个必须要\n        InputSampler.writePartitionFile(job, sampler);\n        job.setPartitionerClass(TotalOrderPartitioner.class);\n\n        boolean success = job.waitForCompletion(true);\n        return success ? 0:1;\n    }\n    public static void main(String[] args) throws Exception {\n        int ret = ToolRunner.run(new TotalSortTest(), args);\n        System.exit(ret);\n    }\n}\n\n\n```\n## **注意的地方**  \n\n```\n InputSampler.Sampler<Text, Text> sampler = new InputSampler.RandomSampler<>(0.01, 10, 100);中三个参数要注意\n``` \n```\nInputSampler.Sampler<Text, Text> 只能是Text,Text的类型\n```\n```\n3. TotalOrderPartitioner.setPartitionFile(job.getConfiguration(), new Path(args[2]));用来给TotalOrderPartitioner初始化赋值，job.setPartitionerClass(TotalOrderPartitioner.class); 进行分区，就不需要自己写分区函数了  \n```\n```\n4. job.setInputFormatClass(KeyValueTextInputFormat.class); 注意里面是KeyValueTextInputFormat.class，而不是TextInputFormat.class。\n```\n```\n5. 在集群上，次程序才能体现出来\n```\n\n6.由于我这里，map的输入和输出都是用的(Text,Text),所以我只需要设置 \n```\njob.setOutputKeyClass(Text.class); \njob.setOutputValueClass(Text.class);\n```\n如果不一样，那么 应该设置4个,前两个为map的输出类型，后两个为reduce的输出类型\n```\njob.setMapOutputKeyClass(Text.class);\njob.setMapOutputValueClass(IntWritable.class);\njob.setOutputKeyClass(IntWritable.class);\njob.setOutputValueClass(NullWritable.class);\n```  \n\n\n更多文章：\n自定义分区全排序  \n  http://blog.csdn.net/t1dmzks/article/details/73032796    \nhttp://blog.csdn.net/t1dmzks/article/details/73028776\n","slug":"hadoop-example-5","published":1,"updated":"2018-01-23T14:16:35.001Z","_id":"cjcrpnzoj000s2wv35yqdy8iu","comments":1,"layout":"post","photos":[],"link":"","content":"<p><strong>为什么用这种方法</strong><br>我们之前的是自定义分区的，但是如果我们不知道数据的分布，手动分区不太容易，稍有不慎，会导致数据倾斜较大，这时候，我们应该使用采样点进行排序，本文使用的是 Hadoop内置的名为 TotalOrderPartitioner 的全排序，采样器使用的是 InputSampler.Sampler，关键解释已经存在于代码之中。 文章参考了<br>国外  <a href=\"http://blog.ditullio.fr/2016/01/04/hadoop-basics-total-order-sorting-mapreduce/#The_TotalOrderPartitioner\" target=\"_blank\" rel=\"external\">http://blog.ditullio.fr/2016/01/04/hadoop-basics-total-order-sorting-mapreduce/#The_TotalOrderPartitioner</a><br>国内  <a href=\"http://www.cnblogs.com/one--way/p/5931308.html\" target=\"_blank\" rel=\"external\">http://www.cnblogs.com/one--way/p/5931308.html</a><br><a id=\"more\"></a></p>\n<h2 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a><strong>代码</strong></h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div></pre></td><td class=\"code\"><pre><div class=\"line\">package com.myhadoop.mapreduce.test;</div><div class=\"line\">import org.apache.hadoop.conf.Configured;</div><div class=\"line\">import org.apache.hadoop.fs.Path;</div><div class=\"line\">import org.apache.hadoop.io.LongWritable;</div><div class=\"line\">import org.apache.hadoop.io.Text;</div><div class=\"line\">import org.apache.hadoop.mapred.lib.InputSampler;</div><div class=\"line\">import org.apache.hadoop.mapred.lib.TotalOrderPartitioner;</div><div class=\"line\">import org.apache.hadoop.mapreduce.Reducer;</div><div class=\"line\">import org.apache.hadoop.mapreduce.Job;</div><div class=\"line\">import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</div><div class=\"line\">import org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat;</div><div class=\"line\">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</div><div class=\"line\">import org.apache.hadoop.util.Tool;</div><div class=\"line\">import org.apache.hadoop.util.ToolRunner;</div><div class=\"line\">import java.io.IOException;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Created by kaishun on 2017/6/10.</div><div class=\"line\"> */</div><div class=\"line\">public class TotalOrderSort extends Configured implements Tool&#123;</div><div class=\"line\"></div><div class=\"line\">    public static class myMap extends org.apache.hadoop.mapreduce.Mapper&lt;LongWritable, Text, Text, Text&gt;&#123;</div><div class=\"line\"></div><div class=\"line\">        public void map(LongWritable key,Text value,Context context) throws IOException,InterruptedException&#123;</div><div class=\"line\">            String[] split = value.toString().split(&quot;\\\\s+&quot;);</div><div class=\"line\">            for (int i = 0; i &lt;split.length ; i++) &#123;</div><div class=\"line\">                Text word = new Text(split[i]);</div><div class=\"line\">                context.write(word,new Text(&quot;&quot;));</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">    public static class myReduce extends Reducer&lt;Text,Text,Text,Text&gt;&#123;</div><div class=\"line\"></div><div class=\"line\">        public void reduce(Text key, Iterable&lt;Text&gt; values,Context context) throws IOException,InterruptedException</div><div class=\"line\">        &#123;</div><div class=\"line\">            context.write(key, new Text(&quot;&quot;));</div><div class=\"line\"></div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    @Override</div><div class=\"line\">    public int run(String[] args) throws Exception &#123;</div><div class=\"line\">        Job job = Job.getInstance(getConf());</div><div class=\"line\">        job.setJarByClass(TotalSort.class);</div><div class=\"line\">        job.setJobName(&quot;TotalSortTest&quot;);</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">        job.setInputFormatClass(KeyValueTextInputFormat.class);</div><div class=\"line\"></div><div class=\"line\">        job.setNumReduceTasks(3);</div><div class=\"line\"></div><div class=\"line\">        //因为map和reduce的输出是同样的类型，所以输出一个就可以了</div><div class=\"line\">        job.setOutputKeyClass(Text.class);</div><div class=\"line\">        job.setOutputValueClass(Text.class);</div><div class=\"line\"></div><div class=\"line\">        job.setMapperClass(myMap.class);</div><div class=\"line\">        job.setReducerClass(myReduce.class);</div><div class=\"line\"></div><div class=\"line\">        FileInputFormat.setInputPaths(job, new Path(args[0]));</div><div class=\"line\">        FileOutputFormat.setOutputPath(job, new Path(args[1]));</div><div class=\"line\"></div><div class=\"line\">        // 设置分区文件，即采样后放在的文件的文件名，不是完整路径</div><div class=\"line\">        TotalOrderPartitioner.setPartitionFile(job.getConfiguration(), new Path(args[2]));</div><div class=\"line\">         //采样器：三个参数</div><div class=\"line\">        /* 第一个参数 freq: 表示来一个样本，将其作为采样点的概率。如果样本数目很大</div><div class=\"line\">         *第二个参数 numSamples：表示采样点最大数目为，我这里设置10代表我的采样点最大为10，如果超过10，那么每次有新的采样点生成时</div><div class=\"line\">         * ，会删除原有的一个采样点,此参数大数据的时候尽量设置多一些</div><div class=\"line\">         * 第三个参数 maxSplitSampled：表示的是最大的分区数：我这里设置100不会起作用，因为我设置的分区只有4个而已</div><div class=\"line\">         */</div><div class=\"line\"></div><div class=\"line\">        InputSampler.Sampler&lt;Text, Text&gt; sampler = new InputSampler.RandomSampler&lt;&gt;(0.01, 10, 100);</div><div class=\"line\"></div><div class=\"line\">        //把分区文件放在hdfs上，对程序没什么效果，方便我们查看而已</div><div class=\"line\">        FileInputFormat.addInputPath(job, new Path(&quot;/test/sort&quot;));</div><div class=\"line\">        //将采样点写入到分区文件中，这个必须要</div><div class=\"line\">        InputSampler.writePartitionFile(job, sampler);</div><div class=\"line\">        job.setPartitionerClass(TotalOrderPartitioner.class);</div><div class=\"line\"></div><div class=\"line\">        boolean success = job.waitForCompletion(true);</div><div class=\"line\">        return success ? 0:1;</div><div class=\"line\">    &#125;</div><div class=\"line\">    public static void main(String[] args) throws Exception &#123;</div><div class=\"line\">        int ret = ToolRunner.run(new TotalSortTest(), args);</div><div class=\"line\">        System.exit(ret);</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h2 id=\"注意的地方\"><a href=\"#注意的地方\" class=\"headerlink\" title=\"注意的地方\"></a><strong>注意的地方</strong></h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"> InputSampler.Sampler&lt;Text, Text&gt; sampler = new InputSampler.RandomSampler&lt;&gt;(0.01, 10, 100);中三个参数要注意</div><div class=\"line\">```</div></pre></td></tr></table></figure>\n<p>InputSampler.Sampler<text, text=\"\"> 只能是Text,Text的类型<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">```</div><div class=\"line\">3. TotalOrderPartitioner.setPartitionFile(job.getConfiguration(), new Path(args[2]));用来给TotalOrderPartitioner初始化赋值，job.setPartitionerClass(TotalOrderPartitioner.class); 进行分区，就不需要自己写分区函数了</div></pre></td></tr></table></figure></text,></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">4. job.setInputFormatClass(KeyValueTextInputFormat.class); 注意里面是KeyValueTextInputFormat.class，而不是TextInputFormat.class。</div></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">5. 在集群上，次程序才能体现出来</div></pre></td></tr></table></figure>\n<p>6.由于我这里，map的输入和输出都是用的(Text,Text),所以我只需要设置<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">job.setOutputKeyClass(Text.class); </div><div class=\"line\">job.setOutputValueClass(Text.class);</div></pre></td></tr></table></figure></p>\n<p>如果不一样，那么 应该设置4个,前两个为map的输出类型，后两个为reduce的输出类型</p>\n<pre><code>job.setMapOutputKeyClass(Text.class);\njob.setMapOutputValueClass(IntWritable.class);\njob.setOutputKeyClass(IntWritable.class);\njob.setOutputValueClass(NullWritable.class);\n</code></pre><p>更多文章：<br>自定义分区全排序<br>  <a href=\"http://blog.csdn.net/t1dmzks/article/details/73032796\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/t1dmzks/article/details/73032796</a><br><a href=\"http://blog.csdn.net/t1dmzks/article/details/73028776\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/t1dmzks/article/details/73028776</a></p>\n","site":{"data":{}},"excerpt":"<p><strong>为什么用这种方法</strong><br>我们之前的是自定义分区的，但是如果我们不知道数据的分布，手动分区不太容易，稍有不慎，会导致数据倾斜较大，这时候，我们应该使用采样点进行排序，本文使用的是 Hadoop内置的名为 TotalOrderPartitioner 的全排序，采样器使用的是 InputSampler.Sampler，关键解释已经存在于代码之中。 文章参考了<br>国外  <a href=\"http://blog.ditullio.fr/2016/01/04/hadoop-basics-total-order-sorting-mapreduce/#The_TotalOrderPartitioner\" target=\"_blank\" rel=\"external\">http://blog.ditullio.fr/2016/01/04/hadoop-basics-total-order-sorting-mapreduce/#The_TotalOrderPartitioner</a><br>国内  <a href=\"http://www.cnblogs.com/one--way/p/5931308.html\" target=\"_blank\" rel=\"external\">http://www.cnblogs.com/one--way/p/5931308.html</a><br>","more":"</p>\n<h2 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a><strong>代码</strong></h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div></pre></td><td class=\"code\"><pre><div class=\"line\">package com.myhadoop.mapreduce.test;</div><div class=\"line\">import org.apache.hadoop.conf.Configured;</div><div class=\"line\">import org.apache.hadoop.fs.Path;</div><div class=\"line\">import org.apache.hadoop.io.LongWritable;</div><div class=\"line\">import org.apache.hadoop.io.Text;</div><div class=\"line\">import org.apache.hadoop.mapred.lib.InputSampler;</div><div class=\"line\">import org.apache.hadoop.mapred.lib.TotalOrderPartitioner;</div><div class=\"line\">import org.apache.hadoop.mapreduce.Reducer;</div><div class=\"line\">import org.apache.hadoop.mapreduce.Job;</div><div class=\"line\">import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</div><div class=\"line\">import org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat;</div><div class=\"line\">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</div><div class=\"line\">import org.apache.hadoop.util.Tool;</div><div class=\"line\">import org.apache.hadoop.util.ToolRunner;</div><div class=\"line\">import java.io.IOException;</div><div class=\"line\"></div><div class=\"line\">/**</div><div class=\"line\"> * Created by kaishun on 2017/6/10.</div><div class=\"line\"> */</div><div class=\"line\">public class TotalOrderSort extends Configured implements Tool&#123;</div><div class=\"line\"></div><div class=\"line\">    public static class myMap extends org.apache.hadoop.mapreduce.Mapper&lt;LongWritable, Text, Text, Text&gt;&#123;</div><div class=\"line\"></div><div class=\"line\">        public void map(LongWritable key,Text value,Context context) throws IOException,InterruptedException&#123;</div><div class=\"line\">            String[] split = value.toString().split(&quot;\\\\s+&quot;);</div><div class=\"line\">            for (int i = 0; i &lt;split.length ; i++) &#123;</div><div class=\"line\">                Text word = new Text(split[i]);</div><div class=\"line\">                context.write(word,new Text(&quot;&quot;));</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">    public static class myReduce extends Reducer&lt;Text,Text,Text,Text&gt;&#123;</div><div class=\"line\"></div><div class=\"line\">        public void reduce(Text key, Iterable&lt;Text&gt; values,Context context) throws IOException,InterruptedException</div><div class=\"line\">        &#123;</div><div class=\"line\">            context.write(key, new Text(&quot;&quot;));</div><div class=\"line\"></div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    @Override</div><div class=\"line\">    public int run(String[] args) throws Exception &#123;</div><div class=\"line\">        Job job = Job.getInstance(getConf());</div><div class=\"line\">        job.setJarByClass(TotalSort.class);</div><div class=\"line\">        job.setJobName(&quot;TotalSortTest&quot;);</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">        job.setInputFormatClass(KeyValueTextInputFormat.class);</div><div class=\"line\"></div><div class=\"line\">        job.setNumReduceTasks(3);</div><div class=\"line\"></div><div class=\"line\">        //因为map和reduce的输出是同样的类型，所以输出一个就可以了</div><div class=\"line\">        job.setOutputKeyClass(Text.class);</div><div class=\"line\">        job.setOutputValueClass(Text.class);</div><div class=\"line\"></div><div class=\"line\">        job.setMapperClass(myMap.class);</div><div class=\"line\">        job.setReducerClass(myReduce.class);</div><div class=\"line\"></div><div class=\"line\">        FileInputFormat.setInputPaths(job, new Path(args[0]));</div><div class=\"line\">        FileOutputFormat.setOutputPath(job, new Path(args[1]));</div><div class=\"line\"></div><div class=\"line\">        // 设置分区文件，即采样后放在的文件的文件名，不是完整路径</div><div class=\"line\">        TotalOrderPartitioner.setPartitionFile(job.getConfiguration(), new Path(args[2]));</div><div class=\"line\">         //采样器：三个参数</div><div class=\"line\">        /* 第一个参数 freq: 表示来一个样本，将其作为采样点的概率。如果样本数目很大</div><div class=\"line\">         *第二个参数 numSamples：表示采样点最大数目为，我这里设置10代表我的采样点最大为10，如果超过10，那么每次有新的采样点生成时</div><div class=\"line\">         * ，会删除原有的一个采样点,此参数大数据的时候尽量设置多一些</div><div class=\"line\">         * 第三个参数 maxSplitSampled：表示的是最大的分区数：我这里设置100不会起作用，因为我设置的分区只有4个而已</div><div class=\"line\">         */</div><div class=\"line\"></div><div class=\"line\">        InputSampler.Sampler&lt;Text, Text&gt; sampler = new InputSampler.RandomSampler&lt;&gt;(0.01, 10, 100);</div><div class=\"line\"></div><div class=\"line\">        //把分区文件放在hdfs上，对程序没什么效果，方便我们查看而已</div><div class=\"line\">        FileInputFormat.addInputPath(job, new Path(&quot;/test/sort&quot;));</div><div class=\"line\">        //将采样点写入到分区文件中，这个必须要</div><div class=\"line\">        InputSampler.writePartitionFile(job, sampler);</div><div class=\"line\">        job.setPartitionerClass(TotalOrderPartitioner.class);</div><div class=\"line\"></div><div class=\"line\">        boolean success = job.waitForCompletion(true);</div><div class=\"line\">        return success ? 0:1;</div><div class=\"line\">    &#125;</div><div class=\"line\">    public static void main(String[] args) throws Exception &#123;</div><div class=\"line\">        int ret = ToolRunner.run(new TotalSortTest(), args);</div><div class=\"line\">        System.exit(ret);</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h2 id=\"注意的地方\"><a href=\"#注意的地方\" class=\"headerlink\" title=\"注意的地方\"></a><strong>注意的地方</strong></h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"> InputSampler.Sampler&lt;Text, Text&gt; sampler = new InputSampler.RandomSampler&lt;&gt;(0.01, 10, 100);中三个参数要注意</div><div class=\"line\">```</div></pre></td></tr></table></figure>\n<p>InputSampler.Sampler<text, text=\"\"> 只能是Text,Text的类型<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">```</div><div class=\"line\">3. TotalOrderPartitioner.setPartitionFile(job.getConfiguration(), new Path(args[2]));用来给TotalOrderPartitioner初始化赋值，job.setPartitionerClass(TotalOrderPartitioner.class); 进行分区，就不需要自己写分区函数了</div></pre></td></tr></table></figure></text,></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">4. job.setInputFormatClass(KeyValueTextInputFormat.class); 注意里面是KeyValueTextInputFormat.class，而不是TextInputFormat.class。</div></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">5. 在集群上，次程序才能体现出来</div></pre></td></tr></table></figure>\n<p>6.由于我这里，map的输入和输出都是用的(Text,Text),所以我只需要设置<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">job.setOutputKeyClass(Text.class); </div><div class=\"line\">job.setOutputValueClass(Text.class);</div></pre></td></tr></table></figure></p>\n<p>如果不一样，那么 应该设置4个,前两个为map的输出类型，后两个为reduce的输出类型</p>\n<pre><code>job.setMapOutputKeyClass(Text.class);\njob.setMapOutputValueClass(IntWritable.class);\njob.setOutputKeyClass(IntWritable.class);\njob.setOutputValueClass(NullWritable.class);\n</code></pre><p>更多文章：<br>自定义分区全排序<br>  <a href=\"http://blog.csdn.net/t1dmzks/article/details/73032796\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/t1dmzks/article/details/73032796</a><br><a href=\"http://blog.csdn.net/t1dmzks/article/details/73028776\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/t1dmzks/article/details/73028776</a></p>"},{"title":"Hadoop入门案例（八）之 表 关联","date":"2016-08-20T13:25:21.000Z","author":"kaishun","id":"21","blogexcerpt":"表关联:两个表关联，代码和以后再贴一起在github上贴出来，mapreduce表关联思路很简单。主要如下 思路： mapper阶段:(key，value) value中加一个字符，用来区别多个表 .reduce时 new 多个数组，例如我们是两个表关联，通过value中的字符，用来判断哪个是哪个表，分别放在不同的数组中，然后数组中进行笛卡尔乘积即可","_content":"\n\n\n表关联:\n两个表关联，代码和以后再贴一起在github上贴出来，mapreduce表关联思路很简单。主要如下  \n\n## 思路\n**mapper阶段:** \n(key，value) value中加一个字符，用来区别多个表  \n**reduce阶段**:\nreduce时，new 多个数组，例如我们是两个表关联，通过value中的字符，用来判断哪个是哪个表，分别放在不同的数组中，然后数组中进行笛卡尔乘积即可 \n**如果是单表关联**：\n找出单表中哪些字段需要关联，理清楚思路，和sql中的单表join一样，和多表关联相同的思路\n\n## 优化的地方\n看具体需求如何，如果是大小表，最好不要用这种方法，因为会经过shuffle，导致大量资源浪费。可以在map段就做关联，而不经过reduce。类似于spark中，用flatmap代替join的方法  \n下面是网上常见的方法，我觉得自己的笔记，用这个代码就挺好的了，如果是多表关联，这网上关于爷孙的关联，也是很好的入门\n\n\n```\npublic class LeftJoin extends Configured implements Tool{\n\n    public static final String DELIMITER = \",\";\n\n    public static class LeftJoinMapper extends\n        Mapper<LongWritable,Text,Text,Text> {\n\n        protected void map(LongWritable key, Text value, Context context)\n            throws IOException,InterruptedException {\n            /*\n             * 拿到两个不同文件，区分出到底是哪个文件，然后分别输出\n             */\n            String filepath = ((FileSplit)context.getInputSplit()).getPath().toString();\n            String line = value.toString();\n            if (line == null || line.equals(\"\")) return; \n\n            if (filepath.indexOf(\"employee\") != -1) {\n                String[] lines = line.split(DELIMITER);\n                if(lines.length < 2) return;\n\n                String company_id = lines[0];\n                String employee = lines[1];\n                context.write(new Text(company_id),new Text(\"a:\"+employee));\n            }\n\n            else if(filepath.indexOf(\"salary\") != -1) {\n                String[] lines = line.split(DELIMITER);\n                if(lines.length < 2) return;\n\n                String company_id = lines[0];\n                String salary = lines[1];\n                context.write(new Text(company_id), new Text(\"b:\" + salary));\n            }\n        }\n    }\n\n    public static class LeftJoinReduce extends \n            Reducer<Text, Text, Text, Text> {\n        protected void reduce(Text key, Iterable<Text> values,\n                Context context) throws IOException, InterruptedException{\n            Vector<String> vecA = new Vector<String>();\n            Vector<String> vecB = new Vector<String>();\n\n            for(Text each_val:values) {\n                String each = each_val.toString();\n                if(each.startsWith(\"a:\")) {\n                    vecA.add(each.substring(2));\n                } else if(each.startsWith(\"b:\")) {\n                    vecB.add(each.substring(2));\n                }\n            }\n\n            for (int i = 0; i < vecA.size(); i++) {\n                /*\n                 * 如果vecB为空的话，将A里的输出，B的位置补null。\n                 */\n                if (vecB.size() == 0) {\n                    context.write(key, new Text(vecA.get(i) + DELIMITER + \"null\"));\n                } else {\n                    for (int j = 0; j < vecB.size(); j++) {\n                        context.write(key, new Text(vecA.get(i) + DELIMITER + vecB.get(j)));\n                    }\n                }\n            }\n        }\n    }\n\n    public int run(String[] args) throws Exception {\n        Configuration conf = getConf();\n        GenericOptionsParser optionparser = new GenericOptionsParser(conf, args);\n        conf = optionparser.getConfiguration();\n\n        Job job = new Job(conf,\"leftjoin\");\n        job.setJarByClass(LeftJoin.class);\n        FileInputFormat.addInputPaths(job, conf.get(\"input_dir\"));\n        Path out = new Path(conf.get(\"output_dir\"));\n        FileOutputFormat.setOutputPath(job, out);\n        job.setNumReduceTasks(conf.getInt(\"reduce_num\",2));\n\n        job.setMapperClass(LeftJoinMapper.class);\n        job.setReducerClass(LeftJoinReduce.class);\n        job.setInputFormatClass(TextInputFormat.class);\n        job.setOutputFormatClass(TextOutputFormat.class);\n\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(Text.class);\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(Text.class);\n        conf.set(\"mapred.textoutputformat.separator\", \",\");\n\n        return (job.waitForCompletion(true) ? 0 : 1);\n    }\n\n    public static void main(String[] args) throws Exception{\n        int res = ToolRunner.run(new Configuration(),new LeftJoin(),args);\n        System.exit(res);\n    }\n\n}\n```","source":"_posts/Hadoop入门案例（八）之 表 关联.md","raw":"---\ntitle: Hadoop入门案例（八）之 表 关联\ndate: 2016-08-20 21:25:21\ntags: [hadoop]\ncategories: [大数据,hadoop]\nauthor: kaishun\nid: 21\npermalink: hadoop-example-8\nblogexcerpt: 表关联:两个表关联，代码和以后再贴一起在github上贴出来，mapreduce表关联思路很简单。主要如下 思路： mapper阶段:(key，value) value中加一个字符，用来区别多个表 .reduce时 new 多个数组，例如我们是两个表关联，通过value中的字符，用来判断哪个是哪个表，分别放在不同的数组中，然后数组中进行笛卡尔乘积即可\n---\n\n\n\n表关联:\n两个表关联，代码和以后再贴一起在github上贴出来，mapreduce表关联思路很简单。主要如下  \n\n## 思路\n**mapper阶段:** \n(key，value) value中加一个字符，用来区别多个表  \n**reduce阶段**:\nreduce时，new 多个数组，例如我们是两个表关联，通过value中的字符，用来判断哪个是哪个表，分别放在不同的数组中，然后数组中进行笛卡尔乘积即可 \n**如果是单表关联**：\n找出单表中哪些字段需要关联，理清楚思路，和sql中的单表join一样，和多表关联相同的思路\n\n## 优化的地方\n看具体需求如何，如果是大小表，最好不要用这种方法，因为会经过shuffle，导致大量资源浪费。可以在map段就做关联，而不经过reduce。类似于spark中，用flatmap代替join的方法  \n下面是网上常见的方法，我觉得自己的笔记，用这个代码就挺好的了，如果是多表关联，这网上关于爷孙的关联，也是很好的入门\n\n\n```\npublic class LeftJoin extends Configured implements Tool{\n\n    public static final String DELIMITER = \",\";\n\n    public static class LeftJoinMapper extends\n        Mapper<LongWritable,Text,Text,Text> {\n\n        protected void map(LongWritable key, Text value, Context context)\n            throws IOException,InterruptedException {\n            /*\n             * 拿到两个不同文件，区分出到底是哪个文件，然后分别输出\n             */\n            String filepath = ((FileSplit)context.getInputSplit()).getPath().toString();\n            String line = value.toString();\n            if (line == null || line.equals(\"\")) return; \n\n            if (filepath.indexOf(\"employee\") != -1) {\n                String[] lines = line.split(DELIMITER);\n                if(lines.length < 2) return;\n\n                String company_id = lines[0];\n                String employee = lines[1];\n                context.write(new Text(company_id),new Text(\"a:\"+employee));\n            }\n\n            else if(filepath.indexOf(\"salary\") != -1) {\n                String[] lines = line.split(DELIMITER);\n                if(lines.length < 2) return;\n\n                String company_id = lines[0];\n                String salary = lines[1];\n                context.write(new Text(company_id), new Text(\"b:\" + salary));\n            }\n        }\n    }\n\n    public static class LeftJoinReduce extends \n            Reducer<Text, Text, Text, Text> {\n        protected void reduce(Text key, Iterable<Text> values,\n                Context context) throws IOException, InterruptedException{\n            Vector<String> vecA = new Vector<String>();\n            Vector<String> vecB = new Vector<String>();\n\n            for(Text each_val:values) {\n                String each = each_val.toString();\n                if(each.startsWith(\"a:\")) {\n                    vecA.add(each.substring(2));\n                } else if(each.startsWith(\"b:\")) {\n                    vecB.add(each.substring(2));\n                }\n            }\n\n            for (int i = 0; i < vecA.size(); i++) {\n                /*\n                 * 如果vecB为空的话，将A里的输出，B的位置补null。\n                 */\n                if (vecB.size() == 0) {\n                    context.write(key, new Text(vecA.get(i) + DELIMITER + \"null\"));\n                } else {\n                    for (int j = 0; j < vecB.size(); j++) {\n                        context.write(key, new Text(vecA.get(i) + DELIMITER + vecB.get(j)));\n                    }\n                }\n            }\n        }\n    }\n\n    public int run(String[] args) throws Exception {\n        Configuration conf = getConf();\n        GenericOptionsParser optionparser = new GenericOptionsParser(conf, args);\n        conf = optionparser.getConfiguration();\n\n        Job job = new Job(conf,\"leftjoin\");\n        job.setJarByClass(LeftJoin.class);\n        FileInputFormat.addInputPaths(job, conf.get(\"input_dir\"));\n        Path out = new Path(conf.get(\"output_dir\"));\n        FileOutputFormat.setOutputPath(job, out);\n        job.setNumReduceTasks(conf.getInt(\"reduce_num\",2));\n\n        job.setMapperClass(LeftJoinMapper.class);\n        job.setReducerClass(LeftJoinReduce.class);\n        job.setInputFormatClass(TextInputFormat.class);\n        job.setOutputFormatClass(TextOutputFormat.class);\n\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(Text.class);\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(Text.class);\n        conf.set(\"mapred.textoutputformat.separator\", \",\");\n\n        return (job.waitForCompletion(true) ? 0 : 1);\n    }\n\n    public static void main(String[] args) throws Exception{\n        int res = ToolRunner.run(new Configuration(),new LeftJoin(),args);\n        System.exit(res);\n    }\n\n}\n```","slug":"hadoop-example-8","published":1,"updated":"2018-01-23T14:17:49.742Z","_id":"cjcrpnzoj000v2wv3tosc38eg","comments":1,"layout":"post","photos":[],"link":"","content":"<p>表关联:<br>两个表关联，代码和以后再贴一起在github上贴出来，mapreduce表关联思路很简单。主要如下  </p>\n<h2 id=\"思路\"><a href=\"#思路\" class=\"headerlink\" title=\"思路\"></a>思路</h2><p><strong>mapper阶段:</strong><br>(key，value) value中加一个字符，用来区别多个表<br><strong>reduce阶段</strong>:<br>reduce时，new 多个数组，例如我们是两个表关联，通过value中的字符，用来判断哪个是哪个表，分别放在不同的数组中，然后数组中进行笛卡尔乘积即可<br><strong>如果是单表关联</strong>：<br>找出单表中哪些字段需要关联，理清楚思路，和sql中的单表join一样，和多表关联相同的思路</p>\n<h2 id=\"优化的地方\"><a href=\"#优化的地方\" class=\"headerlink\" title=\"优化的地方\"></a>优化的地方</h2><p>看具体需求如何，如果是大小表，最好不要用这种方法，因为会经过shuffle，导致大量资源浪费。可以在map段就做关联，而不经过reduce。类似于spark中，用flatmap代替join的方法<br>下面是网上常见的方法，我觉得自己的笔记，用这个代码就挺好的了，如果是多表关联，这网上关于爷孙的关联，也是很好的入门</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div></pre></td><td class=\"code\"><pre><div class=\"line\">public class LeftJoin extends Configured implements Tool&#123;</div><div class=\"line\"></div><div class=\"line\">    public static final String DELIMITER = &quot;,&quot;;</div><div class=\"line\"></div><div class=\"line\">    public static class LeftJoinMapper extends</div><div class=\"line\">        Mapper&lt;LongWritable,Text,Text,Text&gt; &#123;</div><div class=\"line\"></div><div class=\"line\">        protected void map(LongWritable key, Text value, Context context)</div><div class=\"line\">            throws IOException,InterruptedException &#123;</div><div class=\"line\">            /*</div><div class=\"line\">             * 拿到两个不同文件，区分出到底是哪个文件，然后分别输出</div><div class=\"line\">             */</div><div class=\"line\">            String filepath = ((FileSplit)context.getInputSplit()).getPath().toString();</div><div class=\"line\">            String line = value.toString();</div><div class=\"line\">            if (line == null || line.equals(&quot;&quot;)) return; </div><div class=\"line\"></div><div class=\"line\">            if (filepath.indexOf(&quot;employee&quot;) != -1) &#123;</div><div class=\"line\">                String[] lines = line.split(DELIMITER);</div><div class=\"line\">                if(lines.length &lt; 2) return;</div><div class=\"line\"></div><div class=\"line\">                String company_id = lines[0];</div><div class=\"line\">                String employee = lines[1];</div><div class=\"line\">                context.write(new Text(company_id),new Text(&quot;a:&quot;+employee));</div><div class=\"line\">            &#125;</div><div class=\"line\"></div><div class=\"line\">            else if(filepath.indexOf(&quot;salary&quot;) != -1) &#123;</div><div class=\"line\">                String[] lines = line.split(DELIMITER);</div><div class=\"line\">                if(lines.length &lt; 2) return;</div><div class=\"line\"></div><div class=\"line\">                String company_id = lines[0];</div><div class=\"line\">                String salary = lines[1];</div><div class=\"line\">                context.write(new Text(company_id), new Text(&quot;b:&quot; + salary));</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    public static class LeftJoinReduce extends </div><div class=\"line\">            Reducer&lt;Text, Text, Text, Text&gt; &#123;</div><div class=\"line\">        protected void reduce(Text key, Iterable&lt;Text&gt; values,</div><div class=\"line\">                Context context) throws IOException, InterruptedException&#123;</div><div class=\"line\">            Vector&lt;String&gt; vecA = new Vector&lt;String&gt;();</div><div class=\"line\">            Vector&lt;String&gt; vecB = new Vector&lt;String&gt;();</div><div class=\"line\"></div><div class=\"line\">            for(Text each_val:values) &#123;</div><div class=\"line\">                String each = each_val.toString();</div><div class=\"line\">                if(each.startsWith(&quot;a:&quot;)) &#123;</div><div class=\"line\">                    vecA.add(each.substring(2));</div><div class=\"line\">                &#125; else if(each.startsWith(&quot;b:&quot;)) &#123;</div><div class=\"line\">                    vecB.add(each.substring(2));</div><div class=\"line\">                &#125;</div><div class=\"line\">            &#125;</div><div class=\"line\"></div><div class=\"line\">            for (int i = 0; i &lt; vecA.size(); i++) &#123;</div><div class=\"line\">                /*</div><div class=\"line\">                 * 如果vecB为空的话，将A里的输出，B的位置补null。</div><div class=\"line\">                 */</div><div class=\"line\">                if (vecB.size() == 0) &#123;</div><div class=\"line\">                    context.write(key, new Text(vecA.get(i) + DELIMITER + &quot;null&quot;));</div><div class=\"line\">                &#125; else &#123;</div><div class=\"line\">                    for (int j = 0; j &lt; vecB.size(); j++) &#123;</div><div class=\"line\">                        context.write(key, new Text(vecA.get(i) + DELIMITER + vecB.get(j)));</div><div class=\"line\">                    &#125;</div><div class=\"line\">                &#125;</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    public int run(String[] args) throws Exception &#123;</div><div class=\"line\">        Configuration conf = getConf();</div><div class=\"line\">        GenericOptionsParser optionparser = new GenericOptionsParser(conf, args);</div><div class=\"line\">        conf = optionparser.getConfiguration();</div><div class=\"line\"></div><div class=\"line\">        Job job = new Job(conf,&quot;leftjoin&quot;);</div><div class=\"line\">        job.setJarByClass(LeftJoin.class);</div><div class=\"line\">        FileInputFormat.addInputPaths(job, conf.get(&quot;input_dir&quot;));</div><div class=\"line\">        Path out = new Path(conf.get(&quot;output_dir&quot;));</div><div class=\"line\">        FileOutputFormat.setOutputPath(job, out);</div><div class=\"line\">        job.setNumReduceTasks(conf.getInt(&quot;reduce_num&quot;,2));</div><div class=\"line\"></div><div class=\"line\">        job.setMapperClass(LeftJoinMapper.class);</div><div class=\"line\">        job.setReducerClass(LeftJoinReduce.class);</div><div class=\"line\">        job.setInputFormatClass(TextInputFormat.class);</div><div class=\"line\">        job.setOutputFormatClass(TextOutputFormat.class);</div><div class=\"line\"></div><div class=\"line\">        job.setMapOutputKeyClass(Text.class);</div><div class=\"line\">        job.setMapOutputValueClass(Text.class);</div><div class=\"line\">        job.setOutputKeyClass(Text.class);</div><div class=\"line\">        job.setOutputValueClass(Text.class);</div><div class=\"line\">        conf.set(&quot;mapred.textoutputformat.separator&quot;, &quot;,&quot;);</div><div class=\"line\"></div><div class=\"line\">        return (job.waitForCompletion(true) ? 0 : 1);</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    public static void main(String[] args) throws Exception&#123;</div><div class=\"line\">        int res = ToolRunner.run(new Configuration(),new LeftJoin(),args);</div><div class=\"line\">        System.exit(res);</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<p>表关联:<br>两个表关联，代码和以后再贴一起在github上贴出来，mapreduce表关联思路很简单。主要如下  </p>\n<h2 id=\"思路\"><a href=\"#思路\" class=\"headerlink\" title=\"思路\"></a>思路</h2><p><strong>mapper阶段:</strong><br>(key，value) value中加一个字符，用来区别多个表<br><strong>reduce阶段</strong>:<br>reduce时，new 多个数组，例如我们是两个表关联，通过value中的字符，用来判断哪个是哪个表，分别放在不同的数组中，然后数组中进行笛卡尔乘积即可<br><strong>如果是单表关联</strong>：<br>找出单表中哪些字段需要关联，理清楚思路，和sql中的单表join一样，和多表关联相同的思路</p>\n<h2 id=\"优化的地方\"><a href=\"#优化的地方\" class=\"headerlink\" title=\"优化的地方\"></a>优化的地方</h2><p>看具体需求如何，如果是大小表，最好不要用这种方法，因为会经过shuffle，导致大量资源浪费。可以在map段就做关联，而不经过reduce。类似于spark中，用flatmap代替join的方法<br>下面是网上常见的方法，我觉得自己的笔记，用这个代码就挺好的了，如果是多表关联，这网上关于爷孙的关联，也是很好的入门</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div></pre></td><td class=\"code\"><pre><div class=\"line\">public class LeftJoin extends Configured implements Tool&#123;</div><div class=\"line\"></div><div class=\"line\">    public static final String DELIMITER = &quot;,&quot;;</div><div class=\"line\"></div><div class=\"line\">    public static class LeftJoinMapper extends</div><div class=\"line\">        Mapper&lt;LongWritable,Text,Text,Text&gt; &#123;</div><div class=\"line\"></div><div class=\"line\">        protected void map(LongWritable key, Text value, Context context)</div><div class=\"line\">            throws IOException,InterruptedException &#123;</div><div class=\"line\">            /*</div><div class=\"line\">             * 拿到两个不同文件，区分出到底是哪个文件，然后分别输出</div><div class=\"line\">             */</div><div class=\"line\">            String filepath = ((FileSplit)context.getInputSplit()).getPath().toString();</div><div class=\"line\">            String line = value.toString();</div><div class=\"line\">            if (line == null || line.equals(&quot;&quot;)) return; </div><div class=\"line\"></div><div class=\"line\">            if (filepath.indexOf(&quot;employee&quot;) != -1) &#123;</div><div class=\"line\">                String[] lines = line.split(DELIMITER);</div><div class=\"line\">                if(lines.length &lt; 2) return;</div><div class=\"line\"></div><div class=\"line\">                String company_id = lines[0];</div><div class=\"line\">                String employee = lines[1];</div><div class=\"line\">                context.write(new Text(company_id),new Text(&quot;a:&quot;+employee));</div><div class=\"line\">            &#125;</div><div class=\"line\"></div><div class=\"line\">            else if(filepath.indexOf(&quot;salary&quot;) != -1) &#123;</div><div class=\"line\">                String[] lines = line.split(DELIMITER);</div><div class=\"line\">                if(lines.length &lt; 2) return;</div><div class=\"line\"></div><div class=\"line\">                String company_id = lines[0];</div><div class=\"line\">                String salary = lines[1];</div><div class=\"line\">                context.write(new Text(company_id), new Text(&quot;b:&quot; + salary));</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    public static class LeftJoinReduce extends </div><div class=\"line\">            Reducer&lt;Text, Text, Text, Text&gt; &#123;</div><div class=\"line\">        protected void reduce(Text key, Iterable&lt;Text&gt; values,</div><div class=\"line\">                Context context) throws IOException, InterruptedException&#123;</div><div class=\"line\">            Vector&lt;String&gt; vecA = new Vector&lt;String&gt;();</div><div class=\"line\">            Vector&lt;String&gt; vecB = new Vector&lt;String&gt;();</div><div class=\"line\"></div><div class=\"line\">            for(Text each_val:values) &#123;</div><div class=\"line\">                String each = each_val.toString();</div><div class=\"line\">                if(each.startsWith(&quot;a:&quot;)) &#123;</div><div class=\"line\">                    vecA.add(each.substring(2));</div><div class=\"line\">                &#125; else if(each.startsWith(&quot;b:&quot;)) &#123;</div><div class=\"line\">                    vecB.add(each.substring(2));</div><div class=\"line\">                &#125;</div><div class=\"line\">            &#125;</div><div class=\"line\"></div><div class=\"line\">            for (int i = 0; i &lt; vecA.size(); i++) &#123;</div><div class=\"line\">                /*</div><div class=\"line\">                 * 如果vecB为空的话，将A里的输出，B的位置补null。</div><div class=\"line\">                 */</div><div class=\"line\">                if (vecB.size() == 0) &#123;</div><div class=\"line\">                    context.write(key, new Text(vecA.get(i) + DELIMITER + &quot;null&quot;));</div><div class=\"line\">                &#125; else &#123;</div><div class=\"line\">                    for (int j = 0; j &lt; vecB.size(); j++) &#123;</div><div class=\"line\">                        context.write(key, new Text(vecA.get(i) + DELIMITER + vecB.get(j)));</div><div class=\"line\">                    &#125;</div><div class=\"line\">                &#125;</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    public int run(String[] args) throws Exception &#123;</div><div class=\"line\">        Configuration conf = getConf();</div><div class=\"line\">        GenericOptionsParser optionparser = new GenericOptionsParser(conf, args);</div><div class=\"line\">        conf = optionparser.getConfiguration();</div><div class=\"line\"></div><div class=\"line\">        Job job = new Job(conf,&quot;leftjoin&quot;);</div><div class=\"line\">        job.setJarByClass(LeftJoin.class);</div><div class=\"line\">        FileInputFormat.addInputPaths(job, conf.get(&quot;input_dir&quot;));</div><div class=\"line\">        Path out = new Path(conf.get(&quot;output_dir&quot;));</div><div class=\"line\">        FileOutputFormat.setOutputPath(job, out);</div><div class=\"line\">        job.setNumReduceTasks(conf.getInt(&quot;reduce_num&quot;,2));</div><div class=\"line\"></div><div class=\"line\">        job.setMapperClass(LeftJoinMapper.class);</div><div class=\"line\">        job.setReducerClass(LeftJoinReduce.class);</div><div class=\"line\">        job.setInputFormatClass(TextInputFormat.class);</div><div class=\"line\">        job.setOutputFormatClass(TextOutputFormat.class);</div><div class=\"line\"></div><div class=\"line\">        job.setMapOutputKeyClass(Text.class);</div><div class=\"line\">        job.setMapOutputValueClass(Text.class);</div><div class=\"line\">        job.setOutputKeyClass(Text.class);</div><div class=\"line\">        job.setOutputValueClass(Text.class);</div><div class=\"line\">        conf.set(&quot;mapred.textoutputformat.separator&quot;, &quot;,&quot;);</div><div class=\"line\"></div><div class=\"line\">        return (job.waitForCompletion(true) ? 0 : 1);</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    public static void main(String[] args) throws Exception&#123;</div><div class=\"line\">        int res = ToolRunner.run(new Configuration(),new LeftJoin(),args);</div><div class=\"line\">        System.exit(res);</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>"},{"title":"Linux  定时任务Crontab","date":"2017-05-06T13:25:21.000Z","author":"kaishun","id":"58","_content":"\n# Crontab 介绍\nCrontab是Linux系统中在固定时间执行某一个程序的工具，类似于Windows系统中的任务计划程序，在一些自动化的程序或者运维中，起到很大的作用，在很多数据处理方面，例如定时备份数据库，定时指定某些任务等等，用linux的Crontab往往比较方便，下面介绍如何安装与使用crontab\n# crontab安装\n## 检查是否已经安装，若已经安装那么不需要重复安装\n先查看crontab有没有安装，有的话就不需要安装了,==root用户下==\n```\n[root@master crontest]# service crond status  \n若没有出现status说明已经安装\n```  \n若没有安装，则按以下步骤进行\n## 安装\n```shell\nyum -y install vixie-cron\nyum -y install crontabs\n```\n说明：\nvixie-cron 软件包是 cron 的主程序；\ncrontabs 软件包是用来安装、卸装、或列举用来驱动 cron 守护进程的表格的程序。  \n## 配置\n```linux\nservice crond start     //启动服务\nservice crond stop      //关闭服务\nservice crond restart   //重启服务\nservice crond reload    //重新载入配置\nservice crond status    //查看crontab服务状态\nchkconfig --level 345 crond on  //在CentOS系统中加入开机自动启动: \n```\n## 添加定时器任务  \n```\ncrontab -e\n```\n如果是root用户，可以直接编写下面的  \n```\n/etc/crontab\n```\n\n## cron文件语法:  \n下面是打开/etc/crontab显示的内容  \n```\nSHELL=/bin/bash\nPATH=/sbin:/bin:/usr/sbin:/usr/bin\nMAILTO=root\nHOME=/\n\n# For details see man 4 crontabs\n\n# Example of job definition:\n# .---------------- minute (0 - 59)\n# |  .------------- hour (0 - 23)\n# |  |  .---------- day of month (1 - 31)\n# |  |  |  .------- month (1 - 12) OR jan,feb,mar,apr ...\n# |  |  |  |  .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat\n# |  |  |  |  |\n# *  *  *  *  * user-name command to be executed\n# */1 * * * * hadoop /home/hadoop/crontest/testcronmkdir.sh\n```\n\n分     时    日       月       周    用户  命令  \n注意最后一个是周，不是年！！！  ,/etc/crontab 需要指定用户, crontable -e就没有  \n 0-59   0-23   1-31   1-12     0-6     command     (取值范围,0表示周日一般一行对应一个任务)  \n \n下面有一些特殊符号需要记住，通过这些特殊符号，可以很灵活的设置定时任务   \n```\n“*”代表取值范围内的数字,\n“/”代表”每”,\n“-”代表从某个数字到某个数字,\n“,”分开几个离散的数字\n```\n如果把最后的命令换做成脚本，例如我上面的那个，记得必须得是.sh脚本，那就需要在最前面引入  \n```\n#!/bin/bash\n```","source":"_posts/Linux  定时任务Crontab.md","raw":"---\ntitle: Linux  定时任务Crontab\ndate: 2017-05-06 21:25:21\ntags: [linux,crontab]\ncategories: [Linux]\nauthor: kaishun\nid: 58\npermalink: linux-crontab\n---\n\n# Crontab 介绍\nCrontab是Linux系统中在固定时间执行某一个程序的工具，类似于Windows系统中的任务计划程序，在一些自动化的程序或者运维中，起到很大的作用，在很多数据处理方面，例如定时备份数据库，定时指定某些任务等等，用linux的Crontab往往比较方便，下面介绍如何安装与使用crontab\n# crontab安装\n## 检查是否已经安装，若已经安装那么不需要重复安装\n先查看crontab有没有安装，有的话就不需要安装了,==root用户下==\n```\n[root@master crontest]# service crond status  \n若没有出现status说明已经安装\n```  \n若没有安装，则按以下步骤进行\n## 安装\n```shell\nyum -y install vixie-cron\nyum -y install crontabs\n```\n说明：\nvixie-cron 软件包是 cron 的主程序；\ncrontabs 软件包是用来安装、卸装、或列举用来驱动 cron 守护进程的表格的程序。  \n## 配置\n```linux\nservice crond start     //启动服务\nservice crond stop      //关闭服务\nservice crond restart   //重启服务\nservice crond reload    //重新载入配置\nservice crond status    //查看crontab服务状态\nchkconfig --level 345 crond on  //在CentOS系统中加入开机自动启动: \n```\n## 添加定时器任务  \n```\ncrontab -e\n```\n如果是root用户，可以直接编写下面的  \n```\n/etc/crontab\n```\n\n## cron文件语法:  \n下面是打开/etc/crontab显示的内容  \n```\nSHELL=/bin/bash\nPATH=/sbin:/bin:/usr/sbin:/usr/bin\nMAILTO=root\nHOME=/\n\n# For details see man 4 crontabs\n\n# Example of job definition:\n# .---------------- minute (0 - 59)\n# |  .------------- hour (0 - 23)\n# |  |  .---------- day of month (1 - 31)\n# |  |  |  .------- month (1 - 12) OR jan,feb,mar,apr ...\n# |  |  |  |  .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat\n# |  |  |  |  |\n# *  *  *  *  * user-name command to be executed\n# */1 * * * * hadoop /home/hadoop/crontest/testcronmkdir.sh\n```\n\n分     时    日       月       周    用户  命令  \n注意最后一个是周，不是年！！！  ,/etc/crontab 需要指定用户, crontable -e就没有  \n 0-59   0-23   1-31   1-12     0-6     command     (取值范围,0表示周日一般一行对应一个任务)  \n \n下面有一些特殊符号需要记住，通过这些特殊符号，可以很灵活的设置定时任务   \n```\n“*”代表取值范围内的数字,\n“/”代表”每”,\n“-”代表从某个数字到某个数字,\n“,”分开几个离散的数字\n```\n如果把最后的命令换做成脚本，例如我上面的那个，记得必须得是.sh脚本，那就需要在最前面引入  \n```\n#!/bin/bash\n```","slug":"linux-crontab","published":1,"updated":"2018-01-22T16:11:15.964Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcrpnzoy000y2wv3gr4tilph","content":"<h1 id=\"Crontab-介绍\"><a href=\"#Crontab-介绍\" class=\"headerlink\" title=\"Crontab 介绍\"></a>Crontab 介绍</h1><p>Crontab是Linux系统中在固定时间执行某一个程序的工具，类似于Windows系统中的任务计划程序，在一些自动化的程序或者运维中，起到很大的作用，在很多数据处理方面，例如定时备份数据库，定时指定某些任务等等，用linux的Crontab往往比较方便，下面介绍如何安装与使用crontab</p>\n<h1 id=\"crontab安装\"><a href=\"#crontab安装\" class=\"headerlink\" title=\"crontab安装\"></a>crontab安装</h1><h2 id=\"检查是否已经安装，若已经安装那么不需要重复安装\"><a href=\"#检查是否已经安装，若已经安装那么不需要重复安装\" class=\"headerlink\" title=\"检查是否已经安装，若已经安装那么不需要重复安装\"></a>检查是否已经安装，若已经安装那么不需要重复安装</h2><p>先查看crontab有没有安装，有的话就不需要安装了,==root用户下==<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@master crontest]# service crond status  </div><div class=\"line\">若没有出现status说明已经安装</div><div class=\"line\">```  </div><div class=\"line\">若没有安装，则按以下步骤进行</div><div class=\"line\">## 安装</div><div class=\"line\">```shell</div><div class=\"line\">yum -y install vixie-cron</div><div class=\"line\">yum -y install crontabs</div></pre></td></tr></table></figure></p>\n<p>说明：<br>vixie-cron 软件包是 cron 的主程序；<br>crontabs 软件包是用来安装、卸装、或列举用来驱动 cron 守护进程的表格的程序。  </p>\n<h2 id=\"配置\"><a href=\"#配置\" class=\"headerlink\" title=\"配置\"></a>配置</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">service crond start     //启动服务</div><div class=\"line\">service crond stop      //关闭服务</div><div class=\"line\">service crond restart   //重启服务</div><div class=\"line\">service crond reload    //重新载入配置</div><div class=\"line\">service crond status    //查看crontab服务状态</div><div class=\"line\">chkconfig --level 345 crond on  //在CentOS系统中加入开机自动启动:</div></pre></td></tr></table></figure>\n<h2 id=\"添加定时器任务\"><a href=\"#添加定时器任务\" class=\"headerlink\" title=\"添加定时器任务\"></a>添加定时器任务</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">crontab -e</div></pre></td></tr></table></figure>\n<p>如果是root用户，可以直接编写下面的<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">/etc/crontab</div></pre></td></tr></table></figure></p>\n<h2 id=\"cron文件语法\"><a href=\"#cron文件语法\" class=\"headerlink\" title=\"cron文件语法:\"></a>cron文件语法:</h2><p>下面是打开/etc/crontab显示的内容<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\">SHELL=/bin/bash</div><div class=\"line\">PATH=/sbin:/bin:/usr/sbin:/usr/bin</div><div class=\"line\">MAILTO=root</div><div class=\"line\">HOME=/</div><div class=\"line\"></div><div class=\"line\"># For details see man 4 crontabs</div><div class=\"line\"></div><div class=\"line\"># Example of job definition:</div><div class=\"line\"># .---------------- minute (0 - 59)</div><div class=\"line\"># |  .------------- hour (0 - 23)</div><div class=\"line\"># |  |  .---------- day of month (1 - 31)</div><div class=\"line\"># |  |  |  .------- month (1 - 12) OR jan,feb,mar,apr ...</div><div class=\"line\"># |  |  |  |  .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat</div><div class=\"line\"># |  |  |  |  |</div><div class=\"line\"># *  *  *  *  * user-name command to be executed</div><div class=\"line\"># */1 * * * * hadoop /home/hadoop/crontest/testcronmkdir.sh</div></pre></td></tr></table></figure></p>\n<p>分     时    日       月       周    用户  命令<br>注意最后一个是周，不是年！！！  ,/etc/crontab 需要指定用户, crontable -e就没有<br> 0-59   0-23   1-31   1-12     0-6     command     (取值范围,0表示周日一般一行对应一个任务)  </p>\n<p>下面有一些特殊符号需要记住，通过这些特殊符号，可以很灵活的设置定时任务<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">“*”代表取值范围内的数字,</div><div class=\"line\">“/”代表”每”,</div><div class=\"line\">“-”代表从某个数字到某个数字,</div><div class=\"line\">“,”分开几个离散的数字</div></pre></td></tr></table></figure></p>\n<p>如果把最后的命令换做成脚本，例如我上面的那个，记得必须得是.sh脚本，那就需要在最前面引入<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">#!/bin/bash</div></pre></td></tr></table></figure></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Crontab-介绍\"><a href=\"#Crontab-介绍\" class=\"headerlink\" title=\"Crontab 介绍\"></a>Crontab 介绍</h1><p>Crontab是Linux系统中在固定时间执行某一个程序的工具，类似于Windows系统中的任务计划程序，在一些自动化的程序或者运维中，起到很大的作用，在很多数据处理方面，例如定时备份数据库，定时指定某些任务等等，用linux的Crontab往往比较方便，下面介绍如何安装与使用crontab</p>\n<h1 id=\"crontab安装\"><a href=\"#crontab安装\" class=\"headerlink\" title=\"crontab安装\"></a>crontab安装</h1><h2 id=\"检查是否已经安装，若已经安装那么不需要重复安装\"><a href=\"#检查是否已经安装，若已经安装那么不需要重复安装\" class=\"headerlink\" title=\"检查是否已经安装，若已经安装那么不需要重复安装\"></a>检查是否已经安装，若已经安装那么不需要重复安装</h2><p>先查看crontab有没有安装，有的话就不需要安装了,==root用户下==<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@master crontest]# service crond status  </div><div class=\"line\">若没有出现status说明已经安装</div><div class=\"line\">```  </div><div class=\"line\">若没有安装，则按以下步骤进行</div><div class=\"line\">## 安装</div><div class=\"line\">```shell</div><div class=\"line\">yum -y install vixie-cron</div><div class=\"line\">yum -y install crontabs</div></pre></td></tr></table></figure></p>\n<p>说明：<br>vixie-cron 软件包是 cron 的主程序；<br>crontabs 软件包是用来安装、卸装、或列举用来驱动 cron 守护进程的表格的程序。  </p>\n<h2 id=\"配置\"><a href=\"#配置\" class=\"headerlink\" title=\"配置\"></a>配置</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">service crond start     //启动服务</div><div class=\"line\">service crond stop      //关闭服务</div><div class=\"line\">service crond restart   //重启服务</div><div class=\"line\">service crond reload    //重新载入配置</div><div class=\"line\">service crond status    //查看crontab服务状态</div><div class=\"line\">chkconfig --level 345 crond on  //在CentOS系统中加入开机自动启动:</div></pre></td></tr></table></figure>\n<h2 id=\"添加定时器任务\"><a href=\"#添加定时器任务\" class=\"headerlink\" title=\"添加定时器任务\"></a>添加定时器任务</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">crontab -e</div></pre></td></tr></table></figure>\n<p>如果是root用户，可以直接编写下面的<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">/etc/crontab</div></pre></td></tr></table></figure></p>\n<h2 id=\"cron文件语法\"><a href=\"#cron文件语法\" class=\"headerlink\" title=\"cron文件语法:\"></a>cron文件语法:</h2><p>下面是打开/etc/crontab显示的内容<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\">SHELL=/bin/bash</div><div class=\"line\">PATH=/sbin:/bin:/usr/sbin:/usr/bin</div><div class=\"line\">MAILTO=root</div><div class=\"line\">HOME=/</div><div class=\"line\"></div><div class=\"line\"># For details see man 4 crontabs</div><div class=\"line\"></div><div class=\"line\"># Example of job definition:</div><div class=\"line\"># .---------------- minute (0 - 59)</div><div class=\"line\"># |  .------------- hour (0 - 23)</div><div class=\"line\"># |  |  .---------- day of month (1 - 31)</div><div class=\"line\"># |  |  |  .------- month (1 - 12) OR jan,feb,mar,apr ...</div><div class=\"line\"># |  |  |  |  .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat</div><div class=\"line\"># |  |  |  |  |</div><div class=\"line\"># *  *  *  *  * user-name command to be executed</div><div class=\"line\"># */1 * * * * hadoop /home/hadoop/crontest/testcronmkdir.sh</div></pre></td></tr></table></figure></p>\n<p>分     时    日       月       周    用户  命令<br>注意最后一个是周，不是年！！！  ,/etc/crontab 需要指定用户, crontable -e就没有<br> 0-59   0-23   1-31   1-12     0-6     command     (取值范围,0表示周日一般一行对应一个任务)  </p>\n<p>下面有一些特殊符号需要记住，通过这些特殊符号，可以很灵活的设置定时任务<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">“*”代表取值范围内的数字,</div><div class=\"line\">“/”代表”每”,</div><div class=\"line\">“-”代表从某个数字到某个数字,</div><div class=\"line\">“,”分开几个离散的数字</div></pre></td></tr></table></figure></p>\n<p>如果把最后的命令换做成脚本，例如我上面的那个，记得必须得是.sh脚本，那就需要在最前面引入<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">#!/bin/bash</div></pre></td></tr></table></figure></p>\n"},{"title":"Hadoop入门案例（六）之二次排序，全排序基础下的二次排序","date":"2016-08-03T13:25:21.000Z","author":"kaishun","id":"19","blogexcerpt":"对于二次排序，定义是在一个字段排好顺序的前提下，另外一个字段也进行排序。类似于sql中的order by多个字段，然而，网上大多数二次排序，对于partitioner都没有利用，因为网上的partition竟然都是是按照hash分组的，而且也没设置reduceTaskNum，这不能充分利用集群的资源，只有一个reduce，，即使如果有多个reduce，partition按照hash分区也不属于全排序，准确的说属于分组排序。我对网上的代码做了稍加修改，可以满足全排序的情况下，再进行二次排序。测试数和代码都参考自网上，具体哪个不记得了，反正网上都差不多测试数据","_content":"\n\n## **前言**：\n对于二次排序，定义是在一个字段排好顺序的前提下，另外一个字段也进行排序。类似于sql中的order by多个字段，然而，网上大多数二次排序，对于partitioner都没有利用，因为网上的partition竟然都是是按照hash分组的，而且也没设置reduceTaskNum，这不能充分利用集群的资源，只有一个reduce，，即使如果有多个reduce，partition按照hash分区也不属于全排序，准确的说属于分组排序。 我对网上的代码做了稍加修改，可以满足全排序的情况下，再进行二次排序。  \n测试数和代码都参考自网上，具体哪个不记得了，反正网上都差不多\n测试数据\n```\n20 21 \n50 51 \n50 52 \n50 53 \n50 54 \n60 51 \n60 53 \n60 52 \n60 56 \n60 57 \n70 58 \n60 61 \n70 54 \n70 55 \n70 56 \n70 57 \n70 58 \n1 2 \n3 4 \n5 6 \n7 82 \n203 21 \n50 512 \n50 522 \n50 53 \n530 54 \n40 511 \n20 53 \n20 522 \n60 56 \n60 57 \n740 58 \n63 61 \n730 54 \n71 55 \n71 56 \n73 57 \n74 58 \n12 211 \n31 42 \n50 62 \n7 8\n```\n\n## **代码如下**\n```java\npackage com.myhadoop.mapreduce.test;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.*;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Partitioner;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.input.TextInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;\n\nimport java.io.DataInput;\nimport java.io.DataOutput;\nimport java.io.IOException;\nimport java.util.StringTokenizer;\n\n/**\n * Created by Administrator on 2017/6/14.\n */\npublic class SecondSort {\n    public static class IntPair implements WritableComparable<IntPair>\n    {\n        int first;\n        int second;\n        /**\n         * Set the left and right values.\n         */\n        public void set(int left, int right)\n        {\n            first = left;\n            second = right;\n        }\n        public int getFirst()\n        {\n            return first;\n        }\n        public int getSecond()\n        {\n            return second;\n        }\n        @Override\n        //反序列化，从流中的二进制转换成IntPair\n        public void readFields(DataInput in) throws IOException\n        {\n            // TODO Auto-generated method stub\n            first = in.readInt();\n            second = in.readInt();\n        }\n        @Override\n        //序列化，将IntPair转化成使用流传送的二进制\n        public void write(DataOutput out) throws IOException\n        {\n            // TODO Auto-generated method stub\n            out.writeInt(first);\n            out.writeInt(second);\n        }\n        @Override\n        //key的比较\n        public int compareTo(IntPair o)\n        {\n            // TODO Auto-generated method stub\n            if (first != o.first)\n            {\n                return first < o.first ? -1 : 1;\n            }\n            else if (second != o.second)\n            {\n                return second < o.second ? -1 : 1;\n            }\n            else\n            {\n                return 0;\n            }\n        }\n\n        //新定义类应该重写的两个方法\n        @Override\n        //The hashCode() method is used by the HashPartitioner (the default partitioner in MapReduce)\n        public int hashCode()\n        {\n            return first * 157 + second;\n        }\n        @Override\n        public boolean equals(Object right)\n        {\n            if (right == null)\n                return false;\n            if (this == right)\n                return true;\n            if (right instanceof IntPair)\n            {\n                IntPair r = (IntPair) right;\n                return r.first == first && r.second == second;\n            }\n            else\n            {\n                return false;\n            }\n        }\n    }\n\n    /**\n     * 分区函数类。根据first确定Partition。\n     */\n    public static class FirstPartitioner extends Partitioner<IntPair, IntWritable>\n    {\n        @Override\n        public int getPartition(IntPair key, IntWritable value,int numPartitions)\n        {\n            int first = key.getFirst();\n            if(first<60){\n                return 0;\n            }else if(first<80){\n                return 1;\n            }\n            return 2;\n\n\n        }\n    }\n\n    /**\n     * 分组函数类。只要first相同就属于同一个组。\n     */\n    public static class GroupingComparator extends WritableComparator\n    {\n        protected GroupingComparator()\n        {\n            super(IntPair.class, true);\n        }\n        @Override\n        //Compare two WritableComparables.\n        public int compare(WritableComparable w1, WritableComparable w2)\n        {\n            IntPair ip1 = (IntPair) w1;\n            IntPair ip2 = (IntPair) w2;\n            int l = ip1.getFirst();\n            int r = ip2.getFirst();\n            return l == r ? 0 : (l < r ? -1 : 1);\n        }\n    }\n    \n    // 自定义map\n    public static class Map extends Mapper<LongWritable, Text, IntPair, IntWritable>\n    {\n        private final IntPair intkey = new IntPair();\n        private final IntWritable intvalue = new IntWritable();\n        public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException\n        {\n            String line = value.toString();\n            StringTokenizer tokenizer = new StringTokenizer(line);\n            int left = 0;\n            int right = 0;\n            if (tokenizer.hasMoreTokens())\n            {\n                left = Integer.parseInt(tokenizer.nextToken());\n                if (tokenizer.hasMoreTokens())\n                    right = Integer.parseInt(tokenizer.nextToken());\n                intkey.set(left, right);\n                intvalue.set(right);\n                context.write(intkey, intvalue);\n            }\n        }\n    }\n    // 自定义reduce\n    //\n    public static class Reduce extends Reducer<IntPair, IntWritable, Text, IntWritable>\n    {\n        private final Text left = new Text();\n        private static final Text SEPARATOR = new Text(\"------------------------------------------------\");\n\n        public void reduce(IntPair key, Iterable<IntWritable> values,Context context) throws IOException, InterruptedException\n        {\n            context.write(SEPARATOR, null);\n            left.set(Integer.toString(key.getFirst()));\n            for (IntWritable val : values)\n            {\n                context.write(left, val);\n            }\n        }\n    }\n    /**\n     * @param args\n     */\n    public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException\n    {\n        // TODO Auto-generated method stub\n        // 读取hadoop配置\n        Configuration conf = new Configuration();\n        // 实例化一道作业\n        Job job = new Job(conf, \"secondarysort\");\n        job.setJarByClass(SecondSort.class);\n        // Mapper类型\n        job.setMapperClass(Map.class);\n        // 不再需要Combiner类型，因为Combiner的输出类型<Text, IntWritable>对Reduce的输入类型<IntPair, IntWritable>不适用\n        //job.setCombinerClass(Reduce.class);\n        // Reducer类型\n        job.setReducerClass(Reduce.class);\n        //设置NumReduceTasks 的数量为三\n        job.setNumReduceTasks(3);\n        // 分区函数\n        job.setPartitionerClass(FirstPartitioner.class);\n\n\n        // 分组函数\n        job.setGroupingComparatorClass(GroupingComparator.class);\n\n        // map 输出Key的类型\n        job.setMapOutputKeyClass(IntPair.class);\n        // map输出Value的类型\n        job.setMapOutputValueClass(IntWritable.class);\n        // rduce输出Key的类型，是Text，因为使用的OutputFormatClass是TextOutputFormat\n        job.setOutputKeyClass(Text.class);\n        // rduce输出Value的类型\n        job.setOutputValueClass(IntWritable.class);\n\n        // 将输入的数据集分割成小数据块splites，同时提供一个RecordReder的实现。\n        job.setInputFormatClass(TextInputFormat.class);\n        // 提供一个RecordWriter的实现，负责数据输出。\n        job.setOutputFormatClass(TextOutputFormat.class);\n\n        // 输入hdfs路径\n        FileInputFormat.setInputPaths(job, new Path(args[0]));\n        // 输出hdfs路径\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n        // 提交job\n        System.exit(job.waitForCompletion(true) ? 0 : 1);\n    }\n}\n\n```  \n## 运行结果\n生成三个文件 partition-00000，partition-00001，partition-00002  \n每个文件第一个字段都按照进行了全排序，满足上一个条件下对第二个字段进行了排序，并且按照第一个字段进行了分组  \n输出的文件分别为 \n**partition-00000**\n```\n------------------------------------------------ \n1       2 \n------------------------------------------------ \n3       4 \n------------------------------------------------ \n5       6 \n------------------------------------------------ \n7       8 \n7       82 \n------------------------------------------------ \n12      211 \n------------------------------------------------ \n20      21 \n20      53 \n20      522 \n------------------------------------------------ \n31      42 \n------------------------------------------------ \n40      511 \n------------------------------------------------ \n50      51 \n50      52 \n50      53 \n50      53 \n50      54 \n50      62 \n50      512 \n50      522 \n```\n**partition-00001**\n```\n------------------------------------------------ \n60      51 \n60      52 \n60      53 \n60      56 \n60      56 \n60      57 \n60      57 \n60      61 \n------------------------------------------------ \n63      61 \n------------------------------------------------ \n70      54 \n70      55 \n70      56 \n70      57 \n70      58 \n70      58 \n------------------------------------------------ \n71      55 \n71      56 \n------------------------------------------------ \n73      57 \n------------------------------------------------ \n74      58 \n```\n**partion-00003**\n```\n------------------------------------------------ \n203     21 \n------------------------------------------------ \n530     54 \n------------------------------------------------ \n730     54 \n------------------------------------------------ \n740     58\n```\n\n## **分析：**\n网上说的已经够清楚了，我这代码主要也是参考于网上的，主要有三步  \n1. 自定义一个对象，需要实现WritableComparable接口，这个对象包含要排序的字段，并且内部有一些方法需要重写，具体参考我的代码  \n2. 自定义分区，网上的hash分区是有小问题的，网上的要么第一个字段不属于全排序，要么只有一个reduce，我们这里参考全排序，自己定义分区，保证整体有序  \n3. 自定义分组（没要求就不需要了），比如只按照第一个字段分组，参考代码中\n4. 正常的map，reduce，map中的key使用我们自定义的对象\n\n## 感悟：\n主要代码还是网上的，但是做程序开发还是要有自己的独立思考，网上的并不是真正的二次排序，或者说即使满足二次排序，但是也是只有一个reduce任务","source":"_posts/Hadoop入门案例（六）之二次排序，全排序基础下的二次排序.md","raw":"---\ntitle: Hadoop入门案例（六）之二次排序，全排序基础下的二次排序\ndate: 2016-08-03 21:25:21\ntags: [hadoop]\ncategories: [大数据,hadoop]\nauthor: kaishun\nid: 19\npermalink: hadoop-example-6\nblogexcerpt: 对于二次排序，定义是在一个字段排好顺序的前提下，另外一个字段也进行排序。类似于sql中的order by多个字段，然而，网上大多数二次排序，对于partitioner都没有利用，因为网上的partition竟然都是是按照hash分组的，而且也没设置reduceTaskNum，这不能充分利用集群的资源，只有一个reduce，，即使如果有多个reduce，partition按照hash分区也不属于全排序，准确的说属于分组排序。我对网上的代码做了稍加修改，可以满足全排序的情况下，再进行二次排序。测试数和代码都参考自网上，具体哪个不记得了，反正网上都差不多测试数据\n---\n\n\n## **前言**：\n对于二次排序，定义是在一个字段排好顺序的前提下，另外一个字段也进行排序。类似于sql中的order by多个字段，然而，网上大多数二次排序，对于partitioner都没有利用，因为网上的partition竟然都是是按照hash分组的，而且也没设置reduceTaskNum，这不能充分利用集群的资源，只有一个reduce，，即使如果有多个reduce，partition按照hash分区也不属于全排序，准确的说属于分组排序。 我对网上的代码做了稍加修改，可以满足全排序的情况下，再进行二次排序。  \n测试数和代码都参考自网上，具体哪个不记得了，反正网上都差不多\n测试数据\n```\n20 21 \n50 51 \n50 52 \n50 53 \n50 54 \n60 51 \n60 53 \n60 52 \n60 56 \n60 57 \n70 58 \n60 61 \n70 54 \n70 55 \n70 56 \n70 57 \n70 58 \n1 2 \n3 4 \n5 6 \n7 82 \n203 21 \n50 512 \n50 522 \n50 53 \n530 54 \n40 511 \n20 53 \n20 522 \n60 56 \n60 57 \n740 58 \n63 61 \n730 54 \n71 55 \n71 56 \n73 57 \n74 58 \n12 211 \n31 42 \n50 62 \n7 8\n```\n\n## **代码如下**\n```java\npackage com.myhadoop.mapreduce.test;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.*;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Partitioner;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.input.TextInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;\n\nimport java.io.DataInput;\nimport java.io.DataOutput;\nimport java.io.IOException;\nimport java.util.StringTokenizer;\n\n/**\n * Created by Administrator on 2017/6/14.\n */\npublic class SecondSort {\n    public static class IntPair implements WritableComparable<IntPair>\n    {\n        int first;\n        int second;\n        /**\n         * Set the left and right values.\n         */\n        public void set(int left, int right)\n        {\n            first = left;\n            second = right;\n        }\n        public int getFirst()\n        {\n            return first;\n        }\n        public int getSecond()\n        {\n            return second;\n        }\n        @Override\n        //反序列化，从流中的二进制转换成IntPair\n        public void readFields(DataInput in) throws IOException\n        {\n            // TODO Auto-generated method stub\n            first = in.readInt();\n            second = in.readInt();\n        }\n        @Override\n        //序列化，将IntPair转化成使用流传送的二进制\n        public void write(DataOutput out) throws IOException\n        {\n            // TODO Auto-generated method stub\n            out.writeInt(first);\n            out.writeInt(second);\n        }\n        @Override\n        //key的比较\n        public int compareTo(IntPair o)\n        {\n            // TODO Auto-generated method stub\n            if (first != o.first)\n            {\n                return first < o.first ? -1 : 1;\n            }\n            else if (second != o.second)\n            {\n                return second < o.second ? -1 : 1;\n            }\n            else\n            {\n                return 0;\n            }\n        }\n\n        //新定义类应该重写的两个方法\n        @Override\n        //The hashCode() method is used by the HashPartitioner (the default partitioner in MapReduce)\n        public int hashCode()\n        {\n            return first * 157 + second;\n        }\n        @Override\n        public boolean equals(Object right)\n        {\n            if (right == null)\n                return false;\n            if (this == right)\n                return true;\n            if (right instanceof IntPair)\n            {\n                IntPair r = (IntPair) right;\n                return r.first == first && r.second == second;\n            }\n            else\n            {\n                return false;\n            }\n        }\n    }\n\n    /**\n     * 分区函数类。根据first确定Partition。\n     */\n    public static class FirstPartitioner extends Partitioner<IntPair, IntWritable>\n    {\n        @Override\n        public int getPartition(IntPair key, IntWritable value,int numPartitions)\n        {\n            int first = key.getFirst();\n            if(first<60){\n                return 0;\n            }else if(first<80){\n                return 1;\n            }\n            return 2;\n\n\n        }\n    }\n\n    /**\n     * 分组函数类。只要first相同就属于同一个组。\n     */\n    public static class GroupingComparator extends WritableComparator\n    {\n        protected GroupingComparator()\n        {\n            super(IntPair.class, true);\n        }\n        @Override\n        //Compare two WritableComparables.\n        public int compare(WritableComparable w1, WritableComparable w2)\n        {\n            IntPair ip1 = (IntPair) w1;\n            IntPair ip2 = (IntPair) w2;\n            int l = ip1.getFirst();\n            int r = ip2.getFirst();\n            return l == r ? 0 : (l < r ? -1 : 1);\n        }\n    }\n    \n    // 自定义map\n    public static class Map extends Mapper<LongWritable, Text, IntPair, IntWritable>\n    {\n        private final IntPair intkey = new IntPair();\n        private final IntWritable intvalue = new IntWritable();\n        public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException\n        {\n            String line = value.toString();\n            StringTokenizer tokenizer = new StringTokenizer(line);\n            int left = 0;\n            int right = 0;\n            if (tokenizer.hasMoreTokens())\n            {\n                left = Integer.parseInt(tokenizer.nextToken());\n                if (tokenizer.hasMoreTokens())\n                    right = Integer.parseInt(tokenizer.nextToken());\n                intkey.set(left, right);\n                intvalue.set(right);\n                context.write(intkey, intvalue);\n            }\n        }\n    }\n    // 自定义reduce\n    //\n    public static class Reduce extends Reducer<IntPair, IntWritable, Text, IntWritable>\n    {\n        private final Text left = new Text();\n        private static final Text SEPARATOR = new Text(\"------------------------------------------------\");\n\n        public void reduce(IntPair key, Iterable<IntWritable> values,Context context) throws IOException, InterruptedException\n        {\n            context.write(SEPARATOR, null);\n            left.set(Integer.toString(key.getFirst()));\n            for (IntWritable val : values)\n            {\n                context.write(left, val);\n            }\n        }\n    }\n    /**\n     * @param args\n     */\n    public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException\n    {\n        // TODO Auto-generated method stub\n        // 读取hadoop配置\n        Configuration conf = new Configuration();\n        // 实例化一道作业\n        Job job = new Job(conf, \"secondarysort\");\n        job.setJarByClass(SecondSort.class);\n        // Mapper类型\n        job.setMapperClass(Map.class);\n        // 不再需要Combiner类型，因为Combiner的输出类型<Text, IntWritable>对Reduce的输入类型<IntPair, IntWritable>不适用\n        //job.setCombinerClass(Reduce.class);\n        // Reducer类型\n        job.setReducerClass(Reduce.class);\n        //设置NumReduceTasks 的数量为三\n        job.setNumReduceTasks(3);\n        // 分区函数\n        job.setPartitionerClass(FirstPartitioner.class);\n\n\n        // 分组函数\n        job.setGroupingComparatorClass(GroupingComparator.class);\n\n        // map 输出Key的类型\n        job.setMapOutputKeyClass(IntPair.class);\n        // map输出Value的类型\n        job.setMapOutputValueClass(IntWritable.class);\n        // rduce输出Key的类型，是Text，因为使用的OutputFormatClass是TextOutputFormat\n        job.setOutputKeyClass(Text.class);\n        // rduce输出Value的类型\n        job.setOutputValueClass(IntWritable.class);\n\n        // 将输入的数据集分割成小数据块splites，同时提供一个RecordReder的实现。\n        job.setInputFormatClass(TextInputFormat.class);\n        // 提供一个RecordWriter的实现，负责数据输出。\n        job.setOutputFormatClass(TextOutputFormat.class);\n\n        // 输入hdfs路径\n        FileInputFormat.setInputPaths(job, new Path(args[0]));\n        // 输出hdfs路径\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n        // 提交job\n        System.exit(job.waitForCompletion(true) ? 0 : 1);\n    }\n}\n\n```  \n## 运行结果\n生成三个文件 partition-00000，partition-00001，partition-00002  \n每个文件第一个字段都按照进行了全排序，满足上一个条件下对第二个字段进行了排序，并且按照第一个字段进行了分组  \n输出的文件分别为 \n**partition-00000**\n```\n------------------------------------------------ \n1       2 \n------------------------------------------------ \n3       4 \n------------------------------------------------ \n5       6 \n------------------------------------------------ \n7       8 \n7       82 \n------------------------------------------------ \n12      211 \n------------------------------------------------ \n20      21 \n20      53 \n20      522 \n------------------------------------------------ \n31      42 \n------------------------------------------------ \n40      511 \n------------------------------------------------ \n50      51 \n50      52 \n50      53 \n50      53 \n50      54 \n50      62 \n50      512 \n50      522 \n```\n**partition-00001**\n```\n------------------------------------------------ \n60      51 \n60      52 \n60      53 \n60      56 \n60      56 \n60      57 \n60      57 \n60      61 \n------------------------------------------------ \n63      61 \n------------------------------------------------ \n70      54 \n70      55 \n70      56 \n70      57 \n70      58 \n70      58 \n------------------------------------------------ \n71      55 \n71      56 \n------------------------------------------------ \n73      57 \n------------------------------------------------ \n74      58 \n```\n**partion-00003**\n```\n------------------------------------------------ \n203     21 \n------------------------------------------------ \n530     54 \n------------------------------------------------ \n730     54 \n------------------------------------------------ \n740     58\n```\n\n## **分析：**\n网上说的已经够清楚了，我这代码主要也是参考于网上的，主要有三步  \n1. 自定义一个对象，需要实现WritableComparable接口，这个对象包含要排序的字段，并且内部有一些方法需要重写，具体参考我的代码  \n2. 自定义分区，网上的hash分区是有小问题的，网上的要么第一个字段不属于全排序，要么只有一个reduce，我们这里参考全排序，自己定义分区，保证整体有序  \n3. 自定义分组（没要求就不需要了），比如只按照第一个字段分组，参考代码中\n4. 正常的map，reduce，map中的key使用我们自定义的对象\n\n## 感悟：\n主要代码还是网上的，但是做程序开发还是要有自己的独立思考，网上的并不是真正的二次排序，或者说即使满足二次排序，但是也是只有一个reduce任务","slug":"hadoop-example-6","published":1,"updated":"2018-01-23T14:22:23.657Z","_id":"cjcrpnzoy00122wv33zzvehxb","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"前言：\"><a href=\"#前言：\" class=\"headerlink\" title=\"前言：\"></a><strong>前言</strong>：</h2><p>对于二次排序，定义是在一个字段排好顺序的前提下，另外一个字段也进行排序。类似于sql中的order by多个字段，然而，网上大多数二次排序，对于partitioner都没有利用，因为网上的partition竟然都是是按照hash分组的，而且也没设置reduceTaskNum，这不能充分利用集群的资源，只有一个reduce，，即使如果有多个reduce，partition按照hash分区也不属于全排序，准确的说属于分组排序。 我对网上的代码做了稍加修改，可以满足全排序的情况下，再进行二次排序。<br>测试数和代码都参考自网上，具体哪个不记得了，反正网上都差不多<br>测试数据<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div></pre></td><td class=\"code\"><pre><div class=\"line\">20 21 </div><div class=\"line\">50 51 </div><div class=\"line\">50 52 </div><div class=\"line\">50 53 </div><div class=\"line\">50 54 </div><div class=\"line\">60 51 </div><div class=\"line\">60 53 </div><div class=\"line\">60 52 </div><div class=\"line\">60 56 </div><div class=\"line\">60 57 </div><div class=\"line\">70 58 </div><div class=\"line\">60 61 </div><div class=\"line\">70 54 </div><div class=\"line\">70 55 </div><div class=\"line\">70 56 </div><div class=\"line\">70 57 </div><div class=\"line\">70 58 </div><div class=\"line\">1 2 </div><div class=\"line\">3 4 </div><div class=\"line\">5 6 </div><div class=\"line\">7 82 </div><div class=\"line\">203 21 </div><div class=\"line\">50 512 </div><div class=\"line\">50 522 </div><div class=\"line\">50 53 </div><div class=\"line\">530 54 </div><div class=\"line\">40 511 </div><div class=\"line\">20 53 </div><div class=\"line\">20 522 </div><div class=\"line\">60 56 </div><div class=\"line\">60 57 </div><div class=\"line\">740 58 </div><div class=\"line\">63 61 </div><div class=\"line\">730 54 </div><div class=\"line\">71 55 </div><div class=\"line\">71 56 </div><div class=\"line\">73 57 </div><div class=\"line\">74 58 </div><div class=\"line\">12 211 </div><div class=\"line\">31 42 </div><div class=\"line\">50 62 </div><div class=\"line\">7 8</div></pre></td></tr></table></figure></p>\n<h2 id=\"代码如下\"><a href=\"#代码如下\" class=\"headerlink\" title=\"代码如下\"></a><strong>代码如下</strong></h2><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div><div class=\"line\">106</div><div class=\"line\">107</div><div class=\"line\">108</div><div class=\"line\">109</div><div class=\"line\">110</div><div class=\"line\">111</div><div class=\"line\">112</div><div class=\"line\">113</div><div class=\"line\">114</div><div class=\"line\">115</div><div class=\"line\">116</div><div class=\"line\">117</div><div class=\"line\">118</div><div class=\"line\">119</div><div class=\"line\">120</div><div class=\"line\">121</div><div class=\"line\">122</div><div class=\"line\">123</div><div class=\"line\">124</div><div class=\"line\">125</div><div class=\"line\">126</div><div class=\"line\">127</div><div class=\"line\">128</div><div class=\"line\">129</div><div class=\"line\">130</div><div class=\"line\">131</div><div class=\"line\">132</div><div class=\"line\">133</div><div class=\"line\">134</div><div class=\"line\">135</div><div class=\"line\">136</div><div class=\"line\">137</div><div class=\"line\">138</div><div class=\"line\">139</div><div class=\"line\">140</div><div class=\"line\">141</div><div class=\"line\">142</div><div class=\"line\">143</div><div class=\"line\">144</div><div class=\"line\">145</div><div class=\"line\">146</div><div class=\"line\">147</div><div class=\"line\">148</div><div class=\"line\">149</div><div class=\"line\">150</div><div class=\"line\">151</div><div class=\"line\">152</div><div class=\"line\">153</div><div class=\"line\">154</div><div class=\"line\">155</div><div class=\"line\">156</div><div class=\"line\">157</div><div class=\"line\">158</div><div class=\"line\">159</div><div class=\"line\">160</div><div class=\"line\">161</div><div class=\"line\">162</div><div class=\"line\">163</div><div class=\"line\">164</div><div class=\"line\">165</div><div class=\"line\">166</div><div class=\"line\">167</div><div class=\"line\">168</div><div class=\"line\">169</div><div class=\"line\">170</div><div class=\"line\">171</div><div class=\"line\">172</div><div class=\"line\">173</div><div class=\"line\">174</div><div class=\"line\">175</div><div class=\"line\">176</div><div class=\"line\">177</div><div class=\"line\">178</div><div class=\"line\">179</div><div class=\"line\">180</div><div class=\"line\">181</div><div class=\"line\">182</div><div class=\"line\">183</div><div class=\"line\">184</div><div class=\"line\">185</div><div class=\"line\">186</div><div class=\"line\">187</div><div class=\"line\">188</div><div class=\"line\">189</div><div class=\"line\">190</div><div class=\"line\">191</div><div class=\"line\">192</div><div class=\"line\">193</div><div class=\"line\">194</div><div class=\"line\">195</div><div class=\"line\">196</div><div class=\"line\">197</div><div class=\"line\">198</div><div class=\"line\">199</div><div class=\"line\">200</div><div class=\"line\">201</div><div class=\"line\">202</div><div class=\"line\">203</div><div class=\"line\">204</div><div class=\"line\">205</div><div class=\"line\">206</div><div class=\"line\">207</div><div class=\"line\">208</div><div class=\"line\">209</div><div class=\"line\">210</div><div class=\"line\">211</div><div class=\"line\">212</div><div class=\"line\">213</div><div class=\"line\">214</div><div class=\"line\">215</div><div class=\"line\">216</div><div class=\"line\">217</div><div class=\"line\">218</div><div class=\"line\">219</div><div class=\"line\">220</div><div class=\"line\">221</div><div class=\"line\">222</div><div class=\"line\">223</div><div class=\"line\">224</div><div class=\"line\">225</div><div class=\"line\">226</div><div class=\"line\">227</div><div class=\"line\">228</div><div class=\"line\">229</div><div class=\"line\">230</div><div class=\"line\">231</div><div class=\"line\">232</div><div class=\"line\">233</div><div class=\"line\">234</div><div class=\"line\">235</div><div class=\"line\">236</div><div class=\"line\">237</div><div class=\"line\">238</div><div class=\"line\">239</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">package</span> com.myhadoop.mapreduce.test;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.conf.Configuration;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.fs.Path;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.*;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Job;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Mapper;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Partitioner;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Reducer;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">import</span> java.io.DataInput;</div><div class=\"line\"><span class=\"keyword\">import</span> java.io.DataOutput;</div><div class=\"line\"><span class=\"keyword\">import</span> java.io.IOException;</div><div class=\"line\"><span class=\"keyword\">import</span> java.util.StringTokenizer;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\"> * Created by Administrator on 2017/6/14.</span></div><div class=\"line\"><span class=\"comment\"> */</span></div><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SecondSort</span> </span>&#123;</div><div class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">IntPair</span> <span class=\"keyword\">implements</span> <span class=\"title\">WritableComparable</span>&lt;<span class=\"title\">IntPair</span>&gt;</span></div><div class=\"line\"><span class=\"class\">    </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">int</span> first;</div><div class=\"line\">        <span class=\"keyword\">int</span> second;</div><div class=\"line\">        <span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\">         * Set the left and right values.</span></div><div class=\"line\"><span class=\"comment\">         */</span></div><div class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">set</span><span class=\"params\">(<span class=\"keyword\">int</span> left, <span class=\"keyword\">int</span> right)</span></span></div><div class=\"line\"><span class=\"function\">        </span>&#123;</div><div class=\"line\">            first = left;</div><div class=\"line\">            second = right;</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">getFirst</span><span class=\"params\">()</span></span></div><div class=\"line\"><span class=\"function\">        </span>&#123;</div><div class=\"line\">            <span class=\"keyword\">return</span> first;</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">getSecond</span><span class=\"params\">()</span></span></div><div class=\"line\"><span class=\"function\">        </span>&#123;</div><div class=\"line\">            <span class=\"keyword\">return</span> second;</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"meta\">@Override</span></div><div class=\"line\">        <span class=\"comment\">//反序列化，从流中的二进制转换成IntPair</span></div><div class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">readFields</span><span class=\"params\">(DataInput in)</span> <span class=\"keyword\">throws</span> IOException</span></div><div class=\"line\"><span class=\"function\">        </span>&#123;</div><div class=\"line\">            <span class=\"comment\">// TODO Auto-generated method stub</span></div><div class=\"line\">            first = in.readInt();</div><div class=\"line\">            second = in.readInt();</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"meta\">@Override</span></div><div class=\"line\">        <span class=\"comment\">//序列化，将IntPair转化成使用流传送的二进制</span></div><div class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">write</span><span class=\"params\">(DataOutput out)</span> <span class=\"keyword\">throws</span> IOException</span></div><div class=\"line\"><span class=\"function\">        </span>&#123;</div><div class=\"line\">            <span class=\"comment\">// TODO Auto-generated method stub</span></div><div class=\"line\">            out.writeInt(first);</div><div class=\"line\">            out.writeInt(second);</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"meta\">@Override</span></div><div class=\"line\">        <span class=\"comment\">//key的比较</span></div><div class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">compareTo</span><span class=\"params\">(IntPair o)</span></span></div><div class=\"line\"><span class=\"function\">        </span>&#123;</div><div class=\"line\">            <span class=\"comment\">// TODO Auto-generated method stub</span></div><div class=\"line\">            <span class=\"keyword\">if</span> (first != o.first)</div><div class=\"line\">            &#123;</div><div class=\"line\">                <span class=\"keyword\">return</span> first &lt; o.first ? -<span class=\"number\">1</span> : <span class=\"number\">1</span>;</div><div class=\"line\">            &#125;</div><div class=\"line\">            <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (second != o.second)</div><div class=\"line\">            &#123;</div><div class=\"line\">                <span class=\"keyword\">return</span> second &lt; o.second ? -<span class=\"number\">1</span> : <span class=\"number\">1</span>;</div><div class=\"line\">            &#125;</div><div class=\"line\">            <span class=\"keyword\">else</span></div><div class=\"line\">            &#123;</div><div class=\"line\">                <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\">//新定义类应该重写的两个方法</span></div><div class=\"line\">        <span class=\"meta\">@Override</span></div><div class=\"line\">        <span class=\"comment\">//The hashCode() method is used by the HashPartitioner (the default partitioner in MapReduce)</span></div><div class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">hashCode</span><span class=\"params\">()</span></span></div><div class=\"line\"><span class=\"function\">        </span>&#123;</div><div class=\"line\">            <span class=\"keyword\">return</span> first * <span class=\"number\">157</span> + second;</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"meta\">@Override</span></div><div class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">equals</span><span class=\"params\">(Object right)</span></span></div><div class=\"line\"><span class=\"function\">        </span>&#123;</div><div class=\"line\">            <span class=\"keyword\">if</span> (right == <span class=\"keyword\">null</span>)</div><div class=\"line\">                <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</div><div class=\"line\">            <span class=\"keyword\">if</span> (<span class=\"keyword\">this</span> == right)</div><div class=\"line\">                <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</div><div class=\"line\">            <span class=\"keyword\">if</span> (right <span class=\"keyword\">instanceof</span> IntPair)</div><div class=\"line\">            &#123;</div><div class=\"line\">                IntPair r = (IntPair) right;</div><div class=\"line\">                <span class=\"keyword\">return</span> r.first == first &amp;&amp; r.second == second;</div><div class=\"line\">            &#125;</div><div class=\"line\">            <span class=\"keyword\">else</span></div><div class=\"line\">            &#123;</div><div class=\"line\">                <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\">     * 分区函数类。根据first确定Partition。</span></div><div class=\"line\"><span class=\"comment\">     */</span></div><div class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">FirstPartitioner</span> <span class=\"keyword\">extends</span> <span class=\"title\">Partitioner</span>&lt;<span class=\"title\">IntPair</span>, <span class=\"title\">IntWritable</span>&gt;</span></div><div class=\"line\"><span class=\"class\">    </span>&#123;</div><div class=\"line\">        <span class=\"meta\">@Override</span></div><div class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">getPartition</span><span class=\"params\">(IntPair key, IntWritable value,<span class=\"keyword\">int</span> numPartitions)</span></span></div><div class=\"line\"><span class=\"function\">        </span>&#123;</div><div class=\"line\">            <span class=\"keyword\">int</span> first = key.getFirst();</div><div class=\"line\">            <span class=\"keyword\">if</span>(first&lt;<span class=\"number\">60</span>)&#123;</div><div class=\"line\">                <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</div><div class=\"line\">            &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(first&lt;<span class=\"number\">80</span>)&#123;</div><div class=\"line\">                <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</div><div class=\"line\">            &#125;</div><div class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">2</span>;</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\">     * 分组函数类。只要first相同就属于同一个组。</span></div><div class=\"line\"><span class=\"comment\">     */</span></div><div class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">GroupingComparator</span> <span class=\"keyword\">extends</span> <span class=\"title\">WritableComparator</span></span></div><div class=\"line\"><span class=\"class\">    </span>&#123;</div><div class=\"line\">        <span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"title\">GroupingComparator</span><span class=\"params\">()</span></span></div><div class=\"line\"><span class=\"function\">        </span>&#123;</div><div class=\"line\">            <span class=\"keyword\">super</span>(IntPair.class, <span class=\"keyword\">true</span>);</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"meta\">@Override</span></div><div class=\"line\">        <span class=\"comment\">//Compare two WritableComparables.</span></div><div class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">compare</span><span class=\"params\">(WritableComparable w1, WritableComparable w2)</span></span></div><div class=\"line\"><span class=\"function\">        </span>&#123;</div><div class=\"line\">            IntPair ip1 = (IntPair) w1;</div><div class=\"line\">            IntPair ip2 = (IntPair) w2;</div><div class=\"line\">            <span class=\"keyword\">int</span> l = ip1.getFirst();</div><div class=\"line\">            <span class=\"keyword\">int</span> r = ip2.getFirst();</div><div class=\"line\">            <span class=\"keyword\">return</span> l == r ? <span class=\"number\">0</span> : (l &lt; r ? -<span class=\"number\">1</span> : <span class=\"number\">1</span>);</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">    </div><div class=\"line\">    <span class=\"comment\">// 自定义map</span></div><div class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Map</span> <span class=\"keyword\">extends</span> <span class=\"title\">Mapper</span>&lt;<span class=\"title\">LongWritable</span>, <span class=\"title\">Text</span>, <span class=\"title\">IntPair</span>, <span class=\"title\">IntWritable</span>&gt;</span></div><div class=\"line\"><span class=\"class\">    </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">private</span> <span class=\"keyword\">final</span> IntPair intkey = <span class=\"keyword\">new</span> IntPair();</div><div class=\"line\">        <span class=\"keyword\">private</span> <span class=\"keyword\">final</span> IntWritable intvalue = <span class=\"keyword\">new</span> IntWritable();</div><div class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">map</span><span class=\"params\">(LongWritable key, Text value, Context context)</span> <span class=\"keyword\">throws</span> IOException, InterruptedException</span></div><div class=\"line\"><span class=\"function\">        </span>&#123;</div><div class=\"line\">            String line = value.toString();</div><div class=\"line\">            StringTokenizer tokenizer = <span class=\"keyword\">new</span> StringTokenizer(line);</div><div class=\"line\">            <span class=\"keyword\">int</span> left = <span class=\"number\">0</span>;</div><div class=\"line\">            <span class=\"keyword\">int</span> right = <span class=\"number\">0</span>;</div><div class=\"line\">            <span class=\"keyword\">if</span> (tokenizer.hasMoreTokens())</div><div class=\"line\">            &#123;</div><div class=\"line\">                left = Integer.parseInt(tokenizer.nextToken());</div><div class=\"line\">                <span class=\"keyword\">if</span> (tokenizer.hasMoreTokens())</div><div class=\"line\">                    right = Integer.parseInt(tokenizer.nextToken());</div><div class=\"line\">                intkey.set(left, right);</div><div class=\"line\">                intvalue.set(right);</div><div class=\"line\">                context.write(intkey, intvalue);</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">    <span class=\"comment\">// 自定义reduce</span></div><div class=\"line\">    <span class=\"comment\">//</span></div><div class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Reduce</span> <span class=\"keyword\">extends</span> <span class=\"title\">Reducer</span>&lt;<span class=\"title\">IntPair</span>, <span class=\"title\">IntWritable</span>, <span class=\"title\">Text</span>, <span class=\"title\">IntWritable</span>&gt;</span></div><div class=\"line\"><span class=\"class\">    </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">private</span> <span class=\"keyword\">final</span> Text left = <span class=\"keyword\">new</span> Text();</div><div class=\"line\">        <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> Text SEPARATOR = <span class=\"keyword\">new</span> Text(<span class=\"string\">\"------------------------------------------------\"</span>);</div><div class=\"line\"></div><div class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">reduce</span><span class=\"params\">(IntPair key, Iterable&lt;IntWritable&gt; values,Context context)</span> <span class=\"keyword\">throws</span> IOException, InterruptedException</span></div><div class=\"line\"><span class=\"function\">        </span>&#123;</div><div class=\"line\">            context.write(SEPARATOR, <span class=\"keyword\">null</span>);</div><div class=\"line\">            left.set(Integer.toString(key.getFirst()));</div><div class=\"line\">            <span class=\"keyword\">for</span> (IntWritable val : values)</div><div class=\"line\">            &#123;</div><div class=\"line\">                context.write(left, val);</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">    <span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> args</span></div><div class=\"line\"><span class=\"comment\">     */</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> IOException, InterruptedException, ClassNotFoundException</span></div><div class=\"line\"><span class=\"function\">    </span>&#123;</div><div class=\"line\">        <span class=\"comment\">// TODO Auto-generated method stub</span></div><div class=\"line\">        <span class=\"comment\">// 读取hadoop配置</span></div><div class=\"line\">        Configuration conf = <span class=\"keyword\">new</span> Configuration();</div><div class=\"line\">        <span class=\"comment\">// 实例化一道作业</span></div><div class=\"line\">        Job job = <span class=\"keyword\">new</span> Job(conf, <span class=\"string\">\"secondarysort\"</span>);</div><div class=\"line\">        job.setJarByClass(SecondSort.class);</div><div class=\"line\">        <span class=\"comment\">// Mapper类型</span></div><div class=\"line\">        job.setMapperClass(Map.class);</div><div class=\"line\">        <span class=\"comment\">// 不再需要Combiner类型，因为Combiner的输出类型&lt;Text, IntWritable&gt;对Reduce的输入类型&lt;IntPair, IntWritable&gt;不适用</span></div><div class=\"line\">        <span class=\"comment\">//job.setCombinerClass(Reduce.class);</span></div><div class=\"line\">        <span class=\"comment\">// Reducer类型</span></div><div class=\"line\">        job.setReducerClass(Reduce.class);</div><div class=\"line\">        <span class=\"comment\">//设置NumReduceTasks 的数量为三</span></div><div class=\"line\">        job.setNumReduceTasks(<span class=\"number\">3</span>);</div><div class=\"line\">        <span class=\"comment\">// 分区函数</span></div><div class=\"line\">        job.setPartitionerClass(FirstPartitioner.class);</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\">// 分组函数</span></div><div class=\"line\">        job.setGroupingComparatorClass(GroupingComparator.class);</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\">// map 输出Key的类型</span></div><div class=\"line\">        job.setMapOutputKeyClass(IntPair.class);</div><div class=\"line\">        <span class=\"comment\">// map输出Value的类型</span></div><div class=\"line\">        job.setMapOutputValueClass(IntWritable.class);</div><div class=\"line\">        <span class=\"comment\">// rduce输出Key的类型，是Text，因为使用的OutputFormatClass是TextOutputFormat</span></div><div class=\"line\">        job.setOutputKeyClass(Text.class);</div><div class=\"line\">        <span class=\"comment\">// rduce输出Value的类型</span></div><div class=\"line\">        job.setOutputValueClass(IntWritable.class);</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\">// 将输入的数据集分割成小数据块splites，同时提供一个RecordReder的实现。</span></div><div class=\"line\">        job.setInputFormatClass(TextInputFormat.class);</div><div class=\"line\">        <span class=\"comment\">// 提供一个RecordWriter的实现，负责数据输出。</span></div><div class=\"line\">        job.setOutputFormatClass(TextOutputFormat.class);</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\">// 输入hdfs路径</span></div><div class=\"line\">        FileInputFormat.setInputPaths(job, <span class=\"keyword\">new</span> Path(args[<span class=\"number\">0</span>]));</div><div class=\"line\">        <span class=\"comment\">// 输出hdfs路径</span></div><div class=\"line\">        FileOutputFormat.setOutputPath(job, <span class=\"keyword\">new</span> Path(args[<span class=\"number\">1</span>]));</div><div class=\"line\">        <span class=\"comment\">// 提交job</span></div><div class=\"line\">        System.exit(job.waitForCompletion(<span class=\"keyword\">true</span>) ? <span class=\"number\">0</span> : <span class=\"number\">1</span>);</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">```  </div><div class=\"line\">## 运行结果</div><div class=\"line\">生成三个文件 partition-<span class=\"number\">00000</span>，partition-<span class=\"number\">00001</span>，partition-<span class=\"number\">00002</span>  </div><div class=\"line\">每个文件第一个字段都按照进行了全排序，满足上一个条件下对第二个字段进行了排序，并且按照第一个字段进行了分组  </div><div class=\"line\">输出的文件分别为 </div><div class=\"line\">**partition-<span class=\"number\">00000</span>**</div></pre></td></tr></table></figure>\n<hr>\n<h2 id=\"1-2\"><a href=\"#1-2\" class=\"headerlink\" title=\"1       2 \"></a>1       2 </h2><h2 id=\"3-4\"><a href=\"#3-4\" class=\"headerlink\" title=\"3       4 \"></a>3       4 </h2><h2 id=\"5-6\"><a href=\"#5-6\" class=\"headerlink\" title=\"5       6 \"></a>5       6 </h2><p>7       8 </p>\n<h2 id=\"7-82\"><a href=\"#7-82\" class=\"headerlink\" title=\"7       82 \"></a>7       82 </h2><h2 id=\"12-211\"><a href=\"#12-211\" class=\"headerlink\" title=\"12      211 \"></a>12      211 </h2><p>20      21<br>20      53 </p>\n<h2 id=\"20-522\"><a href=\"#20-522\" class=\"headerlink\" title=\"20      522 \"></a>20      522 </h2><h2 id=\"31-42\"><a href=\"#31-42\" class=\"headerlink\" title=\"31      42 \"></a>31      42 </h2><h2 id=\"40-511\"><a href=\"#40-511\" class=\"headerlink\" title=\"40      511 \"></a>40      511 </h2><p>50      51<br>50      52<br>50      53<br>50      53<br>50      54<br>50      62<br>50      512<br>50      522<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">**partition-00001**</div></pre></td></tr></table></figure></p>\n<hr>\n<p>60      51<br>60      52<br>60      53<br>60      56<br>60      56<br>60      57<br>60      57 </p>\n<h2 id=\"60-61\"><a href=\"#60-61\" class=\"headerlink\" title=\"60      61 \"></a>60      61 </h2><h2 id=\"63-61\"><a href=\"#63-61\" class=\"headerlink\" title=\"63      61 \"></a>63      61 </h2><p>70      54<br>70      55<br>70      56<br>70      57<br>70      58 </p>\n<h2 id=\"70-58\"><a href=\"#70-58\" class=\"headerlink\" title=\"70      58 \"></a>70      58 </h2><p>71      55 </p>\n<h2 id=\"71-56\"><a href=\"#71-56\" class=\"headerlink\" title=\"71      56 \"></a>71      56 </h2><h2 id=\"73-57\"><a href=\"#73-57\" class=\"headerlink\" title=\"73      57 \"></a>73      57 </h2><p>74      58<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">**partion-00003**</div></pre></td></tr></table></figure></p>\n<hr>\n<h2 id=\"203-21\"><a href=\"#203-21\" class=\"headerlink\" title=\"203     21 \"></a>203     21 </h2><h2 id=\"530-54\"><a href=\"#530-54\" class=\"headerlink\" title=\"530     54 \"></a>530     54 </h2><h2 id=\"730-54\"><a href=\"#730-54\" class=\"headerlink\" title=\"730     54 \"></a>730     54 </h2><p>740     58<br>```</p>\n<h2 id=\"分析：\"><a href=\"#分析：\" class=\"headerlink\" title=\"分析：\"></a><strong>分析：</strong></h2><p>网上说的已经够清楚了，我这代码主要也是参考于网上的，主要有三步  </p>\n<ol>\n<li>自定义一个对象，需要实现WritableComparable接口，这个对象包含要排序的字段，并且内部有一些方法需要重写，具体参考我的代码  </li>\n<li>自定义分区，网上的hash分区是有小问题的，网上的要么第一个字段不属于全排序，要么只有一个reduce，我们这里参考全排序，自己定义分区，保证整体有序  </li>\n<li>自定义分组（没要求就不需要了），比如只按照第一个字段分组，参考代码中</li>\n<li>正常的map，reduce，map中的key使用我们自定义的对象</li>\n</ol>\n<h2 id=\"感悟：\"><a href=\"#感悟：\" class=\"headerlink\" title=\"感悟：\"></a>感悟：</h2><p>主要代码还是网上的，但是做程序开发还是要有自己的独立思考，网上的并不是真正的二次排序，或者说即使满足二次排序，但是也是只有一个reduce任务</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"前言：\"><a href=\"#前言：\" class=\"headerlink\" title=\"前言：\"></a><strong>前言</strong>：</h2><p>对于二次排序，定义是在一个字段排好顺序的前提下，另外一个字段也进行排序。类似于sql中的order by多个字段，然而，网上大多数二次排序，对于partitioner都没有利用，因为网上的partition竟然都是是按照hash分组的，而且也没设置reduceTaskNum，这不能充分利用集群的资源，只有一个reduce，，即使如果有多个reduce，partition按照hash分区也不属于全排序，准确的说属于分组排序。 我对网上的代码做了稍加修改，可以满足全排序的情况下，再进行二次排序。<br>测试数和代码都参考自网上，具体哪个不记得了，反正网上都差不多<br>测试数据<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div></pre></td><td class=\"code\"><pre><div class=\"line\">20 21 </div><div class=\"line\">50 51 </div><div class=\"line\">50 52 </div><div class=\"line\">50 53 </div><div class=\"line\">50 54 </div><div class=\"line\">60 51 </div><div class=\"line\">60 53 </div><div class=\"line\">60 52 </div><div class=\"line\">60 56 </div><div class=\"line\">60 57 </div><div class=\"line\">70 58 </div><div class=\"line\">60 61 </div><div class=\"line\">70 54 </div><div class=\"line\">70 55 </div><div class=\"line\">70 56 </div><div class=\"line\">70 57 </div><div class=\"line\">70 58 </div><div class=\"line\">1 2 </div><div class=\"line\">3 4 </div><div class=\"line\">5 6 </div><div class=\"line\">7 82 </div><div class=\"line\">203 21 </div><div class=\"line\">50 512 </div><div class=\"line\">50 522 </div><div class=\"line\">50 53 </div><div class=\"line\">530 54 </div><div class=\"line\">40 511 </div><div class=\"line\">20 53 </div><div class=\"line\">20 522 </div><div class=\"line\">60 56 </div><div class=\"line\">60 57 </div><div class=\"line\">740 58 </div><div class=\"line\">63 61 </div><div class=\"line\">730 54 </div><div class=\"line\">71 55 </div><div class=\"line\">71 56 </div><div class=\"line\">73 57 </div><div class=\"line\">74 58 </div><div class=\"line\">12 211 </div><div class=\"line\">31 42 </div><div class=\"line\">50 62 </div><div class=\"line\">7 8</div></pre></td></tr></table></figure></p>\n<h2 id=\"代码如下\"><a href=\"#代码如下\" class=\"headerlink\" title=\"代码如下\"></a><strong>代码如下</strong></h2><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div><div class=\"line\">106</div><div class=\"line\">107</div><div class=\"line\">108</div><div class=\"line\">109</div><div class=\"line\">110</div><div class=\"line\">111</div><div class=\"line\">112</div><div class=\"line\">113</div><div class=\"line\">114</div><div class=\"line\">115</div><div class=\"line\">116</div><div class=\"line\">117</div><div class=\"line\">118</div><div class=\"line\">119</div><div class=\"line\">120</div><div class=\"line\">121</div><div class=\"line\">122</div><div class=\"line\">123</div><div class=\"line\">124</div><div class=\"line\">125</div><div class=\"line\">126</div><div class=\"line\">127</div><div class=\"line\">128</div><div class=\"line\">129</div><div class=\"line\">130</div><div class=\"line\">131</div><div class=\"line\">132</div><div class=\"line\">133</div><div class=\"line\">134</div><div class=\"line\">135</div><div class=\"line\">136</div><div class=\"line\">137</div><div class=\"line\">138</div><div class=\"line\">139</div><div class=\"line\">140</div><div class=\"line\">141</div><div class=\"line\">142</div><div class=\"line\">143</div><div class=\"line\">144</div><div class=\"line\">145</div><div class=\"line\">146</div><div class=\"line\">147</div><div class=\"line\">148</div><div class=\"line\">149</div><div class=\"line\">150</div><div class=\"line\">151</div><div class=\"line\">152</div><div class=\"line\">153</div><div class=\"line\">154</div><div class=\"line\">155</div><div class=\"line\">156</div><div class=\"line\">157</div><div class=\"line\">158</div><div class=\"line\">159</div><div class=\"line\">160</div><div class=\"line\">161</div><div class=\"line\">162</div><div class=\"line\">163</div><div class=\"line\">164</div><div class=\"line\">165</div><div class=\"line\">166</div><div class=\"line\">167</div><div class=\"line\">168</div><div class=\"line\">169</div><div class=\"line\">170</div><div class=\"line\">171</div><div class=\"line\">172</div><div class=\"line\">173</div><div class=\"line\">174</div><div class=\"line\">175</div><div class=\"line\">176</div><div class=\"line\">177</div><div class=\"line\">178</div><div class=\"line\">179</div><div class=\"line\">180</div><div class=\"line\">181</div><div class=\"line\">182</div><div class=\"line\">183</div><div class=\"line\">184</div><div class=\"line\">185</div><div class=\"line\">186</div><div class=\"line\">187</div><div class=\"line\">188</div><div class=\"line\">189</div><div class=\"line\">190</div><div class=\"line\">191</div><div class=\"line\">192</div><div class=\"line\">193</div><div class=\"line\">194</div><div class=\"line\">195</div><div class=\"line\">196</div><div class=\"line\">197</div><div class=\"line\">198</div><div class=\"line\">199</div><div class=\"line\">200</div><div class=\"line\">201</div><div class=\"line\">202</div><div class=\"line\">203</div><div class=\"line\">204</div><div class=\"line\">205</div><div class=\"line\">206</div><div class=\"line\">207</div><div class=\"line\">208</div><div class=\"line\">209</div><div class=\"line\">210</div><div class=\"line\">211</div><div class=\"line\">212</div><div class=\"line\">213</div><div class=\"line\">214</div><div class=\"line\">215</div><div class=\"line\">216</div><div class=\"line\">217</div><div class=\"line\">218</div><div class=\"line\">219</div><div class=\"line\">220</div><div class=\"line\">221</div><div class=\"line\">222</div><div class=\"line\">223</div><div class=\"line\">224</div><div class=\"line\">225</div><div class=\"line\">226</div><div class=\"line\">227</div><div class=\"line\">228</div><div class=\"line\">229</div><div class=\"line\">230</div><div class=\"line\">231</div><div class=\"line\">232</div><div class=\"line\">233</div><div class=\"line\">234</div><div class=\"line\">235</div><div class=\"line\">236</div><div class=\"line\">237</div><div class=\"line\">238</div><div class=\"line\">239</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">package</span> com.myhadoop.mapreduce.test;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.conf.Configuration;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.fs.Path;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.*;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Job;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Mapper;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Partitioner;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Reducer;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">import</span> java.io.DataInput;</div><div class=\"line\"><span class=\"keyword\">import</span> java.io.DataOutput;</div><div class=\"line\"><span class=\"keyword\">import</span> java.io.IOException;</div><div class=\"line\"><span class=\"keyword\">import</span> java.util.StringTokenizer;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\"> * Created by Administrator on 2017/6/14.</span></div><div class=\"line\"><span class=\"comment\"> */</span></div><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">SecondSort</span> </span>&#123;</div><div class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">IntPair</span> <span class=\"keyword\">implements</span> <span class=\"title\">WritableComparable</span>&lt;<span class=\"title\">IntPair</span>&gt;</span></div><div class=\"line\"><span class=\"class\">    </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">int</span> first;</div><div class=\"line\">        <span class=\"keyword\">int</span> second;</div><div class=\"line\">        <span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\">         * Set the left and right values.</span></div><div class=\"line\"><span class=\"comment\">         */</span></div><div class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">set</span><span class=\"params\">(<span class=\"keyword\">int</span> left, <span class=\"keyword\">int</span> right)</span></span></div><div class=\"line\"><span class=\"function\">        </span>&#123;</div><div class=\"line\">            first = left;</div><div class=\"line\">            second = right;</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">getFirst</span><span class=\"params\">()</span></span></div><div class=\"line\"><span class=\"function\">        </span>&#123;</div><div class=\"line\">            <span class=\"keyword\">return</span> first;</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">getSecond</span><span class=\"params\">()</span></span></div><div class=\"line\"><span class=\"function\">        </span>&#123;</div><div class=\"line\">            <span class=\"keyword\">return</span> second;</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"meta\">@Override</span></div><div class=\"line\">        <span class=\"comment\">//反序列化，从流中的二进制转换成IntPair</span></div><div class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">readFields</span><span class=\"params\">(DataInput in)</span> <span class=\"keyword\">throws</span> IOException</span></div><div class=\"line\"><span class=\"function\">        </span>&#123;</div><div class=\"line\">            <span class=\"comment\">// TODO Auto-generated method stub</span></div><div class=\"line\">            first = in.readInt();</div><div class=\"line\">            second = in.readInt();</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"meta\">@Override</span></div><div class=\"line\">        <span class=\"comment\">//序列化，将IntPair转化成使用流传送的二进制</span></div><div class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">write</span><span class=\"params\">(DataOutput out)</span> <span class=\"keyword\">throws</span> IOException</span></div><div class=\"line\"><span class=\"function\">        </span>&#123;</div><div class=\"line\">            <span class=\"comment\">// TODO Auto-generated method stub</span></div><div class=\"line\">            out.writeInt(first);</div><div class=\"line\">            out.writeInt(second);</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"meta\">@Override</span></div><div class=\"line\">        <span class=\"comment\">//key的比较</span></div><div class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">compareTo</span><span class=\"params\">(IntPair o)</span></span></div><div class=\"line\"><span class=\"function\">        </span>&#123;</div><div class=\"line\">            <span class=\"comment\">// TODO Auto-generated method stub</span></div><div class=\"line\">            <span class=\"keyword\">if</span> (first != o.first)</div><div class=\"line\">            &#123;</div><div class=\"line\">                <span class=\"keyword\">return</span> first &lt; o.first ? -<span class=\"number\">1</span> : <span class=\"number\">1</span>;</div><div class=\"line\">            &#125;</div><div class=\"line\">            <span class=\"keyword\">else</span> <span class=\"keyword\">if</span> (second != o.second)</div><div class=\"line\">            &#123;</div><div class=\"line\">                <span class=\"keyword\">return</span> second &lt; o.second ? -<span class=\"number\">1</span> : <span class=\"number\">1</span>;</div><div class=\"line\">            &#125;</div><div class=\"line\">            <span class=\"keyword\">else</span></div><div class=\"line\">            &#123;</div><div class=\"line\">                <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\">//新定义类应该重写的两个方法</span></div><div class=\"line\">        <span class=\"meta\">@Override</span></div><div class=\"line\">        <span class=\"comment\">//The hashCode() method is used by the HashPartitioner (the default partitioner in MapReduce)</span></div><div class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">hashCode</span><span class=\"params\">()</span></span></div><div class=\"line\"><span class=\"function\">        </span>&#123;</div><div class=\"line\">            <span class=\"keyword\">return</span> first * <span class=\"number\">157</span> + second;</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"meta\">@Override</span></div><div class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">boolean</span> <span class=\"title\">equals</span><span class=\"params\">(Object right)</span></span></div><div class=\"line\"><span class=\"function\">        </span>&#123;</div><div class=\"line\">            <span class=\"keyword\">if</span> (right == <span class=\"keyword\">null</span>)</div><div class=\"line\">                <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</div><div class=\"line\">            <span class=\"keyword\">if</span> (<span class=\"keyword\">this</span> == right)</div><div class=\"line\">                <span class=\"keyword\">return</span> <span class=\"keyword\">true</span>;</div><div class=\"line\">            <span class=\"keyword\">if</span> (right <span class=\"keyword\">instanceof</span> IntPair)</div><div class=\"line\">            &#123;</div><div class=\"line\">                IntPair r = (IntPair) right;</div><div class=\"line\">                <span class=\"keyword\">return</span> r.first == first &amp;&amp; r.second == second;</div><div class=\"line\">            &#125;</div><div class=\"line\">            <span class=\"keyword\">else</span></div><div class=\"line\">            &#123;</div><div class=\"line\">                <span class=\"keyword\">return</span> <span class=\"keyword\">false</span>;</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\">     * 分区函数类。根据first确定Partition。</span></div><div class=\"line\"><span class=\"comment\">     */</span></div><div class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">FirstPartitioner</span> <span class=\"keyword\">extends</span> <span class=\"title\">Partitioner</span>&lt;<span class=\"title\">IntPair</span>, <span class=\"title\">IntWritable</span>&gt;</span></div><div class=\"line\"><span class=\"class\">    </span>&#123;</div><div class=\"line\">        <span class=\"meta\">@Override</span></div><div class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">getPartition</span><span class=\"params\">(IntPair key, IntWritable value,<span class=\"keyword\">int</span> numPartitions)</span></span></div><div class=\"line\"><span class=\"function\">        </span>&#123;</div><div class=\"line\">            <span class=\"keyword\">int</span> first = key.getFirst();</div><div class=\"line\">            <span class=\"keyword\">if</span>(first&lt;<span class=\"number\">60</span>)&#123;</div><div class=\"line\">                <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</div><div class=\"line\">            &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(first&lt;<span class=\"number\">80</span>)&#123;</div><div class=\"line\">                <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</div><div class=\"line\">            &#125;</div><div class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">2</span>;</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\">     * 分组函数类。只要first相同就属于同一个组。</span></div><div class=\"line\"><span class=\"comment\">     */</span></div><div class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">GroupingComparator</span> <span class=\"keyword\">extends</span> <span class=\"title\">WritableComparator</span></span></div><div class=\"line\"><span class=\"class\">    </span>&#123;</div><div class=\"line\">        <span class=\"function\"><span class=\"keyword\">protected</span> <span class=\"title\">GroupingComparator</span><span class=\"params\">()</span></span></div><div class=\"line\"><span class=\"function\">        </span>&#123;</div><div class=\"line\">            <span class=\"keyword\">super</span>(IntPair.class, <span class=\"keyword\">true</span>);</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"meta\">@Override</span></div><div class=\"line\">        <span class=\"comment\">//Compare two WritableComparables.</span></div><div class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">compare</span><span class=\"params\">(WritableComparable w1, WritableComparable w2)</span></span></div><div class=\"line\"><span class=\"function\">        </span>&#123;</div><div class=\"line\">            IntPair ip1 = (IntPair) w1;</div><div class=\"line\">            IntPair ip2 = (IntPair) w2;</div><div class=\"line\">            <span class=\"keyword\">int</span> l = ip1.getFirst();</div><div class=\"line\">            <span class=\"keyword\">int</span> r = ip2.getFirst();</div><div class=\"line\">            <span class=\"keyword\">return</span> l == r ? <span class=\"number\">0</span> : (l &lt; r ? -<span class=\"number\">1</span> : <span class=\"number\">1</span>);</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">    </div><div class=\"line\">    <span class=\"comment\">// 自定义map</span></div><div class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Map</span> <span class=\"keyword\">extends</span> <span class=\"title\">Mapper</span>&lt;<span class=\"title\">LongWritable</span>, <span class=\"title\">Text</span>, <span class=\"title\">IntPair</span>, <span class=\"title\">IntWritable</span>&gt;</span></div><div class=\"line\"><span class=\"class\">    </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">private</span> <span class=\"keyword\">final</span> IntPair intkey = <span class=\"keyword\">new</span> IntPair();</div><div class=\"line\">        <span class=\"keyword\">private</span> <span class=\"keyword\">final</span> IntWritable intvalue = <span class=\"keyword\">new</span> IntWritable();</div><div class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">map</span><span class=\"params\">(LongWritable key, Text value, Context context)</span> <span class=\"keyword\">throws</span> IOException, InterruptedException</span></div><div class=\"line\"><span class=\"function\">        </span>&#123;</div><div class=\"line\">            String line = value.toString();</div><div class=\"line\">            StringTokenizer tokenizer = <span class=\"keyword\">new</span> StringTokenizer(line);</div><div class=\"line\">            <span class=\"keyword\">int</span> left = <span class=\"number\">0</span>;</div><div class=\"line\">            <span class=\"keyword\">int</span> right = <span class=\"number\">0</span>;</div><div class=\"line\">            <span class=\"keyword\">if</span> (tokenizer.hasMoreTokens())</div><div class=\"line\">            &#123;</div><div class=\"line\">                left = Integer.parseInt(tokenizer.nextToken());</div><div class=\"line\">                <span class=\"keyword\">if</span> (tokenizer.hasMoreTokens())</div><div class=\"line\">                    right = Integer.parseInt(tokenizer.nextToken());</div><div class=\"line\">                intkey.set(left, right);</div><div class=\"line\">                intvalue.set(right);</div><div class=\"line\">                context.write(intkey, intvalue);</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">    <span class=\"comment\">// 自定义reduce</span></div><div class=\"line\">    <span class=\"comment\">//</span></div><div class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Reduce</span> <span class=\"keyword\">extends</span> <span class=\"title\">Reducer</span>&lt;<span class=\"title\">IntPair</span>, <span class=\"title\">IntWritable</span>, <span class=\"title\">Text</span>, <span class=\"title\">IntWritable</span>&gt;</span></div><div class=\"line\"><span class=\"class\">    </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">private</span> <span class=\"keyword\">final</span> Text left = <span class=\"keyword\">new</span> Text();</div><div class=\"line\">        <span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> Text SEPARATOR = <span class=\"keyword\">new</span> Text(<span class=\"string\">\"------------------------------------------------\"</span>);</div><div class=\"line\"></div><div class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">reduce</span><span class=\"params\">(IntPair key, Iterable&lt;IntWritable&gt; values,Context context)</span> <span class=\"keyword\">throws</span> IOException, InterruptedException</span></div><div class=\"line\"><span class=\"function\">        </span>&#123;</div><div class=\"line\">            context.write(SEPARATOR, <span class=\"keyword\">null</span>);</div><div class=\"line\">            left.set(Integer.toString(key.getFirst()));</div><div class=\"line\">            <span class=\"keyword\">for</span> (IntWritable val : values)</div><div class=\"line\">            &#123;</div><div class=\"line\">                context.write(left, val);</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">    <span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\">     * <span class=\"doctag\">@param</span> args</span></div><div class=\"line\"><span class=\"comment\">     */</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> IOException, InterruptedException, ClassNotFoundException</span></div><div class=\"line\"><span class=\"function\">    </span>&#123;</div><div class=\"line\">        <span class=\"comment\">// TODO Auto-generated method stub</span></div><div class=\"line\">        <span class=\"comment\">// 读取hadoop配置</span></div><div class=\"line\">        Configuration conf = <span class=\"keyword\">new</span> Configuration();</div><div class=\"line\">        <span class=\"comment\">// 实例化一道作业</span></div><div class=\"line\">        Job job = <span class=\"keyword\">new</span> Job(conf, <span class=\"string\">\"secondarysort\"</span>);</div><div class=\"line\">        job.setJarByClass(SecondSort.class);</div><div class=\"line\">        <span class=\"comment\">// Mapper类型</span></div><div class=\"line\">        job.setMapperClass(Map.class);</div><div class=\"line\">        <span class=\"comment\">// 不再需要Combiner类型，因为Combiner的输出类型&lt;Text, IntWritable&gt;对Reduce的输入类型&lt;IntPair, IntWritable&gt;不适用</span></div><div class=\"line\">        <span class=\"comment\">//job.setCombinerClass(Reduce.class);</span></div><div class=\"line\">        <span class=\"comment\">// Reducer类型</span></div><div class=\"line\">        job.setReducerClass(Reduce.class);</div><div class=\"line\">        <span class=\"comment\">//设置NumReduceTasks 的数量为三</span></div><div class=\"line\">        job.setNumReduceTasks(<span class=\"number\">3</span>);</div><div class=\"line\">        <span class=\"comment\">// 分区函数</span></div><div class=\"line\">        job.setPartitionerClass(FirstPartitioner.class);</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\">// 分组函数</span></div><div class=\"line\">        job.setGroupingComparatorClass(GroupingComparator.class);</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\">// map 输出Key的类型</span></div><div class=\"line\">        job.setMapOutputKeyClass(IntPair.class);</div><div class=\"line\">        <span class=\"comment\">// map输出Value的类型</span></div><div class=\"line\">        job.setMapOutputValueClass(IntWritable.class);</div><div class=\"line\">        <span class=\"comment\">// rduce输出Key的类型，是Text，因为使用的OutputFormatClass是TextOutputFormat</span></div><div class=\"line\">        job.setOutputKeyClass(Text.class);</div><div class=\"line\">        <span class=\"comment\">// rduce输出Value的类型</span></div><div class=\"line\">        job.setOutputValueClass(IntWritable.class);</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\">// 将输入的数据集分割成小数据块splites，同时提供一个RecordReder的实现。</span></div><div class=\"line\">        job.setInputFormatClass(TextInputFormat.class);</div><div class=\"line\">        <span class=\"comment\">// 提供一个RecordWriter的实现，负责数据输出。</span></div><div class=\"line\">        job.setOutputFormatClass(TextOutputFormat.class);</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\">// 输入hdfs路径</span></div><div class=\"line\">        FileInputFormat.setInputPaths(job, <span class=\"keyword\">new</span> Path(args[<span class=\"number\">0</span>]));</div><div class=\"line\">        <span class=\"comment\">// 输出hdfs路径</span></div><div class=\"line\">        FileOutputFormat.setOutputPath(job, <span class=\"keyword\">new</span> Path(args[<span class=\"number\">1</span>]));</div><div class=\"line\">        <span class=\"comment\">// 提交job</span></div><div class=\"line\">        System.exit(job.waitForCompletion(<span class=\"keyword\">true</span>) ? <span class=\"number\">0</span> : <span class=\"number\">1</span>);</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">```  </div><div class=\"line\">## 运行结果</div><div class=\"line\">生成三个文件 partition-<span class=\"number\">00000</span>，partition-<span class=\"number\">00001</span>，partition-<span class=\"number\">00002</span>  </div><div class=\"line\">每个文件第一个字段都按照进行了全排序，满足上一个条件下对第二个字段进行了排序，并且按照第一个字段进行了分组  </div><div class=\"line\">输出的文件分别为 </div><div class=\"line\">**partition-<span class=\"number\">00000</span>**</div></pre></td></tr></table></figure>\n<hr>\n<h2 id=\"1-2\"><a href=\"#1-2\" class=\"headerlink\" title=\"1       2 \"></a>1       2 </h2><h2 id=\"3-4\"><a href=\"#3-4\" class=\"headerlink\" title=\"3       4 \"></a>3       4 </h2><h2 id=\"5-6\"><a href=\"#5-6\" class=\"headerlink\" title=\"5       6 \"></a>5       6 </h2><p>7       8 </p>\n<h2 id=\"7-82\"><a href=\"#7-82\" class=\"headerlink\" title=\"7       82 \"></a>7       82 </h2><h2 id=\"12-211\"><a href=\"#12-211\" class=\"headerlink\" title=\"12      211 \"></a>12      211 </h2><p>20      21<br>20      53 </p>\n<h2 id=\"20-522\"><a href=\"#20-522\" class=\"headerlink\" title=\"20      522 \"></a>20      522 </h2><h2 id=\"31-42\"><a href=\"#31-42\" class=\"headerlink\" title=\"31      42 \"></a>31      42 </h2><h2 id=\"40-511\"><a href=\"#40-511\" class=\"headerlink\" title=\"40      511 \"></a>40      511 </h2><p>50      51<br>50      52<br>50      53<br>50      53<br>50      54<br>50      62<br>50      512<br>50      522<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">**partition-00001**</div></pre></td></tr></table></figure></p>\n<hr>\n<p>60      51<br>60      52<br>60      53<br>60      56<br>60      56<br>60      57<br>60      57 </p>\n<h2 id=\"60-61\"><a href=\"#60-61\" class=\"headerlink\" title=\"60      61 \"></a>60      61 </h2><h2 id=\"63-61\"><a href=\"#63-61\" class=\"headerlink\" title=\"63      61 \"></a>63      61 </h2><p>70      54<br>70      55<br>70      56<br>70      57<br>70      58 </p>\n<h2 id=\"70-58\"><a href=\"#70-58\" class=\"headerlink\" title=\"70      58 \"></a>70      58 </h2><p>71      55 </p>\n<h2 id=\"71-56\"><a href=\"#71-56\" class=\"headerlink\" title=\"71      56 \"></a>71      56 </h2><h2 id=\"73-57\"><a href=\"#73-57\" class=\"headerlink\" title=\"73      57 \"></a>73      57 </h2><p>74      58<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">**partion-00003**</div></pre></td></tr></table></figure></p>\n<hr>\n<h2 id=\"203-21\"><a href=\"#203-21\" class=\"headerlink\" title=\"203     21 \"></a>203     21 </h2><h2 id=\"530-54\"><a href=\"#530-54\" class=\"headerlink\" title=\"530     54 \"></a>530     54 </h2><h2 id=\"730-54\"><a href=\"#730-54\" class=\"headerlink\" title=\"730     54 \"></a>730     54 </h2><p>740     58<br>```</p>\n<h2 id=\"分析：\"><a href=\"#分析：\" class=\"headerlink\" title=\"分析：\"></a><strong>分析：</strong></h2><p>网上说的已经够清楚了，我这代码主要也是参考于网上的，主要有三步  </p>\n<ol>\n<li>自定义一个对象，需要实现WritableComparable接口，这个对象包含要排序的字段，并且内部有一些方法需要重写，具体参考我的代码  </li>\n<li>自定义分区，网上的hash分区是有小问题的，网上的要么第一个字段不属于全排序，要么只有一个reduce，我们这里参考全排序，自己定义分区，保证整体有序  </li>\n<li>自定义分组（没要求就不需要了），比如只按照第一个字段分组，参考代码中</li>\n<li>正常的map，reduce，map中的key使用我们自定义的对象</li>\n</ol>\n<h2 id=\"感悟：\"><a href=\"#感悟：\" class=\"headerlink\" title=\"感悟：\"></a>感悟：</h2><p>主要代码还是网上的，但是做程序开发还是要有自己的独立思考，网上的并不是真正的二次排序，或者说即使满足二次排序，但是也是只有一个reduce任务</p>\n"},{"title":"Hadoop入门案例（四）全排序之自定义分区 字符串（单词）排序","date":"2016-08-01T13:25:21.000Z","author":"kaishun","id":"17","blogexcerpt":"大量文本中有很多单词，需要对这些单词进行排序，排序规则按照字符进行排序。 和上一篇对数字进行排序是一样的 http://blog.csdn.net/T1DMzks/article/details/73028776 ， 只不过是自定义分区有点变化, 利用mapReduce中map到reduce端的shuffle进行排序，MapReduce只能保证各个分区内部有序，但不能保证全局有序，于是我还自定义了分区，在map后、shuffle之前，我先将小于c的放在0分区，c-f的放在1分区，其余的放在2分区，这样，首先保证了分区与分区之间是整体有序，然后各个分区进行各自的shuffle，使其分区内部有序。","_content":"\n\n## 需求  \n大量文本中有很多单词，需要对这些单词进行排序，排序规则按照字符进行排序\n\n## 测试文本\n```\nba bac\ndf gh hgg dft dfa dfga df fdaf qqq we fsf aa bb ab\nrr\nty ioo zks huawei mingtong jyzt beijing shanghai shenzhen wuhan nanning guilin \nzhejiang hanzhou anhui hefei xiaoshan xiaohao anqian zheli guiyang\n```\n## 原理分析 \n和上一篇对数字进行排序是一样的 http://blog.csdn.net/T1DMzks/article/details/73028776 ， 只不过是自定义分区有点变化, 利用mapReduce中map到reduce端的shuffle进行排序，MapReduce只能保证各个分区内部有序，但不能保证全局有序，于是我还自定义了分区，在map后、shuffle之前，我先将小于c的放在0分区，c-f的放在1分区，其余的放在2分区，这样，首先保证了分区与分区之间是整体有序，然后各个分区进行各自的shuffle，使其分区内部有序。\n\n## 代码\n```java\npackage com.myhadoop.mapreduce.test;\n\nimport org.apache.hadoop.conf.Configured;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapred.Mapper;\nimport org.apache.hadoop.mapred.OutputCollector;\nimport org.apache.hadoop.mapreduce.Partitioner;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapred.Reporter;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.util.Tool;\nimport org.apache.hadoop.util.ToolRunner;\nimport java.io.IOException;\n\n/**\n * Created by kaishun on 2017/6/10.\n */\npublic class TotalSortTest extends Configured implements Tool{\n\n    public static class myMap extends org.apache.hadoop.mapreduce.Mapper<LongWritable, Text, Text, Text>{\n\n        public void map(LongWritable key,Text value,Context context) throws IOException,InterruptedException{\n            String[] split = value.toString().split(\"\\\\s+\");\n            for (int i = 0; i <split.length ; i++) {\n                Text word = new Text(split[i]);\n                context.write(word,new Text(\"\"));\n            }\n        }\n    }\n    public static class myReduce extends Reducer<Text,Text,Text,Text>{\n\n        public void reduce(Text key, Iterable<Text> values,Context context) throws IOException,InterruptedException\n        {\n                context.write(key, new Text(\"\"));\n\n        }\n    }\n    public static class Partition extends Partitioner<Text,Text>{\n        @Override\n        public int getPartition(Text value1, Text value2, int i) {\n\n            if(value1.toString().compareTo(\"c\")<0){\n                return 0;\n            }else if(value1.toString().compareTo(\"f\")<0){\n                return 1;\n            }\n            return 2;\n        }\n    }\n\n\n\n    @Override\n    public int run(String[] args) throws Exception {\n        Job job = Job.getInstance(getConf());\n        job.setJarByClass(TotalSort.class);\n        job.setJobName(\"TotalSortTest\");\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(Text.class);\n\n        job.setPartitionerClass(Partition.class);\n        job.setMapperClass(myMap.class);\n        job.setReducerClass(myReduce.class);\n        job.setNumReduceTasks(3);\n\n        FileInputFormat.setInputPaths(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n        boolean success = job.waitForCompletion(true);\n        return success ? 0:1;\n    }\n    public static void main(String[] args) throws Exception {\n        int ret = ToolRunner.run(new TotalSortTest(), args);\n        System.exit(ret);\n    }\n}\n\n```\n\n## 测试结果\n生成了三个文件part-r-00000，part-r-00001，part-r-00002\n各个分区之间有顺序，分区内部也有顺序，分别为\n```\naa\t\nab\t\nanhui\t\nanqian\t\nba\t\nbac\t\nbb\t\nbeijing\t\n\n```\n```\ndf\t\ndfa\t\ndfga\t\ndft\t\n\n```\n```\nfdaf\t\nfsf\t\ngh\t\nguilin\t\nguiyang\t\nhanzhou\t\nhefei\t\nhgg\t\nhuawei\t\nioo\t\njyzt\t\nmingtong\t\nnanning\t\nqqq\t\nrr\t\nshanghai\t\nshenzhen\t\nty\t\nwe\t\nwuhan\t\nxiaohao\t\nxiaoshan\t\nzhejiang\t\nzheli\t\nzks\t\n\n```  \n\n## 总结\nmapreduce的shuffle是对key值得hashcode进行排序的，所以单词的全排序也是一样的，类似于数据库中的order by 一样， 利用自定义分区，保证整体有序，利用mapreduce内部的shuffle，对key进行排序，保证了局部有序，从而实现了全排序\n","source":"_posts/Hadoop入门案例（四）全排序之自定义分区 字符串（单词）排序.md","raw":"---\ntitle: Hadoop入门案例（四）全排序之自定义分区 字符串（单词）排序\ndate: 2016-08-01 21:25:21\ntags: [hadoop]\ncategories: [大数据,hadoop]\nauthor: kaishun\nid: 17\npermalink: hadoop-example-4\nblogexcerpt: 大量文本中有很多单词，需要对这些单词进行排序，排序规则按照字符进行排序。 和上一篇对数字进行排序是一样的 http://blog.csdn.net/T1DMzks/article/details/73028776 ， 只不过是自定义分区有点变化, 利用mapReduce中map到reduce端的shuffle进行排序，MapReduce只能保证各个分区内部有序，但不能保证全局有序，于是我还自定义了分区，在map后、shuffle之前，我先将小于c的放在0分区，c-f的放在1分区，其余的放在2分区，这样，首先保证了分区与分区之间是整体有序，然后各个分区进行各自的shuffle，使其分区内部有序。\n---\n\n\n## 需求  \n大量文本中有很多单词，需要对这些单词进行排序，排序规则按照字符进行排序\n\n## 测试文本\n```\nba bac\ndf gh hgg dft dfa dfga df fdaf qqq we fsf aa bb ab\nrr\nty ioo zks huawei mingtong jyzt beijing shanghai shenzhen wuhan nanning guilin \nzhejiang hanzhou anhui hefei xiaoshan xiaohao anqian zheli guiyang\n```\n## 原理分析 \n和上一篇对数字进行排序是一样的 http://blog.csdn.net/T1DMzks/article/details/73028776 ， 只不过是自定义分区有点变化, 利用mapReduce中map到reduce端的shuffle进行排序，MapReduce只能保证各个分区内部有序，但不能保证全局有序，于是我还自定义了分区，在map后、shuffle之前，我先将小于c的放在0分区，c-f的放在1分区，其余的放在2分区，这样，首先保证了分区与分区之间是整体有序，然后各个分区进行各自的shuffle，使其分区内部有序。\n\n## 代码\n```java\npackage com.myhadoop.mapreduce.test;\n\nimport org.apache.hadoop.conf.Configured;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapred.Mapper;\nimport org.apache.hadoop.mapred.OutputCollector;\nimport org.apache.hadoop.mapreduce.Partitioner;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapred.Reporter;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.util.Tool;\nimport org.apache.hadoop.util.ToolRunner;\nimport java.io.IOException;\n\n/**\n * Created by kaishun on 2017/6/10.\n */\npublic class TotalSortTest extends Configured implements Tool{\n\n    public static class myMap extends org.apache.hadoop.mapreduce.Mapper<LongWritable, Text, Text, Text>{\n\n        public void map(LongWritable key,Text value,Context context) throws IOException,InterruptedException{\n            String[] split = value.toString().split(\"\\\\s+\");\n            for (int i = 0; i <split.length ; i++) {\n                Text word = new Text(split[i]);\n                context.write(word,new Text(\"\"));\n            }\n        }\n    }\n    public static class myReduce extends Reducer<Text,Text,Text,Text>{\n\n        public void reduce(Text key, Iterable<Text> values,Context context) throws IOException,InterruptedException\n        {\n                context.write(key, new Text(\"\"));\n\n        }\n    }\n    public static class Partition extends Partitioner<Text,Text>{\n        @Override\n        public int getPartition(Text value1, Text value2, int i) {\n\n            if(value1.toString().compareTo(\"c\")<0){\n                return 0;\n            }else if(value1.toString().compareTo(\"f\")<0){\n                return 1;\n            }\n            return 2;\n        }\n    }\n\n\n\n    @Override\n    public int run(String[] args) throws Exception {\n        Job job = Job.getInstance(getConf());\n        job.setJarByClass(TotalSort.class);\n        job.setJobName(\"TotalSortTest\");\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(Text.class);\n\n        job.setPartitionerClass(Partition.class);\n        job.setMapperClass(myMap.class);\n        job.setReducerClass(myReduce.class);\n        job.setNumReduceTasks(3);\n\n        FileInputFormat.setInputPaths(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n        boolean success = job.waitForCompletion(true);\n        return success ? 0:1;\n    }\n    public static void main(String[] args) throws Exception {\n        int ret = ToolRunner.run(new TotalSortTest(), args);\n        System.exit(ret);\n    }\n}\n\n```\n\n## 测试结果\n生成了三个文件part-r-00000，part-r-00001，part-r-00002\n各个分区之间有顺序，分区内部也有顺序，分别为\n```\naa\t\nab\t\nanhui\t\nanqian\t\nba\t\nbac\t\nbb\t\nbeijing\t\n\n```\n```\ndf\t\ndfa\t\ndfga\t\ndft\t\n\n```\n```\nfdaf\t\nfsf\t\ngh\t\nguilin\t\nguiyang\t\nhanzhou\t\nhefei\t\nhgg\t\nhuawei\t\nioo\t\njyzt\t\nmingtong\t\nnanning\t\nqqq\t\nrr\t\nshanghai\t\nshenzhen\t\nty\t\nwe\t\nwuhan\t\nxiaohao\t\nxiaoshan\t\nzhejiang\t\nzheli\t\nzks\t\n\n```  \n\n## 总结\nmapreduce的shuffle是对key值得hashcode进行排序的，所以单词的全排序也是一样的，类似于数据库中的order by 一样， 利用自定义分区，保证整体有序，利用mapreduce内部的shuffle，对key进行排序，保证了局部有序，从而实现了全排序\n","slug":"hadoop-example-4","published":1,"updated":"2018-01-23T14:18:43.182Z","_id":"cjcrpnzoy00162wv33yp7xp5w","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"需求\"><a href=\"#需求\" class=\"headerlink\" title=\"需求\"></a>需求</h2><p>大量文本中有很多单词，需要对这些单词进行排序，排序规则按照字符进行排序</p>\n<h2 id=\"测试文本\"><a href=\"#测试文本\" class=\"headerlink\" title=\"测试文本\"></a>测试文本</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">ba bac</div><div class=\"line\">df gh hgg dft dfa dfga df fdaf qqq we fsf aa bb ab</div><div class=\"line\">rr</div><div class=\"line\">ty ioo zks huawei mingtong jyzt beijing shanghai shenzhen wuhan nanning guilin </div><div class=\"line\">zhejiang hanzhou anhui hefei xiaoshan xiaohao anqian zheli guiyang</div></pre></td></tr></table></figure>\n<h2 id=\"原理分析\"><a href=\"#原理分析\" class=\"headerlink\" title=\"原理分析\"></a>原理分析</h2><p>和上一篇对数字进行排序是一样的 <a href=\"http://blog.csdn.net/T1DMzks/article/details/73028776\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/T1DMzks/article/details/73028776</a> ， 只不过是自定义分区有点变化, 利用mapReduce中map到reduce端的shuffle进行排序，MapReduce只能保证各个分区内部有序，但不能保证全局有序，于是我还自定义了分区，在map后、shuffle之前，我先将小于c的放在0分区，c-f的放在1分区，其余的放在2分区，这样，首先保证了分区与分区之间是整体有序，然后各个分区进行各自的shuffle，使其分区内部有序。</p>\n<h2 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h2><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">package</span> com.myhadoop.mapreduce.test;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.conf.Configured;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.fs.Path;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.IntWritable;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.LongWritable;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.NullWritable;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.Text;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapred.Mapper;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapred.OutputCollector;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Partitioner;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Reducer;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapred.Reporter;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Job;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.util.Tool;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.util.ToolRunner;</div><div class=\"line\"><span class=\"keyword\">import</span> java.io.IOException;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\"> * Created by kaishun on 2017/6/10.</span></div><div class=\"line\"><span class=\"comment\"> */</span></div><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TotalSortTest</span> <span class=\"keyword\">extends</span> <span class=\"title\">Configured</span> <span class=\"keyword\">implements</span> <span class=\"title\">Tool</span></span>&#123;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">myMap</span> <span class=\"keyword\">extends</span> <span class=\"title\">org</span>.<span class=\"title\">apache</span>.<span class=\"title\">hadoop</span>.<span class=\"title\">mapreduce</span>.<span class=\"title\">Mapper</span>&lt;<span class=\"title\">LongWritable</span>, <span class=\"title\">Text</span>, <span class=\"title\">Text</span>, <span class=\"title\">Text</span>&gt;</span>&#123;</div><div class=\"line\"></div><div class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">map</span><span class=\"params\">(LongWritable key,Text value,Context context)</span> <span class=\"keyword\">throws</span> IOException,InterruptedException</span>&#123;</div><div class=\"line\">            String[] split = value.toString().split(<span class=\"string\">\"\\\\s+\"</span>);</div><div class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt;split.length ; i++) &#123;</div><div class=\"line\">                Text word = <span class=\"keyword\">new</span> Text(split[i]);</div><div class=\"line\">                context.write(word,<span class=\"keyword\">new</span> Text(<span class=\"string\">\"\"</span>));</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">myReduce</span> <span class=\"keyword\">extends</span> <span class=\"title\">Reducer</span>&lt;<span class=\"title\">Text</span>,<span class=\"title\">Text</span>,<span class=\"title\">Text</span>,<span class=\"title\">Text</span>&gt;</span>&#123;</div><div class=\"line\"></div><div class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">reduce</span><span class=\"params\">(Text key, Iterable&lt;Text&gt; values,Context context)</span> <span class=\"keyword\">throws</span> IOException,InterruptedException</span></div><div class=\"line\"><span class=\"function\">        </span>&#123;</div><div class=\"line\">                context.write(key, <span class=\"keyword\">new</span> Text(<span class=\"string\">\"\"</span>));</div><div class=\"line\"></div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Partition</span> <span class=\"keyword\">extends</span> <span class=\"title\">Partitioner</span>&lt;<span class=\"title\">Text</span>,<span class=\"title\">Text</span>&gt;</span>&#123;</div><div class=\"line\">        <span class=\"meta\">@Override</span></div><div class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">getPartition</span><span class=\"params\">(Text value1, Text value2, <span class=\"keyword\">int</span> i)</span> </span>&#123;</div><div class=\"line\"></div><div class=\"line\">            <span class=\"keyword\">if</span>(value1.toString().compareTo(<span class=\"string\">\"c\"</span>)&lt;<span class=\"number\">0</span>)&#123;</div><div class=\"line\">                <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</div><div class=\"line\">            &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(value1.toString().compareTo(<span class=\"string\">\"f\"</span>)&lt;<span class=\"number\">0</span>)&#123;</div><div class=\"line\">                <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</div><div class=\"line\">            &#125;</div><div class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">2</span>;</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">    <span class=\"meta\">@Override</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">run</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">        Job job = Job.getInstance(getConf());</div><div class=\"line\">        job.setJarByClass(TotalSort.class);</div><div class=\"line\">        job.setJobName(<span class=\"string\">\"TotalSortTest\"</span>);</div><div class=\"line\">        job.setOutputKeyClass(Text.class);</div><div class=\"line\">        job.setOutputValueClass(Text.class);</div><div class=\"line\"></div><div class=\"line\">        job.setPartitionerClass(Partition.class);</div><div class=\"line\">        job.setMapperClass(myMap.class);</div><div class=\"line\">        job.setReducerClass(myReduce.class);</div><div class=\"line\">        job.setNumReduceTasks(<span class=\"number\">3</span>);</div><div class=\"line\"></div><div class=\"line\">        FileInputFormat.setInputPaths(job, <span class=\"keyword\">new</span> Path(args[<span class=\"number\">0</span>]));</div><div class=\"line\">        FileOutputFormat.setOutputPath(job, <span class=\"keyword\">new</span> Path(args[<span class=\"number\">1</span>]));</div><div class=\"line\"></div><div class=\"line\">        <span class=\"keyword\">boolean</span> success = job.waitForCompletion(<span class=\"keyword\">true</span>);</div><div class=\"line\">        <span class=\"keyword\">return</span> success ? <span class=\"number\">0</span>:<span class=\"number\">1</span>;</div><div class=\"line\">    &#125;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">int</span> ret = ToolRunner.run(<span class=\"keyword\">new</span> TotalSortTest(), args);</div><div class=\"line\">        System.exit(ret);</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h2 id=\"测试结果\"><a href=\"#测试结果\" class=\"headerlink\" title=\"测试结果\"></a>测试结果</h2><p>生成了三个文件part-r-00000，part-r-00001，part-r-00002<br>各个分区之间有顺序，分区内部也有顺序，分别为<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">aa\t</div><div class=\"line\">ab\t</div><div class=\"line\">anhui\t</div><div class=\"line\">anqian\t</div><div class=\"line\">ba\t</div><div class=\"line\">bac\t</div><div class=\"line\">bb\t</div><div class=\"line\">beijing</div></pre></td></tr></table></figure></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">df\t</div><div class=\"line\">dfa\t</div><div class=\"line\">dfga\t</div><div class=\"line\">dft</div></pre></td></tr></table></figure>\n<pre><code>fdaf    \nfsf    \ngh    \nguilin    \nguiyang    \nhanzhou    \nhefei    \nhgg    \nhuawei    \nioo    \njyzt    \nmingtong    \nnanning    \nqqq    \nrr    \nshanghai    \nshenzhen    \nty    \nwe    \nwuhan    \nxiaohao    \nxiaoshan    \nzhejiang    \nzheli    \nzks\n</code></pre><h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>mapreduce的shuffle是对key值得hashcode进行排序的，所以单词的全排序也是一样的，类似于数据库中的order by 一样， 利用自定义分区，保证整体有序，利用mapreduce内部的shuffle，对key进行排序，保证了局部有序，从而实现了全排序</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"需求\"><a href=\"#需求\" class=\"headerlink\" title=\"需求\"></a>需求</h2><p>大量文本中有很多单词，需要对这些单词进行排序，排序规则按照字符进行排序</p>\n<h2 id=\"测试文本\"><a href=\"#测试文本\" class=\"headerlink\" title=\"测试文本\"></a>测试文本</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">ba bac</div><div class=\"line\">df gh hgg dft dfa dfga df fdaf qqq we fsf aa bb ab</div><div class=\"line\">rr</div><div class=\"line\">ty ioo zks huawei mingtong jyzt beijing shanghai shenzhen wuhan nanning guilin </div><div class=\"line\">zhejiang hanzhou anhui hefei xiaoshan xiaohao anqian zheli guiyang</div></pre></td></tr></table></figure>\n<h2 id=\"原理分析\"><a href=\"#原理分析\" class=\"headerlink\" title=\"原理分析\"></a>原理分析</h2><p>和上一篇对数字进行排序是一样的 <a href=\"http://blog.csdn.net/T1DMzks/article/details/73028776\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/T1DMzks/article/details/73028776</a> ， 只不过是自定义分区有点变化, 利用mapReduce中map到reduce端的shuffle进行排序，MapReduce只能保证各个分区内部有序，但不能保证全局有序，于是我还自定义了分区，在map后、shuffle之前，我先将小于c的放在0分区，c-f的放在1分区，其余的放在2分区，这样，首先保证了分区与分区之间是整体有序，然后各个分区进行各自的shuffle，使其分区内部有序。</p>\n<h2 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h2><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">package</span> com.myhadoop.mapreduce.test;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.conf.Configured;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.fs.Path;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.IntWritable;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.LongWritable;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.NullWritable;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.Text;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapred.Mapper;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapred.OutputCollector;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Partitioner;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Reducer;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapred.Reporter;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Job;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.util.Tool;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.util.ToolRunner;</div><div class=\"line\"><span class=\"keyword\">import</span> java.io.IOException;</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\"> * Created by kaishun on 2017/6/10.</span></div><div class=\"line\"><span class=\"comment\"> */</span></div><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TotalSortTest</span> <span class=\"keyword\">extends</span> <span class=\"title\">Configured</span> <span class=\"keyword\">implements</span> <span class=\"title\">Tool</span></span>&#123;</div><div class=\"line\"></div><div class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">myMap</span> <span class=\"keyword\">extends</span> <span class=\"title\">org</span>.<span class=\"title\">apache</span>.<span class=\"title\">hadoop</span>.<span class=\"title\">mapreduce</span>.<span class=\"title\">Mapper</span>&lt;<span class=\"title\">LongWritable</span>, <span class=\"title\">Text</span>, <span class=\"title\">Text</span>, <span class=\"title\">Text</span>&gt;</span>&#123;</div><div class=\"line\"></div><div class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">map</span><span class=\"params\">(LongWritable key,Text value,Context context)</span> <span class=\"keyword\">throws</span> IOException,InterruptedException</span>&#123;</div><div class=\"line\">            String[] split = value.toString().split(<span class=\"string\">\"\\\\s+\"</span>);</div><div class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt;split.length ; i++) &#123;</div><div class=\"line\">                Text word = <span class=\"keyword\">new</span> Text(split[i]);</div><div class=\"line\">                context.write(word,<span class=\"keyword\">new</span> Text(<span class=\"string\">\"\"</span>));</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">myReduce</span> <span class=\"keyword\">extends</span> <span class=\"title\">Reducer</span>&lt;<span class=\"title\">Text</span>,<span class=\"title\">Text</span>,<span class=\"title\">Text</span>,<span class=\"title\">Text</span>&gt;</span>&#123;</div><div class=\"line\"></div><div class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">reduce</span><span class=\"params\">(Text key, Iterable&lt;Text&gt; values,Context context)</span> <span class=\"keyword\">throws</span> IOException,InterruptedException</span></div><div class=\"line\"><span class=\"function\">        </span>&#123;</div><div class=\"line\">                context.write(key, <span class=\"keyword\">new</span> Text(<span class=\"string\">\"\"</span>));</div><div class=\"line\"></div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Partition</span> <span class=\"keyword\">extends</span> <span class=\"title\">Partitioner</span>&lt;<span class=\"title\">Text</span>,<span class=\"title\">Text</span>&gt;</span>&#123;</div><div class=\"line\">        <span class=\"meta\">@Override</span></div><div class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">getPartition</span><span class=\"params\">(Text value1, Text value2, <span class=\"keyword\">int</span> i)</span> </span>&#123;</div><div class=\"line\"></div><div class=\"line\">            <span class=\"keyword\">if</span>(value1.toString().compareTo(<span class=\"string\">\"c\"</span>)&lt;<span class=\"number\">0</span>)&#123;</div><div class=\"line\">                <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</div><div class=\"line\">            &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(value1.toString().compareTo(<span class=\"string\">\"f\"</span>)&lt;<span class=\"number\">0</span>)&#123;</div><div class=\"line\">                <span class=\"keyword\">return</span> <span class=\"number\">1</span>;</div><div class=\"line\">            &#125;</div><div class=\"line\">            <span class=\"keyword\">return</span> <span class=\"number\">2</span>;</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">    <span class=\"meta\">@Override</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">int</span> <span class=\"title\">run</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">        Job job = Job.getInstance(getConf());</div><div class=\"line\">        job.setJarByClass(TotalSort.class);</div><div class=\"line\">        job.setJobName(<span class=\"string\">\"TotalSortTest\"</span>);</div><div class=\"line\">        job.setOutputKeyClass(Text.class);</div><div class=\"line\">        job.setOutputValueClass(Text.class);</div><div class=\"line\"></div><div class=\"line\">        job.setPartitionerClass(Partition.class);</div><div class=\"line\">        job.setMapperClass(myMap.class);</div><div class=\"line\">        job.setReducerClass(myReduce.class);</div><div class=\"line\">        job.setNumReduceTasks(<span class=\"number\">3</span>);</div><div class=\"line\"></div><div class=\"line\">        FileInputFormat.setInputPaths(job, <span class=\"keyword\">new</span> Path(args[<span class=\"number\">0</span>]));</div><div class=\"line\">        FileOutputFormat.setOutputPath(job, <span class=\"keyword\">new</span> Path(args[<span class=\"number\">1</span>]));</div><div class=\"line\"></div><div class=\"line\">        <span class=\"keyword\">boolean</span> success = job.waitForCompletion(<span class=\"keyword\">true</span>);</div><div class=\"line\">        <span class=\"keyword\">return</span> success ? <span class=\"number\">0</span>:<span class=\"number\">1</span>;</div><div class=\"line\">    &#125;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">int</span> ret = ToolRunner.run(<span class=\"keyword\">new</span> TotalSortTest(), args);</div><div class=\"line\">        System.exit(ret);</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h2 id=\"测试结果\"><a href=\"#测试结果\" class=\"headerlink\" title=\"测试结果\"></a>测试结果</h2><p>生成了三个文件part-r-00000，part-r-00001，part-r-00002<br>各个分区之间有顺序，分区内部也有顺序，分别为<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">aa\t</div><div class=\"line\">ab\t</div><div class=\"line\">anhui\t</div><div class=\"line\">anqian\t</div><div class=\"line\">ba\t</div><div class=\"line\">bac\t</div><div class=\"line\">bb\t</div><div class=\"line\">beijing</div></pre></td></tr></table></figure></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">df\t</div><div class=\"line\">dfa\t</div><div class=\"line\">dfga\t</div><div class=\"line\">dft</div></pre></td></tr></table></figure>\n<pre><code>fdaf    \nfsf    \ngh    \nguilin    \nguiyang    \nhanzhou    \nhefei    \nhgg    \nhuawei    \nioo    \njyzt    \nmingtong    \nnanning    \nqqq    \nrr    \nshanghai    \nshenzhen    \nty    \nwe    \nwuhan    \nxiaohao    \nxiaoshan    \nzhejiang    \nzheli    \nzks\n</code></pre><h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>mapreduce的shuffle是对key值得hashcode进行排序的，所以单词的全排序也是一样的，类似于数据库中的order by 一样， 利用自定义分区，保证整体有序，利用mapreduce内部的shuffle，对key进行排序，保证了局部有序，从而实现了全排序</p>\n"},{"title":"Linux 常见的查找相关笔记","date":"2017-05-05T13:25:21.000Z","author":"kaishun","id":"56","_content":" \n## 查找文件名  \n find -name filename \n \n 1. 按照文件名查找\n```\nfind / -name httpd.conf　　#在根目录下查找文件httpd.conf，表示在整个硬盘查找\nfind /etc -name httpd.conf　　#在/etc目录下文件httpd.conf\nfind /etc -name '*srm*'　　#使用通配符*(0或者任意多个)。表示在/etc目录下查找文件名中含有字符串‘srm’的文件\nfind . -name 'srm*' 　　#表示当前目录下查找文件名开头是字符串‘srm’的文件\n```\n2. 按照文件特征查找 \n```\nfind / -amin -10 　　# 查找在系统中最后10分钟访问的文件(access time)\nfind / -atime -2　　 # 查找在系统中最后48小时访问的文件\nfind / -empty 　　# 查找在系统中为空的文件或者文件夹\nfind / -group cat 　　# 查找在系统中属于 group为cat的文件\nfind / -mmin -5 　　# 查找在系统中最后5分钟里修改过的文件(modify time)\nfind / -mtime -1 　　#查找在系统中最后24小时里修改过的文件\nfind / -user fred 　　#查找在系统中属于fred这个用户的文件\nfind / -size +10000c　　#查找出大于10000000字节的文件(c:字节，w:双字，k:KB，M:MB，G:GB)\nfind / -size -1000k 　　#查找出小于1000KB的文件\n```\n3. 使用混合查找方式查找文件  \n**参数有： ！，-and(-a)，-or(-o)。**\n```\nfind /tmp -size +10000c -and -mtime +2 　　#在/tmp目录下查找大于10000字节并在最后2分钟内修改的文件\nfind / -user fred -or -user george 　　#在/目录下查找用户是fred或者george的文件文件\nfind /tmp ! -user panda　　#在/tmp目录中查找所有不属于panda用户的文件\n```\n\n## 查找包含字符串的文件    \n \n grep  \"string\"  ./*  \n “string\"为待查找串  ， ./* 表示当前目录下所有文件\n**二、grep命令**\n\n　　　  基本格式：find  expression\n\n 　　　 1.主要参数\n\n　　　　[options]主要参数：  \n　　　　－c：只输出匹配行的计数。  \n　　　　－i：不区分大小写  \n　　　　－h：查询多文件时不显示文件名。  \n　　　　－l：查询多文件时只输出包含匹配字符的文件名。  \n　　　　－n：显示匹配行及行号。  \n　　　　－s：不显示不存在或无匹配文本的错误信息。  \n　　　　－v：显示不包含匹配文本的所有行。  \n\n　　　　pattern正则表达式主要参数：  \n　　　　\\： 忽略正则表达式中特殊字符的原有含义。  \n　　　　^：匹配正则表达式的开始行。  \n　　　　$: 匹配正则表达式的结束行。  \n　　　　\\<：从匹配正则表达 式的行开始。  \n　　　　\\>：到匹配正则表达式的行结束。  \n　　　　[ ]：单个字符，如[A]即A符合要求 。  \n　　　　[ - ]：范围，如[A-Z]，即A、B、C一直到Z都符合要求 。  \n　　　　.：所有的单个字符。  \n　　　　* ：有字符，长度可以为0。  \n\n**2.实例**　   \n\n　　(1)grep 'test' d*　　#显示所有以d开头的文件中包含 test的行  \n　　(2)grep ‘test’ aa bb cc 　　 #显示在aa，bb，cc文件中包含test的行  \n　　(3)grep ‘[a-z]\\{5\\}’ aa   　　#显示所有包含每行字符串至少有5个连续小写字符的字符串的行  \n　　(4)grep magic /usr/src　　#显示/usr/src目录下的文件(不含子目录)包含magic的行  \n　　(5)grep -r magic   /usr/src　　#显示/usr/src目录下的文件(包含子目录)包含magic的行  \n\n　　(6)grep -w pattern files   ：只匹配整个单词，而不是字符串的一部分(如匹配’magic’，而不是’magical’)，  ","source":"_posts/Linux 常见的查找相关笔记.md","raw":"---\ntitle: Linux 常见的查找相关笔记\ndate: 2017-05-05 21:25:21\ntags: [linux]\ncategories: [Linux]\nauthor: kaishun\nid: 56\npermalink: linux-find-xargs\n---\n \n## 查找文件名  \n find -name filename \n \n 1. 按照文件名查找\n```\nfind / -name httpd.conf　　#在根目录下查找文件httpd.conf，表示在整个硬盘查找\nfind /etc -name httpd.conf　　#在/etc目录下文件httpd.conf\nfind /etc -name '*srm*'　　#使用通配符*(0或者任意多个)。表示在/etc目录下查找文件名中含有字符串‘srm’的文件\nfind . -name 'srm*' 　　#表示当前目录下查找文件名开头是字符串‘srm’的文件\n```\n2. 按照文件特征查找 \n```\nfind / -amin -10 　　# 查找在系统中最后10分钟访问的文件(access time)\nfind / -atime -2　　 # 查找在系统中最后48小时访问的文件\nfind / -empty 　　# 查找在系统中为空的文件或者文件夹\nfind / -group cat 　　# 查找在系统中属于 group为cat的文件\nfind / -mmin -5 　　# 查找在系统中最后5分钟里修改过的文件(modify time)\nfind / -mtime -1 　　#查找在系统中最后24小时里修改过的文件\nfind / -user fred 　　#查找在系统中属于fred这个用户的文件\nfind / -size +10000c　　#查找出大于10000000字节的文件(c:字节，w:双字，k:KB，M:MB，G:GB)\nfind / -size -1000k 　　#查找出小于1000KB的文件\n```\n3. 使用混合查找方式查找文件  \n**参数有： ！，-and(-a)，-or(-o)。**\n```\nfind /tmp -size +10000c -and -mtime +2 　　#在/tmp目录下查找大于10000字节并在最后2分钟内修改的文件\nfind / -user fred -or -user george 　　#在/目录下查找用户是fred或者george的文件文件\nfind /tmp ! -user panda　　#在/tmp目录中查找所有不属于panda用户的文件\n```\n\n## 查找包含字符串的文件    \n \n grep  \"string\"  ./*  \n “string\"为待查找串  ， ./* 表示当前目录下所有文件\n**二、grep命令**\n\n　　　  基本格式：find  expression\n\n 　　　 1.主要参数\n\n　　　　[options]主要参数：  \n　　　　－c：只输出匹配行的计数。  \n　　　　－i：不区分大小写  \n　　　　－h：查询多文件时不显示文件名。  \n　　　　－l：查询多文件时只输出包含匹配字符的文件名。  \n　　　　－n：显示匹配行及行号。  \n　　　　－s：不显示不存在或无匹配文本的错误信息。  \n　　　　－v：显示不包含匹配文本的所有行。  \n\n　　　　pattern正则表达式主要参数：  \n　　　　\\： 忽略正则表达式中特殊字符的原有含义。  \n　　　　^：匹配正则表达式的开始行。  \n　　　　$: 匹配正则表达式的结束行。  \n　　　　\\<：从匹配正则表达 式的行开始。  \n　　　　\\>：到匹配正则表达式的行结束。  \n　　　　[ ]：单个字符，如[A]即A符合要求 。  \n　　　　[ - ]：范围，如[A-Z]，即A、B、C一直到Z都符合要求 。  \n　　　　.：所有的单个字符。  \n　　　　* ：有字符，长度可以为0。  \n\n**2.实例**　   \n\n　　(1)grep 'test' d*　　#显示所有以d开头的文件中包含 test的行  \n　　(2)grep ‘test’ aa bb cc 　　 #显示在aa，bb，cc文件中包含test的行  \n　　(3)grep ‘[a-z]\\{5\\}’ aa   　　#显示所有包含每行字符串至少有5个连续小写字符的字符串的行  \n　　(4)grep magic /usr/src　　#显示/usr/src目录下的文件(不含子目录)包含magic的行  \n　　(5)grep -r magic   /usr/src　　#显示/usr/src目录下的文件(包含子目录)包含magic的行  \n\n　　(6)grep -w pattern files   ：只匹配整个单词，而不是字符串的一部分(如匹配’magic’，而不是’magical’)，  ","slug":"linux-find-xargs","published":1,"updated":"2018-01-22T16:07:01.975Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcrpnzpe001a2wv3tgbg7ew3","content":"<h2 id=\"查找文件名\"><a href=\"#查找文件名\" class=\"headerlink\" title=\"查找文件名\"></a>查找文件名</h2><p> find -name filename </p>\n<ol>\n<li>按照文件名查找<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">find / -name httpd.conf　　#在根目录下查找文件httpd.conf，表示在整个硬盘查找</div><div class=\"line\">find /etc -name httpd.conf　　#在/etc目录下文件httpd.conf</div><div class=\"line\">find /etc -name &apos;*srm*&apos;　　#使用通配符*(0或者任意多个)。表示在/etc目录下查找文件名中含有字符串‘srm’的文件</div><div class=\"line\">find . -name &apos;srm*&apos; 　　#表示当前目录下查找文件名开头是字符串‘srm’的文件</div></pre></td></tr></table></figure>\n</li>\n</ol>\n<ol>\n<li><p>按照文件特征查找 </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">find / -amin -10 　　# 查找在系统中最后10分钟访问的文件(access time)</div><div class=\"line\">find / -atime -2　　 # 查找在系统中最后48小时访问的文件</div><div class=\"line\">find / -empty 　　# 查找在系统中为空的文件或者文件夹</div><div class=\"line\">find / -group cat 　　# 查找在系统中属于 group为cat的文件</div><div class=\"line\">find / -mmin -5 　　# 查找在系统中最后5分钟里修改过的文件(modify time)</div><div class=\"line\">find / -mtime -1 　　#查找在系统中最后24小时里修改过的文件</div><div class=\"line\">find / -user fred 　　#查找在系统中属于fred这个用户的文件</div><div class=\"line\">find / -size +10000c　　#查找出大于10000000字节的文件(c:字节，w:双字，k:KB，M:MB，G:GB)</div><div class=\"line\">find / -size -1000k 　　#查找出小于1000KB的文件</div></pre></td></tr></table></figure>\n</li>\n<li><p>使用混合查找方式查找文件<br><strong>参数有： ！，-and(-a)，-or(-o)。</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">find /tmp -size +10000c -and -mtime +2 　　#在/tmp目录下查找大于10000字节并在最后2分钟内修改的文件</div><div class=\"line\">find / -user fred -or -user george 　　#在/目录下查找用户是fred或者george的文件文件</div><div class=\"line\">find /tmp ! -user panda　　#在/tmp目录中查找所有不属于panda用户的文件</div></pre></td></tr></table></figure>\n</li>\n</ol>\n<h2 id=\"查找包含字符串的文件\"><a href=\"#查找包含字符串的文件\" class=\"headerlink\" title=\"查找包含字符串的文件\"></a>查找包含字符串的文件</h2><p> grep  “string”  ./<em><br> “string”为待查找串  ， ./</em> 表示当前目录下所有文件<br><strong>二、grep命令</strong></p>\n<p>　　　  基本格式：find  expression</p>\n<p> 　　　 1.主要参数</p>\n<p>　　　　[options]主要参数：<br>　　　　－c：只输出匹配行的计数。<br>　　　　－i：不区分大小写<br>　　　　－h：查询多文件时不显示文件名。<br>　　　　－l：查询多文件时只输出包含匹配字符的文件名。<br>　　　　－n：显示匹配行及行号。<br>　　　　－s：不显示不存在或无匹配文本的错误信息。<br>　　　　－v：显示不包含匹配文本的所有行。  </p>\n<p>　　　　pattern正则表达式主要参数：<br>　　　　\\： 忽略正则表达式中特殊字符的原有含义。<br>　　　　^：匹配正则表达式的开始行。<br>　　　　$: 匹配正则表达式的结束行。<br>　　　　\\&lt;：从匹配正则表达 式的行开始。<br>　　　　>：到匹配正则表达式的行结束。<br>　　　　[ ]：单个字符，如[A]即A符合要求 。<br>　　　　[ - ]：范围，如[A-Z]，即A、B、C一直到Z都符合要求 。<br>　　　　.：所有的单个字符。<br>　　　　* ：有字符，长度可以为0。  </p>\n<p><strong>2.实例</strong>　   </p>\n<p>　　(1)grep ‘test’ d*　　#显示所有以d开头的文件中包含 test的行<br>　　(2)grep ‘test’ aa bb cc 　　 #显示在aa，bb，cc文件中包含test的行<br>　　(3)grep ‘[a-z]{5}’ aa   　　#显示所有包含每行字符串至少有5个连续小写字符的字符串的行<br>　　(4)grep magic /usr/src　　#显示/usr/src目录下的文件(不含子目录)包含magic的行<br>　　(5)grep -r magic   /usr/src　　#显示/usr/src目录下的文件(包含子目录)包含magic的行  </p>\n<p>　　(6)grep -w pattern files   ：只匹配整个单词，而不是字符串的一部分(如匹配’magic’，而不是’magical’)，  </p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"查找文件名\"><a href=\"#查找文件名\" class=\"headerlink\" title=\"查找文件名\"></a>查找文件名</h2><p> find -name filename </p>\n<ol>\n<li>按照文件名查找<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">find / -name httpd.conf　　#在根目录下查找文件httpd.conf，表示在整个硬盘查找</div><div class=\"line\">find /etc -name httpd.conf　　#在/etc目录下文件httpd.conf</div><div class=\"line\">find /etc -name &apos;*srm*&apos;　　#使用通配符*(0或者任意多个)。表示在/etc目录下查找文件名中含有字符串‘srm’的文件</div><div class=\"line\">find . -name &apos;srm*&apos; 　　#表示当前目录下查找文件名开头是字符串‘srm’的文件</div></pre></td></tr></table></figure>\n</li>\n</ol>\n<ol>\n<li><p>按照文件特征查找 </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">find / -amin -10 　　# 查找在系统中最后10分钟访问的文件(access time)</div><div class=\"line\">find / -atime -2　　 # 查找在系统中最后48小时访问的文件</div><div class=\"line\">find / -empty 　　# 查找在系统中为空的文件或者文件夹</div><div class=\"line\">find / -group cat 　　# 查找在系统中属于 group为cat的文件</div><div class=\"line\">find / -mmin -5 　　# 查找在系统中最后5分钟里修改过的文件(modify time)</div><div class=\"line\">find / -mtime -1 　　#查找在系统中最后24小时里修改过的文件</div><div class=\"line\">find / -user fred 　　#查找在系统中属于fred这个用户的文件</div><div class=\"line\">find / -size +10000c　　#查找出大于10000000字节的文件(c:字节，w:双字，k:KB，M:MB，G:GB)</div><div class=\"line\">find / -size -1000k 　　#查找出小于1000KB的文件</div></pre></td></tr></table></figure>\n</li>\n<li><p>使用混合查找方式查找文件<br><strong>参数有： ！，-and(-a)，-or(-o)。</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">find /tmp -size +10000c -and -mtime +2 　　#在/tmp目录下查找大于10000字节并在最后2分钟内修改的文件</div><div class=\"line\">find / -user fred -or -user george 　　#在/目录下查找用户是fred或者george的文件文件</div><div class=\"line\">find /tmp ! -user panda　　#在/tmp目录中查找所有不属于panda用户的文件</div></pre></td></tr></table></figure>\n</li>\n</ol>\n<h2 id=\"查找包含字符串的文件\"><a href=\"#查找包含字符串的文件\" class=\"headerlink\" title=\"查找包含字符串的文件\"></a>查找包含字符串的文件</h2><p> grep  “string”  ./<em><br> “string”为待查找串  ， ./</em> 表示当前目录下所有文件<br><strong>二、grep命令</strong></p>\n<p>　　　  基本格式：find  expression</p>\n<p> 　　　 1.主要参数</p>\n<p>　　　　[options]主要参数：<br>　　　　－c：只输出匹配行的计数。<br>　　　　－i：不区分大小写<br>　　　　－h：查询多文件时不显示文件名。<br>　　　　－l：查询多文件时只输出包含匹配字符的文件名。<br>　　　　－n：显示匹配行及行号。<br>　　　　－s：不显示不存在或无匹配文本的错误信息。<br>　　　　－v：显示不包含匹配文本的所有行。  </p>\n<p>　　　　pattern正则表达式主要参数：<br>　　　　\\： 忽略正则表达式中特殊字符的原有含义。<br>　　　　^：匹配正则表达式的开始行。<br>　　　　$: 匹配正则表达式的结束行。<br>　　　　\\&lt;：从匹配正则表达 式的行开始。<br>　　　　>：到匹配正则表达式的行结束。<br>　　　　[ ]：单个字符，如[A]即A符合要求 。<br>　　　　[ - ]：范围，如[A-Z]，即A、B、C一直到Z都符合要求 。<br>　　　　.：所有的单个字符。<br>　　　　* ：有字符，长度可以为0。  </p>\n<p><strong>2.实例</strong>　   </p>\n<p>　　(1)grep ‘test’ d*　　#显示所有以d开头的文件中包含 test的行<br>　　(2)grep ‘test’ aa bb cc 　　 #显示在aa，bb，cc文件中包含test的行<br>　　(3)grep ‘[a-z]{5}’ aa   　　#显示所有包含每行字符串至少有5个连续小写字符的字符串的行<br>　　(4)grep magic /usr/src　　#显示/usr/src目录下的文件(不含子目录)包含magic的行<br>　　(5)grep -r magic   /usr/src　　#显示/usr/src目录下的文件(包含子目录)包含magic的行  </p>\n<p>　　(6)grep -w pattern files   ：只匹配整个单词，而不是字符串的一部分(如匹配’magic’，而不是’magical’)，  </p>\n"},{"title":"JDBC批量插入与更新工具类","date":"2017-05-04T13:25:21.000Z","author":"kaishun","id":"55","_content":"\n## 批量添加\n```\n\tpublic void UpdateFileData(String sSQL, ArrayList<String[]> objParams)\n\t{\n\t\tGetConn();\n\t\tint iResult = 0;\n\n\t\ttry\n\t\t{\n//\t\t\tStatement ps = _CONN.createStatement();\n\t\t\tPreparedStatement preparedStatement = _CONN.prepareStatement(sSQL);\n\t\t\tfor (int i = 0; i <objParams.size() ; i++) {\n\t\t\t\tpreparedStatement.setString(1,objParams.get(i)[0]);\n\t\t\t\tpreparedStatement.setString(2,objParams.get(i)[1]);\n\t\t\t\tpreparedStatement.setString(3,objParams.get(i)[2]);\n\t\t\t\tpreparedStatement.setLong(4,Integer.parseInt(objParams.get(i)[3]));\n\t\t\t\tpreparedStatement.setString(5,objParams.get(i)[4]);\n\t\t\t\tpreparedStatement.setString(6,objParams.get(i)[5]);\n\t\t\t\tpreparedStatement.addBatch();\n\t\t\t}\n\n\t\t\tpreparedStatement.executeBatch();\n\t\t} catch (Exception ex)\n\t\t{\n\t\t\tSystem.out.println(ex.getMessage());\n\n\t\t} finally\n\t\t{\n\t\t\tCloseConn();\n\t\t}\n\n\t}\n``` \n## 参考网上的例子\n```\nimport java.sql.Connection;\nimport java.sql.Date;\nimport java.sql.PreparedStatement;\nimport java.sql.Statement;\n \nimport org.junit.Test;\n \nimport xuezaipiao1.JDBC_Tools;\n/**\n * 向Oracle 的 temp 数据表中添加  10万 条记录\n * 测试如何插入，用时最短\n */\n \npublic class JDBCTest {\n     \n    /**\n     * \n     * 1.使用 Statement .\n     * 测试用时:35535\n     */\n    @Test\n    public void testBbatchStatement() {\n        Connection conn = null;\n        Statement statement = null;\n        String sql = null;\n        try {\n            conn = JDBC_Tools.getConnection();\n            JDBC_Tools.beginTx(conn);\n             \n            long beginTime = System.currentTimeMillis();\n            statement = conn.createStatement();\n            for(int i = 0;i<100000;i++){\n                sql = \"INSERT INTO temp values(\"+(i+1)\n                        +\",'name_\"+(i+1)+\"','13-6月 -15')\";\n                statement.executeUpdate(sql);\n            }\n            long endTime = System.currentTimeMillis();\n            System.out.println(\"Time : \"+(endTime - beginTime));\n            JDBC_Tools.commit(conn);\n        } catch (Exception e) {\n            e.printStackTrace();\n            JDBC_Tools.rollback(conn);\n        }finally{\n            JDBC_Tools.relaseSource(conn, statement);\n        }\n    }\n     \n    /**\n     * 使用PreparedStatement \n     * 测试用时:9717\n     */\n    @Test\n    public void testBbatchPreparedStatement() {\n        Connection conn = null;\n        PreparedStatement ps = null;\n        String sql = null;\n        try {\n            conn = JDBC_Tools.getConnection();\n            JDBC_Tools.beginTx(conn);\n             \n            long beginTime = System.currentTimeMillis();\n            sql = \"INSERT INTO temp values(?,?,?)\";\n            ps = conn.prepareStatement(sql);\n            Date date = new Date(new java.util.Date().getTime());\n            for(int i = 0;i<100000;i++){\n                ps.setInt(1, i+1);\n                ps.setString(2, \"name_\"+i);\n                ps.setDate(3, date);\n                ps.executeUpdate();//9717\n            }\n            long endTime = System.currentTimeMillis();\n            System.out.println(\"Time : \"+(endTime - beginTime));\n            JDBC_Tools.commit(conn);\n        } catch (Exception e) {\n             \n            e.printStackTrace();\n            JDBC_Tools.rollback(conn);\n        }finally{\n            JDBC_Tools.relaseSource(conn, ps);\n        }\n    }\n         \n    /**\n     * 测试用时 : 658\n     */\n    @Test\n    public void testBbatch() {\n        Connection conn = null;\n        PreparedStatement ps = null;\n        String sql = null;\n        try {\n            conn = JDBC_Tools.getConnection();\n            JDBC_Tools.beginTx(conn);\n             \n            long beginTime = System.currentTimeMillis();\n            sql = \"INSERT INTO temp values(?,?,?)\";\n            ps = conn.prepareStatement(sql);\n            Date date = new Date(new java.util.Date().getTime());\n            for(int i = 0;i<100000;i++){\n                ps.setInt(1, i+1);\n                ps.setString(2, \"name_\"+i);\n                ps.setDate(3, date);\n                 \n                //积攒SQL\n                ps.addBatch();\n                 \n                //当积攒到一定程度,就执行一次,并且清空记录\n                if((i+1) % 300==0){\n                    ps.executeBatch();\n                    ps.clearBatch();\n                }\n            }\n            //总条数不是批量值整数倍,则还需要在执行一次\n            if(100000 % 300 != 0){\n                ps.executeBatch();\n                ps.clearBatch();\n            }\n            long endTime = System.currentTimeMillis();\n            System.out.println(\"Time : \"+(endTime - beginTime));\n            JDBC_Tools.commit(conn);\n        } catch (Exception e) {\n             \n            e.printStackTrace();\n            JDBC_Tools.rollback(conn);\n        }finally{\n            JDBC_Tools.relaseSource(conn, ps);\n        }\n    }\n}\n```\n\n\n## JDBC批量更新\n```\npublic void insertInTransaction(String[] sqls) throws SQLException{\n          try ( Connection conn = DriverManager.getConnection(url, user, pass)) {\n              //关闭自动提交，即开启事务\n              conn.setAutoCommit(false);\n              try(Statement stmt = conn.createStatement()) {\n                  for (String sql : sqls) {\n                      stmt.addBatch(sql);\n                  }\n                  stmt.executeBatch();\n                 //提交事务\n                 conn.commit();\n             } catch (SQLException e) {\n                 conn.rollback();\n                 System.out.println(\"Exception:\");\n                 System.out.println(e.getMessage());\n             } finally {\n                 conn.setAutoCommit(true);\n             }\n         } \n     }\n```","source":"_posts/JDBC批量插入与更新.md","raw":"---\ntitle: JDBC批量插入与更新工具类\ndate: 2017-05-04 21:25:21\ntags: [java,JDBC,工具类]\ncategories: [programme]\nauthor: kaishun\nid: 55\npermalink: jdbc-util\n---\n\n## 批量添加\n```\n\tpublic void UpdateFileData(String sSQL, ArrayList<String[]> objParams)\n\t{\n\t\tGetConn();\n\t\tint iResult = 0;\n\n\t\ttry\n\t\t{\n//\t\t\tStatement ps = _CONN.createStatement();\n\t\t\tPreparedStatement preparedStatement = _CONN.prepareStatement(sSQL);\n\t\t\tfor (int i = 0; i <objParams.size() ; i++) {\n\t\t\t\tpreparedStatement.setString(1,objParams.get(i)[0]);\n\t\t\t\tpreparedStatement.setString(2,objParams.get(i)[1]);\n\t\t\t\tpreparedStatement.setString(3,objParams.get(i)[2]);\n\t\t\t\tpreparedStatement.setLong(4,Integer.parseInt(objParams.get(i)[3]));\n\t\t\t\tpreparedStatement.setString(5,objParams.get(i)[4]);\n\t\t\t\tpreparedStatement.setString(6,objParams.get(i)[5]);\n\t\t\t\tpreparedStatement.addBatch();\n\t\t\t}\n\n\t\t\tpreparedStatement.executeBatch();\n\t\t} catch (Exception ex)\n\t\t{\n\t\t\tSystem.out.println(ex.getMessage());\n\n\t\t} finally\n\t\t{\n\t\t\tCloseConn();\n\t\t}\n\n\t}\n``` \n## 参考网上的例子\n```\nimport java.sql.Connection;\nimport java.sql.Date;\nimport java.sql.PreparedStatement;\nimport java.sql.Statement;\n \nimport org.junit.Test;\n \nimport xuezaipiao1.JDBC_Tools;\n/**\n * 向Oracle 的 temp 数据表中添加  10万 条记录\n * 测试如何插入，用时最短\n */\n \npublic class JDBCTest {\n     \n    /**\n     * \n     * 1.使用 Statement .\n     * 测试用时:35535\n     */\n    @Test\n    public void testBbatchStatement() {\n        Connection conn = null;\n        Statement statement = null;\n        String sql = null;\n        try {\n            conn = JDBC_Tools.getConnection();\n            JDBC_Tools.beginTx(conn);\n             \n            long beginTime = System.currentTimeMillis();\n            statement = conn.createStatement();\n            for(int i = 0;i<100000;i++){\n                sql = \"INSERT INTO temp values(\"+(i+1)\n                        +\",'name_\"+(i+1)+\"','13-6月 -15')\";\n                statement.executeUpdate(sql);\n            }\n            long endTime = System.currentTimeMillis();\n            System.out.println(\"Time : \"+(endTime - beginTime));\n            JDBC_Tools.commit(conn);\n        } catch (Exception e) {\n            e.printStackTrace();\n            JDBC_Tools.rollback(conn);\n        }finally{\n            JDBC_Tools.relaseSource(conn, statement);\n        }\n    }\n     \n    /**\n     * 使用PreparedStatement \n     * 测试用时:9717\n     */\n    @Test\n    public void testBbatchPreparedStatement() {\n        Connection conn = null;\n        PreparedStatement ps = null;\n        String sql = null;\n        try {\n            conn = JDBC_Tools.getConnection();\n            JDBC_Tools.beginTx(conn);\n             \n            long beginTime = System.currentTimeMillis();\n            sql = \"INSERT INTO temp values(?,?,?)\";\n            ps = conn.prepareStatement(sql);\n            Date date = new Date(new java.util.Date().getTime());\n            for(int i = 0;i<100000;i++){\n                ps.setInt(1, i+1);\n                ps.setString(2, \"name_\"+i);\n                ps.setDate(3, date);\n                ps.executeUpdate();//9717\n            }\n            long endTime = System.currentTimeMillis();\n            System.out.println(\"Time : \"+(endTime - beginTime));\n            JDBC_Tools.commit(conn);\n        } catch (Exception e) {\n             \n            e.printStackTrace();\n            JDBC_Tools.rollback(conn);\n        }finally{\n            JDBC_Tools.relaseSource(conn, ps);\n        }\n    }\n         \n    /**\n     * 测试用时 : 658\n     */\n    @Test\n    public void testBbatch() {\n        Connection conn = null;\n        PreparedStatement ps = null;\n        String sql = null;\n        try {\n            conn = JDBC_Tools.getConnection();\n            JDBC_Tools.beginTx(conn);\n             \n            long beginTime = System.currentTimeMillis();\n            sql = \"INSERT INTO temp values(?,?,?)\";\n            ps = conn.prepareStatement(sql);\n            Date date = new Date(new java.util.Date().getTime());\n            for(int i = 0;i<100000;i++){\n                ps.setInt(1, i+1);\n                ps.setString(2, \"name_\"+i);\n                ps.setDate(3, date);\n                 \n                //积攒SQL\n                ps.addBatch();\n                 \n                //当积攒到一定程度,就执行一次,并且清空记录\n                if((i+1) % 300==0){\n                    ps.executeBatch();\n                    ps.clearBatch();\n                }\n            }\n            //总条数不是批量值整数倍,则还需要在执行一次\n            if(100000 % 300 != 0){\n                ps.executeBatch();\n                ps.clearBatch();\n            }\n            long endTime = System.currentTimeMillis();\n            System.out.println(\"Time : \"+(endTime - beginTime));\n            JDBC_Tools.commit(conn);\n        } catch (Exception e) {\n             \n            e.printStackTrace();\n            JDBC_Tools.rollback(conn);\n        }finally{\n            JDBC_Tools.relaseSource(conn, ps);\n        }\n    }\n}\n```\n\n\n## JDBC批量更新\n```\npublic void insertInTransaction(String[] sqls) throws SQLException{\n          try ( Connection conn = DriverManager.getConnection(url, user, pass)) {\n              //关闭自动提交，即开启事务\n              conn.setAutoCommit(false);\n              try(Statement stmt = conn.createStatement()) {\n                  for (String sql : sqls) {\n                      stmt.addBatch(sql);\n                  }\n                  stmt.executeBatch();\n                 //提交事务\n                 conn.commit();\n             } catch (SQLException e) {\n                 conn.rollback();\n                 System.out.println(\"Exception:\");\n                 System.out.println(e.getMessage());\n             } finally {\n                 conn.setAutoCommit(true);\n             }\n         } \n     }\n```","slug":"jdbc-util","published":1,"updated":"2018-01-22T16:05:22.412Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcrpnzpe001d2wv38jzjpdkh","content":"<h2 id=\"批量添加\"><a href=\"#批量添加\" class=\"headerlink\" title=\"批量添加\"></a>批量添加</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div></pre></td><td class=\"code\"><pre><div class=\"line\">\tpublic void UpdateFileData(String sSQL, ArrayList&lt;String[]&gt; objParams)</div><div class=\"line\">\t&#123;</div><div class=\"line\">\t\tGetConn();</div><div class=\"line\">\t\tint iResult = 0;</div><div class=\"line\"></div><div class=\"line\">\t\ttry</div><div class=\"line\">\t\t&#123;</div><div class=\"line\">//\t\t\tStatement ps = _CONN.createStatement();</div><div class=\"line\">\t\t\tPreparedStatement preparedStatement = _CONN.prepareStatement(sSQL);</div><div class=\"line\">\t\t\tfor (int i = 0; i &lt;objParams.size() ; i++) &#123;</div><div class=\"line\">\t\t\t\tpreparedStatement.setString(1,objParams.get(i)[0]);</div><div class=\"line\">\t\t\t\tpreparedStatement.setString(2,objParams.get(i)[1]);</div><div class=\"line\">\t\t\t\tpreparedStatement.setString(3,objParams.get(i)[2]);</div><div class=\"line\">\t\t\t\tpreparedStatement.setLong(4,Integer.parseInt(objParams.get(i)[3]));</div><div class=\"line\">\t\t\t\tpreparedStatement.setString(5,objParams.get(i)[4]);</div><div class=\"line\">\t\t\t\tpreparedStatement.setString(6,objParams.get(i)[5]);</div><div class=\"line\">\t\t\t\tpreparedStatement.addBatch();</div><div class=\"line\">\t\t\t&#125;</div><div class=\"line\"></div><div class=\"line\">\t\t\tpreparedStatement.executeBatch();</div><div class=\"line\">\t\t&#125; catch (Exception ex)</div><div class=\"line\">\t\t&#123;</div><div class=\"line\">\t\t\tSystem.out.println(ex.getMessage());</div><div class=\"line\"></div><div class=\"line\">\t\t&#125; finally</div><div class=\"line\">\t\t&#123;</div><div class=\"line\">\t\t\tCloseConn();</div><div class=\"line\">\t\t&#125;</div><div class=\"line\"></div><div class=\"line\">\t&#125;</div><div class=\"line\">``` </div><div class=\"line\">## 参考网上的例子</div></pre></td></tr></table></figure>\n<p>import java.sql.Connection;<br>import java.sql.Date;<br>import java.sql.PreparedStatement;<br>import java.sql.Statement;</p>\n<p>import org.junit.Test;</p>\n<p>import xuezaipiao1.JDBC_Tools;<br>/**</p>\n<ul>\n<li>向Oracle 的 temp 数据表中添加  10万 条记录</li>\n<li>测试如何插入，用时最短<br>*/</li>\n</ul>\n<p>public class JDBCTest {</p>\n<pre><code>/**\n * \n * 1.使用 Statement .\n * 测试用时:35535\n */\n@Test\npublic void testBbatchStatement() {\n    Connection conn = null;\n    Statement statement = null;\n    String sql = null;\n    try {\n        conn = JDBC_Tools.getConnection();\n        JDBC_Tools.beginTx(conn);\n\n        long beginTime = System.currentTimeMillis();\n        statement = conn.createStatement();\n        for(int i = 0;i&lt;100000;i++){\n            sql = &quot;INSERT INTO temp values(&quot;+(i+1)\n                    +&quot;,&apos;name_&quot;+(i+1)+&quot;&apos;,&apos;13-6月 -15&apos;)&quot;;\n            statement.executeUpdate(sql);\n        }\n        long endTime = System.currentTimeMillis();\n        System.out.println(&quot;Time : &quot;+(endTime - beginTime));\n        JDBC_Tools.commit(conn);\n    } catch (Exception e) {\n        e.printStackTrace();\n        JDBC_Tools.rollback(conn);\n    }finally{\n        JDBC_Tools.relaseSource(conn, statement);\n    }\n}\n\n/**\n * 使用PreparedStatement \n * 测试用时:9717\n */\n@Test\npublic void testBbatchPreparedStatement() {\n    Connection conn = null;\n    PreparedStatement ps = null;\n    String sql = null;\n    try {\n        conn = JDBC_Tools.getConnection();\n        JDBC_Tools.beginTx(conn);\n\n        long beginTime = System.currentTimeMillis();\n        sql = &quot;INSERT INTO temp values(?,?,?)&quot;;\n        ps = conn.prepareStatement(sql);\n        Date date = new Date(new java.util.Date().getTime());\n        for(int i = 0;i&lt;100000;i++){\n            ps.setInt(1, i+1);\n            ps.setString(2, &quot;name_&quot;+i);\n            ps.setDate(3, date);\n            ps.executeUpdate();//9717\n        }\n        long endTime = System.currentTimeMillis();\n        System.out.println(&quot;Time : &quot;+(endTime - beginTime));\n        JDBC_Tools.commit(conn);\n    } catch (Exception e) {\n\n        e.printStackTrace();\n        JDBC_Tools.rollback(conn);\n    }finally{\n        JDBC_Tools.relaseSource(conn, ps);\n    }\n}\n\n/**\n * 测试用时 : 658\n */\n@Test\npublic void testBbatch() {\n    Connection conn = null;\n    PreparedStatement ps = null;\n    String sql = null;\n    try {\n        conn = JDBC_Tools.getConnection();\n        JDBC_Tools.beginTx(conn);\n\n        long beginTime = System.currentTimeMillis();\n        sql = &quot;INSERT INTO temp values(?,?,?)&quot;;\n        ps = conn.prepareStatement(sql);\n        Date date = new Date(new java.util.Date().getTime());\n        for(int i = 0;i&lt;100000;i++){\n            ps.setInt(1, i+1);\n            ps.setString(2, &quot;name_&quot;+i);\n            ps.setDate(3, date);\n\n            //积攒SQL\n            ps.addBatch();\n\n            //当积攒到一定程度,就执行一次,并且清空记录\n            if((i+1) % 300==0){\n                ps.executeBatch();\n                ps.clearBatch();\n            }\n        }\n        //总条数不是批量值整数倍,则还需要在执行一次\n        if(100000 % 300 != 0){\n            ps.executeBatch();\n            ps.clearBatch();\n        }\n        long endTime = System.currentTimeMillis();\n        System.out.println(&quot;Time : &quot;+(endTime - beginTime));\n        JDBC_Tools.commit(conn);\n    } catch (Exception e) {\n\n        e.printStackTrace();\n        JDBC_Tools.rollback(conn);\n    }finally{\n        JDBC_Tools.relaseSource(conn, ps);\n    }\n}\n</code></pre><p>}<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">## JDBC批量更新</div></pre></td></tr></table></figure></p>\n<p>public void insertInTransaction(String[] sqls) throws SQLException{<br>          try ( Connection conn = DriverManager.getConnection(url, user, pass)) {<br>              //关闭自动提交，即开启事务<br>              conn.setAutoCommit(false);<br>              try(Statement stmt = conn.createStatement()) {<br>                  for (String sql : sqls) {<br>                      stmt.addBatch(sql);<br>                  }<br>                  stmt.executeBatch();<br>                 //提交事务<br>                 conn.commit();<br>             } catch (SQLException e) {<br>                 conn.rollback();<br>                 System.out.println(“Exception:”);<br>                 System.out.println(e.getMessage());<br>             } finally {<br>                 conn.setAutoCommit(true);<br>             }<br>         }<br>     }<br>```</p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"批量添加\"><a href=\"#批量添加\" class=\"headerlink\" title=\"批量添加\"></a>批量添加</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div></pre></td><td class=\"code\"><pre><div class=\"line\">\tpublic void UpdateFileData(String sSQL, ArrayList&lt;String[]&gt; objParams)</div><div class=\"line\">\t&#123;</div><div class=\"line\">\t\tGetConn();</div><div class=\"line\">\t\tint iResult = 0;</div><div class=\"line\"></div><div class=\"line\">\t\ttry</div><div class=\"line\">\t\t&#123;</div><div class=\"line\">//\t\t\tStatement ps = _CONN.createStatement();</div><div class=\"line\">\t\t\tPreparedStatement preparedStatement = _CONN.prepareStatement(sSQL);</div><div class=\"line\">\t\t\tfor (int i = 0; i &lt;objParams.size() ; i++) &#123;</div><div class=\"line\">\t\t\t\tpreparedStatement.setString(1,objParams.get(i)[0]);</div><div class=\"line\">\t\t\t\tpreparedStatement.setString(2,objParams.get(i)[1]);</div><div class=\"line\">\t\t\t\tpreparedStatement.setString(3,objParams.get(i)[2]);</div><div class=\"line\">\t\t\t\tpreparedStatement.setLong(4,Integer.parseInt(objParams.get(i)[3]));</div><div class=\"line\">\t\t\t\tpreparedStatement.setString(5,objParams.get(i)[4]);</div><div class=\"line\">\t\t\t\tpreparedStatement.setString(6,objParams.get(i)[5]);</div><div class=\"line\">\t\t\t\tpreparedStatement.addBatch();</div><div class=\"line\">\t\t\t&#125;</div><div class=\"line\"></div><div class=\"line\">\t\t\tpreparedStatement.executeBatch();</div><div class=\"line\">\t\t&#125; catch (Exception ex)</div><div class=\"line\">\t\t&#123;</div><div class=\"line\">\t\t\tSystem.out.println(ex.getMessage());</div><div class=\"line\"></div><div class=\"line\">\t\t&#125; finally</div><div class=\"line\">\t\t&#123;</div><div class=\"line\">\t\t\tCloseConn();</div><div class=\"line\">\t\t&#125;</div><div class=\"line\"></div><div class=\"line\">\t&#125;</div><div class=\"line\">``` </div><div class=\"line\">## 参考网上的例子</div></pre></td></tr></table></figure>\n<p>import java.sql.Connection;<br>import java.sql.Date;<br>import java.sql.PreparedStatement;<br>import java.sql.Statement;</p>\n<p>import org.junit.Test;</p>\n<p>import xuezaipiao1.JDBC_Tools;<br>/**</p>\n<ul>\n<li>向Oracle 的 temp 数据表中添加  10万 条记录</li>\n<li>测试如何插入，用时最短<br>*/</li>\n</ul>\n<p>public class JDBCTest {</p>\n<pre><code>/**\n * \n * 1.使用 Statement .\n * 测试用时:35535\n */\n@Test\npublic void testBbatchStatement() {\n    Connection conn = null;\n    Statement statement = null;\n    String sql = null;\n    try {\n        conn = JDBC_Tools.getConnection();\n        JDBC_Tools.beginTx(conn);\n\n        long beginTime = System.currentTimeMillis();\n        statement = conn.createStatement();\n        for(int i = 0;i&lt;100000;i++){\n            sql = &quot;INSERT INTO temp values(&quot;+(i+1)\n                    +&quot;,&apos;name_&quot;+(i+1)+&quot;&apos;,&apos;13-6月 -15&apos;)&quot;;\n            statement.executeUpdate(sql);\n        }\n        long endTime = System.currentTimeMillis();\n        System.out.println(&quot;Time : &quot;+(endTime - beginTime));\n        JDBC_Tools.commit(conn);\n    } catch (Exception e) {\n        e.printStackTrace();\n        JDBC_Tools.rollback(conn);\n    }finally{\n        JDBC_Tools.relaseSource(conn, statement);\n    }\n}\n\n/**\n * 使用PreparedStatement \n * 测试用时:9717\n */\n@Test\npublic void testBbatchPreparedStatement() {\n    Connection conn = null;\n    PreparedStatement ps = null;\n    String sql = null;\n    try {\n        conn = JDBC_Tools.getConnection();\n        JDBC_Tools.beginTx(conn);\n\n        long beginTime = System.currentTimeMillis();\n        sql = &quot;INSERT INTO temp values(?,?,?)&quot;;\n        ps = conn.prepareStatement(sql);\n        Date date = new Date(new java.util.Date().getTime());\n        for(int i = 0;i&lt;100000;i++){\n            ps.setInt(1, i+1);\n            ps.setString(2, &quot;name_&quot;+i);\n            ps.setDate(3, date);\n            ps.executeUpdate();//9717\n        }\n        long endTime = System.currentTimeMillis();\n        System.out.println(&quot;Time : &quot;+(endTime - beginTime));\n        JDBC_Tools.commit(conn);\n    } catch (Exception e) {\n\n        e.printStackTrace();\n        JDBC_Tools.rollback(conn);\n    }finally{\n        JDBC_Tools.relaseSource(conn, ps);\n    }\n}\n\n/**\n * 测试用时 : 658\n */\n@Test\npublic void testBbatch() {\n    Connection conn = null;\n    PreparedStatement ps = null;\n    String sql = null;\n    try {\n        conn = JDBC_Tools.getConnection();\n        JDBC_Tools.beginTx(conn);\n\n        long beginTime = System.currentTimeMillis();\n        sql = &quot;INSERT INTO temp values(?,?,?)&quot;;\n        ps = conn.prepareStatement(sql);\n        Date date = new Date(new java.util.Date().getTime());\n        for(int i = 0;i&lt;100000;i++){\n            ps.setInt(1, i+1);\n            ps.setString(2, &quot;name_&quot;+i);\n            ps.setDate(3, date);\n\n            //积攒SQL\n            ps.addBatch();\n\n            //当积攒到一定程度,就执行一次,并且清空记录\n            if((i+1) % 300==0){\n                ps.executeBatch();\n                ps.clearBatch();\n            }\n        }\n        //总条数不是批量值整数倍,则还需要在执行一次\n        if(100000 % 300 != 0){\n            ps.executeBatch();\n            ps.clearBatch();\n        }\n        long endTime = System.currentTimeMillis();\n        System.out.println(&quot;Time : &quot;+(endTime - beginTime));\n        JDBC_Tools.commit(conn);\n    } catch (Exception e) {\n\n        e.printStackTrace();\n        JDBC_Tools.rollback(conn);\n    }finally{\n        JDBC_Tools.relaseSource(conn, ps);\n    }\n}\n</code></pre><p>}<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">## JDBC批量更新</div></pre></td></tr></table></figure></p>\n<p>public void insertInTransaction(String[] sqls) throws SQLException{<br>          try ( Connection conn = DriverManager.getConnection(url, user, pass)) {<br>              //关闭自动提交，即开启事务<br>              conn.setAutoCommit(false);<br>              try(Statement stmt = conn.createStatement()) {<br>                  for (String sql : sqls) {<br>                      stmt.addBatch(sql);<br>                  }<br>                  stmt.executeBatch();<br>                 //提交事务<br>                 conn.commit();<br>             } catch (SQLException e) {<br>                 conn.rollback();<br>                 System.out.println(“Exception:”);<br>                 System.out.println(e.getMessage());<br>             } finally {<br>                 conn.setAutoCommit(true);<br>             }<br>         }<br>     }<br>```</p>\n"},{"title":"Spark集群环境的搭建","date":"2016-09-05T13:25:21.000Z","author":"kaishun","id":"31","_content":"\n\n前提：已经安装好了hadoop, 说明，这次安装的是spark-1.4.0-bin-hadoop2.6.tgz版本，这个版本是兼容hadoop2.7的hdfs和yarn的\n## 软件准备:   \nscala-2.10.4 ,[下载地址](http://www.scala-lang.org/)  \nspark-1.4.0-bin-hadoop2.6.tgz [下载地址](http://spark.apache.org/downloads.html)\n## 1. 安装scala-2.10.4\n将下载好的scala-2.11.4.tgz复制到/usr/lib  \n### 1.1 解压安装包\n```shell\nsudo tar -zxf scala-2.11.4.tgz\n```  \n### 1.2 设置scala环境变量\n```shell\nsudo vim /etc/profile\n```   \n打开之后在末尾添加\n```shell\n#scale\nexport SCALA_HOME=/usr/lib/scala-2.10.4\nexport PATH=$SCALA_HOME/bin:$PATH\n```\n使配置生效\n```shell\nsource /etc/profile\n```\n### 1.3 检查是否安装成功\n```shell\nscala -version\n```\n出现Scala code runner version 2.10.4 -- Copyright 2002-2013, LAMP/EPFL说明安装成功  \n\n## 2. 安装spark-1.4.0-bin-hadoop2.6\n将下载的 spark-1.4.0-bin-hadoop2.6.tar.gz 复制到/usr目录  \n### 2.1 解压并且改名\n```shell\nsudo tar zxvf spark-1.4.0-bin-hadoop2.6.tgz\nsudo mv spark-1.4.0-bin-hadoop2.6/ spark-1.4.0-hadoop2.6\n```\n### 2.2 配置环境\n```shell\nsudo vi /etc/profile\n``` \n打开之后在末尾添加\n```shell\n#spark\nexport SPARK_HOME=/usr/spark-1.4.0-hadoop2.6\nexport PATH=$SPARK_HOME/bin:$PATH\n```  \n使资源生效\n```\nsource /etc/profile\n```  \n### 2.3 配置Spark环境\n#### 配置 spark-env.sh文件\n进入到 spark的conf目录, 拷贝一份spark-env.sh.template  并且改名为 spark-env.sh\n```shell\ncd $SPARK_HOME/conf\nsudo cp spark-env.sh.template spark-env.sh\n```\nsudo vi spark-env.sh 添加以下内容：\n```shell\nexport JAVA_HOME=/usr/local/java/jdk1.7.0_79\nexport HADOOP_HOME=/home/hadoop/MyCloudera/APP/hadoop/hadoop\nexport HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop\nexport SCALA_HOME==/usr/lib/scala-2.10.4\nexport SPARK_HOME=/usr/spark-1.4.0-hadoop2.6\nexport SPARK_MASTER_IP=master\nexport SPARK_MASTER_PORT=7077\nexport SPARK_MASTER_WEBUI_PORT=8099\n\n#每个Worker使用的CPU核数\nexport SPARK_WORKER_CORES=1\n#每个Slave中启动几个Worker实例\nexport SPARK_WORKER_INSTANCES=1\n#每个Worker使用多大的内存\nexport SPARK_WORKER_MEMORY=2G\n#Worker的WebUI端口号\nexport SPARK_WORKER_WEBUI_PORT=8081\n#每个Executor使用使用的核数\nexport SPARK_EXECUTOR_CORES=1\n#每个Executor使用的内存\nexport SPARK_EXECUTOR_MEMORY=1G\n\nexport SPARK_CLASSPATH=$SPARK_HOME/conf/:$SPARK_HOME/lib/*\nexport SPARK_CLASSPATH=$SPARK_CLASSPATH:$CLASSPATH\nexport LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:$HADOOP_HOME/lib/nativ\n```\n#### 配置Slave文件  \n拷贝一份slaves.template并且改名为slaves  \n```shell\nsudo cp slaves.template slaves\n```  \nsudo vim slaves，然后根据自己对节点的设置，按照下面类似的方法添加内容\n```shell\nmaster\nnode01\nnode02\n```  \n### 2.4 把配置好的，所有文件，复制一份到node节点的服务器上\n```shell\n建议先打包，然后使用scp传到所有的node节点，然后在所有的node的节点的/usr/目录下解压\n```\n\n### 2.5 启动Spark并且测试\n启动Spark(前提是先启动hadoop，以后会用到其hdfs的系统) ,先启动master,再启动slave， 操作全部在主节点进行, 或者直接全部启动  \n如果启动失败，根据提示进行处理\n```shell\n#方法一:\ncd $SPARK_HOME/sbin/\n./start-master.sh\n./start-slaves.sh\n#方法二\n./start-all.sh\n```\n启动 master机器jps有以下显示,相对于hadoop, 多了一个master,一个worker\n\n```\n14930 ResourceManager\n16437 Jps\n15033 NodeManager\n16324 Worker\n14510 DataNode\n16166 Master\n14753 SecondaryNameNode\n14409 NameNode\n```\nslave机器jps有如下显示，多了一个worker\n```\n2610 NodeManager\n2816 Worker\n2888 Jps\n2466 DataNode\n```\n\n\n使用spark shell进行测试\n在hadoop用户下，进入spark 的bin目录，打开spark-shell命令编程\n```shell \ncd $SPARK_HOME/bin\nspark-shell \n打开后可以看到一个大大的Spark的欢迎图和版本，说明启动成功\n```  \n开始测试\n```\nval text= sc.parallelize(Seq(\"aa\",\"aa\",\"bb\",\"cc\"))\n#回车可以看到 text: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at parallelize at <console>:21\n```\n我们看一看其第一个元素\n```\ntext.take(1)\n#回车后最后出现一堆info信息，最后一行出现 res0: Array[String] = Array(aa)\n```\n我们统计其元组数量\n```\ntext.count\n#回车后出现 res1: Long = 4\n```\n过滤，统计不含aa的元组数量\n```\n# 注意这条命令的 > 和！之间要有一个空格\ntext.filter(r => !r.contains(\"aa\")).count\n#回车后出现\nres3: Long = 2\n```  \n\n如果想自己写相应的spark测试，可以参考这篇文章，这篇文章使用的是idea的编译器，eclipse的我没用过，估计除了打包这里不一样，其他都是一样的  \n[Spark Idea搭建实战](http://www.cnblogs.com/shishanyuan/p/4721120.html)\n\n官网给了很多例子，均在 spark-1.4.0-bin-hadoop2.7\\examples\\src\\main 文件夹下，可做参考\n","source":"_posts/Spark集群环境的搭建.md","raw":"---\ntitle: Spark集群环境的搭建\ndate: 2016-09-05 21:25:21\ntags: [spark]\ncategories: [大数据,spark]\nauthor: kaishun\nid: 31\npermalink: spark-cluster-install\n---\n\n\n前提：已经安装好了hadoop, 说明，这次安装的是spark-1.4.0-bin-hadoop2.6.tgz版本，这个版本是兼容hadoop2.7的hdfs和yarn的\n## 软件准备:   \nscala-2.10.4 ,[下载地址](http://www.scala-lang.org/)  \nspark-1.4.0-bin-hadoop2.6.tgz [下载地址](http://spark.apache.org/downloads.html)\n## 1. 安装scala-2.10.4\n将下载好的scala-2.11.4.tgz复制到/usr/lib  \n### 1.1 解压安装包\n```shell\nsudo tar -zxf scala-2.11.4.tgz\n```  \n### 1.2 设置scala环境变量\n```shell\nsudo vim /etc/profile\n```   \n打开之后在末尾添加\n```shell\n#scale\nexport SCALA_HOME=/usr/lib/scala-2.10.4\nexport PATH=$SCALA_HOME/bin:$PATH\n```\n使配置生效\n```shell\nsource /etc/profile\n```\n### 1.3 检查是否安装成功\n```shell\nscala -version\n```\n出现Scala code runner version 2.10.4 -- Copyright 2002-2013, LAMP/EPFL说明安装成功  \n\n## 2. 安装spark-1.4.0-bin-hadoop2.6\n将下载的 spark-1.4.0-bin-hadoop2.6.tar.gz 复制到/usr目录  \n### 2.1 解压并且改名\n```shell\nsudo tar zxvf spark-1.4.0-bin-hadoop2.6.tgz\nsudo mv spark-1.4.0-bin-hadoop2.6/ spark-1.4.0-hadoop2.6\n```\n### 2.2 配置环境\n```shell\nsudo vi /etc/profile\n``` \n打开之后在末尾添加\n```shell\n#spark\nexport SPARK_HOME=/usr/spark-1.4.0-hadoop2.6\nexport PATH=$SPARK_HOME/bin:$PATH\n```  \n使资源生效\n```\nsource /etc/profile\n```  \n### 2.3 配置Spark环境\n#### 配置 spark-env.sh文件\n进入到 spark的conf目录, 拷贝一份spark-env.sh.template  并且改名为 spark-env.sh\n```shell\ncd $SPARK_HOME/conf\nsudo cp spark-env.sh.template spark-env.sh\n```\nsudo vi spark-env.sh 添加以下内容：\n```shell\nexport JAVA_HOME=/usr/local/java/jdk1.7.0_79\nexport HADOOP_HOME=/home/hadoop/MyCloudera/APP/hadoop/hadoop\nexport HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop\nexport SCALA_HOME==/usr/lib/scala-2.10.4\nexport SPARK_HOME=/usr/spark-1.4.0-hadoop2.6\nexport SPARK_MASTER_IP=master\nexport SPARK_MASTER_PORT=7077\nexport SPARK_MASTER_WEBUI_PORT=8099\n\n#每个Worker使用的CPU核数\nexport SPARK_WORKER_CORES=1\n#每个Slave中启动几个Worker实例\nexport SPARK_WORKER_INSTANCES=1\n#每个Worker使用多大的内存\nexport SPARK_WORKER_MEMORY=2G\n#Worker的WebUI端口号\nexport SPARK_WORKER_WEBUI_PORT=8081\n#每个Executor使用使用的核数\nexport SPARK_EXECUTOR_CORES=1\n#每个Executor使用的内存\nexport SPARK_EXECUTOR_MEMORY=1G\n\nexport SPARK_CLASSPATH=$SPARK_HOME/conf/:$SPARK_HOME/lib/*\nexport SPARK_CLASSPATH=$SPARK_CLASSPATH:$CLASSPATH\nexport LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:$HADOOP_HOME/lib/nativ\n```\n#### 配置Slave文件  \n拷贝一份slaves.template并且改名为slaves  \n```shell\nsudo cp slaves.template slaves\n```  \nsudo vim slaves，然后根据自己对节点的设置，按照下面类似的方法添加内容\n```shell\nmaster\nnode01\nnode02\n```  \n### 2.4 把配置好的，所有文件，复制一份到node节点的服务器上\n```shell\n建议先打包，然后使用scp传到所有的node节点，然后在所有的node的节点的/usr/目录下解压\n```\n\n### 2.5 启动Spark并且测试\n启动Spark(前提是先启动hadoop，以后会用到其hdfs的系统) ,先启动master,再启动slave， 操作全部在主节点进行, 或者直接全部启动  \n如果启动失败，根据提示进行处理\n```shell\n#方法一:\ncd $SPARK_HOME/sbin/\n./start-master.sh\n./start-slaves.sh\n#方法二\n./start-all.sh\n```\n启动 master机器jps有以下显示,相对于hadoop, 多了一个master,一个worker\n\n```\n14930 ResourceManager\n16437 Jps\n15033 NodeManager\n16324 Worker\n14510 DataNode\n16166 Master\n14753 SecondaryNameNode\n14409 NameNode\n```\nslave机器jps有如下显示，多了一个worker\n```\n2610 NodeManager\n2816 Worker\n2888 Jps\n2466 DataNode\n```\n\n\n使用spark shell进行测试\n在hadoop用户下，进入spark 的bin目录，打开spark-shell命令编程\n```shell \ncd $SPARK_HOME/bin\nspark-shell \n打开后可以看到一个大大的Spark的欢迎图和版本，说明启动成功\n```  \n开始测试\n```\nval text= sc.parallelize(Seq(\"aa\",\"aa\",\"bb\",\"cc\"))\n#回车可以看到 text: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at parallelize at <console>:21\n```\n我们看一看其第一个元素\n```\ntext.take(1)\n#回车后最后出现一堆info信息，最后一行出现 res0: Array[String] = Array(aa)\n```\n我们统计其元组数量\n```\ntext.count\n#回车后出现 res1: Long = 4\n```\n过滤，统计不含aa的元组数量\n```\n# 注意这条命令的 > 和！之间要有一个空格\ntext.filter(r => !r.contains(\"aa\")).count\n#回车后出现\nres3: Long = 2\n```  \n\n如果想自己写相应的spark测试，可以参考这篇文章，这篇文章使用的是idea的编译器，eclipse的我没用过，估计除了打包这里不一样，其他都是一样的  \n[Spark Idea搭建实战](http://www.cnblogs.com/shishanyuan/p/4721120.html)\n\n官网给了很多例子，均在 spark-1.4.0-bin-hadoop2.7\\examples\\src\\main 文件夹下，可做参考\n","slug":"spark-cluster-install","published":1,"updated":"2018-01-22T15:18:48.304Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcrpnzpe001i2wv39qwh8u1g","content":"<p>前提：已经安装好了hadoop, 说明，这次安装的是spark-1.4.0-bin-hadoop2.6.tgz版本，这个版本是兼容hadoop2.7的hdfs和yarn的</p>\n<h2 id=\"软件准备\"><a href=\"#软件准备\" class=\"headerlink\" title=\"软件准备:\"></a>软件准备:</h2><p>scala-2.10.4 ,<a href=\"http://www.scala-lang.org/\" target=\"_blank\" rel=\"external\">下载地址</a><br>spark-1.4.0-bin-hadoop2.6.tgz <a href=\"http://spark.apache.org/downloads.html\" target=\"_blank\" rel=\"external\">下载地址</a></p>\n<h2 id=\"1-安装scala-2-10-4\"><a href=\"#1-安装scala-2-10-4\" class=\"headerlink\" title=\"1. 安装scala-2.10.4\"></a>1. 安装scala-2.10.4</h2><p>将下载好的scala-2.11.4.tgz复制到/usr/lib  </p>\n<h3 id=\"1-1-解压安装包\"><a href=\"#1-1-解压安装包\" class=\"headerlink\" title=\"1.1 解压安装包\"></a>1.1 解压安装包</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">sudo tar -zxf scala-2.11.4.tgz</div><div class=\"line\">```  </div><div class=\"line\"><span class=\"meta\">#</span>## 1.2 设置scala环境变量</div><div class=\"line\">```shell</div><div class=\"line\">sudo vim /etc/profile</div><div class=\"line\">```   </div><div class=\"line\">打开之后在末尾添加</div><div class=\"line\">```shell</div><div class=\"line\"><span class=\"meta\">#</span>scale</div><div class=\"line\">export SCALA_HOME=/usr/lib/scala-2.10.4</div><div class=\"line\">export PATH=$SCALA_HOME/bin:$PATH</div></pre></td></tr></table></figure>\n<p>使配置生效<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">source /etc/profile</div></pre></td></tr></table></figure></p>\n<h3 id=\"1-3-检查是否安装成功\"><a href=\"#1-3-检查是否安装成功\" class=\"headerlink\" title=\"1.3 检查是否安装成功\"></a>1.3 检查是否安装成功</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala -version</div></pre></td></tr></table></figure>\n<p>出现Scala code runner version 2.10.4 – Copyright 2002-2013, LAMP/EPFL说明安装成功  </p>\n<h2 id=\"2-安装spark-1-4-0-bin-hadoop2-6\"><a href=\"#2-安装spark-1-4-0-bin-hadoop2-6\" class=\"headerlink\" title=\"2. 安装spark-1.4.0-bin-hadoop2.6\"></a>2. 安装spark-1.4.0-bin-hadoop2.6</h2><p>将下载的 spark-1.4.0-bin-hadoop2.6.tar.gz 复制到/usr目录  </p>\n<h3 id=\"2-1-解压并且改名\"><a href=\"#2-1-解压并且改名\" class=\"headerlink\" title=\"2.1 解压并且改名\"></a>2.1 解压并且改名</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">sudo tar zxvf spark-1.4.0-bin-hadoop2.6.tgz</div><div class=\"line\">sudo mv spark-1.4.0-bin-hadoop2.6/ spark-1.4.0-hadoop2.6</div></pre></td></tr></table></figure>\n<h3 id=\"2-2-配置环境\"><a href=\"#2-2-配置环境\" class=\"headerlink\" title=\"2.2 配置环境\"></a>2.2 配置环境</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">sudo vi /etc/profile</div><div class=\"line\">``` </div><div class=\"line\">打开之后在末尾添加</div><div class=\"line\">```shell</div><div class=\"line\"><span class=\"meta\">#</span>spark</div><div class=\"line\">export SPARK_HOME=/usr/spark-1.4.0-hadoop2.6</div><div class=\"line\">export PATH=$SPARK_HOME/bin:$PATH</div><div class=\"line\">```  </div><div class=\"line\">使资源生效</div></pre></td></tr></table></figure>\n<p>source /etc/profile<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">### 2.3 配置Spark环境</div><div class=\"line\">#### 配置 spark-env.sh文件</div><div class=\"line\">进入到 spark的conf目录, 拷贝一份spark-env.sh.template  并且改名为 spark-env.sh</div><div class=\"line\">```shell</div><div class=\"line\">cd $SPARK_HOME/conf</div><div class=\"line\">sudo cp spark-env.sh.template spark-env.sh</div></pre></td></tr></table></figure></p>\n<p>sudo vi spark-env.sh 添加以下内容：<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div></pre></td><td class=\"code\"><pre><div class=\"line\">export JAVA_HOME=/usr/local/java/jdk1.7.0_79</div><div class=\"line\">export HADOOP_HOME=/home/hadoop/MyCloudera/APP/hadoop/hadoop</div><div class=\"line\">export HADOOP_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoop</div><div class=\"line\">export SCALA_HOME==/usr/lib/scala-2.10.4</div><div class=\"line\">export SPARK_HOME=/usr/spark-1.4.0-hadoop2.6</div><div class=\"line\">export SPARK_MASTER_IP=master</div><div class=\"line\">export SPARK_MASTER_PORT=7077</div><div class=\"line\">export SPARK_MASTER_WEBUI_PORT=8099</div><div class=\"line\"></div><div class=\"line\"><span class=\"meta\">#</span>每个Worker使用的CPU核数</div><div class=\"line\">export SPARK_WORKER_CORES=1</div><div class=\"line\"><span class=\"meta\">#</span>每个Slave中启动几个Worker实例</div><div class=\"line\">export SPARK_WORKER_INSTANCES=1</div><div class=\"line\"><span class=\"meta\">#</span>每个Worker使用多大的内存</div><div class=\"line\">export SPARK_WORKER_MEMORY=2G</div><div class=\"line\"><span class=\"meta\">#</span>Worker的WebUI端口号</div><div class=\"line\">export SPARK_WORKER_WEBUI_PORT=8081</div><div class=\"line\"><span class=\"meta\">#</span>每个Executor使用使用的核数</div><div class=\"line\">export SPARK_EXECUTOR_CORES=1</div><div class=\"line\"><span class=\"meta\">#</span>每个Executor使用的内存</div><div class=\"line\">export SPARK_EXECUTOR_MEMORY=1G</div><div class=\"line\"></div><div class=\"line\">export SPARK_CLASSPATH=$SPARK_HOME/conf/:$SPARK_HOME/lib/*</div><div class=\"line\">export SPARK_CLASSPATH=$SPARK_CLASSPATH:$CLASSPATH</div><div class=\"line\">export LD_LIBRARY_PATH=$&#123;LD_LIBRARY_PATH&#125;:$HADOOP_HOME/lib/nativ</div></pre></td></tr></table></figure></p>\n<h4 id=\"配置Slave文件\"><a href=\"#配置Slave文件\" class=\"headerlink\" title=\"配置Slave文件\"></a>配置Slave文件</h4><p>拷贝一份slaves.template并且改名为slaves<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">sudo cp slaves.template slaves</div><div class=\"line\">```  </div><div class=\"line\">sudo vim slaves，然后根据自己对节点的设置，按照下面类似的方法添加内容</div><div class=\"line\">```shell</div><div class=\"line\">master</div><div class=\"line\">node01</div><div class=\"line\">node02</div><div class=\"line\">```  </div><div class=\"line\"><span class=\"meta\">#</span>## 2.4 把配置好的，所有文件，复制一份到node节点的服务器上</div><div class=\"line\">```shell</div><div class=\"line\">建议先打包，然后使用scp传到所有的node节点，然后在所有的node的节点的/usr/目录下解压</div></pre></td></tr></table></figure></p>\n<h3 id=\"2-5-启动Spark并且测试\"><a href=\"#2-5-启动Spark并且测试\" class=\"headerlink\" title=\"2.5 启动Spark并且测试\"></a>2.5 启动Spark并且测试</h3><p>启动Spark(前提是先启动hadoop，以后会用到其hdfs的系统) ,先启动master,再启动slave， 操作全部在主节点进行, 或者直接全部启动<br>如果启动失败，根据提示进行处理<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">#</span>方法一:</div><div class=\"line\">cd $SPARK_HOME/sbin/</div><div class=\"line\">./start-master.sh</div><div class=\"line\">./start-slaves.sh</div><div class=\"line\"><span class=\"meta\">#</span>方法二</div><div class=\"line\">./start-all.sh</div></pre></td></tr></table></figure></p>\n<p>启动 master机器jps有以下显示,相对于hadoop, 多了一个master,一个worker</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">14930 ResourceManager</div><div class=\"line\">16437 Jps</div><div class=\"line\">15033 NodeManager</div><div class=\"line\">16324 Worker</div><div class=\"line\">14510 DataNode</div><div class=\"line\">16166 Master</div><div class=\"line\">14753 SecondaryNameNode</div><div class=\"line\">14409 NameNode</div></pre></td></tr></table></figure>\n<p>slave机器jps有如下显示，多了一个worker<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">2610 NodeManager</div><div class=\"line\">2816 Worker</div><div class=\"line\">2888 Jps</div><div class=\"line\">2466 DataNode</div></pre></td></tr></table></figure></p>\n<p>使用spark shell进行测试<br>在hadoop用户下，进入spark 的bin目录，打开spark-shell命令编程<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">cd $SPARK_HOME/bin</div><div class=\"line\">spark-shell </div><div class=\"line\">打开后可以看到一个大大的Spark的欢迎图和版本，说明启动成功</div><div class=\"line\">```  </div><div class=\"line\">开始测试</div></pre></td></tr></table></figure></p>\n<p>val text= sc.parallelize(Seq(“aa”,”aa”,”bb”,”cc”))</p>\n<p>#回车可以看到 text: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at parallelize at <console>:21<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">我们看一看其第一个元素</div></pre></td></tr></table></figure></console></p>\n<p>text.take(1)</p>\n<p>#回车后最后出现一堆info信息，最后一行出现 res0: Array[String] = Array(aa)<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">我们统计其元组数量</div></pre></td></tr></table></figure></p>\n<p>text.count</p>\n<p>#回车后出现 res1: Long = 4<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">过滤，统计不含aa的元组数量</div></pre></td></tr></table></figure></p>\n<h1 id=\"注意这条命令的-gt-和！之间要有一个空格\"><a href=\"#注意这条命令的-gt-和！之间要有一个空格\" class=\"headerlink\" title=\"注意这条命令的 &gt; 和！之间要有一个空格\"></a>注意这条命令的 &gt; 和！之间要有一个空格</h1><p>text.filter(r =&gt; !r.contains(“aa”)).count</p>\n<p>#回车后出现<br>res3: Long = 2<br>```  </p>\n<p>如果想自己写相应的spark测试，可以参考这篇文章，这篇文章使用的是idea的编译器，eclipse的我没用过，估计除了打包这里不一样，其他都是一样的<br><a href=\"http://www.cnblogs.com/shishanyuan/p/4721120.html\" target=\"_blank\" rel=\"external\">Spark Idea搭建实战</a></p>\n<p>官网给了很多例子，均在 spark-1.4.0-bin-hadoop2.7\\examples\\src\\main 文件夹下，可做参考</p>\n","site":{"data":{}},"excerpt":"","more":"<p>前提：已经安装好了hadoop, 说明，这次安装的是spark-1.4.0-bin-hadoop2.6.tgz版本，这个版本是兼容hadoop2.7的hdfs和yarn的</p>\n<h2 id=\"软件准备\"><a href=\"#软件准备\" class=\"headerlink\" title=\"软件准备:\"></a>软件准备:</h2><p>scala-2.10.4 ,<a href=\"http://www.scala-lang.org/\" target=\"_blank\" rel=\"external\">下载地址</a><br>spark-1.4.0-bin-hadoop2.6.tgz <a href=\"http://spark.apache.org/downloads.html\" target=\"_blank\" rel=\"external\">下载地址</a></p>\n<h2 id=\"1-安装scala-2-10-4\"><a href=\"#1-安装scala-2-10-4\" class=\"headerlink\" title=\"1. 安装scala-2.10.4\"></a>1. 安装scala-2.10.4</h2><p>将下载好的scala-2.11.4.tgz复制到/usr/lib  </p>\n<h3 id=\"1-1-解压安装包\"><a href=\"#1-1-解压安装包\" class=\"headerlink\" title=\"1.1 解压安装包\"></a>1.1 解压安装包</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">sudo tar -zxf scala-2.11.4.tgz</div><div class=\"line\">```  </div><div class=\"line\"><span class=\"meta\">#</span>## 1.2 设置scala环境变量</div><div class=\"line\">```shell</div><div class=\"line\">sudo vim /etc/profile</div><div class=\"line\">```   </div><div class=\"line\">打开之后在末尾添加</div><div class=\"line\">```shell</div><div class=\"line\"><span class=\"meta\">#</span>scale</div><div class=\"line\">export SCALA_HOME=/usr/lib/scala-2.10.4</div><div class=\"line\">export PATH=$SCALA_HOME/bin:$PATH</div></pre></td></tr></table></figure>\n<p>使配置生效<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">source /etc/profile</div></pre></td></tr></table></figure></p>\n<h3 id=\"1-3-检查是否安装成功\"><a href=\"#1-3-检查是否安装成功\" class=\"headerlink\" title=\"1.3 检查是否安装成功\"></a>1.3 检查是否安装成功</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala -version</div></pre></td></tr></table></figure>\n<p>出现Scala code runner version 2.10.4 – Copyright 2002-2013, LAMP/EPFL说明安装成功  </p>\n<h2 id=\"2-安装spark-1-4-0-bin-hadoop2-6\"><a href=\"#2-安装spark-1-4-0-bin-hadoop2-6\" class=\"headerlink\" title=\"2. 安装spark-1.4.0-bin-hadoop2.6\"></a>2. 安装spark-1.4.0-bin-hadoop2.6</h2><p>将下载的 spark-1.4.0-bin-hadoop2.6.tar.gz 复制到/usr目录  </p>\n<h3 id=\"2-1-解压并且改名\"><a href=\"#2-1-解压并且改名\" class=\"headerlink\" title=\"2.1 解压并且改名\"></a>2.1 解压并且改名</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">sudo tar zxvf spark-1.4.0-bin-hadoop2.6.tgz</div><div class=\"line\">sudo mv spark-1.4.0-bin-hadoop2.6/ spark-1.4.0-hadoop2.6</div></pre></td></tr></table></figure>\n<h3 id=\"2-2-配置环境\"><a href=\"#2-2-配置环境\" class=\"headerlink\" title=\"2.2 配置环境\"></a>2.2 配置环境</h3><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">sudo vi /etc/profile</div><div class=\"line\">``` </div><div class=\"line\">打开之后在末尾添加</div><div class=\"line\">```shell</div><div class=\"line\"><span class=\"meta\">#</span>spark</div><div class=\"line\">export SPARK_HOME=/usr/spark-1.4.0-hadoop2.6</div><div class=\"line\">export PATH=$SPARK_HOME/bin:$PATH</div><div class=\"line\">```  </div><div class=\"line\">使资源生效</div></pre></td></tr></table></figure>\n<p>source /etc/profile<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">### 2.3 配置Spark环境</div><div class=\"line\">#### 配置 spark-env.sh文件</div><div class=\"line\">进入到 spark的conf目录, 拷贝一份spark-env.sh.template  并且改名为 spark-env.sh</div><div class=\"line\">```shell</div><div class=\"line\">cd $SPARK_HOME/conf</div><div class=\"line\">sudo cp spark-env.sh.template spark-env.sh</div></pre></td></tr></table></figure></p>\n<p>sudo vi spark-env.sh 添加以下内容：<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div></pre></td><td class=\"code\"><pre><div class=\"line\">export JAVA_HOME=/usr/local/java/jdk1.7.0_79</div><div class=\"line\">export HADOOP_HOME=/home/hadoop/MyCloudera/APP/hadoop/hadoop</div><div class=\"line\">export HADOOP_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoop</div><div class=\"line\">export SCALA_HOME==/usr/lib/scala-2.10.4</div><div class=\"line\">export SPARK_HOME=/usr/spark-1.4.0-hadoop2.6</div><div class=\"line\">export SPARK_MASTER_IP=master</div><div class=\"line\">export SPARK_MASTER_PORT=7077</div><div class=\"line\">export SPARK_MASTER_WEBUI_PORT=8099</div><div class=\"line\"></div><div class=\"line\"><span class=\"meta\">#</span>每个Worker使用的CPU核数</div><div class=\"line\">export SPARK_WORKER_CORES=1</div><div class=\"line\"><span class=\"meta\">#</span>每个Slave中启动几个Worker实例</div><div class=\"line\">export SPARK_WORKER_INSTANCES=1</div><div class=\"line\"><span class=\"meta\">#</span>每个Worker使用多大的内存</div><div class=\"line\">export SPARK_WORKER_MEMORY=2G</div><div class=\"line\"><span class=\"meta\">#</span>Worker的WebUI端口号</div><div class=\"line\">export SPARK_WORKER_WEBUI_PORT=8081</div><div class=\"line\"><span class=\"meta\">#</span>每个Executor使用使用的核数</div><div class=\"line\">export SPARK_EXECUTOR_CORES=1</div><div class=\"line\"><span class=\"meta\">#</span>每个Executor使用的内存</div><div class=\"line\">export SPARK_EXECUTOR_MEMORY=1G</div><div class=\"line\"></div><div class=\"line\">export SPARK_CLASSPATH=$SPARK_HOME/conf/:$SPARK_HOME/lib/*</div><div class=\"line\">export SPARK_CLASSPATH=$SPARK_CLASSPATH:$CLASSPATH</div><div class=\"line\">export LD_LIBRARY_PATH=$&#123;LD_LIBRARY_PATH&#125;:$HADOOP_HOME/lib/nativ</div></pre></td></tr></table></figure></p>\n<h4 id=\"配置Slave文件\"><a href=\"#配置Slave文件\" class=\"headerlink\" title=\"配置Slave文件\"></a>配置Slave文件</h4><p>拷贝一份slaves.template并且改名为slaves<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">sudo cp slaves.template slaves</div><div class=\"line\">```  </div><div class=\"line\">sudo vim slaves，然后根据自己对节点的设置，按照下面类似的方法添加内容</div><div class=\"line\">```shell</div><div class=\"line\">master</div><div class=\"line\">node01</div><div class=\"line\">node02</div><div class=\"line\">```  </div><div class=\"line\"><span class=\"meta\">#</span>## 2.4 把配置好的，所有文件，复制一份到node节点的服务器上</div><div class=\"line\">```shell</div><div class=\"line\">建议先打包，然后使用scp传到所有的node节点，然后在所有的node的节点的/usr/目录下解压</div></pre></td></tr></table></figure></p>\n<h3 id=\"2-5-启动Spark并且测试\"><a href=\"#2-5-启动Spark并且测试\" class=\"headerlink\" title=\"2.5 启动Spark并且测试\"></a>2.5 启动Spark并且测试</h3><p>启动Spark(前提是先启动hadoop，以后会用到其hdfs的系统) ,先启动master,再启动slave， 操作全部在主节点进行, 或者直接全部启动<br>如果启动失败，根据提示进行处理<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">#</span>方法一:</div><div class=\"line\">cd $SPARK_HOME/sbin/</div><div class=\"line\">./start-master.sh</div><div class=\"line\">./start-slaves.sh</div><div class=\"line\"><span class=\"meta\">#</span>方法二</div><div class=\"line\">./start-all.sh</div></pre></td></tr></table></figure></p>\n<p>启动 master机器jps有以下显示,相对于hadoop, 多了一个master,一个worker</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">14930 ResourceManager</div><div class=\"line\">16437 Jps</div><div class=\"line\">15033 NodeManager</div><div class=\"line\">16324 Worker</div><div class=\"line\">14510 DataNode</div><div class=\"line\">16166 Master</div><div class=\"line\">14753 SecondaryNameNode</div><div class=\"line\">14409 NameNode</div></pre></td></tr></table></figure>\n<p>slave机器jps有如下显示，多了一个worker<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">2610 NodeManager</div><div class=\"line\">2816 Worker</div><div class=\"line\">2888 Jps</div><div class=\"line\">2466 DataNode</div></pre></td></tr></table></figure></p>\n<p>使用spark shell进行测试<br>在hadoop用户下，进入spark 的bin目录，打开spark-shell命令编程<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">cd $SPARK_HOME/bin</div><div class=\"line\">spark-shell </div><div class=\"line\">打开后可以看到一个大大的Spark的欢迎图和版本，说明启动成功</div><div class=\"line\">```  </div><div class=\"line\">开始测试</div></pre></td></tr></table></figure></p>\n<p>val text= sc.parallelize(Seq(“aa”,”aa”,”bb”,”cc”))</p>\n<p>#回车可以看到 text: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at parallelize at <console>:21<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">我们看一看其第一个元素</div></pre></td></tr></table></figure></console></p>\n<p>text.take(1)</p>\n<p>#回车后最后出现一堆info信息，最后一行出现 res0: Array[String] = Array(aa)<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">我们统计其元组数量</div></pre></td></tr></table></figure></p>\n<p>text.count</p>\n<p>#回车后出现 res1: Long = 4<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">过滤，统计不含aa的元组数量</div></pre></td></tr></table></figure></p>\n<h1 id=\"注意这条命令的-gt-和！之间要有一个空格\"><a href=\"#注意这条命令的-gt-和！之间要有一个空格\" class=\"headerlink\" title=\"注意这条命令的 &gt; 和！之间要有一个空格\"></a>注意这条命令的 &gt; 和！之间要有一个空格</h1><p>text.filter(r =&gt; !r.contains(“aa”)).count</p>\n<p>#回车后出现<br>res3: Long = 2<br>```  </p>\n<p>如果想自己写相应的spark测试，可以参考这篇文章，这篇文章使用的是idea的编译器，eclipse的我没用过，估计除了打包这里不一样，其他都是一样的<br><a href=\"http://www.cnblogs.com/shishanyuan/p/4721120.html\" target=\"_blank\" rel=\"external\">Spark Idea搭建实战</a></p>\n<p>官网给了很多例子，均在 spark-1.4.0-bin-hadoop2.7\\examples\\src\\main 文件夹下，可做参考</p>\n"},{"title":"hadoop集群搭建教程","date":"2016-07-15T13:25:21.000Z","author":"kaishun","id":"12","_content":"\nhadoop的配置参看github https://github.com/zhaikaishun/hadoop_cluster  \n作者: 翟开顺\n  \n关键字: \n集群环境介绍，Hadoop简介，网络配置，所需软件\nSSH免密码登陆配置，java环境安装，卸载原有的JDK， 安装jdk17， 配置java环境变量，验证是否成功，Hadoop集群安装，安装Hadoop，验证hadoop\nhadoop错误分析\n<!-- more -->\n# **集群环境介绍**\n## **1.  Hadoop简介**  \n　Hadoop是Apache软件基金会旗下的一个开源分布式计算平台。以Hadoop分布式文件系统（HDFS，Hadoop Distributed Filesystem）和MapReduce（Google MapReduce的开源实现）为核心的Hadoop为用户提供了系统底层细节透明的分布式基础架构。\n　　对于Hadoop的集群来讲，可以分成两大类角色：Master和Salve。一个HDFS集群是由一个NameNode和若干个DataNode组成的。其中NameNode作为主服务器，管理文件系统的命名空间和客户端对文件系统的访问操作；集群中的DataNode管理存储的数据。MapReduce框架是由一个单独运行在主节点上的JobTracker和运行在每个集群从节点的TaskTracker共同组成的。主节点负责调度构成一个作业的所有任务，这些任务分布在不同的从节点上。主节点监控它们的执行情况，并且重新执行之前的失败任务；从节点仅负责由主节点指派的任务。当一个Job被提交时，JobTracker接收到提交作业和配置信息之后，就会将配置信息等分发给从节点，同时调度任务并监控TaskTracker的执行。\n　　从上面的介绍可以看出，HDFS和MapReduce共同组成了Hadoop分布式系统体系结构的核心。HDFS在集群上实现分布式文件系统，MapReduce在集群上实现了分布式计算和任务处理。HDFS在MapReduce任务处理过程中提供了文件操作和存储等支持，MapReduce在HDFS的基础上实现了任务的分发、跟踪、执行等工作，并收集结果，二者相互作用，完成了Hadoop分布式集群的主要任务。\n\n\n## **2.  环境说明**  \n本教程为了简单起见只设置两个节点： master为主节点，node01为数据节点，节点之间局域网连接，相互可以ping通，节点IP分布如下\n\n机器名称 | IP地址\n---|---\nmaster | 192.168.200.128\nnode01 | 192.168.200.129\n\n两个节点都是centos6.5系统，都有同一个用户，用户名叫Hadoop  \n\n### **给hadoop用户赋予root权限**  \n切换到root用户 赋予etc/sudoers777权限，然后打开\n```shell\n[root@kaishun etc]# chmod 777 /etc/sudoers\n[root@kaishun etc]# vim /etc/sudoers\n```  \n找到Allows people in group wheel to run all commands，把下面%wheel的#给去掉,在Allow root to run any commands anywhere下，加上hadoop  ALL=(ALL)       ALL，然后保存\n```shell\n## Allow root to run any commands anywhere\nroot    ALL=(ALL)       ALL     \nhadoop  ALL=(ALL)       ALL\n\n## Allows people in group wheel to run all commands\n%wheel        ALL=(ALL)       ALL\n```\n把sudoers的权限改回来成440\n```shell\n[root@kaishun etc]# chmod 440 /etc/sudoers\n```    \n测试是否成功  \n在普通用户下\n```shell\n[hadoop@kaishun ~]$ sudo mkdir test\n输入密码如果可以成功创建文件夹，说明成功\n```\n### **网络配置**\n\n**1. 查看当前机器名**\n**在root用户下**输入，显示\n```shell\n[root@kaishun hadoop]# hostname\n显示 kaishun， 与我们规划的master不符合\n```  \n**2. 在root用户下修改当前机器名称**\n```shell\n[root@kaishun hadoop]# vim /etc/sysconfig/network\n```\n修改HOSTNAME 为 master\n```shell\nHOSTNAME=master\n```\n同理，192.168.200.129这台机器修改成node01\n修改之后，可能不会立即生效，我是重启后才生效的  \n**3. 在root用户下配置hosts文件, 每台机器都需要配置（必须）**  \n```shell\n[root@master hadoop]# vim /etc/hosts\n```\n添加\n```shell\n127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n192.168.200.128 master\n192.168.200.129 node01\n```  \n**测试是否成功**，如果能相互使用ping node01   ping master能成功，说明hosts文件配置成功\n```shell\n[hadoop@master ~]$ ping node01\nPING node01 (192.168.200.129) 56(84) bytes of data.\n64 bytes from node01 (192.168.200.129): icmp_seq=1 ttl=64 time=0.391 ms\n64 bytes from node01 (192.168.200.129): icmp_seq=2 ttl=64 time=0.435 ms\n64 bytes from node01 (192.168.200.129): icmp_seq=3 ttl=64 time=0.442 ms\n\n[hadoop@node01 ~]$ ping master\nPING master (192.168.200.128) 56(84) bytes of data.\n64 bytes from master (192.168.200.128): icmp_seq=1 ttl=64 time=0.379 ms\n64 bytes from master (192.168.200.128): icmp_seq=2 ttl=64 time=0.411 ms\n64 bytes from master (192.168.200.128): icmp_seq=3 ttl=64 time=0.460 ms\n```\n\n## **3.  所需软件**\n1.  JDK版本1.7  \n2.  hadoop版本hadoop-2.7.1  去官网的华科镜像下载hadoop-2.7.1.tar.gz， [地址](http://mirrors.hust.edu.cn/apache/hadoop/common/hadoop-2.7.1/)  \n3. 数据传输工具FileZilla， ssh连接工具 secureCRT  \n\n## **4.  SSH免密码登陆配置**\nHadoop运行过程中需要管理远端Hadoop守护进程，在Hadoop启动以后，NameNode是通过SSH（Secure Shell）来启动和停止各个DataNode上的各种守护进程的。这就必须在节点之间执行指令的时候是不需要输入密码的形式，故我们需要配置SSH运用无密码公钥认证的形式，这样NameNode使用SSH无密码登录并启动DataName进程，同样原理，DataNode上也能使用SSH无密码登录到NameNode。    \n安装CentOS6.5时，我们选择了一些基本安装包，所以我们需要两个服务：ssh和rsync已经安装了。可以通过下面命令查看结果显示如下：\n```\n[hadoop@master ~]$ rpm –qa | grep openssh\n[hadoop@master ~]$ rpm –qa | grep rsync\n如果有相应的提示，说明这两个是装好了的，我这里是系统自带的\n```  \n**4.1 配置master无密码登陆所有的node**  \n原理请百度\n在master节点上执行以下命令 然后按几次回车键：\n```shell\n[hadoop@master ~]$ ssh-keygen -t rsa\n```  \n出现下图  \n![ssh免密码登陆](http://i4.buimg.com/567571/b22c79e94cbc2297.png)\n\n我们看到这句话  \nYour identification has been saved in /home/hadoop/.ssh/id_rsa.  \nYour public key has been saved in /home/hadoop/.ssh/id_rsa.pub.  \n说明默认目录在 /home/hadoop/.ssh/ 下\n\n接着在master节点上做如下配置，把id_rsa.pub追加到授权的key里面去。  \n```shell\n[hadoop@master ~]$ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n```\n现在我们进入~/.ssh目录可以看到\n```\n[hadoop@master ~]$ cd ~/.ssh/\n[hadoop@master .ssh]$ ll\ntotal 12\n-rw-rw-r--. 1 hadoop hadoop  395 Apr  2 16:22 authorized_keys\n-rw-------. 1 hadoop hadoop 1675 Apr  2 16:17 id_rsa\n-rw-r--r--. 1 hadoop hadoop  395 Apr  2 16:17 id_rsa.pub\n```\n**4.1.1. 修改文件\"authorized_keys权限**  \n```\n[hadoop@master .ssh]$ chmod 600 ~/.ssh/authorized_keys\n```\n![authorized_keys权限](http://i2.muimg.com/567571/41ecd0b8634028ae.png)\n\n**4.1.2. 设置SSH配置**  \n用root用户登录服务器修改SSH配置文件\"/etc/ssh/sshd_config\"的下列内容。这里找到这些内容，把前面的#去掉即可\n```shell\n[root@master .ssh]# vim /etc/ssh/sshd_config\nRSAAuthentication yes # 启用 RSA 认证\nPubkeyAuthentication yes # 启用公钥私钥配对认证方式\nAuthorizedKeysFile .ssh/authorized_keys # 公钥文件路径（和上面生成的文件同）\n```\n**重启SSH服务**\n```\n[root@master .ssh]# /etc/rc.d/init.d/sshd restart\nStopping sshd:                                             [  OK  ]\nStarting sshd:                                             [  OK  ]\n[root@master .ssh]# \n```\n退出root用户，使用hadoop普通用户验证是否成功, ssh localhost, 如果不需要输入密码，那么验证成功\n```shell\n[hadoop@master .ssh]$ ssh localhost\nThe authenticity of host 'localhost (::1)' can't be established.\nRSA key fingerprint is 48:0b:ee:9b:67:85:4c:19:35:10:d1:1d:e1:5d:fa:c4.\nAre you sure you want to continue connecting (yes/no)? yes\nWarning: Permanently added 'localhost' (RSA) to the list of known hosts.\nLast login: Sun Apr  2 16:09:39 2017 from 192.168.200.1\n```\n\n**4.1.3. 把公钥复制到所有的node机器上**\n从上图中得知无密码登录本级已经设置完毕，接下来的事儿是把公钥复制所有的node机器上。使用下面的命令格式进行复制公钥  \nscp ~/.ssh/id_rsa.pub 远程用户名@远程服务器IP:~/\n我本地这样使用 scp ~/.ssh/id_rsa.pub hadoop@192.168.200.129:~/  ,然后根据提示输入需要复制的远程服务器的密码，最后出现下面的提示说明复制成功\n```shell\n[hadoop@master ~]$ scp ~/.ssh/id_rsa.pub hadoop@192.168.200.129:~/\nhadoop@192.168.200.129's password:  # 这里输入远程密码\nid_rsa.pub                                                                                                                        100%  395     0.4KB/s   00:00    \n[hadoop@master ~]$ \n```  \n**4.1.4. 对节点机器进行配置**\n下面就针对IP为\"192.168.200.129\"的node01的节点进行配置。\n4.1  ll -a查看是否有.ssh目录，如果没有，我们需要创建一个.ssh目录，并且赋予这个权限 drwx------.  2 hadoop hadoop 4096 Apr  2 16:40 .ssh  具体权限参照master的机器， centos6.5一般都是默认带有.ssh目录的\n```\n[hadoop@node01 ~]$ ll -a \ndrwx------.  2 hadoop hadoop 4096 Apr  2 16:40 .ssh\n```\n如果有这个目录了，我们把刚才的文件追加到authorized_keys 中去，然后修改authorized_keys文件权限\n```\n[hadoop@node01 ~]$ cat ~/id_rsa.pub >> ~/.ssh/authorized_keys\n[hadoop@node01 .ssh]$ chmod 600 ~/.ssh/authorized_keys\n\n```\n进入到ssh 目录，ll 看到如下所示说明成功，注意权限是否正确\n```shell\n[hadoop@node01 .ssh]$ ll\ntotal 8\n-rw-------. 1 hadoop hadoop 395 Apr  2 16:52 authorized_keys\n-rw-r--r--. 1 hadoop hadoop 391 Apr  2 16:40 known_hosts\n```\n**4.2  用root用户修改/etc/ssh/sshd_config**\n参考前面的master的修改/etc/ssh/sshd_config的方法  \n设置SSH配置  \n用root用户登录服务器修改SSH配置文件\"/etc/ssh/sshd_config\"的下列内容。这里找到这些内容，把前面的#去掉即可\n```shell\n[root@master .ssh]# vim /etc/ssh/sshd_config\nRSAAuthentication yes # 启用 RSA 认证\nPubkeyAuthentication yes # 启用公钥私钥配对认证方式\nAuthorizedKeysFile .ssh/authorized_keys # 公钥文件路径（和上面生成的文件同）\n```\n**重启SSH服务**\n```\n[root@master .ssh]# /etc/rc.d/init.d/sshd restart\nStopping sshd:                                             [  OK  ]\nStarting sshd:                                             [  OK  ]\n[root@master .ssh]# \n```\n最后记得把\"/home/hadoop/\"目录下的\"id_rsa.pub\"文件删除掉 \n\n到此为止，我们经过的步骤已经实现了从\"master\"到\"node01\"SSH无密码登录\n\n验证master到node01的无密码登陆,在master机器上，使用hadoop用户 ssh node01或者ssh 192.168.200.129, 下面是成功的的结果\n```\n[hadoop@master ~]$ ssh node01\nLast login: Sun Apr  2 17:25:50 2017 from localhost\n[hadoop@node01 ~]$ \n\n[hadoop@node01 ~]$ ssh master\nLast login: Sun Apr  2 17:26:04 2017 from node01\n[hadoop@master ~]$ \n```\n\n\n## **5 java安装环境**\n### **5.1 卸载原有的JDK**\n因为有的系统自带有JDK, 安装前先卸载  \n查看所装的JDK\n```shell\n[hadoop@master ~]$ rpm -qa | grep jdk\n出现\njava-1.7.0-openjdk-1.7.0.45-2.4.3.3.el6.x86_64\njava-1.6.0-openjdk-1.6.0.0-1.66.1.13.0.el6.x86_64\n```\nroot下卸载前面查出的这两个\n```shell\n[root@master hadoop]#  yum -y remove java-1.7.0-openjdk-1.7.0.45-2.4.3.3.el6.x86_64  \n[root@master hadoop]#  yum -y remove java-1.6.0-openjdk-1.6.0.0-1.66.1.13.0.el6.x86_64  \n成功后会出现一个complete\n\n```  \n### **5.2 安装jdk1.7**\n首先用root身份登录master后在/usr/local下创建java文件夹\n```shell\n[root@master hadoop]# mkdir -p /usr/local/java\n```  \n我们把FTP传来的jdk-7u79-linux-x64.tar.gz复制到/usr/local/java 文件夹下\n```shell\n[root@master Downloads]# cp jdk-7u79-linux-x64.tar.gz /usr/local/java/\n```\n解压并且\n```shell\n[root@master java]# tar zxvf jdk-7u79-linux-x64.tar.gz \n解压完成后出现\n[root@master java]# ll\ntotal 149920\ndrwxr-xr-x. 8 uucp  143      4096 Apr 11  2015 jdk1.7.0_79\n-rw-r--r--. 1 root root 153512879 Apr  2 18:17 jdk-7u79-linux-x64.tar.gz\n```\n给所有者权限\n```shell\n[root@master java]# chown hadoop:hadoop jdk1.7.0_79/ -R\n```\n### **5.3 配置java环境变量**\n编辑\"/etc/profile\"文件\n```shell\n[root@master java]#  vim /etc/profile\n```\n在尾部加入\n```shell\n# set java environment\nexport JAVA_HOME=/usr/local/java/jdk1.7.0_79\nexport JRE_HOME=/usr/local/java/jdk1.7.0_79/jre\nexport PATH=$PATH:/usr/local/java/jdk1.7.0_79/bin\nexport CLASSPATH=./:/usr/local/java/jdk1.7.0_79/lib:/usr/local/java/jdk1.7.0_79/jre/lib\n```\n使配置生效\n```shell\n[root@master java]# source /etc/profile\n```\n### **5.4 验证是否成功**\n```\njava -version  出现 java version \"1.7.0_79\"\njavac  有提示\njava  有提示\n```\n确保是按照我上面的步骤，权限不能有错，否则可能会有问题, 同样，在另外的节点上也安装好jdk\n\n\n## **6. Hadoop集群安装**  \n所有的机器上都要安装hadoop，现在就先在Master服务器安装，然后其他服务器按照步骤重复进行即可。**安装和配置hadoop需要以\"root\"的身份进行。**\n### **6.1 安装Hadoop**\n#### **6.1.1 建立一个目录，用来存放hadoop**\n```shell\n[root@master Downloads]#  mkdir -p /home/hadoop/MyCloudera/APP/hadoop/\n```\n#### **6.1.2 把下载好得hadoop-2.7.1.tar.gz 复制到这个目录下，解压并且命名为hadoop**\n```shell\n复制到我们建立得目录\n[root@master Downloads]# cp hadoop-2.7.1.tar.gz  /home/hadoop/MyCloudera/APP/hadoop/\n进入到我们复制得目录\n[root@master Downloads]# cd /home/hadoop/MyCloudera/APP/hadoop  \n对此tar.gz解压\n[root@master hadoop]# tar zxvf hadoop-2.7.1.tar.gz \n改名为hadoop\n[root@master hadoop]# mv hadoop-2.7.1 hadoop\n```\n#### **6.1.3 将文件夹得读写权限赋予给hadoop用户**\n```shell\n[root@master APP]# chown -R hadoop:hadoop hadoop  \nll 查看权限，是这样得\n[root@master APP]# ll\ntotal 4\ndrwxr-xr-x. 3 hadoop hadoop 4096 Apr  2 23:29 hadoop\n```\n\n\n#### **6.1.4 配置/etc/profile**\n```shell\n[root@master APP]# vim /etc/profile\n```\n在末尾加上如下配置，其中HADOOP_HOME填写前面得hadoop存放得位置\n```\n# set hadoop path\nexport HADOOP_HOME=/home/hadoop/MyCloudera/APP/hadoop/hadoop \nexport PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin\nexport HADOOP_MAPRED_HOME=$HADOOP_HOME\nexport HADOOP_COMMON_HOME=$HADOOP_HOME\nexport HADOOP_HDFS_HOME=$HADOOP_HOME\nexport HADOOP_YARN_HOME=$HADOOP_HOME\nexport HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop\nexport YARN_CONF_DIR=${HADOOP_HOME}/etc/hadoop\n\n```\n让配置生效\n```shell\n[root@master APP]# source /etc/profile\n```\n\n### **6.2 配置hadoop**\nHadoop配置文件在conf目录下，之前的版本的配置文件主要是Hadoop-default.xml和Hadoop-site.xml。由于Hadoop发展迅速，代码量急剧增加，代码开发分为了core，hdfs和map/reduce三部分，配置文件也被分成了三个core-site.xml、hdfs-site.xml、mapred-site.xml。core-site.xml和hdfs-site.xml是站在HDFS角度上配置文件；core-site.xml和mapred-site.xml是站在MapReduce角度上配置文件。\n\n#### **6.2.1 配置hadoop-env.sh**\n该hadoop-env.sh文件位于/home/hadoop/MyCloudera/APP/hadoop/hadoop/etc/hadoop目录下\n在文件的末尾添加下面内容\n```shell\n# The java environment\nexport JAVA_HOME=/usr/local/java/jdk1.7.0_79\n```\n#### **6.2.2 配置core-site.xml文件**\n我们先在本地建立几个目录，用来存放一些hadoop得文件  \n在根目录下，建立一个data\n```shell\n根目录路径\n[root@master /]# pwd\n/\n创建一个data目录\n[root@master /]# mkdir data\n创建/data/tmpdata/hadoop/data/tmp目录\n[root@master /]# mkdir -p /data/tmpdata/hadoop/data/tmp\n```\n然后对core-site.xml做如下配置, 具体得hadoop.tmp.dir和fs.default.name得功能参看百度google\n```xml\n<configuration>\n  <property>\n      <name>hadoop.tmp.dir</name>\n      <value>/data/tmpdata/hadoop/data/tmp</value>\n  </property>\n\n  <property>\n      <name>fs.default.name</name>\n      <value>hdfs://master:9000</value>\n  </property>\n</configuration>\n```\n#### **6.2.3 配置hdfs-site.xml文件**  \n我这里配置的比较完整，如果想简单点，有的其实可以默认设置，具体参看其他文章\n**1. 创建namenode和datanode的存放目录,然后对/data目录赋予权限。 注意权限不能有错**\n```shell\n[root@master /]# mkdir -p /data/hadoop/data/name\n[root@master data]# mkdir -p /data/hadoop/data/data\n\n[root@master /]#  chown hadoop:hadoop data/ -R\n[root@master /]# chmod 777 data/ -R \n\n```\n**2. 创建SecondaryNameNode的目录**\n在根目录下创建hadoop目录，然后创建/hadoop/SecondaryNameNode/目录，最后赋予hadoop目录权限\n```shell\n[root@master /]# mkdir hadoop  \n[root@master /]# mkdir -p /hadoop/SecondaryNameNode/  \n\n[root@master /]# chown hadoop:hadoop hadoop/ -R \n[root@master /]#  chmod 777 hadoop/ -R\n\n```\n\n**hdfs-site.xml配置**\n```xml\n<configuration>\n     <property>\n            <name>dfs.namenode.name.dir</name>\n            <value>/data/hadoop/data/name/</value>\n     </property>\n     <property>\n            <name>dfs.datanode.data.dir</name>\n            <value>/data/hadoop/data/data/</value>\n     </property>\n     <property>\n           <name>dfs.replication</name>\n           <value>2</value>\n     </property>\n     <property>\n            <name>dfs.namenode.checkpoint.dir</name>\n            <value>/hadoop/SecondaryNameNode/</value>\n     </property>\n    \n     <property>\n            <name>dfs.http.address</name>\n            <value>master:50070</value>\n     </property>\n    \n     <property>\n            <name>dfs.secondary.http.address</name>\n            <value>master:50090</value>\n     </property>\n\n\n    <property>\n    <name>dfs.datanode.du.reserved</name>\n    <value>0</value>\n    <description> 每个卷预留的空闲空间数量 </description>\n    </property>\n    \n    <property>\n    <name>dfs.datanode.max.xcievers</name>\n    <value>32768</value>\n    </property>\n    \n    \n    <property>\n    <name>dfs.datanode.socket.write.timeout</name>\n    <value>0</value>\n    </property>\n    <property>\n    <name>dfs.socket.timeout</name>\n    <value>180000</value>\n    <description>socket通讯超时时间</description>\n    </property>\n\n</configuration> \n```\n#### **6.2.3 配置mapred-site.xml文件**\n我这里配置的比较完整，网上大多数都是用的默认，具体其中的一些参数可以百度  \n这里先建立几个文件\n```shell\n[root@master /]# mkdir -p /hadoop/mapreduce/jobhistory/history/done\n[root@master /]# mkdir -p /hadoop/mapreduce/jobhistory/history/done_intermediate\n[root@master /]# mkdir -p /hadoop/hadoop-yarn/staging  \n\n赋予权限\n[root@master /]# chown hadoop:hadoop hadoop/ -R \n[root@master /]#  chmod 777 hadoop/ -R\n```\n复制一份 mapred-site.xml\n```shell\n[root@master hadoop]# cp mapred-site.xml.template mapred-site.xml\n```\n\n```xml\n<configuration>\n\n<property>\n<name>mapreduce.framework.name</name>\n<value>yarn</value>\n</property>\n\n<property>\n<name>mapreduce.jobtracker.address</name>\n<value>master:9001</value>\n</property>\n\n<property>\n<name>mapreduce.jobtracker.http.address</name>\n<value>master:50030</value>\n</property>\n\n<property>\n<name>mapreduce.jobhistory.address</name>\n<value>master:10020</value>\n</property>\n\n\n<property>\n<name>mapreduce.jobhistory.webapp.address</name>\n<value>master:19888</value>\n</property>\n\n\n<property>\n<name>mapreduce.jobhistory.done-dir</name>\n<value>/hadoop/mapreduce/jobhistory/history/done</value>\n</property>\n\n<property>\n<name>mapreduce.jobhistory.intermediate-done-dir</name>\n<value>/hadoop/mapreduce/jobhistory/history/done_intermediate</value>\n</property>\n\n<property>\n<name>yarn.app.mapreduce.am.staging-dir</name>\n<value>/hadoop/hadoop-yarn/staging</value>\n</property>\n\n<property>\n<name>mapred.hosts.exclude</name>\n<value>/home/hadoop/MyCloudera/APP/hadoop/hadoop/etc/hadoop/excludes</value>\n<final>true</final>\n</property>\n\n\n\n\n<property>\n<name>mapreduce.tasktracker.map.tasks.maximum</name>\n<value>32</value>\n<description> 同一时间允许运行的最大map任务数 </description>\n</property>\n<property>\n<name>mapreduce.tasktracker.reduce.tasks.maximum</name>\n<value>16</value>\n<description> 同一时间允许运行的最大reduce任务数 </description>\n</property>\n\n\n\n<property>\n<name>yarn.nodemanager.resource.memory-mb</name>\n<value>1000</value>\n</property>\n<property>\n<name>mapreduce.map.memory.mb</name>\n<value>512</value>\n<description>map阶段申请的container的内存的大小</description>\n</property>\n\n\n<property>\n<name>mapreduce.reduce.memory.mb</name>\n<value>512</value>\n<description>reduce阶段申请的container的内存的大小</description>\n</property>\n\n\n<property>\n<name>mapreduce.map.java.opts</name>\n<value>-Xmx512M</value>\n<description>用户设定的map/reduce阶段申请的container的JVM参数。最大堆设定要比申请的内存少一些，用于JVM的非堆部分使用。 </description>\n</property>\n<property>\n<name>mapreduce.reduce.java.opts</name>\n<value>-Xmx1024M</value>\n</property>\n<property>\n<name>mapreduce.task.io.sort.mb</name>\n<value>1024</value>\n</property>\n<property>\n<name>mapreduce.reduce.shuffle.parallelcopies</name>\n<value>16</value>\n\n</configuration>\n```\n\n#### **6.2.3 配置yarn-site.xml文件**\n创建一些文件夹，并且赋予权限\n```shell\n[root@master /]# clear\n[root@master /]# mkdir -p /data/nodemanager/tmp/\n[root@master /]# mkdir -p /hadoop/nodemanager/remote\n[root@master /]# mkdir -p /data/hadoop/data/nodemanager/logs\n[root@master /]# chown hadoop:hadoop hadoop/ -R\n[root@master /]# chmod 777 hadoop/ -R\n[root@master /]# chown hadoop:hadoop data/ -R\n[root@master /]# chmod 777 data/ -R\n```\n\n```xml\n<configuration>\n\n<!-- Site specific YARN configuration properties -->\n    <property>\n         <name>yarn.nodemanager.aux-services</name>\n          <value>mapreduce_shuffle</value>\n    </property>\n\n    <property>\n          <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>\n          <value>org.apache.hadoop.mapred.ShuffleHandler</value>\n     </property>\n\n    <property>\n            <name>yarn.resourcemanager.scheduler.address</name>\n            <value>master:8030</value>\n     </property>\n\n    <property>\n            <name>yarn.resourcemanager.resource-tracker.address</name>\n            <value>master:8031</value>\n            </property>\n\n    <property>\n            <name>yarn.resourcemanager.address</name>\n            <value>master:8032</value>\n     </property>\n\n    <property>\n            <name>yarn.resourcemanager.admin.address</name>\n             <value>master:8033</value>\n    </property>\n\n    <property>\n            <name>yarn.nodemanager.address</name>\n            <value>master:9999</value>\n    </property>\n\n    <property>\n            <name>yarn.nodemanager.webapp.address</name>\n            <value>master:8042</value>\n    </property>\n    \n     <property>\n            <name>yarn.resourcemanager.webapp.address</name>\n            <value>master:8088</value>\n    </property>\n\n    <property>\n            <name>yarn.nodemanager.local-dirs</name>\n            <value>/data/nodemanager/tmp/</value>\n            </property>\n\n    <property>\n            <name>yarn.nodemanager.remote-app-log-dir</name>\n            <value>/hadoop/nodemanager/remote</value>\n    </property>\n\n    <property>\n          <name>yarn.nodemanager.log-dirs</name>\n           <value>/data/hadoop/data/nodemanager/logs</value>\n     </property>\n\n    <property>\n            <name>yarn.nodemanager.log.retain-seconds</name>\n            <value>604800</value>\n    </property>\n\n    <property>\n            <name>yarn.nodemanager.resource.cpu-vcores</name>\n            <value>24</value>\n    </property>\n\n    <property>\n            <name>yarn.nodemanager.resource.memory-mb</name>\n            <value>1024</value>\n    </property>\n\n    <property>\n            <name>yarn.nodemanager.vmem-pmem-ratio</name>\n            <value>2</value>\n    </property>\n\n    <property>\n            <name>yarn.scheduler.minimum-allocation-mb</name>\n            <value>256</value>\n    </property>\n\n    <property>\n            <name>yarn.scheduler.maximum-allocation-mb</name>\n            <value>1024</value>\n    </property>\n\n    <property>\n            <name>yarn.scheduler.minimum-allocation-vcores</name>\n            <value>1</value>\n    </property>\n\n    <property>\n            <name>yarn.scheduler.maximum-allocation-vcores</name>\n            <value>24</value>\n    </property>\n\n    <property>\n            <name>yarn.log-aggregation-enable</name>\n            <value>true</value>\n    </property>\n\n</configuration>\n\n```\n\n\n\n\n#### **6.2.3 配置slaves文件**\n这个配置主要记录数据节点的列表，假如集群有3个数据节点，如：node001，node002，node003\n那么在slave文件里面就可以设置为：  \nnode001  \nnode002  \nnode003  \n\n我这里为两个节点，配置如下\n```shell\nmaster\nnode01\n```\n到此，master的hadoop的配置已经完成，对于其他节点，我们建立好相关的目录，复制过去，稍作配置即可了\n\n需要建立的目录总结\n\n```shell\n[root@node01 /]# mkdir -p /home/hadoop/MyCloudera/APP/hadoop/\n[root@node01 /]# mkdir data\n[root@node01 /]# mkdir -p /data/tmpdata/hadoop/data/tmp\n[root@node01 /]# mkdir -p /data/hadoop/data/name\n[root@node01 /]# mkdir -p /data/hadoop/data/data\n[root@node01 /]# mkdir hadoop\n[root@node01 /]# mkdir -p /hadoop/SecondaryNameNode/  \n[root@node01 /]# chown hadoop:hadoop hadoop/ -R \n[root@node01 /]# chmod 777 hadoop/ -R\n[root@node01 /]# mkdir -p /hadoop/mapreduce/jobhistory/history/done\n[root@node01 /]# mkdir -p /hadoop/mapreduce/jobhistory/history/done_intermediate\n[root@node01 /]# mkdir -p /hadoop/hadoop-yarn/staging  \n[root@node01 /]# mkdir -p /data/nodemanager/tmp/\n[root@node01 /]# mkdir -p /hadoop/nodemanager/remote\n[root@node01 /]# mkdir -p /data/hadoop/data/nodemanager/logs\n[root@node01 /]# chown hadoop:hadoop hadoop/ -R\n[root@node01 /]# chmod 777 hadoop/ -R\n[root@node01 /]# chown hadoop:hadoop data/ -R\n[root@node01 /]# chmod 777 data/ -R\n[root@node01 /]# \n\n```\n\n\n为了确保 hadoop目录 权限没有问题，每台机器在hadoop目录下再次执行一下以下命令\n```shell\nchown -R hadoop:hadoop hadoop\n## 为了保险起见，我给了777的权限， 下面的这一步貌似不做也可以\nchmod 777 hadoop/ -R\n```\n\n### **6.3 启动与验证**\n#### **6.3.1 格式化HDFS文件系统**\n**在master上使用普通用户hadoop进行操作**\n如果第一次启动需要对hadoop平台进行格式化，记得第一次，假如原来有数据就不需要格式化：\n```\nhdfs namenode -format\n```\n如果经过多次format之后，**一定要把/data/hadoop/data/data /data/hadoop/data/name目录下的文件删除**\n\n#### **6.3.2 启动hadoop**\n在启动前关闭集群中所有机器的防火墙，不然会出现datanode开后又自动关闭。\n记得永久的关闭防火墙chkconfig iptables off\n```shell\nchkconfig iptables off\n```\n开始启动,在master的普通用户 hadoop下进行操作\n```shell\nstart-all.sh\n```\n验证hadoop: 输入jps命令，会出现以下进程说明成功\n```shell\n[hadoop@master hadoop]$ jps\n[hadoop@master hadoop]$ jps\n4197 ResourceManager\n3851 DataNode\n4602 Jps\n4013 SecondaryNameNode\n4308 NodeManager\n3739 NameNode\n```  \n#### **6.3.3 测试以下hdfs**  \n创建一个目录\n```shell\n[hadoop@node01 ~]$ hadoop fs -mkdir -p /hive/warehouse\n``` \n传一个文件\n```shell\n[hadoop@master hadoop]$ hadoop fs -put slaves /hive/warehouse\n```\n查看文件\n```shell\n[hadoop@master hadoop]$ hadoop fs -cat /hive/warehouse/slaves\n显示\nmaster\nnode01\n```\n经过上面的测试，说明我们集群安装成功\n### 6.4 网页查看集群\n查看hdfs   \nhttp://192.168.200.128:50070\n显示  \n![hdfs验证](http://i2.muimg.com/567571/7435a4693a8a21b4.png)\n\n验证hadoop   \nhttp://192.168.200.128:8088/cluster/nodes\n显示  \n![hadoop验证](http://i1.piimg.com/567571/80a651b3fc692958.png)\n\n\n## **7. hadoop 集群碰到错误的解决办法**\n这里的错误，一般都分为几大类，一类是某些文件夹没有创建，一类是某些文件或者文件夹权限不够，一类就是配置错误  \n这些错误都可以去logs目录下查看，我的logs目录在  /home/hadoop/MyCloudera/APP/hadoop/hadoop/logs  \n哪里有问题就对应哪个文件去查看错误，例如resourcemanager没起来或者出问题，就去yarn-hadoop-resourcemanager-master.log\n```shell\n-rwxrwxrwx. 1 hadoop hadoop 921348 Apr  3 13:19 hadoop-hadoop-datanode-master.log\n-rw-rw-r--. 1 hadoop hadoop   1434 Apr  3 13:18 hadoop-hadoop-datanode-master.out\n-rw-rw-r--. 1 hadoop hadoop      0 Apr  3 13:18 hadoop-hadoop-datanode-master.out.1\n-rw-rw-r--. 1 hadoop hadoop   1434 Apr  3 12:52 hadoop-hadoop-datanode-master.out.2\n-rwxrwxrwx. 1 hadoop hadoop   1434 Apr  3 12:45 hadoop-hadoop-datanode-master.out.3\n-rwxrwxrwx. 1 hadoop hadoop   1434 Apr  3 12:42 hadoop-hadoop-datanode-master.out.4\n-rwxrwxrwx. 1 hadoop hadoop   1434 Apr  3 12:35 hadoop-hadoop-datanode-master.out.5\n-rwxrwxrwx. 1 hadoop hadoop 371773 Apr  3 13:26 hadoop-hadoop-namenode-master.log\n-rw-rw-r--. 1 hadoop hadoop    717 Apr  3 13:18 hadoop-hadoop-namenode-master.out\n-rw-rw-r--. 1 hadoop hadoop    717 Apr  3 13:09 hadoop-hadoop-namenode-master.out.1\n-rw-rw-r--. 1 hadoop hadoop    717 Apr  3 12:52 hadoop-hadoop-namenode-master.out.2\n-rwxrwxrwx. 1 hadoop hadoop    717 Apr  3 12:44 hadoop-hadoop-namenode-master.out.3\n-rwxrwxrwx. 1 hadoop hadoop    717 Apr  3 12:42 hadoop-hadoop-namenode-master.out.4\n-rwxrwxrwx. 1 hadoop hadoop    717 Apr  3 12:35 hadoop-hadoop-namenode-master.out.5\n-rwxrwxrwx. 1 hadoop hadoop      0 Apr  3 01:43 SecurityAuth-hadoop.audit\n-rwxrwxrwx. 1 hadoop hadoop 618506 Apr  3 13:19 yarn-hadoop-nodemanager-master.log\n-rw-rw-r--. 1 hadoop hadoop   1402 Apr  3 13:19 yarn-hadoop-nodemanager-master.out\n-rw-rw-r--. 1 hadoop hadoop      0 Apr  3 13:19 yarn-hadoop-nodemanager-master.out.1\n-rw-rw-r--. 1 hadoop hadoop   1402 Apr  3 13:09 yarn-hadoop-nodemanager-master.out.2\n-rw-rw-r--. 1 hadoop hadoop   1402 Apr  3 12:52 yarn-hadoop-nodemanager-master.out.3\n-rwxrwxrwx. 1 hadoop hadoop   1402 Apr  3 12:36 yarn-hadoop-nodemanager-master.out.4\n-rwxrwxrwx. 1 hadoop hadoop   1402 Apr  3 12:30 yarn-hadoop-nodemanager-master.out.5\n-rwxrwxrwx. 1 hadoop hadoop 343209 Apr  3 13:19 yarn-hadoop-resourcemanager-master.log\n-rw-rw-r--. 1 hadoop hadoop    701 Apr  3 13:19 yarn-hadoop-resourcemanager-master.out\n-rw-rw-r--. 1 hadoop hadoop    701 Apr  3 13:09 yarn-hadoop-resourcemanager-master.out.1\n-rw-rw-r--. 1 hadoop hadoop    701 Apr  3 12:52 yarn-hadoop-resourcemanager-master.out.2\n-rwxrwxrwx. 1 hadoop hadoop    701 Apr  3 12:45 yarn-hadoop-resourcemanager-master.out.3\n-rwxrwxrwx. 1 hadoop hadoop    701 Apr  3 12:36 yarn-hadoop-resourcemanager-master.out.4\n-rwxrwxrwx. 1 hadoop hadoop    701 Apr  3 12:30 yarn-hadoop-resourcemanager-master.out.5\n```\n\n\n\n\n\n\n\n\n\n\n","source":"_posts/hadoop集群搭建教程.md","raw":"---\ntitle: hadoop集群搭建教程\ndate: 2016-07-15 21:25:21\ntags: [hadoop]\ncategories: [大数据,hadoop]\nauthor: kaishun\nid: 12\npermalink: hadoop-cluster-install\n---\n\nhadoop的配置参看github https://github.com/zhaikaishun/hadoop_cluster  \n作者: 翟开顺\n  \n关键字: \n集群环境介绍，Hadoop简介，网络配置，所需软件\nSSH免密码登陆配置，java环境安装，卸载原有的JDK， 安装jdk17， 配置java环境变量，验证是否成功，Hadoop集群安装，安装Hadoop，验证hadoop\nhadoop错误分析\n<!-- more -->\n# **集群环境介绍**\n## **1.  Hadoop简介**  \n　Hadoop是Apache软件基金会旗下的一个开源分布式计算平台。以Hadoop分布式文件系统（HDFS，Hadoop Distributed Filesystem）和MapReduce（Google MapReduce的开源实现）为核心的Hadoop为用户提供了系统底层细节透明的分布式基础架构。\n　　对于Hadoop的集群来讲，可以分成两大类角色：Master和Salve。一个HDFS集群是由一个NameNode和若干个DataNode组成的。其中NameNode作为主服务器，管理文件系统的命名空间和客户端对文件系统的访问操作；集群中的DataNode管理存储的数据。MapReduce框架是由一个单独运行在主节点上的JobTracker和运行在每个集群从节点的TaskTracker共同组成的。主节点负责调度构成一个作业的所有任务，这些任务分布在不同的从节点上。主节点监控它们的执行情况，并且重新执行之前的失败任务；从节点仅负责由主节点指派的任务。当一个Job被提交时，JobTracker接收到提交作业和配置信息之后，就会将配置信息等分发给从节点，同时调度任务并监控TaskTracker的执行。\n　　从上面的介绍可以看出，HDFS和MapReduce共同组成了Hadoop分布式系统体系结构的核心。HDFS在集群上实现分布式文件系统，MapReduce在集群上实现了分布式计算和任务处理。HDFS在MapReduce任务处理过程中提供了文件操作和存储等支持，MapReduce在HDFS的基础上实现了任务的分发、跟踪、执行等工作，并收集结果，二者相互作用，完成了Hadoop分布式集群的主要任务。\n\n\n## **2.  环境说明**  \n本教程为了简单起见只设置两个节点： master为主节点，node01为数据节点，节点之间局域网连接，相互可以ping通，节点IP分布如下\n\n机器名称 | IP地址\n---|---\nmaster | 192.168.200.128\nnode01 | 192.168.200.129\n\n两个节点都是centos6.5系统，都有同一个用户，用户名叫Hadoop  \n\n### **给hadoop用户赋予root权限**  \n切换到root用户 赋予etc/sudoers777权限，然后打开\n```shell\n[root@kaishun etc]# chmod 777 /etc/sudoers\n[root@kaishun etc]# vim /etc/sudoers\n```  \n找到Allows people in group wheel to run all commands，把下面%wheel的#给去掉,在Allow root to run any commands anywhere下，加上hadoop  ALL=(ALL)       ALL，然后保存\n```shell\n## Allow root to run any commands anywhere\nroot    ALL=(ALL)       ALL     \nhadoop  ALL=(ALL)       ALL\n\n## Allows people in group wheel to run all commands\n%wheel        ALL=(ALL)       ALL\n```\n把sudoers的权限改回来成440\n```shell\n[root@kaishun etc]# chmod 440 /etc/sudoers\n```    \n测试是否成功  \n在普通用户下\n```shell\n[hadoop@kaishun ~]$ sudo mkdir test\n输入密码如果可以成功创建文件夹，说明成功\n```\n### **网络配置**\n\n**1. 查看当前机器名**\n**在root用户下**输入，显示\n```shell\n[root@kaishun hadoop]# hostname\n显示 kaishun， 与我们规划的master不符合\n```  \n**2. 在root用户下修改当前机器名称**\n```shell\n[root@kaishun hadoop]# vim /etc/sysconfig/network\n```\n修改HOSTNAME 为 master\n```shell\nHOSTNAME=master\n```\n同理，192.168.200.129这台机器修改成node01\n修改之后，可能不会立即生效，我是重启后才生效的  \n**3. 在root用户下配置hosts文件, 每台机器都需要配置（必须）**  \n```shell\n[root@master hadoop]# vim /etc/hosts\n```\n添加\n```shell\n127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\n192.168.200.128 master\n192.168.200.129 node01\n```  \n**测试是否成功**，如果能相互使用ping node01   ping master能成功，说明hosts文件配置成功\n```shell\n[hadoop@master ~]$ ping node01\nPING node01 (192.168.200.129) 56(84) bytes of data.\n64 bytes from node01 (192.168.200.129): icmp_seq=1 ttl=64 time=0.391 ms\n64 bytes from node01 (192.168.200.129): icmp_seq=2 ttl=64 time=0.435 ms\n64 bytes from node01 (192.168.200.129): icmp_seq=3 ttl=64 time=0.442 ms\n\n[hadoop@node01 ~]$ ping master\nPING master (192.168.200.128) 56(84) bytes of data.\n64 bytes from master (192.168.200.128): icmp_seq=1 ttl=64 time=0.379 ms\n64 bytes from master (192.168.200.128): icmp_seq=2 ttl=64 time=0.411 ms\n64 bytes from master (192.168.200.128): icmp_seq=3 ttl=64 time=0.460 ms\n```\n\n## **3.  所需软件**\n1.  JDK版本1.7  \n2.  hadoop版本hadoop-2.7.1  去官网的华科镜像下载hadoop-2.7.1.tar.gz， [地址](http://mirrors.hust.edu.cn/apache/hadoop/common/hadoop-2.7.1/)  \n3. 数据传输工具FileZilla， ssh连接工具 secureCRT  \n\n## **4.  SSH免密码登陆配置**\nHadoop运行过程中需要管理远端Hadoop守护进程，在Hadoop启动以后，NameNode是通过SSH（Secure Shell）来启动和停止各个DataNode上的各种守护进程的。这就必须在节点之间执行指令的时候是不需要输入密码的形式，故我们需要配置SSH运用无密码公钥认证的形式，这样NameNode使用SSH无密码登录并启动DataName进程，同样原理，DataNode上也能使用SSH无密码登录到NameNode。    \n安装CentOS6.5时，我们选择了一些基本安装包，所以我们需要两个服务：ssh和rsync已经安装了。可以通过下面命令查看结果显示如下：\n```\n[hadoop@master ~]$ rpm –qa | grep openssh\n[hadoop@master ~]$ rpm –qa | grep rsync\n如果有相应的提示，说明这两个是装好了的，我这里是系统自带的\n```  \n**4.1 配置master无密码登陆所有的node**  \n原理请百度\n在master节点上执行以下命令 然后按几次回车键：\n```shell\n[hadoop@master ~]$ ssh-keygen -t rsa\n```  \n出现下图  \n![ssh免密码登陆](http://i4.buimg.com/567571/b22c79e94cbc2297.png)\n\n我们看到这句话  \nYour identification has been saved in /home/hadoop/.ssh/id_rsa.  \nYour public key has been saved in /home/hadoop/.ssh/id_rsa.pub.  \n说明默认目录在 /home/hadoop/.ssh/ 下\n\n接着在master节点上做如下配置，把id_rsa.pub追加到授权的key里面去。  \n```shell\n[hadoop@master ~]$ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n```\n现在我们进入~/.ssh目录可以看到\n```\n[hadoop@master ~]$ cd ~/.ssh/\n[hadoop@master .ssh]$ ll\ntotal 12\n-rw-rw-r--. 1 hadoop hadoop  395 Apr  2 16:22 authorized_keys\n-rw-------. 1 hadoop hadoop 1675 Apr  2 16:17 id_rsa\n-rw-r--r--. 1 hadoop hadoop  395 Apr  2 16:17 id_rsa.pub\n```\n**4.1.1. 修改文件\"authorized_keys权限**  \n```\n[hadoop@master .ssh]$ chmod 600 ~/.ssh/authorized_keys\n```\n![authorized_keys权限](http://i2.muimg.com/567571/41ecd0b8634028ae.png)\n\n**4.1.2. 设置SSH配置**  \n用root用户登录服务器修改SSH配置文件\"/etc/ssh/sshd_config\"的下列内容。这里找到这些内容，把前面的#去掉即可\n```shell\n[root@master .ssh]# vim /etc/ssh/sshd_config\nRSAAuthentication yes # 启用 RSA 认证\nPubkeyAuthentication yes # 启用公钥私钥配对认证方式\nAuthorizedKeysFile .ssh/authorized_keys # 公钥文件路径（和上面生成的文件同）\n```\n**重启SSH服务**\n```\n[root@master .ssh]# /etc/rc.d/init.d/sshd restart\nStopping sshd:                                             [  OK  ]\nStarting sshd:                                             [  OK  ]\n[root@master .ssh]# \n```\n退出root用户，使用hadoop普通用户验证是否成功, ssh localhost, 如果不需要输入密码，那么验证成功\n```shell\n[hadoop@master .ssh]$ ssh localhost\nThe authenticity of host 'localhost (::1)' can't be established.\nRSA key fingerprint is 48:0b:ee:9b:67:85:4c:19:35:10:d1:1d:e1:5d:fa:c4.\nAre you sure you want to continue connecting (yes/no)? yes\nWarning: Permanently added 'localhost' (RSA) to the list of known hosts.\nLast login: Sun Apr  2 16:09:39 2017 from 192.168.200.1\n```\n\n**4.1.3. 把公钥复制到所有的node机器上**\n从上图中得知无密码登录本级已经设置完毕，接下来的事儿是把公钥复制所有的node机器上。使用下面的命令格式进行复制公钥  \nscp ~/.ssh/id_rsa.pub 远程用户名@远程服务器IP:~/\n我本地这样使用 scp ~/.ssh/id_rsa.pub hadoop@192.168.200.129:~/  ,然后根据提示输入需要复制的远程服务器的密码，最后出现下面的提示说明复制成功\n```shell\n[hadoop@master ~]$ scp ~/.ssh/id_rsa.pub hadoop@192.168.200.129:~/\nhadoop@192.168.200.129's password:  # 这里输入远程密码\nid_rsa.pub                                                                                                                        100%  395     0.4KB/s   00:00    \n[hadoop@master ~]$ \n```  \n**4.1.4. 对节点机器进行配置**\n下面就针对IP为\"192.168.200.129\"的node01的节点进行配置。\n4.1  ll -a查看是否有.ssh目录，如果没有，我们需要创建一个.ssh目录，并且赋予这个权限 drwx------.  2 hadoop hadoop 4096 Apr  2 16:40 .ssh  具体权限参照master的机器， centos6.5一般都是默认带有.ssh目录的\n```\n[hadoop@node01 ~]$ ll -a \ndrwx------.  2 hadoop hadoop 4096 Apr  2 16:40 .ssh\n```\n如果有这个目录了，我们把刚才的文件追加到authorized_keys 中去，然后修改authorized_keys文件权限\n```\n[hadoop@node01 ~]$ cat ~/id_rsa.pub >> ~/.ssh/authorized_keys\n[hadoop@node01 .ssh]$ chmod 600 ~/.ssh/authorized_keys\n\n```\n进入到ssh 目录，ll 看到如下所示说明成功，注意权限是否正确\n```shell\n[hadoop@node01 .ssh]$ ll\ntotal 8\n-rw-------. 1 hadoop hadoop 395 Apr  2 16:52 authorized_keys\n-rw-r--r--. 1 hadoop hadoop 391 Apr  2 16:40 known_hosts\n```\n**4.2  用root用户修改/etc/ssh/sshd_config**\n参考前面的master的修改/etc/ssh/sshd_config的方法  \n设置SSH配置  \n用root用户登录服务器修改SSH配置文件\"/etc/ssh/sshd_config\"的下列内容。这里找到这些内容，把前面的#去掉即可\n```shell\n[root@master .ssh]# vim /etc/ssh/sshd_config\nRSAAuthentication yes # 启用 RSA 认证\nPubkeyAuthentication yes # 启用公钥私钥配对认证方式\nAuthorizedKeysFile .ssh/authorized_keys # 公钥文件路径（和上面生成的文件同）\n```\n**重启SSH服务**\n```\n[root@master .ssh]# /etc/rc.d/init.d/sshd restart\nStopping sshd:                                             [  OK  ]\nStarting sshd:                                             [  OK  ]\n[root@master .ssh]# \n```\n最后记得把\"/home/hadoop/\"目录下的\"id_rsa.pub\"文件删除掉 \n\n到此为止，我们经过的步骤已经实现了从\"master\"到\"node01\"SSH无密码登录\n\n验证master到node01的无密码登陆,在master机器上，使用hadoop用户 ssh node01或者ssh 192.168.200.129, 下面是成功的的结果\n```\n[hadoop@master ~]$ ssh node01\nLast login: Sun Apr  2 17:25:50 2017 from localhost\n[hadoop@node01 ~]$ \n\n[hadoop@node01 ~]$ ssh master\nLast login: Sun Apr  2 17:26:04 2017 from node01\n[hadoop@master ~]$ \n```\n\n\n## **5 java安装环境**\n### **5.1 卸载原有的JDK**\n因为有的系统自带有JDK, 安装前先卸载  \n查看所装的JDK\n```shell\n[hadoop@master ~]$ rpm -qa | grep jdk\n出现\njava-1.7.0-openjdk-1.7.0.45-2.4.3.3.el6.x86_64\njava-1.6.0-openjdk-1.6.0.0-1.66.1.13.0.el6.x86_64\n```\nroot下卸载前面查出的这两个\n```shell\n[root@master hadoop]#  yum -y remove java-1.7.0-openjdk-1.7.0.45-2.4.3.3.el6.x86_64  \n[root@master hadoop]#  yum -y remove java-1.6.0-openjdk-1.6.0.0-1.66.1.13.0.el6.x86_64  \n成功后会出现一个complete\n\n```  \n### **5.2 安装jdk1.7**\n首先用root身份登录master后在/usr/local下创建java文件夹\n```shell\n[root@master hadoop]# mkdir -p /usr/local/java\n```  \n我们把FTP传来的jdk-7u79-linux-x64.tar.gz复制到/usr/local/java 文件夹下\n```shell\n[root@master Downloads]# cp jdk-7u79-linux-x64.tar.gz /usr/local/java/\n```\n解压并且\n```shell\n[root@master java]# tar zxvf jdk-7u79-linux-x64.tar.gz \n解压完成后出现\n[root@master java]# ll\ntotal 149920\ndrwxr-xr-x. 8 uucp  143      4096 Apr 11  2015 jdk1.7.0_79\n-rw-r--r--. 1 root root 153512879 Apr  2 18:17 jdk-7u79-linux-x64.tar.gz\n```\n给所有者权限\n```shell\n[root@master java]# chown hadoop:hadoop jdk1.7.0_79/ -R\n```\n### **5.3 配置java环境变量**\n编辑\"/etc/profile\"文件\n```shell\n[root@master java]#  vim /etc/profile\n```\n在尾部加入\n```shell\n# set java environment\nexport JAVA_HOME=/usr/local/java/jdk1.7.0_79\nexport JRE_HOME=/usr/local/java/jdk1.7.0_79/jre\nexport PATH=$PATH:/usr/local/java/jdk1.7.0_79/bin\nexport CLASSPATH=./:/usr/local/java/jdk1.7.0_79/lib:/usr/local/java/jdk1.7.0_79/jre/lib\n```\n使配置生效\n```shell\n[root@master java]# source /etc/profile\n```\n### **5.4 验证是否成功**\n```\njava -version  出现 java version \"1.7.0_79\"\njavac  有提示\njava  有提示\n```\n确保是按照我上面的步骤，权限不能有错，否则可能会有问题, 同样，在另外的节点上也安装好jdk\n\n\n## **6. Hadoop集群安装**  \n所有的机器上都要安装hadoop，现在就先在Master服务器安装，然后其他服务器按照步骤重复进行即可。**安装和配置hadoop需要以\"root\"的身份进行。**\n### **6.1 安装Hadoop**\n#### **6.1.1 建立一个目录，用来存放hadoop**\n```shell\n[root@master Downloads]#  mkdir -p /home/hadoop/MyCloudera/APP/hadoop/\n```\n#### **6.1.2 把下载好得hadoop-2.7.1.tar.gz 复制到这个目录下，解压并且命名为hadoop**\n```shell\n复制到我们建立得目录\n[root@master Downloads]# cp hadoop-2.7.1.tar.gz  /home/hadoop/MyCloudera/APP/hadoop/\n进入到我们复制得目录\n[root@master Downloads]# cd /home/hadoop/MyCloudera/APP/hadoop  \n对此tar.gz解压\n[root@master hadoop]# tar zxvf hadoop-2.7.1.tar.gz \n改名为hadoop\n[root@master hadoop]# mv hadoop-2.7.1 hadoop\n```\n#### **6.1.3 将文件夹得读写权限赋予给hadoop用户**\n```shell\n[root@master APP]# chown -R hadoop:hadoop hadoop  \nll 查看权限，是这样得\n[root@master APP]# ll\ntotal 4\ndrwxr-xr-x. 3 hadoop hadoop 4096 Apr  2 23:29 hadoop\n```\n\n\n#### **6.1.4 配置/etc/profile**\n```shell\n[root@master APP]# vim /etc/profile\n```\n在末尾加上如下配置，其中HADOOP_HOME填写前面得hadoop存放得位置\n```\n# set hadoop path\nexport HADOOP_HOME=/home/hadoop/MyCloudera/APP/hadoop/hadoop \nexport PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin\nexport HADOOP_MAPRED_HOME=$HADOOP_HOME\nexport HADOOP_COMMON_HOME=$HADOOP_HOME\nexport HADOOP_HDFS_HOME=$HADOOP_HOME\nexport HADOOP_YARN_HOME=$HADOOP_HOME\nexport HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop\nexport YARN_CONF_DIR=${HADOOP_HOME}/etc/hadoop\n\n```\n让配置生效\n```shell\n[root@master APP]# source /etc/profile\n```\n\n### **6.2 配置hadoop**\nHadoop配置文件在conf目录下，之前的版本的配置文件主要是Hadoop-default.xml和Hadoop-site.xml。由于Hadoop发展迅速，代码量急剧增加，代码开发分为了core，hdfs和map/reduce三部分，配置文件也被分成了三个core-site.xml、hdfs-site.xml、mapred-site.xml。core-site.xml和hdfs-site.xml是站在HDFS角度上配置文件；core-site.xml和mapred-site.xml是站在MapReduce角度上配置文件。\n\n#### **6.2.1 配置hadoop-env.sh**\n该hadoop-env.sh文件位于/home/hadoop/MyCloudera/APP/hadoop/hadoop/etc/hadoop目录下\n在文件的末尾添加下面内容\n```shell\n# The java environment\nexport JAVA_HOME=/usr/local/java/jdk1.7.0_79\n```\n#### **6.2.2 配置core-site.xml文件**\n我们先在本地建立几个目录，用来存放一些hadoop得文件  \n在根目录下，建立一个data\n```shell\n根目录路径\n[root@master /]# pwd\n/\n创建一个data目录\n[root@master /]# mkdir data\n创建/data/tmpdata/hadoop/data/tmp目录\n[root@master /]# mkdir -p /data/tmpdata/hadoop/data/tmp\n```\n然后对core-site.xml做如下配置, 具体得hadoop.tmp.dir和fs.default.name得功能参看百度google\n```xml\n<configuration>\n  <property>\n      <name>hadoop.tmp.dir</name>\n      <value>/data/tmpdata/hadoop/data/tmp</value>\n  </property>\n\n  <property>\n      <name>fs.default.name</name>\n      <value>hdfs://master:9000</value>\n  </property>\n</configuration>\n```\n#### **6.2.3 配置hdfs-site.xml文件**  \n我这里配置的比较完整，如果想简单点，有的其实可以默认设置，具体参看其他文章\n**1. 创建namenode和datanode的存放目录,然后对/data目录赋予权限。 注意权限不能有错**\n```shell\n[root@master /]# mkdir -p /data/hadoop/data/name\n[root@master data]# mkdir -p /data/hadoop/data/data\n\n[root@master /]#  chown hadoop:hadoop data/ -R\n[root@master /]# chmod 777 data/ -R \n\n```\n**2. 创建SecondaryNameNode的目录**\n在根目录下创建hadoop目录，然后创建/hadoop/SecondaryNameNode/目录，最后赋予hadoop目录权限\n```shell\n[root@master /]# mkdir hadoop  \n[root@master /]# mkdir -p /hadoop/SecondaryNameNode/  \n\n[root@master /]# chown hadoop:hadoop hadoop/ -R \n[root@master /]#  chmod 777 hadoop/ -R\n\n```\n\n**hdfs-site.xml配置**\n```xml\n<configuration>\n     <property>\n            <name>dfs.namenode.name.dir</name>\n            <value>/data/hadoop/data/name/</value>\n     </property>\n     <property>\n            <name>dfs.datanode.data.dir</name>\n            <value>/data/hadoop/data/data/</value>\n     </property>\n     <property>\n           <name>dfs.replication</name>\n           <value>2</value>\n     </property>\n     <property>\n            <name>dfs.namenode.checkpoint.dir</name>\n            <value>/hadoop/SecondaryNameNode/</value>\n     </property>\n    \n     <property>\n            <name>dfs.http.address</name>\n            <value>master:50070</value>\n     </property>\n    \n     <property>\n            <name>dfs.secondary.http.address</name>\n            <value>master:50090</value>\n     </property>\n\n\n    <property>\n    <name>dfs.datanode.du.reserved</name>\n    <value>0</value>\n    <description> 每个卷预留的空闲空间数量 </description>\n    </property>\n    \n    <property>\n    <name>dfs.datanode.max.xcievers</name>\n    <value>32768</value>\n    </property>\n    \n    \n    <property>\n    <name>dfs.datanode.socket.write.timeout</name>\n    <value>0</value>\n    </property>\n    <property>\n    <name>dfs.socket.timeout</name>\n    <value>180000</value>\n    <description>socket通讯超时时间</description>\n    </property>\n\n</configuration> \n```\n#### **6.2.3 配置mapred-site.xml文件**\n我这里配置的比较完整，网上大多数都是用的默认，具体其中的一些参数可以百度  \n这里先建立几个文件\n```shell\n[root@master /]# mkdir -p /hadoop/mapreduce/jobhistory/history/done\n[root@master /]# mkdir -p /hadoop/mapreduce/jobhistory/history/done_intermediate\n[root@master /]# mkdir -p /hadoop/hadoop-yarn/staging  \n\n赋予权限\n[root@master /]# chown hadoop:hadoop hadoop/ -R \n[root@master /]#  chmod 777 hadoop/ -R\n```\n复制一份 mapred-site.xml\n```shell\n[root@master hadoop]# cp mapred-site.xml.template mapred-site.xml\n```\n\n```xml\n<configuration>\n\n<property>\n<name>mapreduce.framework.name</name>\n<value>yarn</value>\n</property>\n\n<property>\n<name>mapreduce.jobtracker.address</name>\n<value>master:9001</value>\n</property>\n\n<property>\n<name>mapreduce.jobtracker.http.address</name>\n<value>master:50030</value>\n</property>\n\n<property>\n<name>mapreduce.jobhistory.address</name>\n<value>master:10020</value>\n</property>\n\n\n<property>\n<name>mapreduce.jobhistory.webapp.address</name>\n<value>master:19888</value>\n</property>\n\n\n<property>\n<name>mapreduce.jobhistory.done-dir</name>\n<value>/hadoop/mapreduce/jobhistory/history/done</value>\n</property>\n\n<property>\n<name>mapreduce.jobhistory.intermediate-done-dir</name>\n<value>/hadoop/mapreduce/jobhistory/history/done_intermediate</value>\n</property>\n\n<property>\n<name>yarn.app.mapreduce.am.staging-dir</name>\n<value>/hadoop/hadoop-yarn/staging</value>\n</property>\n\n<property>\n<name>mapred.hosts.exclude</name>\n<value>/home/hadoop/MyCloudera/APP/hadoop/hadoop/etc/hadoop/excludes</value>\n<final>true</final>\n</property>\n\n\n\n\n<property>\n<name>mapreduce.tasktracker.map.tasks.maximum</name>\n<value>32</value>\n<description> 同一时间允许运行的最大map任务数 </description>\n</property>\n<property>\n<name>mapreduce.tasktracker.reduce.tasks.maximum</name>\n<value>16</value>\n<description> 同一时间允许运行的最大reduce任务数 </description>\n</property>\n\n\n\n<property>\n<name>yarn.nodemanager.resource.memory-mb</name>\n<value>1000</value>\n</property>\n<property>\n<name>mapreduce.map.memory.mb</name>\n<value>512</value>\n<description>map阶段申请的container的内存的大小</description>\n</property>\n\n\n<property>\n<name>mapreduce.reduce.memory.mb</name>\n<value>512</value>\n<description>reduce阶段申请的container的内存的大小</description>\n</property>\n\n\n<property>\n<name>mapreduce.map.java.opts</name>\n<value>-Xmx512M</value>\n<description>用户设定的map/reduce阶段申请的container的JVM参数。最大堆设定要比申请的内存少一些，用于JVM的非堆部分使用。 </description>\n</property>\n<property>\n<name>mapreduce.reduce.java.opts</name>\n<value>-Xmx1024M</value>\n</property>\n<property>\n<name>mapreduce.task.io.sort.mb</name>\n<value>1024</value>\n</property>\n<property>\n<name>mapreduce.reduce.shuffle.parallelcopies</name>\n<value>16</value>\n\n</configuration>\n```\n\n#### **6.2.3 配置yarn-site.xml文件**\n创建一些文件夹，并且赋予权限\n```shell\n[root@master /]# clear\n[root@master /]# mkdir -p /data/nodemanager/tmp/\n[root@master /]# mkdir -p /hadoop/nodemanager/remote\n[root@master /]# mkdir -p /data/hadoop/data/nodemanager/logs\n[root@master /]# chown hadoop:hadoop hadoop/ -R\n[root@master /]# chmod 777 hadoop/ -R\n[root@master /]# chown hadoop:hadoop data/ -R\n[root@master /]# chmod 777 data/ -R\n```\n\n```xml\n<configuration>\n\n<!-- Site specific YARN configuration properties -->\n    <property>\n         <name>yarn.nodemanager.aux-services</name>\n          <value>mapreduce_shuffle</value>\n    </property>\n\n    <property>\n          <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>\n          <value>org.apache.hadoop.mapred.ShuffleHandler</value>\n     </property>\n\n    <property>\n            <name>yarn.resourcemanager.scheduler.address</name>\n            <value>master:8030</value>\n     </property>\n\n    <property>\n            <name>yarn.resourcemanager.resource-tracker.address</name>\n            <value>master:8031</value>\n            </property>\n\n    <property>\n            <name>yarn.resourcemanager.address</name>\n            <value>master:8032</value>\n     </property>\n\n    <property>\n            <name>yarn.resourcemanager.admin.address</name>\n             <value>master:8033</value>\n    </property>\n\n    <property>\n            <name>yarn.nodemanager.address</name>\n            <value>master:9999</value>\n    </property>\n\n    <property>\n            <name>yarn.nodemanager.webapp.address</name>\n            <value>master:8042</value>\n    </property>\n    \n     <property>\n            <name>yarn.resourcemanager.webapp.address</name>\n            <value>master:8088</value>\n    </property>\n\n    <property>\n            <name>yarn.nodemanager.local-dirs</name>\n            <value>/data/nodemanager/tmp/</value>\n            </property>\n\n    <property>\n            <name>yarn.nodemanager.remote-app-log-dir</name>\n            <value>/hadoop/nodemanager/remote</value>\n    </property>\n\n    <property>\n          <name>yarn.nodemanager.log-dirs</name>\n           <value>/data/hadoop/data/nodemanager/logs</value>\n     </property>\n\n    <property>\n            <name>yarn.nodemanager.log.retain-seconds</name>\n            <value>604800</value>\n    </property>\n\n    <property>\n            <name>yarn.nodemanager.resource.cpu-vcores</name>\n            <value>24</value>\n    </property>\n\n    <property>\n            <name>yarn.nodemanager.resource.memory-mb</name>\n            <value>1024</value>\n    </property>\n\n    <property>\n            <name>yarn.nodemanager.vmem-pmem-ratio</name>\n            <value>2</value>\n    </property>\n\n    <property>\n            <name>yarn.scheduler.minimum-allocation-mb</name>\n            <value>256</value>\n    </property>\n\n    <property>\n            <name>yarn.scheduler.maximum-allocation-mb</name>\n            <value>1024</value>\n    </property>\n\n    <property>\n            <name>yarn.scheduler.minimum-allocation-vcores</name>\n            <value>1</value>\n    </property>\n\n    <property>\n            <name>yarn.scheduler.maximum-allocation-vcores</name>\n            <value>24</value>\n    </property>\n\n    <property>\n            <name>yarn.log-aggregation-enable</name>\n            <value>true</value>\n    </property>\n\n</configuration>\n\n```\n\n\n\n\n#### **6.2.3 配置slaves文件**\n这个配置主要记录数据节点的列表，假如集群有3个数据节点，如：node001，node002，node003\n那么在slave文件里面就可以设置为：  \nnode001  \nnode002  \nnode003  \n\n我这里为两个节点，配置如下\n```shell\nmaster\nnode01\n```\n到此，master的hadoop的配置已经完成，对于其他节点，我们建立好相关的目录，复制过去，稍作配置即可了\n\n需要建立的目录总结\n\n```shell\n[root@node01 /]# mkdir -p /home/hadoop/MyCloudera/APP/hadoop/\n[root@node01 /]# mkdir data\n[root@node01 /]# mkdir -p /data/tmpdata/hadoop/data/tmp\n[root@node01 /]# mkdir -p /data/hadoop/data/name\n[root@node01 /]# mkdir -p /data/hadoop/data/data\n[root@node01 /]# mkdir hadoop\n[root@node01 /]# mkdir -p /hadoop/SecondaryNameNode/  \n[root@node01 /]# chown hadoop:hadoop hadoop/ -R \n[root@node01 /]# chmod 777 hadoop/ -R\n[root@node01 /]# mkdir -p /hadoop/mapreduce/jobhistory/history/done\n[root@node01 /]# mkdir -p /hadoop/mapreduce/jobhistory/history/done_intermediate\n[root@node01 /]# mkdir -p /hadoop/hadoop-yarn/staging  \n[root@node01 /]# mkdir -p /data/nodemanager/tmp/\n[root@node01 /]# mkdir -p /hadoop/nodemanager/remote\n[root@node01 /]# mkdir -p /data/hadoop/data/nodemanager/logs\n[root@node01 /]# chown hadoop:hadoop hadoop/ -R\n[root@node01 /]# chmod 777 hadoop/ -R\n[root@node01 /]# chown hadoop:hadoop data/ -R\n[root@node01 /]# chmod 777 data/ -R\n[root@node01 /]# \n\n```\n\n\n为了确保 hadoop目录 权限没有问题，每台机器在hadoop目录下再次执行一下以下命令\n```shell\nchown -R hadoop:hadoop hadoop\n## 为了保险起见，我给了777的权限， 下面的这一步貌似不做也可以\nchmod 777 hadoop/ -R\n```\n\n### **6.3 启动与验证**\n#### **6.3.1 格式化HDFS文件系统**\n**在master上使用普通用户hadoop进行操作**\n如果第一次启动需要对hadoop平台进行格式化，记得第一次，假如原来有数据就不需要格式化：\n```\nhdfs namenode -format\n```\n如果经过多次format之后，**一定要把/data/hadoop/data/data /data/hadoop/data/name目录下的文件删除**\n\n#### **6.3.2 启动hadoop**\n在启动前关闭集群中所有机器的防火墙，不然会出现datanode开后又自动关闭。\n记得永久的关闭防火墙chkconfig iptables off\n```shell\nchkconfig iptables off\n```\n开始启动,在master的普通用户 hadoop下进行操作\n```shell\nstart-all.sh\n```\n验证hadoop: 输入jps命令，会出现以下进程说明成功\n```shell\n[hadoop@master hadoop]$ jps\n[hadoop@master hadoop]$ jps\n4197 ResourceManager\n3851 DataNode\n4602 Jps\n4013 SecondaryNameNode\n4308 NodeManager\n3739 NameNode\n```  \n#### **6.3.3 测试以下hdfs**  \n创建一个目录\n```shell\n[hadoop@node01 ~]$ hadoop fs -mkdir -p /hive/warehouse\n``` \n传一个文件\n```shell\n[hadoop@master hadoop]$ hadoop fs -put slaves /hive/warehouse\n```\n查看文件\n```shell\n[hadoop@master hadoop]$ hadoop fs -cat /hive/warehouse/slaves\n显示\nmaster\nnode01\n```\n经过上面的测试，说明我们集群安装成功\n### 6.4 网页查看集群\n查看hdfs   \nhttp://192.168.200.128:50070\n显示  \n![hdfs验证](http://i2.muimg.com/567571/7435a4693a8a21b4.png)\n\n验证hadoop   \nhttp://192.168.200.128:8088/cluster/nodes\n显示  \n![hadoop验证](http://i1.piimg.com/567571/80a651b3fc692958.png)\n\n\n## **7. hadoop 集群碰到错误的解决办法**\n这里的错误，一般都分为几大类，一类是某些文件夹没有创建，一类是某些文件或者文件夹权限不够，一类就是配置错误  \n这些错误都可以去logs目录下查看，我的logs目录在  /home/hadoop/MyCloudera/APP/hadoop/hadoop/logs  \n哪里有问题就对应哪个文件去查看错误，例如resourcemanager没起来或者出问题，就去yarn-hadoop-resourcemanager-master.log\n```shell\n-rwxrwxrwx. 1 hadoop hadoop 921348 Apr  3 13:19 hadoop-hadoop-datanode-master.log\n-rw-rw-r--. 1 hadoop hadoop   1434 Apr  3 13:18 hadoop-hadoop-datanode-master.out\n-rw-rw-r--. 1 hadoop hadoop      0 Apr  3 13:18 hadoop-hadoop-datanode-master.out.1\n-rw-rw-r--. 1 hadoop hadoop   1434 Apr  3 12:52 hadoop-hadoop-datanode-master.out.2\n-rwxrwxrwx. 1 hadoop hadoop   1434 Apr  3 12:45 hadoop-hadoop-datanode-master.out.3\n-rwxrwxrwx. 1 hadoop hadoop   1434 Apr  3 12:42 hadoop-hadoop-datanode-master.out.4\n-rwxrwxrwx. 1 hadoop hadoop   1434 Apr  3 12:35 hadoop-hadoop-datanode-master.out.5\n-rwxrwxrwx. 1 hadoop hadoop 371773 Apr  3 13:26 hadoop-hadoop-namenode-master.log\n-rw-rw-r--. 1 hadoop hadoop    717 Apr  3 13:18 hadoop-hadoop-namenode-master.out\n-rw-rw-r--. 1 hadoop hadoop    717 Apr  3 13:09 hadoop-hadoop-namenode-master.out.1\n-rw-rw-r--. 1 hadoop hadoop    717 Apr  3 12:52 hadoop-hadoop-namenode-master.out.2\n-rwxrwxrwx. 1 hadoop hadoop    717 Apr  3 12:44 hadoop-hadoop-namenode-master.out.3\n-rwxrwxrwx. 1 hadoop hadoop    717 Apr  3 12:42 hadoop-hadoop-namenode-master.out.4\n-rwxrwxrwx. 1 hadoop hadoop    717 Apr  3 12:35 hadoop-hadoop-namenode-master.out.5\n-rwxrwxrwx. 1 hadoop hadoop      0 Apr  3 01:43 SecurityAuth-hadoop.audit\n-rwxrwxrwx. 1 hadoop hadoop 618506 Apr  3 13:19 yarn-hadoop-nodemanager-master.log\n-rw-rw-r--. 1 hadoop hadoop   1402 Apr  3 13:19 yarn-hadoop-nodemanager-master.out\n-rw-rw-r--. 1 hadoop hadoop      0 Apr  3 13:19 yarn-hadoop-nodemanager-master.out.1\n-rw-rw-r--. 1 hadoop hadoop   1402 Apr  3 13:09 yarn-hadoop-nodemanager-master.out.2\n-rw-rw-r--. 1 hadoop hadoop   1402 Apr  3 12:52 yarn-hadoop-nodemanager-master.out.3\n-rwxrwxrwx. 1 hadoop hadoop   1402 Apr  3 12:36 yarn-hadoop-nodemanager-master.out.4\n-rwxrwxrwx. 1 hadoop hadoop   1402 Apr  3 12:30 yarn-hadoop-nodemanager-master.out.5\n-rwxrwxrwx. 1 hadoop hadoop 343209 Apr  3 13:19 yarn-hadoop-resourcemanager-master.log\n-rw-rw-r--. 1 hadoop hadoop    701 Apr  3 13:19 yarn-hadoop-resourcemanager-master.out\n-rw-rw-r--. 1 hadoop hadoop    701 Apr  3 13:09 yarn-hadoop-resourcemanager-master.out.1\n-rw-rw-r--. 1 hadoop hadoop    701 Apr  3 12:52 yarn-hadoop-resourcemanager-master.out.2\n-rwxrwxrwx. 1 hadoop hadoop    701 Apr  3 12:45 yarn-hadoop-resourcemanager-master.out.3\n-rwxrwxrwx. 1 hadoop hadoop    701 Apr  3 12:36 yarn-hadoop-resourcemanager-master.out.4\n-rwxrwxrwx. 1 hadoop hadoop    701 Apr  3 12:30 yarn-hadoop-resourcemanager-master.out.5\n```\n\n\n\n\n\n\n\n\n\n\n","slug":"hadoop-cluster-install","published":1,"updated":"2018-01-23T14:19:06.419Z","_id":"cjcrpnzpt001t2wv3lhxm2rvi","comments":1,"layout":"post","photos":[],"link":"","content":"<p>hadoop的配置参看github <a href=\"https://github.com/zhaikaishun/hadoop_cluster\" target=\"_blank\" rel=\"external\">https://github.com/zhaikaishun/hadoop_cluster</a><br>作者: 翟开顺</p>\n<p>关键字:<br>集群环境介绍，Hadoop简介，网络配置，所需软件<br>SSH免密码登陆配置，java环境安装，卸载原有的JDK， 安装jdk17， 配置java环境变量，验证是否成功，Hadoop集群安装，安装Hadoop，验证hadoop<br>hadoop错误分析<br><a id=\"more\"></a></p>\n<h1 id=\"集群环境介绍\"><a href=\"#集群环境介绍\" class=\"headerlink\" title=\"集群环境介绍\"></a><strong>集群环境介绍</strong></h1><h2 id=\"1-Hadoop简介\"><a href=\"#1-Hadoop简介\" class=\"headerlink\" title=\"1.  Hadoop简介\"></a><strong>1.  Hadoop简介</strong></h2><p>　Hadoop是Apache软件基金会旗下的一个开源分布式计算平台。以Hadoop分布式文件系统（HDFS，Hadoop Distributed Filesystem）和MapReduce（Google MapReduce的开源实现）为核心的Hadoop为用户提供了系统底层细节透明的分布式基础架构。<br>　　对于Hadoop的集群来讲，可以分成两大类角色：Master和Salve。一个HDFS集群是由一个NameNode和若干个DataNode组成的。其中NameNode作为主服务器，管理文件系统的命名空间和客户端对文件系统的访问操作；集群中的DataNode管理存储的数据。MapReduce框架是由一个单独运行在主节点上的JobTracker和运行在每个集群从节点的TaskTracker共同组成的。主节点负责调度构成一个作业的所有任务，这些任务分布在不同的从节点上。主节点监控它们的执行情况，并且重新执行之前的失败任务；从节点仅负责由主节点指派的任务。当一个Job被提交时，JobTracker接收到提交作业和配置信息之后，就会将配置信息等分发给从节点，同时调度任务并监控TaskTracker的执行。<br>　　从上面的介绍可以看出，HDFS和MapReduce共同组成了Hadoop分布式系统体系结构的核心。HDFS在集群上实现分布式文件系统，MapReduce在集群上实现了分布式计算和任务处理。HDFS在MapReduce任务处理过程中提供了文件操作和存储等支持，MapReduce在HDFS的基础上实现了任务的分发、跟踪、执行等工作，并收集结果，二者相互作用，完成了Hadoop分布式集群的主要任务。</p>\n<h2 id=\"2-环境说明\"><a href=\"#2-环境说明\" class=\"headerlink\" title=\"2.  环境说明\"></a><strong>2.  环境说明</strong></h2><p>本教程为了简单起见只设置两个节点： master为主节点，node01为数据节点，节点之间局域网连接，相互可以ping通，节点IP分布如下</p>\n<table>\n<thead>\n<tr>\n<th>机器名称</th>\n<th>IP地址</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>master</td>\n<td>192.168.200.128</td>\n</tr>\n<tr>\n<td>node01</td>\n<td>192.168.200.129</td>\n</tr>\n</tbody>\n</table>\n<p>两个节点都是centos6.5系统，都有同一个用户，用户名叫Hadoop  </p>\n<h3 id=\"给hadoop用户赋予root权限\"><a href=\"#给hadoop用户赋予root权限\" class=\"headerlink\" title=\"给hadoop用户赋予root权限\"></a><strong>给hadoop用户赋予root权限</strong></h3><p>切换到root用户 赋予etc/sudoers777权限，然后打开<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@kaishun etc]# chmod 777 /etc/sudoers</div><div class=\"line\">[root@kaishun etc]# vim /etc/sudoers</div><div class=\"line\">```  </div><div class=\"line\">找到Allows people in group wheel to run all commands，把下面%wheel的#给去掉,在Allow root to run any commands anywhere下，加上hadoop  ALL=(ALL)       ALL，然后保存</div><div class=\"line\">```shell</div><div class=\"line\"><span class=\"meta\">#</span># Allow root to run any commands anywhere</div><div class=\"line\">root    ALL=(ALL)       ALL     </div><div class=\"line\">hadoop  ALL=(ALL)       ALL</div><div class=\"line\"></div><div class=\"line\"><span class=\"meta\">#</span># Allows people in group wheel to run all commands</div><div class=\"line\"><span class=\"meta\">%</span>wheel        ALL=(ALL)       ALL</div></pre></td></tr></table></figure></p>\n<p>把sudoers的权限改回来成440<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@kaishun etc]# chmod 440 /etc/sudoers</div><div class=\"line\">```    </div><div class=\"line\">测试是否成功  </div><div class=\"line\">在普通用户下</div><div class=\"line\">```shell</div><div class=\"line\">[hadoop@kaishun ~]$ sudo mkdir test</div><div class=\"line\">输入密码如果可以成功创建文件夹，说明成功</div></pre></td></tr></table></figure></p>\n<h3 id=\"网络配置\"><a href=\"#网络配置\" class=\"headerlink\" title=\"网络配置\"></a><strong>网络配置</strong></h3><p><strong>1. 查看当前机器名</strong><br><strong>在root用户下</strong>输入，显示<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@kaishun hadoop]# hostname</div><div class=\"line\">显示 kaishun， 与我们规划的master不符合</div><div class=\"line\">```  </div><div class=\"line\">**2. 在root用户下修改当前机器名称**</div><div class=\"line\">```shell</div><div class=\"line\">[root@kaishun hadoop]# vim /etc/sysconfig/network</div></pre></td></tr></table></figure></p>\n<p>修改HOSTNAME 为 master<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">HOSTNAME=master</div></pre></td></tr></table></figure></p>\n<p>同理，192.168.200.129这台机器修改成node01<br>修改之后，可能不会立即生效，我是重启后才生效的<br><strong>3. 在root用户下配置hosts文件, 每台机器都需要配置（必须）</strong><br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@master hadoop]# vim /etc/hosts</div></pre></td></tr></table></figure></p>\n<p>添加<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div></pre></td><td class=\"code\"><pre><div class=\"line\">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</div><div class=\"line\">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</div><div class=\"line\">192.168.200.128 master</div><div class=\"line\">192.168.200.129 node01</div><div class=\"line\">```  </div><div class=\"line\">**测试是否成功**，如果能相互使用ping node01   ping master能成功，说明hosts文件配置成功</div><div class=\"line\">```shell</div><div class=\"line\">[hadoop@master ~]$ ping node01</div><div class=\"line\">PING node01 (192.168.200.129) 56(84) bytes of data.</div><div class=\"line\">64 bytes from node01 (192.168.200.129): icmp_seq=1 ttl=64 time=0.391 ms</div><div class=\"line\">64 bytes from node01 (192.168.200.129): icmp_seq=2 ttl=64 time=0.435 ms</div><div class=\"line\">64 bytes from node01 (192.168.200.129): icmp_seq=3 ttl=64 time=0.442 ms</div><div class=\"line\"></div><div class=\"line\">[hadoop@node01 ~]$ ping master</div><div class=\"line\">PING master (192.168.200.128) 56(84) bytes of data.</div><div class=\"line\">64 bytes from master (192.168.200.128): icmp_seq=1 ttl=64 time=0.379 ms</div><div class=\"line\">64 bytes from master (192.168.200.128): icmp_seq=2 ttl=64 time=0.411 ms</div><div class=\"line\">64 bytes from master (192.168.200.128): icmp_seq=3 ttl=64 time=0.460 ms</div></pre></td></tr></table></figure></p>\n<h2 id=\"3-所需软件\"><a href=\"#3-所需软件\" class=\"headerlink\" title=\"3.  所需软件\"></a><strong>3.  所需软件</strong></h2><ol>\n<li>JDK版本1.7  </li>\n<li>hadoop版本hadoop-2.7.1  去官网的华科镜像下载hadoop-2.7.1.tar.gz， <a href=\"http://mirrors.hust.edu.cn/apache/hadoop/common/hadoop-2.7.1/\" target=\"_blank\" rel=\"external\">地址</a>  </li>\n<li>数据传输工具FileZilla， ssh连接工具 secureCRT  </li>\n</ol>\n<h2 id=\"4-SSH免密码登陆配置\"><a href=\"#4-SSH免密码登陆配置\" class=\"headerlink\" title=\"4.  SSH免密码登陆配置\"></a><strong>4.  SSH免密码登陆配置</strong></h2><p>Hadoop运行过程中需要管理远端Hadoop守护进程，在Hadoop启动以后，NameNode是通过SSH（Secure Shell）来启动和停止各个DataNode上的各种守护进程的。这就必须在节点之间执行指令的时候是不需要输入密码的形式，故我们需要配置SSH运用无密码公钥认证的形式，这样NameNode使用SSH无密码登录并启动DataName进程，同样原理，DataNode上也能使用SSH无密码登录到NameNode。<br>安装CentOS6.5时，我们选择了一些基本安装包，所以我们需要两个服务：ssh和rsync已经安装了。可以通过下面命令查看结果显示如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div></pre></td><td class=\"code\"><pre><div class=\"line\">[hadoop@master ~]$ rpm –qa | grep openssh</div><div class=\"line\">[hadoop@master ~]$ rpm –qa | grep rsync</div><div class=\"line\">如果有相应的提示，说明这两个是装好了的，我这里是系统自带的</div><div class=\"line\">```  </div><div class=\"line\">**4.1 配置master无密码登陆所有的node**  </div><div class=\"line\">原理请百度</div><div class=\"line\">在master节点上执行以下命令 然后按几次回车键：</div><div class=\"line\">```shell</div><div class=\"line\">[hadoop@master ~]$ ssh-keygen -t rsa</div><div class=\"line\">```  </div><div class=\"line\">出现下图  </div><div class=\"line\">![ssh免密码登陆](http://i4.buimg.com/567571/b22c79e94cbc2297.png)</div><div class=\"line\"></div><div class=\"line\">我们看到这句话  </div><div class=\"line\">Your identification has been saved in /home/hadoop/.ssh/id_rsa.  </div><div class=\"line\">Your public key has been saved in /home/hadoop/.ssh/id_rsa.pub.  </div><div class=\"line\">说明默认目录在 /home/hadoop/.ssh/ 下</div><div class=\"line\"></div><div class=\"line\">接着在master节点上做如下配置，把id_rsa.pub追加到授权的key里面去。  </div><div class=\"line\">```shell</div><div class=\"line\">[hadoop@master ~]$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</div></pre></td></tr></table></figure></p>\n<p>现在我们进入~/.ssh目录可以看到<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">[hadoop@master ~]$ cd ~/.ssh/</div><div class=\"line\">[hadoop@master .ssh]$ ll</div><div class=\"line\">total 12</div><div class=\"line\">-rw-rw-r--. 1 hadoop hadoop  395 Apr  2 16:22 authorized_keys</div><div class=\"line\">-rw-------. 1 hadoop hadoop 1675 Apr  2 16:17 id_rsa</div><div class=\"line\">-rw-r--r--. 1 hadoop hadoop  395 Apr  2 16:17 id_rsa.pub</div></pre></td></tr></table></figure></p>\n<p><strong>4.1.1. 修改文件”authorized_keys权限</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">[hadoop@master .ssh]$ chmod 600 ~/.ssh/authorized_keys</div></pre></td></tr></table></figure></p>\n<p><img src=\"http://i2.muimg.com/567571/41ecd0b8634028ae.png\" alt=\"authorized_keys权限\"></p>\n<p><strong>4.1.2. 设置SSH配置</strong><br>用root用户登录服务器修改SSH配置文件”/etc/ssh/sshd_config”的下列内容。这里找到这些内容，把前面的#去掉即可<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@master .ssh]# vim /etc/ssh/sshd_config</div><div class=\"line\">RSAAuthentication yes # 启用 RSA 认证</div><div class=\"line\">PubkeyAuthentication yes # 启用公钥私钥配对认证方式</div><div class=\"line\">AuthorizedKeysFile .ssh/authorized_keys # 公钥文件路径（和上面生成的文件同）</div></pre></td></tr></table></figure></p>\n<p><strong>重启SSH服务</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@master .ssh]# /etc/rc.d/init.d/sshd restart</div><div class=\"line\">Stopping sshd:                                             [  OK  ]</div><div class=\"line\">Starting sshd:                                             [  OK  ]</div><div class=\"line\">[root@master .ssh]#</div></pre></td></tr></table></figure></p>\n<p>退出root用户，使用hadoop普通用户验证是否成功, ssh localhost, 如果不需要输入密码，那么验证成功<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">[hadoop@master .ssh]$ ssh localhost</div><div class=\"line\">The authenticity of host 'localhost (::1)' can't be established.</div><div class=\"line\">RSA key fingerprint is 48:0b:ee:9b:67:85:4c:19:35:10:d1:1d:e1:5d:fa:c4.</div><div class=\"line\">Are you sure you want to continue connecting (yes/no)? yes</div><div class=\"line\">Warning: Permanently added 'localhost' (RSA) to the list of known hosts.</div><div class=\"line\">Last login: Sun Apr  2 16:09:39 2017 from 192.168.200.1</div></pre></td></tr></table></figure></p>\n<p><strong>4.1.3. 把公钥复制到所有的node机器上</strong><br>从上图中得知无密码登录本级已经设置完毕，接下来的事儿是把公钥复制所有的node机器上。使用下面的命令格式进行复制公钥<br>scp ~/.ssh/id_rsa.pub 远程用户名@远程服务器IP:~/<br>我本地这样使用 scp ~/.ssh/id_rsa.pub hadoop@192.168.200.129:~/  ,然后根据提示输入需要复制的远程服务器的密码，最后出现下面的提示说明复制成功<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">[hadoop@master ~]$ scp ~/.ssh/id_rsa.pub hadoop@192.168.200.129:~/</div><div class=\"line\">hadoop@192.168.200.129's password:  # 这里输入远程密码</div><div class=\"line\">id_rsa.pub                                                                                                                        100%  395     0.4KB/s   00:00    </div><div class=\"line\">[hadoop@master ~]$ </div><div class=\"line\">```  </div><div class=\"line\">**4.1.4. 对节点机器进行配置**</div><div class=\"line\">下面就针对IP为\"192.168.200.129\"的node01的节点进行配置。</div><div class=\"line\">4.1  ll -a查看是否有.ssh目录，如果没有，我们需要创建一个.ssh目录，并且赋予这个权限 drwx------.  2 hadoop hadoop 4096 Apr  2 16:40 .ssh  具体权限参照master的机器， centos6.5一般都是默认带有.ssh目录的</div></pre></td></tr></table></figure></p>\n<p>[hadoop@node01 ~]$ ll -a<br>drwx——.  2 hadoop hadoop 4096 Apr  2 16:40 .ssh<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">如果有这个目录了，我们把刚才的文件追加到authorized_keys 中去，然后修改authorized_keys文件权限</div></pre></td></tr></table></figure></p>\n<p>[hadoop@node01 ~]$ cat ~/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys<br>[hadoop@node01 .ssh]$ chmod 600 ~/.ssh/authorized_keys</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">进入到ssh 目录，ll 看到如下所示说明成功，注意权限是否正确</div><div class=\"line\">```shell</div><div class=\"line\">[hadoop@node01 .ssh]$ ll</div><div class=\"line\">total 8</div><div class=\"line\">-rw-------. 1 hadoop hadoop 395 Apr  2 16:52 authorized_keys</div><div class=\"line\">-rw-r--r--. 1 hadoop hadoop 391 Apr  2 16:40 known_hosts</div></pre></td></tr></table></figure>\n<p><strong>4.2  用root用户修改/etc/ssh/sshd_config</strong><br>参考前面的master的修改/etc/ssh/sshd_config的方法<br>设置SSH配置<br>用root用户登录服务器修改SSH配置文件”/etc/ssh/sshd_config”的下列内容。这里找到这些内容，把前面的#去掉即可<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@master .ssh]# vim /etc/ssh/sshd_config</div><div class=\"line\">RSAAuthentication yes # 启用 RSA 认证</div><div class=\"line\">PubkeyAuthentication yes # 启用公钥私钥配对认证方式</div><div class=\"line\">AuthorizedKeysFile .ssh/authorized_keys # 公钥文件路径（和上面生成的文件同）</div></pre></td></tr></table></figure></p>\n<p><strong>重启SSH服务</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@master .ssh]# /etc/rc.d/init.d/sshd restart</div><div class=\"line\">Stopping sshd:                                             [  OK  ]</div><div class=\"line\">Starting sshd:                                             [  OK  ]</div><div class=\"line\">[root@master .ssh]#</div></pre></td></tr></table></figure></p>\n<p>最后记得把”/home/hadoop/“目录下的”id_rsa.pub”文件删除掉 </p>\n<p>到此为止，我们经过的步骤已经实现了从”master”到”node01”SSH无密码登录</p>\n<p>验证master到node01的无密码登陆,在master机器上，使用hadoop用户 ssh node01或者ssh 192.168.200.129, 下面是成功的的结果<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">[hadoop@master ~]$ ssh node01</div><div class=\"line\">Last login: Sun Apr  2 17:25:50 2017 from localhost</div><div class=\"line\">[hadoop@node01 ~]$ </div><div class=\"line\"></div><div class=\"line\">[hadoop@node01 ~]$ ssh master</div><div class=\"line\">Last login: Sun Apr  2 17:26:04 2017 from node01</div><div class=\"line\">[hadoop@master ~]$</div></pre></td></tr></table></figure></p>\n<h2 id=\"5-java安装环境\"><a href=\"#5-java安装环境\" class=\"headerlink\" title=\"5 java安装环境\"></a><strong>5 java安装环境</strong></h2><h3 id=\"5-1-卸载原有的JDK\"><a href=\"#5-1-卸载原有的JDK\" class=\"headerlink\" title=\"5.1 卸载原有的JDK\"></a><strong>5.1 卸载原有的JDK</strong></h3><p>因为有的系统自带有JDK, 安装前先卸载<br>查看所装的JDK<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">[hadoop@master ~]$ rpm -qa | grep jdk</div><div class=\"line\">出现</div><div class=\"line\">java-1.7.0-openjdk-1.7.0.45-2.4.3.3.el6.x86_64</div><div class=\"line\">java-1.6.0-openjdk-1.6.0.0-1.66.1.13.0.el6.x86_64</div></pre></td></tr></table></figure></p>\n<p>root下卸载前面查出的这两个<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@master hadoop]#  yum -y remove java-1.7.0-openjdk-1.7.0.45-2.4.3.3.el6.x86_64  </div><div class=\"line\">[root@master hadoop]#  yum -y remove java-1.6.0-openjdk-1.6.0.0-1.66.1.13.0.el6.x86_64  </div><div class=\"line\">成功后会出现一个complete</div><div class=\"line\"></div><div class=\"line\">```  </div><div class=\"line\"><span class=\"meta\">#</span>## **5.2 安装jdk1.7**</div><div class=\"line\">首先用root身份登录master后在/usr/local下创建java文件夹</div><div class=\"line\">```shell</div><div class=\"line\">[root@master hadoop]# mkdir -p /usr/local/java</div><div class=\"line\">```  </div><div class=\"line\">我们把FTP传来的jdk-7u79-linux-x64.tar.gz复制到/usr/local/java 文件夹下</div><div class=\"line\">```shell</div><div class=\"line\">[root@master Downloads]# cp jdk-7u79-linux-x64.tar.gz /usr/local/java/</div></pre></td></tr></table></figure></p>\n<p>解压并且<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@master java]# tar zxvf jdk-7u79-linux-x64.tar.gz </div><div class=\"line\">解压完成后出现</div><div class=\"line\">[root@master java]# ll</div><div class=\"line\">total 149920</div><div class=\"line\">drwxr-xr-x. 8 uucp  143      4096 Apr 11  2015 jdk1.7.0_79</div><div class=\"line\">-rw-r--r--. 1 root root 153512879 Apr  2 18:17 jdk-7u79-linux-x64.tar.gz</div></pre></td></tr></table></figure></p>\n<p>给所有者权限<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@master java]# chown hadoop:hadoop jdk1.7.0_79/ -R</div></pre></td></tr></table></figure></p>\n<h3 id=\"5-3-配置java环境变量\"><a href=\"#5-3-配置java环境变量\" class=\"headerlink\" title=\"5.3 配置java环境变量\"></a><strong>5.3 配置java环境变量</strong></h3><p>编辑”/etc/profile”文件<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@master java]#  vim /etc/profile</div></pre></td></tr></table></figure></p>\n<p>在尾部加入<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">#</span> set java environment</div><div class=\"line\">export JAVA_HOME=/usr/local/java/jdk1.7.0_79</div><div class=\"line\">export JRE_HOME=/usr/local/java/jdk1.7.0_79/jre</div><div class=\"line\">export PATH=$PATH:/usr/local/java/jdk1.7.0_79/bin</div><div class=\"line\">export CLASSPATH=./:/usr/local/java/jdk1.7.0_79/lib:/usr/local/java/jdk1.7.0_79/jre/lib</div></pre></td></tr></table></figure></p>\n<p>使配置生效<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@master java]# source /etc/profile</div></pre></td></tr></table></figure></p>\n<h3 id=\"5-4-验证是否成功\"><a href=\"#5-4-验证是否成功\" class=\"headerlink\" title=\"5.4 验证是否成功\"></a><strong>5.4 验证是否成功</strong></h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">java -version  出现 java version &quot;1.7.0_79&quot;</div><div class=\"line\">javac  有提示</div><div class=\"line\">java  有提示</div></pre></td></tr></table></figure>\n<p>确保是按照我上面的步骤，权限不能有错，否则可能会有问题, 同样，在另外的节点上也安装好jdk</p>\n<h2 id=\"6-Hadoop集群安装\"><a href=\"#6-Hadoop集群安装\" class=\"headerlink\" title=\"6. Hadoop集群安装\"></a><strong>6. Hadoop集群安装</strong></h2><p>所有的机器上都要安装hadoop，现在就先在Master服务器安装，然后其他服务器按照步骤重复进行即可。<strong>安装和配置hadoop需要以”root”的身份进行。</strong></p>\n<h3 id=\"6-1-安装Hadoop\"><a href=\"#6-1-安装Hadoop\" class=\"headerlink\" title=\"6.1 安装Hadoop\"></a><strong>6.1 安装Hadoop</strong></h3><h4 id=\"6-1-1-建立一个目录，用来存放hadoop\"><a href=\"#6-1-1-建立一个目录，用来存放hadoop\" class=\"headerlink\" title=\"6.1.1 建立一个目录，用来存放hadoop\"></a><strong>6.1.1 建立一个目录，用来存放hadoop</strong></h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@master Downloads]#  mkdir -p /home/hadoop/MyCloudera/APP/hadoop/</div></pre></td></tr></table></figure>\n<h4 id=\"6-1-2-把下载好得hadoop-2-7-1-tar-gz-复制到这个目录下，解压并且命名为hadoop\"><a href=\"#6-1-2-把下载好得hadoop-2-7-1-tar-gz-复制到这个目录下，解压并且命名为hadoop\" class=\"headerlink\" title=\"6.1.2 把下载好得hadoop-2.7.1.tar.gz 复制到这个目录下，解压并且命名为hadoop\"></a><strong>6.1.2 把下载好得hadoop-2.7.1.tar.gz 复制到这个目录下，解压并且命名为hadoop</strong></h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">复制到我们建立得目录</div><div class=\"line\">[root@master Downloads]# cp hadoop-2.7.1.tar.gz  /home/hadoop/MyCloudera/APP/hadoop/</div><div class=\"line\">进入到我们复制得目录</div><div class=\"line\">[root@master Downloads]# cd /home/hadoop/MyCloudera/APP/hadoop  </div><div class=\"line\">对此tar.gz解压</div><div class=\"line\">[root@master hadoop]# tar zxvf hadoop-2.7.1.tar.gz </div><div class=\"line\">改名为hadoop</div><div class=\"line\">[root@master hadoop]# mv hadoop-2.7.1 hadoop</div></pre></td></tr></table></figure>\n<h4 id=\"6-1-3-将文件夹得读写权限赋予给hadoop用户\"><a href=\"#6-1-3-将文件夹得读写权限赋予给hadoop用户\" class=\"headerlink\" title=\"6.1.3 将文件夹得读写权限赋予给hadoop用户\"></a><strong>6.1.3 将文件夹得读写权限赋予给hadoop用户</strong></h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@master APP]# chown -R hadoop:hadoop hadoop  </div><div class=\"line\">ll 查看权限，是这样得</div><div class=\"line\">[root@master APP]# ll</div><div class=\"line\">total 4</div><div class=\"line\">drwxr-xr-x. 3 hadoop hadoop 4096 Apr  2 23:29 hadoop</div></pre></td></tr></table></figure>\n<h4 id=\"6-1-4-配置-etc-profile\"><a href=\"#6-1-4-配置-etc-profile\" class=\"headerlink\" title=\"6.1.4 配置/etc/profile\"></a><strong>6.1.4 配置/etc/profile</strong></h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@master APP]# vim /etc/profile</div></pre></td></tr></table></figure>\n<p>在末尾加上如下配置，其中HADOOP_HOME填写前面得hadoop存放得位置<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"># set hadoop path</div><div class=\"line\">export HADOOP_HOME=/home/hadoop/MyCloudera/APP/hadoop/hadoop </div><div class=\"line\">export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin</div><div class=\"line\">export HADOOP_MAPRED_HOME=$HADOOP_HOME</div><div class=\"line\">export HADOOP_COMMON_HOME=$HADOOP_HOME</div><div class=\"line\">export HADOOP_HDFS_HOME=$HADOOP_HOME</div><div class=\"line\">export HADOOP_YARN_HOME=$HADOOP_HOME</div><div class=\"line\">export HADOOP_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoop</div><div class=\"line\">export YARN_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoop</div></pre></td></tr></table></figure></p>\n<p>让配置生效<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@master APP]# source /etc/profile</div></pre></td></tr></table></figure></p>\n<h3 id=\"6-2-配置hadoop\"><a href=\"#6-2-配置hadoop\" class=\"headerlink\" title=\"6.2 配置hadoop\"></a><strong>6.2 配置hadoop</strong></h3><p>Hadoop配置文件在conf目录下，之前的版本的配置文件主要是Hadoop-default.xml和Hadoop-site.xml。由于Hadoop发展迅速，代码量急剧增加，代码开发分为了core，hdfs和map/reduce三部分，配置文件也被分成了三个core-site.xml、hdfs-site.xml、mapred-site.xml。core-site.xml和hdfs-site.xml是站在HDFS角度上配置文件；core-site.xml和mapred-site.xml是站在MapReduce角度上配置文件。</p>\n<h4 id=\"6-2-1-配置hadoop-env-sh\"><a href=\"#6-2-1-配置hadoop-env-sh\" class=\"headerlink\" title=\"6.2.1 配置hadoop-env.sh\"></a><strong>6.2.1 配置hadoop-env.sh</strong></h4><p>该hadoop-env.sh文件位于/home/hadoop/MyCloudera/APP/hadoop/hadoop/etc/hadoop目录下<br>在文件的末尾添加下面内容<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">#</span> The java environment</div><div class=\"line\">export JAVA_HOME=/usr/local/java/jdk1.7.0_79</div></pre></td></tr></table></figure></p>\n<h4 id=\"6-2-2-配置core-site-xml文件\"><a href=\"#6-2-2-配置core-site-xml文件\" class=\"headerlink\" title=\"6.2.2 配置core-site.xml文件\"></a><strong>6.2.2 配置core-site.xml文件</strong></h4><p>我们先在本地建立几个目录，用来存放一些hadoop得文件<br>在根目录下，建立一个data<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">根目录路径</div><div class=\"line\">[root@master /]# pwd</div><div class=\"line\">/</div><div class=\"line\">创建一个data目录</div><div class=\"line\">[root@master /]# mkdir data</div><div class=\"line\">创建/data/tmpdata/hadoop/data/tmp目录</div><div class=\"line\">[root@master /]# mkdir -p /data/tmpdata/hadoop/data/tmp</div></pre></td></tr></table></figure></p>\n<p>然后对core-site.xml做如下配置, 具体得hadoop.tmp.dir和fs.default.name得功能参看百度google<br><figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">configuration</span>&gt;</span></div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>hadoop.tmp.dir<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>/data/tmpdata/hadoop/data/tmp<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>fs.default.name<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>hdfs://master:9000<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">configuration</span>&gt;</span></div></pre></td></tr></table></figure></p>\n<h4 id=\"6-2-3-配置hdfs-site-xml文件\"><a href=\"#6-2-3-配置hdfs-site-xml文件\" class=\"headerlink\" title=\"6.2.3 配置hdfs-site.xml文件\"></a><strong>6.2.3 配置hdfs-site.xml文件</strong></h4><p>我这里配置的比较完整，如果想简单点，有的其实可以默认设置，具体参看其他文章<br><strong>1. 创建namenode和datanode的存放目录,然后对/data目录赋予权限。 注意权限不能有错</strong><br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@master /]# mkdir -p /data/hadoop/data/name</div><div class=\"line\">[root@master data]# mkdir -p /data/hadoop/data/data</div><div class=\"line\"></div><div class=\"line\">[root@master /]#  chown hadoop:hadoop data/ -R</div><div class=\"line\">[root@master /]# chmod 777 data/ -R</div></pre></td></tr></table></figure></p>\n<p><strong>2. 创建SecondaryNameNode的目录</strong><br>在根目录下创建hadoop目录，然后创建/hadoop/SecondaryNameNode/目录，最后赋予hadoop目录权限<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@master /]# mkdir hadoop  </div><div class=\"line\">[root@master /]# mkdir -p /hadoop/SecondaryNameNode/  </div><div class=\"line\"></div><div class=\"line\">[root@master /]# chown hadoop:hadoop hadoop/ -R </div><div class=\"line\">[root@master /]#  chmod 777 hadoop/ -R</div></pre></td></tr></table></figure></p>\n<p><strong>hdfs-site.xml配置</strong><br><figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">configuration</span>&gt;</span></div><div class=\"line\">     <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.namenode.name.dir<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>/data/hadoop/data/name/<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">     <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">     <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.datanode.data.dir<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>/data/hadoop/data/data/<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">     <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">     <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">           <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.replication<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">           <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>2<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">     <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">     <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.namenode.checkpoint.dir<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>/hadoop/SecondaryNameNode/<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">     <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">    </div><div class=\"line\">     <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.http.address<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>master:50070<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">     <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">    </div><div class=\"line\">     <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.secondary.http.address<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>master:50090<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">     <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.datanode.du.reserved<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>0<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span> 每个卷预留的空闲空间数量 <span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">    </div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.datanode.max.xcievers<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>32768<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">    </div><div class=\"line\">    </div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.datanode.socket.write.timeout<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>0<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.socket.timeout<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>180000<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>socket通讯超时时间<span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">configuration</span>&gt;</span></div></pre></td></tr></table></figure></p>\n<h4 id=\"6-2-3-配置mapred-site-xml文件\"><a href=\"#6-2-3-配置mapred-site-xml文件\" class=\"headerlink\" title=\"6.2.3 配置mapred-site.xml文件\"></a><strong>6.2.3 配置mapred-site.xml文件</strong></h4><p>我这里配置的比较完整，网上大多数都是用的默认，具体其中的一些参数可以百度<br>这里先建立几个文件<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@master /]# mkdir -p /hadoop/mapreduce/jobhistory/history/done</div><div class=\"line\">[root@master /]# mkdir -p /hadoop/mapreduce/jobhistory/history/done_intermediate</div><div class=\"line\">[root@master /]# mkdir -p /hadoop/hadoop-yarn/staging  </div><div class=\"line\"></div><div class=\"line\">赋予权限</div><div class=\"line\">[root@master /]# chown hadoop:hadoop hadoop/ -R </div><div class=\"line\">[root@master /]#  chmod 777 hadoop/ -R</div></pre></td></tr></table></figure></p>\n<p>复制一份 mapred-site.xml<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@master hadoop]# cp mapred-site.xml.template mapred-site.xml</div></pre></td></tr></table></figure></p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">configuration</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapreduce.framework.name<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>yarn<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapreduce.jobtracker.address<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>master:9001<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapreduce.jobtracker.http.address<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>master:50030<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapreduce.jobhistory.address<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>master:10020<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>master:19888<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapreduce.jobhistory.done-dir<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>/hadoop/mapreduce/jobhistory/history/done<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapreduce.jobhistory.intermediate-done-dir<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>/hadoop/mapreduce/jobhistory/history/done_intermediate<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.app.mapreduce.am.staging-dir<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>/hadoop/hadoop-yarn/staging<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapred.hosts.exclude<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>/home/hadoop/MyCloudera/APP/hadoop/hadoop/etc/hadoop/excludes<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">final</span>&gt;</span>true<span class=\"tag\">&lt;/<span class=\"name\">final</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapreduce.tasktracker.map.tasks.maximum<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>32<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span> 同一时间允许运行的最大map任务数 <span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapreduce.tasktracker.reduce.tasks.maximum<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>16<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span> 同一时间允许运行的最大reduce任务数 <span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>1000<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapreduce.map.memory.mb<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>512<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>map阶段申请的container的内存的大小<span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapreduce.reduce.memory.mb<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>512<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>reduce阶段申请的container的内存的大小<span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapreduce.map.java.opts<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>-Xmx512M<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>用户设定的map/reduce阶段申请的container的JVM参数。最大堆设定要比申请的内存少一些，用于JVM的非堆部分使用。 <span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapreduce.reduce.java.opts<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>-Xmx1024M<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapreduce.task.io.sort.mb<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>1024<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapreduce.reduce.shuffle.parallelcopies<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>16<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">configuration</span>&gt;</span></div></pre></td></tr></table></figure>\n<h4 id=\"6-2-3-配置yarn-site-xml文件\"><a href=\"#6-2-3-配置yarn-site-xml文件\" class=\"headerlink\" title=\"6.2.3 配置yarn-site.xml文件\"></a><strong>6.2.3 配置yarn-site.xml文件</strong></h4><p>创建一些文件夹，并且赋予权限<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@master /]# clear</div><div class=\"line\">[root@master /]# mkdir -p /data/nodemanager/tmp/</div><div class=\"line\">[root@master /]# mkdir -p /hadoop/nodemanager/remote</div><div class=\"line\">[root@master /]# mkdir -p /data/hadoop/data/nodemanager/logs</div><div class=\"line\">[root@master /]# chown hadoop:hadoop hadoop/ -R</div><div class=\"line\">[root@master /]# chmod 777 hadoop/ -R</div><div class=\"line\">[root@master /]# chown hadoop:hadoop data/ -R</div><div class=\"line\">[root@master /]# chmod 777 data/ -R</div></pre></td></tr></table></figure></p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div><div class=\"line\">106</div><div class=\"line\">107</div><div class=\"line\">108</div><div class=\"line\">109</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">configuration</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">&lt;!-- Site specific YARN configuration properties --&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">         <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.aux-services<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>mapreduce_shuffle<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.aux-services.mapreduce.shuffle.class<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>org.apache.hadoop.mapred.ShuffleHandler<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">     <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.resourcemanager.scheduler.address<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>master:8030<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">     <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.resourcemanager.resource-tracker.address<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>master:8031<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.resourcemanager.address<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>master:8032<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">     <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.resourcemanager.admin.address<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">             <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>master:8033<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.address<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>master:9999<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.webapp.address<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>master:8042<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">    </div><div class=\"line\">     <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.resourcemanager.webapp.address<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>master:8088<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.local-dirs<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>/data/nodemanager/tmp/<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.remote-app-log-dir<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>/hadoop/nodemanager/remote<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.log-dirs<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">           <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>/data/hadoop/data/nodemanager/logs<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">     <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.log.retain-seconds<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>604800<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.resource.cpu-vcores<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>24<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>1024<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.vmem-pmem-ratio<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>2<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.scheduler.minimum-allocation-mb<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>256<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.scheduler.maximum-allocation-mb<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>1024<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.scheduler.minimum-allocation-vcores<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>1<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.scheduler.maximum-allocation-vcores<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>24<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.log-aggregation-enable<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>true<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">configuration</span>&gt;</span></div></pre></td></tr></table></figure>\n<h4 id=\"6-2-3-配置slaves文件\"><a href=\"#6-2-3-配置slaves文件\" class=\"headerlink\" title=\"6.2.3 配置slaves文件\"></a><strong>6.2.3 配置slaves文件</strong></h4><p>这个配置主要记录数据节点的列表，假如集群有3个数据节点，如：node001，node002，node003<br>那么在slave文件里面就可以设置为：<br>node001<br>node002<br>node003  </p>\n<p>我这里为两个节点，配置如下<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">master</div><div class=\"line\">node01</div></pre></td></tr></table></figure></p>\n<p>到此，master的hadoop的配置已经完成，对于其他节点，我们建立好相关的目录，复制过去，稍作配置即可了</p>\n<p>需要建立的目录总结</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@node01 /]# mkdir -p /home/hadoop/MyCloudera/APP/hadoop/</div><div class=\"line\">[root@node01 /]# mkdir data</div><div class=\"line\">[root@node01 /]# mkdir -p /data/tmpdata/hadoop/data/tmp</div><div class=\"line\">[root@node01 /]# mkdir -p /data/hadoop/data/name</div><div class=\"line\">[root@node01 /]# mkdir -p /data/hadoop/data/data</div><div class=\"line\">[root@node01 /]# mkdir hadoop</div><div class=\"line\">[root@node01 /]# mkdir -p /hadoop/SecondaryNameNode/  </div><div class=\"line\">[root@node01 /]# chown hadoop:hadoop hadoop/ -R </div><div class=\"line\">[root@node01 /]# chmod 777 hadoop/ -R</div><div class=\"line\">[root@node01 /]# mkdir -p /hadoop/mapreduce/jobhistory/history/done</div><div class=\"line\">[root@node01 /]# mkdir -p /hadoop/mapreduce/jobhistory/history/done_intermediate</div><div class=\"line\">[root@node01 /]# mkdir -p /hadoop/hadoop-yarn/staging  </div><div class=\"line\">[root@node01 /]# mkdir -p /data/nodemanager/tmp/</div><div class=\"line\">[root@node01 /]# mkdir -p /hadoop/nodemanager/remote</div><div class=\"line\">[root@node01 /]# mkdir -p /data/hadoop/data/nodemanager/logs</div><div class=\"line\">[root@node01 /]# chown hadoop:hadoop hadoop/ -R</div><div class=\"line\">[root@node01 /]# chmod 777 hadoop/ -R</div><div class=\"line\">[root@node01 /]# chown hadoop:hadoop data/ -R</div><div class=\"line\">[root@node01 /]# chmod 777 data/ -R</div><div class=\"line\">[root@node01 /]#</div></pre></td></tr></table></figure>\n<p>为了确保 hadoop目录 权限没有问题，每台机器在hadoop目录下再次执行一下以下命令<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">chown -R hadoop:hadoop hadoop</div><div class=\"line\"><span class=\"meta\">#</span># 为了保险起见，我给了777的权限， 下面的这一步貌似不做也可以</div><div class=\"line\">chmod 777 hadoop/ -R</div></pre></td></tr></table></figure></p>\n<h3 id=\"6-3-启动与验证\"><a href=\"#6-3-启动与验证\" class=\"headerlink\" title=\"6.3 启动与验证\"></a><strong>6.3 启动与验证</strong></h3><h4 id=\"6-3-1-格式化HDFS文件系统\"><a href=\"#6-3-1-格式化HDFS文件系统\" class=\"headerlink\" title=\"6.3.1 格式化HDFS文件系统\"></a><strong>6.3.1 格式化HDFS文件系统</strong></h4><p><strong>在master上使用普通用户hadoop进行操作</strong><br>如果第一次启动需要对hadoop平台进行格式化，记得第一次，假如原来有数据就不需要格式化：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs namenode -format</div></pre></td></tr></table></figure></p>\n<p>如果经过多次format之后，<strong>一定要把/data/hadoop/data/data /data/hadoop/data/name目录下的文件删除</strong></p>\n<h4 id=\"6-3-2-启动hadoop\"><a href=\"#6-3-2-启动hadoop\" class=\"headerlink\" title=\"6.3.2 启动hadoop\"></a><strong>6.3.2 启动hadoop</strong></h4><p>在启动前关闭集群中所有机器的防火墙，不然会出现datanode开后又自动关闭。<br>记得永久的关闭防火墙chkconfig iptables off<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">chkconfig iptables off</div></pre></td></tr></table></figure></p>\n<p>开始启动,在master的普通用户 hadoop下进行操作<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">start-all.sh</div></pre></td></tr></table></figure></p>\n<p>验证hadoop: 输入jps命令，会出现以下进程说明成功<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\">[hadoop@master hadoop]$ jps</div><div class=\"line\">[hadoop@master hadoop]$ jps</div><div class=\"line\">4197 ResourceManager</div><div class=\"line\">3851 DataNode</div><div class=\"line\">4602 Jps</div><div class=\"line\">4013 SecondaryNameNode</div><div class=\"line\">4308 NodeManager</div><div class=\"line\">3739 NameNode</div><div class=\"line\">```  </div><div class=\"line\"><span class=\"meta\">#</span>### **6.3.3 测试以下hdfs**  </div><div class=\"line\">创建一个目录</div><div class=\"line\">```shell</div><div class=\"line\">[hadoop@node01 ~]$ hadoop fs -mkdir -p /hive/warehouse</div><div class=\"line\">``` </div><div class=\"line\">传一个文件</div><div class=\"line\">```shell</div><div class=\"line\">[hadoop@master hadoop]$ hadoop fs -put slaves /hive/warehouse</div></pre></td></tr></table></figure></p>\n<p>查看文件<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">[hadoop@master hadoop]$ hadoop fs -cat /hive/warehouse/slaves</div><div class=\"line\">显示</div><div class=\"line\">master</div><div class=\"line\">node01</div></pre></td></tr></table></figure></p>\n<p>经过上面的测试，说明我们集群安装成功</p>\n<h3 id=\"6-4-网页查看集群\"><a href=\"#6-4-网页查看集群\" class=\"headerlink\" title=\"6.4 网页查看集群\"></a>6.4 网页查看集群</h3><p>查看hdfs<br><a href=\"http://192.168.200.128:50070\" target=\"_blank\" rel=\"external\">http://192.168.200.128:50070</a><br>显示<br><img src=\"http://i2.muimg.com/567571/7435a4693a8a21b4.png\" alt=\"hdfs验证\"></p>\n<p>验证hadoop<br><a href=\"http://192.168.200.128:8088/cluster/nodes\" target=\"_blank\" rel=\"external\">http://192.168.200.128:8088/cluster/nodes</a><br>显示<br><img src=\"http://i1.piimg.com/567571/80a651b3fc692958.png\" alt=\"hadoop验证\"></p>\n<h2 id=\"7-hadoop-集群碰到错误的解决办法\"><a href=\"#7-hadoop-集群碰到错误的解决办法\" class=\"headerlink\" title=\"7. hadoop 集群碰到错误的解决办法\"></a><strong>7. hadoop 集群碰到错误的解决办法</strong></h2><p>这里的错误，一般都分为几大类，一类是某些文件夹没有创建，一类是某些文件或者文件夹权限不够，一类就是配置错误<br>这些错误都可以去logs目录下查看，我的logs目录在  /home/hadoop/MyCloudera/APP/hadoop/hadoop/logs<br>哪里有问题就对应哪个文件去查看错误，例如resourcemanager没起来或者出问题，就去yarn-hadoop-resourcemanager-master.log<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div></pre></td><td class=\"code\"><pre><div class=\"line\">-rwxrwxrwx. 1 hadoop hadoop 921348 Apr  3 13:19 hadoop-hadoop-datanode-master.log</div><div class=\"line\">-rw-rw-r--. 1 hadoop hadoop   1434 Apr  3 13:18 hadoop-hadoop-datanode-master.out</div><div class=\"line\">-rw-rw-r--. 1 hadoop hadoop      0 Apr  3 13:18 hadoop-hadoop-datanode-master.out.1</div><div class=\"line\">-rw-rw-r--. 1 hadoop hadoop   1434 Apr  3 12:52 hadoop-hadoop-datanode-master.out.2</div><div class=\"line\">-rwxrwxrwx. 1 hadoop hadoop   1434 Apr  3 12:45 hadoop-hadoop-datanode-master.out.3</div><div class=\"line\">-rwxrwxrwx. 1 hadoop hadoop   1434 Apr  3 12:42 hadoop-hadoop-datanode-master.out.4</div><div class=\"line\">-rwxrwxrwx. 1 hadoop hadoop   1434 Apr  3 12:35 hadoop-hadoop-datanode-master.out.5</div><div class=\"line\">-rwxrwxrwx. 1 hadoop hadoop 371773 Apr  3 13:26 hadoop-hadoop-namenode-master.log</div><div class=\"line\">-rw-rw-r--. 1 hadoop hadoop    717 Apr  3 13:18 hadoop-hadoop-namenode-master.out</div><div class=\"line\">-rw-rw-r--. 1 hadoop hadoop    717 Apr  3 13:09 hadoop-hadoop-namenode-master.out.1</div><div class=\"line\">-rw-rw-r--. 1 hadoop hadoop    717 Apr  3 12:52 hadoop-hadoop-namenode-master.out.2</div><div class=\"line\">-rwxrwxrwx. 1 hadoop hadoop    717 Apr  3 12:44 hadoop-hadoop-namenode-master.out.3</div><div class=\"line\">-rwxrwxrwx. 1 hadoop hadoop    717 Apr  3 12:42 hadoop-hadoop-namenode-master.out.4</div><div class=\"line\">-rwxrwxrwx. 1 hadoop hadoop    717 Apr  3 12:35 hadoop-hadoop-namenode-master.out.5</div><div class=\"line\">-rwxrwxrwx. 1 hadoop hadoop      0 Apr  3 01:43 SecurityAuth-hadoop.audit</div><div class=\"line\">-rwxrwxrwx. 1 hadoop hadoop 618506 Apr  3 13:19 yarn-hadoop-nodemanager-master.log</div><div class=\"line\">-rw-rw-r--. 1 hadoop hadoop   1402 Apr  3 13:19 yarn-hadoop-nodemanager-master.out</div><div class=\"line\">-rw-rw-r--. 1 hadoop hadoop      0 Apr  3 13:19 yarn-hadoop-nodemanager-master.out.1</div><div class=\"line\">-rw-rw-r--. 1 hadoop hadoop   1402 Apr  3 13:09 yarn-hadoop-nodemanager-master.out.2</div><div class=\"line\">-rw-rw-r--. 1 hadoop hadoop   1402 Apr  3 12:52 yarn-hadoop-nodemanager-master.out.3</div><div class=\"line\">-rwxrwxrwx. 1 hadoop hadoop   1402 Apr  3 12:36 yarn-hadoop-nodemanager-master.out.4</div><div class=\"line\">-rwxrwxrwx. 1 hadoop hadoop   1402 Apr  3 12:30 yarn-hadoop-nodemanager-master.out.5</div><div class=\"line\">-rwxrwxrwx. 1 hadoop hadoop 343209 Apr  3 13:19 yarn-hadoop-resourcemanager-master.log</div><div class=\"line\">-rw-rw-r--. 1 hadoop hadoop    701 Apr  3 13:19 yarn-hadoop-resourcemanager-master.out</div><div class=\"line\">-rw-rw-r--. 1 hadoop hadoop    701 Apr  3 13:09 yarn-hadoop-resourcemanager-master.out.1</div><div class=\"line\">-rw-rw-r--. 1 hadoop hadoop    701 Apr  3 12:52 yarn-hadoop-resourcemanager-master.out.2</div><div class=\"line\">-rwxrwxrwx. 1 hadoop hadoop    701 Apr  3 12:45 yarn-hadoop-resourcemanager-master.out.3</div><div class=\"line\">-rwxrwxrwx. 1 hadoop hadoop    701 Apr  3 12:36 yarn-hadoop-resourcemanager-master.out.4</div><div class=\"line\">-rwxrwxrwx. 1 hadoop hadoop    701 Apr  3 12:30 yarn-hadoop-resourcemanager-master.out.5</div></pre></td></tr></table></figure></p>\n","site":{"data":{}},"excerpt":"<p>hadoop的配置参看github <a href=\"https://github.com/zhaikaishun/hadoop_cluster\" target=\"_blank\" rel=\"external\">https://github.com/zhaikaishun/hadoop_cluster</a><br>作者: 翟开顺</p>\n<p>关键字:<br>集群环境介绍，Hadoop简介，网络配置，所需软件<br>SSH免密码登陆配置，java环境安装，卸载原有的JDK， 安装jdk17， 配置java环境变量，验证是否成功，Hadoop集群安装，安装Hadoop，验证hadoop<br>hadoop错误分析<br>","more":"</p>\n<h1 id=\"集群环境介绍\"><a href=\"#集群环境介绍\" class=\"headerlink\" title=\"集群环境介绍\"></a><strong>集群环境介绍</strong></h1><h2 id=\"1-Hadoop简介\"><a href=\"#1-Hadoop简介\" class=\"headerlink\" title=\"1.  Hadoop简介\"></a><strong>1.  Hadoop简介</strong></h2><p>　Hadoop是Apache软件基金会旗下的一个开源分布式计算平台。以Hadoop分布式文件系统（HDFS，Hadoop Distributed Filesystem）和MapReduce（Google MapReduce的开源实现）为核心的Hadoop为用户提供了系统底层细节透明的分布式基础架构。<br>　　对于Hadoop的集群来讲，可以分成两大类角色：Master和Salve。一个HDFS集群是由一个NameNode和若干个DataNode组成的。其中NameNode作为主服务器，管理文件系统的命名空间和客户端对文件系统的访问操作；集群中的DataNode管理存储的数据。MapReduce框架是由一个单独运行在主节点上的JobTracker和运行在每个集群从节点的TaskTracker共同组成的。主节点负责调度构成一个作业的所有任务，这些任务分布在不同的从节点上。主节点监控它们的执行情况，并且重新执行之前的失败任务；从节点仅负责由主节点指派的任务。当一个Job被提交时，JobTracker接收到提交作业和配置信息之后，就会将配置信息等分发给从节点，同时调度任务并监控TaskTracker的执行。<br>　　从上面的介绍可以看出，HDFS和MapReduce共同组成了Hadoop分布式系统体系结构的核心。HDFS在集群上实现分布式文件系统，MapReduce在集群上实现了分布式计算和任务处理。HDFS在MapReduce任务处理过程中提供了文件操作和存储等支持，MapReduce在HDFS的基础上实现了任务的分发、跟踪、执行等工作，并收集结果，二者相互作用，完成了Hadoop分布式集群的主要任务。</p>\n<h2 id=\"2-环境说明\"><a href=\"#2-环境说明\" class=\"headerlink\" title=\"2.  环境说明\"></a><strong>2.  环境说明</strong></h2><p>本教程为了简单起见只设置两个节点： master为主节点，node01为数据节点，节点之间局域网连接，相互可以ping通，节点IP分布如下</p>\n<table>\n<thead>\n<tr>\n<th>机器名称</th>\n<th>IP地址</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>master</td>\n<td>192.168.200.128</td>\n</tr>\n<tr>\n<td>node01</td>\n<td>192.168.200.129</td>\n</tr>\n</tbody>\n</table>\n<p>两个节点都是centos6.5系统，都有同一个用户，用户名叫Hadoop  </p>\n<h3 id=\"给hadoop用户赋予root权限\"><a href=\"#给hadoop用户赋予root权限\" class=\"headerlink\" title=\"给hadoop用户赋予root权限\"></a><strong>给hadoop用户赋予root权限</strong></h3><p>切换到root用户 赋予etc/sudoers777权限，然后打开<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@kaishun etc]# chmod 777 /etc/sudoers</div><div class=\"line\">[root@kaishun etc]# vim /etc/sudoers</div><div class=\"line\">```  </div><div class=\"line\">找到Allows people in group wheel to run all commands，把下面%wheel的#给去掉,在Allow root to run any commands anywhere下，加上hadoop  ALL=(ALL)       ALL，然后保存</div><div class=\"line\">```shell</div><div class=\"line\"><span class=\"meta\">#</span># Allow root to run any commands anywhere</div><div class=\"line\">root    ALL=(ALL)       ALL     </div><div class=\"line\">hadoop  ALL=(ALL)       ALL</div><div class=\"line\"></div><div class=\"line\"><span class=\"meta\">#</span># Allows people in group wheel to run all commands</div><div class=\"line\"><span class=\"meta\">%</span>wheel        ALL=(ALL)       ALL</div></pre></td></tr></table></figure></p>\n<p>把sudoers的权限改回来成440<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@kaishun etc]# chmod 440 /etc/sudoers</div><div class=\"line\">```    </div><div class=\"line\">测试是否成功  </div><div class=\"line\">在普通用户下</div><div class=\"line\">```shell</div><div class=\"line\">[hadoop@kaishun ~]$ sudo mkdir test</div><div class=\"line\">输入密码如果可以成功创建文件夹，说明成功</div></pre></td></tr></table></figure></p>\n<h3 id=\"网络配置\"><a href=\"#网络配置\" class=\"headerlink\" title=\"网络配置\"></a><strong>网络配置</strong></h3><p><strong>1. 查看当前机器名</strong><br><strong>在root用户下</strong>输入，显示<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@kaishun hadoop]# hostname</div><div class=\"line\">显示 kaishun， 与我们规划的master不符合</div><div class=\"line\">```  </div><div class=\"line\">**2. 在root用户下修改当前机器名称**</div><div class=\"line\">```shell</div><div class=\"line\">[root@kaishun hadoop]# vim /etc/sysconfig/network</div></pre></td></tr></table></figure></p>\n<p>修改HOSTNAME 为 master<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">HOSTNAME=master</div></pre></td></tr></table></figure></p>\n<p>同理，192.168.200.129这台机器修改成node01<br>修改之后，可能不会立即生效，我是重启后才生效的<br><strong>3. 在root用户下配置hosts文件, 每台机器都需要配置（必须）</strong><br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@master hadoop]# vim /etc/hosts</div></pre></td></tr></table></figure></p>\n<p>添加<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div></pre></td><td class=\"code\"><pre><div class=\"line\">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</div><div class=\"line\">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</div><div class=\"line\">192.168.200.128 master</div><div class=\"line\">192.168.200.129 node01</div><div class=\"line\">```  </div><div class=\"line\">**测试是否成功**，如果能相互使用ping node01   ping master能成功，说明hosts文件配置成功</div><div class=\"line\">```shell</div><div class=\"line\">[hadoop@master ~]$ ping node01</div><div class=\"line\">PING node01 (192.168.200.129) 56(84) bytes of data.</div><div class=\"line\">64 bytes from node01 (192.168.200.129): icmp_seq=1 ttl=64 time=0.391 ms</div><div class=\"line\">64 bytes from node01 (192.168.200.129): icmp_seq=2 ttl=64 time=0.435 ms</div><div class=\"line\">64 bytes from node01 (192.168.200.129): icmp_seq=3 ttl=64 time=0.442 ms</div><div class=\"line\"></div><div class=\"line\">[hadoop@node01 ~]$ ping master</div><div class=\"line\">PING master (192.168.200.128) 56(84) bytes of data.</div><div class=\"line\">64 bytes from master (192.168.200.128): icmp_seq=1 ttl=64 time=0.379 ms</div><div class=\"line\">64 bytes from master (192.168.200.128): icmp_seq=2 ttl=64 time=0.411 ms</div><div class=\"line\">64 bytes from master (192.168.200.128): icmp_seq=3 ttl=64 time=0.460 ms</div></pre></td></tr></table></figure></p>\n<h2 id=\"3-所需软件\"><a href=\"#3-所需软件\" class=\"headerlink\" title=\"3.  所需软件\"></a><strong>3.  所需软件</strong></h2><ol>\n<li>JDK版本1.7  </li>\n<li>hadoop版本hadoop-2.7.1  去官网的华科镜像下载hadoop-2.7.1.tar.gz， <a href=\"http://mirrors.hust.edu.cn/apache/hadoop/common/hadoop-2.7.1/\" target=\"_blank\" rel=\"external\">地址</a>  </li>\n<li>数据传输工具FileZilla， ssh连接工具 secureCRT  </li>\n</ol>\n<h2 id=\"4-SSH免密码登陆配置\"><a href=\"#4-SSH免密码登陆配置\" class=\"headerlink\" title=\"4.  SSH免密码登陆配置\"></a><strong>4.  SSH免密码登陆配置</strong></h2><p>Hadoop运行过程中需要管理远端Hadoop守护进程，在Hadoop启动以后，NameNode是通过SSH（Secure Shell）来启动和停止各个DataNode上的各种守护进程的。这就必须在节点之间执行指令的时候是不需要输入密码的形式，故我们需要配置SSH运用无密码公钥认证的形式，这样NameNode使用SSH无密码登录并启动DataName进程，同样原理，DataNode上也能使用SSH无密码登录到NameNode。<br>安装CentOS6.5时，我们选择了一些基本安装包，所以我们需要两个服务：ssh和rsync已经安装了。可以通过下面命令查看结果显示如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div></pre></td><td class=\"code\"><pre><div class=\"line\">[hadoop@master ~]$ rpm –qa | grep openssh</div><div class=\"line\">[hadoop@master ~]$ rpm –qa | grep rsync</div><div class=\"line\">如果有相应的提示，说明这两个是装好了的，我这里是系统自带的</div><div class=\"line\">```  </div><div class=\"line\">**4.1 配置master无密码登陆所有的node**  </div><div class=\"line\">原理请百度</div><div class=\"line\">在master节点上执行以下命令 然后按几次回车键：</div><div class=\"line\">```shell</div><div class=\"line\">[hadoop@master ~]$ ssh-keygen -t rsa</div><div class=\"line\">```  </div><div class=\"line\">出现下图  </div><div class=\"line\">![ssh免密码登陆](http://i4.buimg.com/567571/b22c79e94cbc2297.png)</div><div class=\"line\"></div><div class=\"line\">我们看到这句话  </div><div class=\"line\">Your identification has been saved in /home/hadoop/.ssh/id_rsa.  </div><div class=\"line\">Your public key has been saved in /home/hadoop/.ssh/id_rsa.pub.  </div><div class=\"line\">说明默认目录在 /home/hadoop/.ssh/ 下</div><div class=\"line\"></div><div class=\"line\">接着在master节点上做如下配置，把id_rsa.pub追加到授权的key里面去。  </div><div class=\"line\">```shell</div><div class=\"line\">[hadoop@master ~]$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</div></pre></td></tr></table></figure></p>\n<p>现在我们进入~/.ssh目录可以看到<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">[hadoop@master ~]$ cd ~/.ssh/</div><div class=\"line\">[hadoop@master .ssh]$ ll</div><div class=\"line\">total 12</div><div class=\"line\">-rw-rw-r--. 1 hadoop hadoop  395 Apr  2 16:22 authorized_keys</div><div class=\"line\">-rw-------. 1 hadoop hadoop 1675 Apr  2 16:17 id_rsa</div><div class=\"line\">-rw-r--r--. 1 hadoop hadoop  395 Apr  2 16:17 id_rsa.pub</div></pre></td></tr></table></figure></p>\n<p><strong>4.1.1. 修改文件”authorized_keys权限</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">[hadoop@master .ssh]$ chmod 600 ~/.ssh/authorized_keys</div></pre></td></tr></table></figure></p>\n<p><img src=\"http://i2.muimg.com/567571/41ecd0b8634028ae.png\" alt=\"authorized_keys权限\"></p>\n<p><strong>4.1.2. 设置SSH配置</strong><br>用root用户登录服务器修改SSH配置文件”/etc/ssh/sshd_config”的下列内容。这里找到这些内容，把前面的#去掉即可<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@master .ssh]# vim /etc/ssh/sshd_config</div><div class=\"line\">RSAAuthentication yes # 启用 RSA 认证</div><div class=\"line\">PubkeyAuthentication yes # 启用公钥私钥配对认证方式</div><div class=\"line\">AuthorizedKeysFile .ssh/authorized_keys # 公钥文件路径（和上面生成的文件同）</div></pre></td></tr></table></figure></p>\n<p><strong>重启SSH服务</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@master .ssh]# /etc/rc.d/init.d/sshd restart</div><div class=\"line\">Stopping sshd:                                             [  OK  ]</div><div class=\"line\">Starting sshd:                                             [  OK  ]</div><div class=\"line\">[root@master .ssh]#</div></pre></td></tr></table></figure></p>\n<p>退出root用户，使用hadoop普通用户验证是否成功, ssh localhost, 如果不需要输入密码，那么验证成功<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">[hadoop@master .ssh]$ ssh localhost</div><div class=\"line\">The authenticity of host 'localhost (::1)' can't be established.</div><div class=\"line\">RSA key fingerprint is 48:0b:ee:9b:67:85:4c:19:35:10:d1:1d:e1:5d:fa:c4.</div><div class=\"line\">Are you sure you want to continue connecting (yes/no)? yes</div><div class=\"line\">Warning: Permanently added 'localhost' (RSA) to the list of known hosts.</div><div class=\"line\">Last login: Sun Apr  2 16:09:39 2017 from 192.168.200.1</div></pre></td></tr></table></figure></p>\n<p><strong>4.1.3. 把公钥复制到所有的node机器上</strong><br>从上图中得知无密码登录本级已经设置完毕，接下来的事儿是把公钥复制所有的node机器上。使用下面的命令格式进行复制公钥<br>scp ~/.ssh/id_rsa.pub 远程用户名@远程服务器IP:~/<br>我本地这样使用 scp ~/.ssh/id_rsa.pub hadoop@192.168.200.129:~/  ,然后根据提示输入需要复制的远程服务器的密码，最后出现下面的提示说明复制成功<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">[hadoop@master ~]$ scp ~/.ssh/id_rsa.pub hadoop@192.168.200.129:~/</div><div class=\"line\">hadoop@192.168.200.129's password:  # 这里输入远程密码</div><div class=\"line\">id_rsa.pub                                                                                                                        100%  395     0.4KB/s   00:00    </div><div class=\"line\">[hadoop@master ~]$ </div><div class=\"line\">```  </div><div class=\"line\">**4.1.4. 对节点机器进行配置**</div><div class=\"line\">下面就针对IP为\"192.168.200.129\"的node01的节点进行配置。</div><div class=\"line\">4.1  ll -a查看是否有.ssh目录，如果没有，我们需要创建一个.ssh目录，并且赋予这个权限 drwx------.  2 hadoop hadoop 4096 Apr  2 16:40 .ssh  具体权限参照master的机器， centos6.5一般都是默认带有.ssh目录的</div></pre></td></tr></table></figure></p>\n<p>[hadoop@node01 ~]$ ll -a<br>drwx——.  2 hadoop hadoop 4096 Apr  2 16:40 .ssh<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">如果有这个目录了，我们把刚才的文件追加到authorized_keys 中去，然后修改authorized_keys文件权限</div></pre></td></tr></table></figure></p>\n<p>[hadoop@node01 ~]$ cat ~/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys<br>[hadoop@node01 .ssh]$ chmod 600 ~/.ssh/authorized_keys</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">进入到ssh 目录，ll 看到如下所示说明成功，注意权限是否正确</div><div class=\"line\">```shell</div><div class=\"line\">[hadoop@node01 .ssh]$ ll</div><div class=\"line\">total 8</div><div class=\"line\">-rw-------. 1 hadoop hadoop 395 Apr  2 16:52 authorized_keys</div><div class=\"line\">-rw-r--r--. 1 hadoop hadoop 391 Apr  2 16:40 known_hosts</div></pre></td></tr></table></figure>\n<p><strong>4.2  用root用户修改/etc/ssh/sshd_config</strong><br>参考前面的master的修改/etc/ssh/sshd_config的方法<br>设置SSH配置<br>用root用户登录服务器修改SSH配置文件”/etc/ssh/sshd_config”的下列内容。这里找到这些内容，把前面的#去掉即可<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@master .ssh]# vim /etc/ssh/sshd_config</div><div class=\"line\">RSAAuthentication yes # 启用 RSA 认证</div><div class=\"line\">PubkeyAuthentication yes # 启用公钥私钥配对认证方式</div><div class=\"line\">AuthorizedKeysFile .ssh/authorized_keys # 公钥文件路径（和上面生成的文件同）</div></pre></td></tr></table></figure></p>\n<p><strong>重启SSH服务</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@master .ssh]# /etc/rc.d/init.d/sshd restart</div><div class=\"line\">Stopping sshd:                                             [  OK  ]</div><div class=\"line\">Starting sshd:                                             [  OK  ]</div><div class=\"line\">[root@master .ssh]#</div></pre></td></tr></table></figure></p>\n<p>最后记得把”/home/hadoop/“目录下的”id_rsa.pub”文件删除掉 </p>\n<p>到此为止，我们经过的步骤已经实现了从”master”到”node01”SSH无密码登录</p>\n<p>验证master到node01的无密码登陆,在master机器上，使用hadoop用户 ssh node01或者ssh 192.168.200.129, 下面是成功的的结果<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">[hadoop@master ~]$ ssh node01</div><div class=\"line\">Last login: Sun Apr  2 17:25:50 2017 from localhost</div><div class=\"line\">[hadoop@node01 ~]$ </div><div class=\"line\"></div><div class=\"line\">[hadoop@node01 ~]$ ssh master</div><div class=\"line\">Last login: Sun Apr  2 17:26:04 2017 from node01</div><div class=\"line\">[hadoop@master ~]$</div></pre></td></tr></table></figure></p>\n<h2 id=\"5-java安装环境\"><a href=\"#5-java安装环境\" class=\"headerlink\" title=\"5 java安装环境\"></a><strong>5 java安装环境</strong></h2><h3 id=\"5-1-卸载原有的JDK\"><a href=\"#5-1-卸载原有的JDK\" class=\"headerlink\" title=\"5.1 卸载原有的JDK\"></a><strong>5.1 卸载原有的JDK</strong></h3><p>因为有的系统自带有JDK, 安装前先卸载<br>查看所装的JDK<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">[hadoop@master ~]$ rpm -qa | grep jdk</div><div class=\"line\">出现</div><div class=\"line\">java-1.7.0-openjdk-1.7.0.45-2.4.3.3.el6.x86_64</div><div class=\"line\">java-1.6.0-openjdk-1.6.0.0-1.66.1.13.0.el6.x86_64</div></pre></td></tr></table></figure></p>\n<p>root下卸载前面查出的这两个<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@master hadoop]#  yum -y remove java-1.7.0-openjdk-1.7.0.45-2.4.3.3.el6.x86_64  </div><div class=\"line\">[root@master hadoop]#  yum -y remove java-1.6.0-openjdk-1.6.0.0-1.66.1.13.0.el6.x86_64  </div><div class=\"line\">成功后会出现一个complete</div><div class=\"line\"></div><div class=\"line\">```  </div><div class=\"line\"><span class=\"meta\">#</span>## **5.2 安装jdk1.7**</div><div class=\"line\">首先用root身份登录master后在/usr/local下创建java文件夹</div><div class=\"line\">```shell</div><div class=\"line\">[root@master hadoop]# mkdir -p /usr/local/java</div><div class=\"line\">```  </div><div class=\"line\">我们把FTP传来的jdk-7u79-linux-x64.tar.gz复制到/usr/local/java 文件夹下</div><div class=\"line\">```shell</div><div class=\"line\">[root@master Downloads]# cp jdk-7u79-linux-x64.tar.gz /usr/local/java/</div></pre></td></tr></table></figure></p>\n<p>解压并且<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@master java]# tar zxvf jdk-7u79-linux-x64.tar.gz </div><div class=\"line\">解压完成后出现</div><div class=\"line\">[root@master java]# ll</div><div class=\"line\">total 149920</div><div class=\"line\">drwxr-xr-x. 8 uucp  143      4096 Apr 11  2015 jdk1.7.0_79</div><div class=\"line\">-rw-r--r--. 1 root root 153512879 Apr  2 18:17 jdk-7u79-linux-x64.tar.gz</div></pre></td></tr></table></figure></p>\n<p>给所有者权限<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@master java]# chown hadoop:hadoop jdk1.7.0_79/ -R</div></pre></td></tr></table></figure></p>\n<h3 id=\"5-3-配置java环境变量\"><a href=\"#5-3-配置java环境变量\" class=\"headerlink\" title=\"5.3 配置java环境变量\"></a><strong>5.3 配置java环境变量</strong></h3><p>编辑”/etc/profile”文件<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@master java]#  vim /etc/profile</div></pre></td></tr></table></figure></p>\n<p>在尾部加入<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">#</span> set java environment</div><div class=\"line\">export JAVA_HOME=/usr/local/java/jdk1.7.0_79</div><div class=\"line\">export JRE_HOME=/usr/local/java/jdk1.7.0_79/jre</div><div class=\"line\">export PATH=$PATH:/usr/local/java/jdk1.7.0_79/bin</div><div class=\"line\">export CLASSPATH=./:/usr/local/java/jdk1.7.0_79/lib:/usr/local/java/jdk1.7.0_79/jre/lib</div></pre></td></tr></table></figure></p>\n<p>使配置生效<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@master java]# source /etc/profile</div></pre></td></tr></table></figure></p>\n<h3 id=\"5-4-验证是否成功\"><a href=\"#5-4-验证是否成功\" class=\"headerlink\" title=\"5.4 验证是否成功\"></a><strong>5.4 验证是否成功</strong></h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">java -version  出现 java version &quot;1.7.0_79&quot;</div><div class=\"line\">javac  有提示</div><div class=\"line\">java  有提示</div></pre></td></tr></table></figure>\n<p>确保是按照我上面的步骤，权限不能有错，否则可能会有问题, 同样，在另外的节点上也安装好jdk</p>\n<h2 id=\"6-Hadoop集群安装\"><a href=\"#6-Hadoop集群安装\" class=\"headerlink\" title=\"6. Hadoop集群安装\"></a><strong>6. Hadoop集群安装</strong></h2><p>所有的机器上都要安装hadoop，现在就先在Master服务器安装，然后其他服务器按照步骤重复进行即可。<strong>安装和配置hadoop需要以”root”的身份进行。</strong></p>\n<h3 id=\"6-1-安装Hadoop\"><a href=\"#6-1-安装Hadoop\" class=\"headerlink\" title=\"6.1 安装Hadoop\"></a><strong>6.1 安装Hadoop</strong></h3><h4 id=\"6-1-1-建立一个目录，用来存放hadoop\"><a href=\"#6-1-1-建立一个目录，用来存放hadoop\" class=\"headerlink\" title=\"6.1.1 建立一个目录，用来存放hadoop\"></a><strong>6.1.1 建立一个目录，用来存放hadoop</strong></h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@master Downloads]#  mkdir -p /home/hadoop/MyCloudera/APP/hadoop/</div></pre></td></tr></table></figure>\n<h4 id=\"6-1-2-把下载好得hadoop-2-7-1-tar-gz-复制到这个目录下，解压并且命名为hadoop\"><a href=\"#6-1-2-把下载好得hadoop-2-7-1-tar-gz-复制到这个目录下，解压并且命名为hadoop\" class=\"headerlink\" title=\"6.1.2 把下载好得hadoop-2.7.1.tar.gz 复制到这个目录下，解压并且命名为hadoop\"></a><strong>6.1.2 把下载好得hadoop-2.7.1.tar.gz 复制到这个目录下，解压并且命名为hadoop</strong></h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">复制到我们建立得目录</div><div class=\"line\">[root@master Downloads]# cp hadoop-2.7.1.tar.gz  /home/hadoop/MyCloudera/APP/hadoop/</div><div class=\"line\">进入到我们复制得目录</div><div class=\"line\">[root@master Downloads]# cd /home/hadoop/MyCloudera/APP/hadoop  </div><div class=\"line\">对此tar.gz解压</div><div class=\"line\">[root@master hadoop]# tar zxvf hadoop-2.7.1.tar.gz </div><div class=\"line\">改名为hadoop</div><div class=\"line\">[root@master hadoop]# mv hadoop-2.7.1 hadoop</div></pre></td></tr></table></figure>\n<h4 id=\"6-1-3-将文件夹得读写权限赋予给hadoop用户\"><a href=\"#6-1-3-将文件夹得读写权限赋予给hadoop用户\" class=\"headerlink\" title=\"6.1.3 将文件夹得读写权限赋予给hadoop用户\"></a><strong>6.1.3 将文件夹得读写权限赋予给hadoop用户</strong></h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@master APP]# chown -R hadoop:hadoop hadoop  </div><div class=\"line\">ll 查看权限，是这样得</div><div class=\"line\">[root@master APP]# ll</div><div class=\"line\">total 4</div><div class=\"line\">drwxr-xr-x. 3 hadoop hadoop 4096 Apr  2 23:29 hadoop</div></pre></td></tr></table></figure>\n<h4 id=\"6-1-4-配置-etc-profile\"><a href=\"#6-1-4-配置-etc-profile\" class=\"headerlink\" title=\"6.1.4 配置/etc/profile\"></a><strong>6.1.4 配置/etc/profile</strong></h4><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@master APP]# vim /etc/profile</div></pre></td></tr></table></figure>\n<p>在末尾加上如下配置，其中HADOOP_HOME填写前面得hadoop存放得位置<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"># set hadoop path</div><div class=\"line\">export HADOOP_HOME=/home/hadoop/MyCloudera/APP/hadoop/hadoop </div><div class=\"line\">export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin</div><div class=\"line\">export HADOOP_MAPRED_HOME=$HADOOP_HOME</div><div class=\"line\">export HADOOP_COMMON_HOME=$HADOOP_HOME</div><div class=\"line\">export HADOOP_HDFS_HOME=$HADOOP_HOME</div><div class=\"line\">export HADOOP_YARN_HOME=$HADOOP_HOME</div><div class=\"line\">export HADOOP_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoop</div><div class=\"line\">export YARN_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoop</div></pre></td></tr></table></figure></p>\n<p>让配置生效<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@master APP]# source /etc/profile</div></pre></td></tr></table></figure></p>\n<h3 id=\"6-2-配置hadoop\"><a href=\"#6-2-配置hadoop\" class=\"headerlink\" title=\"6.2 配置hadoop\"></a><strong>6.2 配置hadoop</strong></h3><p>Hadoop配置文件在conf目录下，之前的版本的配置文件主要是Hadoop-default.xml和Hadoop-site.xml。由于Hadoop发展迅速，代码量急剧增加，代码开发分为了core，hdfs和map/reduce三部分，配置文件也被分成了三个core-site.xml、hdfs-site.xml、mapred-site.xml。core-site.xml和hdfs-site.xml是站在HDFS角度上配置文件；core-site.xml和mapred-site.xml是站在MapReduce角度上配置文件。</p>\n<h4 id=\"6-2-1-配置hadoop-env-sh\"><a href=\"#6-2-1-配置hadoop-env-sh\" class=\"headerlink\" title=\"6.2.1 配置hadoop-env.sh\"></a><strong>6.2.1 配置hadoop-env.sh</strong></h4><p>该hadoop-env.sh文件位于/home/hadoop/MyCloudera/APP/hadoop/hadoop/etc/hadoop目录下<br>在文件的末尾添加下面内容<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"meta\">#</span> The java environment</div><div class=\"line\">export JAVA_HOME=/usr/local/java/jdk1.7.0_79</div></pre></td></tr></table></figure></p>\n<h4 id=\"6-2-2-配置core-site-xml文件\"><a href=\"#6-2-2-配置core-site-xml文件\" class=\"headerlink\" title=\"6.2.2 配置core-site.xml文件\"></a><strong>6.2.2 配置core-site.xml文件</strong></h4><p>我们先在本地建立几个目录，用来存放一些hadoop得文件<br>在根目录下，建立一个data<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">根目录路径</div><div class=\"line\">[root@master /]# pwd</div><div class=\"line\">/</div><div class=\"line\">创建一个data目录</div><div class=\"line\">[root@master /]# mkdir data</div><div class=\"line\">创建/data/tmpdata/hadoop/data/tmp目录</div><div class=\"line\">[root@master /]# mkdir -p /data/tmpdata/hadoop/data/tmp</div></pre></td></tr></table></figure></p>\n<p>然后对core-site.xml做如下配置, 具体得hadoop.tmp.dir和fs.default.name得功能参看百度google<br><figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">configuration</span>&gt;</span></div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>hadoop.tmp.dir<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>/data/tmpdata/hadoop/data/tmp<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\">  <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>fs.default.name<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">      <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>hdfs://master:9000<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">  <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">configuration</span>&gt;</span></div></pre></td></tr></table></figure></p>\n<h4 id=\"6-2-3-配置hdfs-site-xml文件\"><a href=\"#6-2-3-配置hdfs-site-xml文件\" class=\"headerlink\" title=\"6.2.3 配置hdfs-site.xml文件\"></a><strong>6.2.3 配置hdfs-site.xml文件</strong></h4><p>我这里配置的比较完整，如果想简单点，有的其实可以默认设置，具体参看其他文章<br><strong>1. 创建namenode和datanode的存放目录,然后对/data目录赋予权限。 注意权限不能有错</strong><br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@master /]# mkdir -p /data/hadoop/data/name</div><div class=\"line\">[root@master data]# mkdir -p /data/hadoop/data/data</div><div class=\"line\"></div><div class=\"line\">[root@master /]#  chown hadoop:hadoop data/ -R</div><div class=\"line\">[root@master /]# chmod 777 data/ -R</div></pre></td></tr></table></figure></p>\n<p><strong>2. 创建SecondaryNameNode的目录</strong><br>在根目录下创建hadoop目录，然后创建/hadoop/SecondaryNameNode/目录，最后赋予hadoop目录权限<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@master /]# mkdir hadoop  </div><div class=\"line\">[root@master /]# mkdir -p /hadoop/SecondaryNameNode/  </div><div class=\"line\"></div><div class=\"line\">[root@master /]# chown hadoop:hadoop hadoop/ -R </div><div class=\"line\">[root@master /]#  chmod 777 hadoop/ -R</div></pre></td></tr></table></figure></p>\n<p><strong>hdfs-site.xml配置</strong><br><figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">configuration</span>&gt;</span></div><div class=\"line\">     <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.namenode.name.dir<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>/data/hadoop/data/name/<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">     <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">     <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.datanode.data.dir<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>/data/hadoop/data/data/<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">     <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">     <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">           <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.replication<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">           <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>2<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">     <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">     <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.namenode.checkpoint.dir<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>/hadoop/SecondaryNameNode/<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">     <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">    </div><div class=\"line\">     <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.http.address<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>master:50070<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">     <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">    </div><div class=\"line\">     <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.secondary.http.address<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>master:50090<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">     <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.datanode.du.reserved<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>0<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span> 每个卷预留的空闲空间数量 <span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">    </div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.datanode.max.xcievers<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>32768<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">    </div><div class=\"line\">    </div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.datanode.socket.write.timeout<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>0<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>dfs.socket.timeout<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>180000<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>socket通讯超时时间<span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">configuration</span>&gt;</span></div></pre></td></tr></table></figure></p>\n<h4 id=\"6-2-3-配置mapred-site-xml文件\"><a href=\"#6-2-3-配置mapred-site-xml文件\" class=\"headerlink\" title=\"6.2.3 配置mapred-site.xml文件\"></a><strong>6.2.3 配置mapred-site.xml文件</strong></h4><p>我这里配置的比较完整，网上大多数都是用的默认，具体其中的一些参数可以百度<br>这里先建立几个文件<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@master /]# mkdir -p /hadoop/mapreduce/jobhistory/history/done</div><div class=\"line\">[root@master /]# mkdir -p /hadoop/mapreduce/jobhistory/history/done_intermediate</div><div class=\"line\">[root@master /]# mkdir -p /hadoop/hadoop-yarn/staging  </div><div class=\"line\"></div><div class=\"line\">赋予权限</div><div class=\"line\">[root@master /]# chown hadoop:hadoop hadoop/ -R </div><div class=\"line\">[root@master /]#  chmod 777 hadoop/ -R</div></pre></td></tr></table></figure></p>\n<p>复制一份 mapred-site.xml<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@master hadoop]# cp mapred-site.xml.template mapred-site.xml</div></pre></td></tr></table></figure></p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">configuration</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapreduce.framework.name<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>yarn<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapreduce.jobtracker.address<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>master:9001<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapreduce.jobtracker.http.address<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>master:50030<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapreduce.jobhistory.address<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>master:10020<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>master:19888<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapreduce.jobhistory.done-dir<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>/hadoop/mapreduce/jobhistory/history/done<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapreduce.jobhistory.intermediate-done-dir<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>/hadoop/mapreduce/jobhistory/history/done_intermediate<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.app.mapreduce.am.staging-dir<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>/hadoop/hadoop-yarn/staging<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapred.hosts.exclude<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>/home/hadoop/MyCloudera/APP/hadoop/hadoop/etc/hadoop/excludes<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">final</span>&gt;</span>true<span class=\"tag\">&lt;/<span class=\"name\">final</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapreduce.tasktracker.map.tasks.maximum<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>32<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span> 同一时间允许运行的最大map任务数 <span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapreduce.tasktracker.reduce.tasks.maximum<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>16<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span> 同一时间允许运行的最大reduce任务数 <span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>1000<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapreduce.map.memory.mb<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>512<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>map阶段申请的container的内存的大小<span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapreduce.reduce.memory.mb<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>512<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>reduce阶段申请的container的内存的大小<span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapreduce.map.java.opts<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>-Xmx512M<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">description</span>&gt;</span>用户设定的map/reduce阶段申请的container的JVM参数。最大堆设定要比申请的内存少一些，用于JVM的非堆部分使用。 <span class=\"tag\">&lt;/<span class=\"name\">description</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapreduce.reduce.java.opts<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>-Xmx1024M<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapreduce.task.io.sort.mb<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>1024<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>mapreduce.reduce.shuffle.parallelcopies<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>16<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">configuration</span>&gt;</span></div></pre></td></tr></table></figure>\n<h4 id=\"6-2-3-配置yarn-site-xml文件\"><a href=\"#6-2-3-配置yarn-site-xml文件\" class=\"headerlink\" title=\"6.2.3 配置yarn-site.xml文件\"></a><strong>6.2.3 配置yarn-site.xml文件</strong></h4><p>创建一些文件夹，并且赋予权限<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@master /]# clear</div><div class=\"line\">[root@master /]# mkdir -p /data/nodemanager/tmp/</div><div class=\"line\">[root@master /]# mkdir -p /hadoop/nodemanager/remote</div><div class=\"line\">[root@master /]# mkdir -p /data/hadoop/data/nodemanager/logs</div><div class=\"line\">[root@master /]# chown hadoop:hadoop hadoop/ -R</div><div class=\"line\">[root@master /]# chmod 777 hadoop/ -R</div><div class=\"line\">[root@master /]# chown hadoop:hadoop data/ -R</div><div class=\"line\">[root@master /]# chmod 777 data/ -R</div></pre></td></tr></table></figure></p>\n<figure class=\"highlight xml\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div><div class=\"line\">106</div><div class=\"line\">107</div><div class=\"line\">108</div><div class=\"line\">109</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">configuration</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">&lt;!-- Site specific YARN configuration properties --&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">         <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.aux-services<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>mapreduce_shuffle<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.aux-services.mapreduce.shuffle.class<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>org.apache.hadoop.mapred.ShuffleHandler<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">     <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.resourcemanager.scheduler.address<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>master:8030<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">     <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.resourcemanager.resource-tracker.address<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>master:8031<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.resourcemanager.address<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>master:8032<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">     <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.resourcemanager.admin.address<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">             <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>master:8033<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.address<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>master:9999<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.webapp.address<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>master:8042<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">    </div><div class=\"line\">     <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.resourcemanager.webapp.address<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>master:8088<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.local-dirs<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>/data/nodemanager/tmp/<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.remote-app-log-dir<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>/hadoop/nodemanager/remote<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">          <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.log-dirs<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">           <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>/data/hadoop/data/nodemanager/logs<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">     <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.log.retain-seconds<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>604800<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.resource.cpu-vcores<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>24<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>1024<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.nodemanager.vmem-pmem-ratio<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>2<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.scheduler.minimum-allocation-mb<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>256<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.scheduler.maximum-allocation-mb<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>1024<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.scheduler.minimum-allocation-vcores<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>1<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.scheduler.maximum-allocation-vcores<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>24<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"tag\">&lt;<span class=\"name\">property</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">name</span>&gt;</span>yarn.log-aggregation-enable<span class=\"tag\">&lt;/<span class=\"name\">name</span>&gt;</span></div><div class=\"line\">            <span class=\"tag\">&lt;<span class=\"name\">value</span>&gt;</span>true<span class=\"tag\">&lt;/<span class=\"name\">value</span>&gt;</span></div><div class=\"line\">    <span class=\"tag\">&lt;/<span class=\"name\">property</span>&gt;</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"tag\">&lt;/<span class=\"name\">configuration</span>&gt;</span></div></pre></td></tr></table></figure>\n<h4 id=\"6-2-3-配置slaves文件\"><a href=\"#6-2-3-配置slaves文件\" class=\"headerlink\" title=\"6.2.3 配置slaves文件\"></a><strong>6.2.3 配置slaves文件</strong></h4><p>这个配置主要记录数据节点的列表，假如集群有3个数据节点，如：node001，node002，node003<br>那么在slave文件里面就可以设置为：<br>node001<br>node002<br>node003  </p>\n<p>我这里为两个节点，配置如下<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">master</div><div class=\"line\">node01</div></pre></td></tr></table></figure></p>\n<p>到此，master的hadoop的配置已经完成，对于其他节点，我们建立好相关的目录，复制过去，稍作配置即可了</p>\n<p>需要建立的目录总结</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div></pre></td><td class=\"code\"><pre><div class=\"line\">[root@node01 /]# mkdir -p /home/hadoop/MyCloudera/APP/hadoop/</div><div class=\"line\">[root@node01 /]# mkdir data</div><div class=\"line\">[root@node01 /]# mkdir -p /data/tmpdata/hadoop/data/tmp</div><div class=\"line\">[root@node01 /]# mkdir -p /data/hadoop/data/name</div><div class=\"line\">[root@node01 /]# mkdir -p /data/hadoop/data/data</div><div class=\"line\">[root@node01 /]# mkdir hadoop</div><div class=\"line\">[root@node01 /]# mkdir -p /hadoop/SecondaryNameNode/  </div><div class=\"line\">[root@node01 /]# chown hadoop:hadoop hadoop/ -R </div><div class=\"line\">[root@node01 /]# chmod 777 hadoop/ -R</div><div class=\"line\">[root@node01 /]# mkdir -p /hadoop/mapreduce/jobhistory/history/done</div><div class=\"line\">[root@node01 /]# mkdir -p /hadoop/mapreduce/jobhistory/history/done_intermediate</div><div class=\"line\">[root@node01 /]# mkdir -p /hadoop/hadoop-yarn/staging  </div><div class=\"line\">[root@node01 /]# mkdir -p /data/nodemanager/tmp/</div><div class=\"line\">[root@node01 /]# mkdir -p /hadoop/nodemanager/remote</div><div class=\"line\">[root@node01 /]# mkdir -p /data/hadoop/data/nodemanager/logs</div><div class=\"line\">[root@node01 /]# chown hadoop:hadoop hadoop/ -R</div><div class=\"line\">[root@node01 /]# chmod 777 hadoop/ -R</div><div class=\"line\">[root@node01 /]# chown hadoop:hadoop data/ -R</div><div class=\"line\">[root@node01 /]# chmod 777 data/ -R</div><div class=\"line\">[root@node01 /]#</div></pre></td></tr></table></figure>\n<p>为了确保 hadoop目录 权限没有问题，每台机器在hadoop目录下再次执行一下以下命令<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">chown -R hadoop:hadoop hadoop</div><div class=\"line\"><span class=\"meta\">#</span># 为了保险起见，我给了777的权限， 下面的这一步貌似不做也可以</div><div class=\"line\">chmod 777 hadoop/ -R</div></pre></td></tr></table></figure></p>\n<h3 id=\"6-3-启动与验证\"><a href=\"#6-3-启动与验证\" class=\"headerlink\" title=\"6.3 启动与验证\"></a><strong>6.3 启动与验证</strong></h3><h4 id=\"6-3-1-格式化HDFS文件系统\"><a href=\"#6-3-1-格式化HDFS文件系统\" class=\"headerlink\" title=\"6.3.1 格式化HDFS文件系统\"></a><strong>6.3.1 格式化HDFS文件系统</strong></h4><p><strong>在master上使用普通用户hadoop进行操作</strong><br>如果第一次启动需要对hadoop平台进行格式化，记得第一次，假如原来有数据就不需要格式化：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">hdfs namenode -format</div></pre></td></tr></table></figure></p>\n<p>如果经过多次format之后，<strong>一定要把/data/hadoop/data/data /data/hadoop/data/name目录下的文件删除</strong></p>\n<h4 id=\"6-3-2-启动hadoop\"><a href=\"#6-3-2-启动hadoop\" class=\"headerlink\" title=\"6.3.2 启动hadoop\"></a><strong>6.3.2 启动hadoop</strong></h4><p>在启动前关闭集群中所有机器的防火墙，不然会出现datanode开后又自动关闭。<br>记得永久的关闭防火墙chkconfig iptables off<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">chkconfig iptables off</div></pre></td></tr></table></figure></p>\n<p>开始启动,在master的普通用户 hadoop下进行操作<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">start-all.sh</div></pre></td></tr></table></figure></p>\n<p>验证hadoop: 输入jps命令，会出现以下进程说明成功<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\">[hadoop@master hadoop]$ jps</div><div class=\"line\">[hadoop@master hadoop]$ jps</div><div class=\"line\">4197 ResourceManager</div><div class=\"line\">3851 DataNode</div><div class=\"line\">4602 Jps</div><div class=\"line\">4013 SecondaryNameNode</div><div class=\"line\">4308 NodeManager</div><div class=\"line\">3739 NameNode</div><div class=\"line\">```  </div><div class=\"line\"><span class=\"meta\">#</span>### **6.3.3 测试以下hdfs**  </div><div class=\"line\">创建一个目录</div><div class=\"line\">```shell</div><div class=\"line\">[hadoop@node01 ~]$ hadoop fs -mkdir -p /hive/warehouse</div><div class=\"line\">``` </div><div class=\"line\">传一个文件</div><div class=\"line\">```shell</div><div class=\"line\">[hadoop@master hadoop]$ hadoop fs -put slaves /hive/warehouse</div></pre></td></tr></table></figure></p>\n<p>查看文件<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">[hadoop@master hadoop]$ hadoop fs -cat /hive/warehouse/slaves</div><div class=\"line\">显示</div><div class=\"line\">master</div><div class=\"line\">node01</div></pre></td></tr></table></figure></p>\n<p>经过上面的测试，说明我们集群安装成功</p>\n<h3 id=\"6-4-网页查看集群\"><a href=\"#6-4-网页查看集群\" class=\"headerlink\" title=\"6.4 网页查看集群\"></a>6.4 网页查看集群</h3><p>查看hdfs<br><a href=\"http://192.168.200.128:50070\" target=\"_blank\" rel=\"external\">http://192.168.200.128:50070</a><br>显示<br><img src=\"http://i2.muimg.com/567571/7435a4693a8a21b4.png\" alt=\"hdfs验证\"></p>\n<p>验证hadoop<br><a href=\"http://192.168.200.128:8088/cluster/nodes\" target=\"_blank\" rel=\"external\">http://192.168.200.128:8088/cluster/nodes</a><br>显示<br><img src=\"http://i1.piimg.com/567571/80a651b3fc692958.png\" alt=\"hadoop验证\"></p>\n<h2 id=\"7-hadoop-集群碰到错误的解决办法\"><a href=\"#7-hadoop-集群碰到错误的解决办法\" class=\"headerlink\" title=\"7. hadoop 集群碰到错误的解决办法\"></a><strong>7. hadoop 集群碰到错误的解决办法</strong></h2><p>这里的错误，一般都分为几大类，一类是某些文件夹没有创建，一类是某些文件或者文件夹权限不够，一类就是配置错误<br>这些错误都可以去logs目录下查看，我的logs目录在  /home/hadoop/MyCloudera/APP/hadoop/hadoop/logs<br>哪里有问题就对应哪个文件去查看错误，例如resourcemanager没起来或者出问题，就去yarn-hadoop-resourcemanager-master.log<br><figure class=\"highlight shell\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div></pre></td><td class=\"code\"><pre><div class=\"line\">-rwxrwxrwx. 1 hadoop hadoop 921348 Apr  3 13:19 hadoop-hadoop-datanode-master.log</div><div class=\"line\">-rw-rw-r--. 1 hadoop hadoop   1434 Apr  3 13:18 hadoop-hadoop-datanode-master.out</div><div class=\"line\">-rw-rw-r--. 1 hadoop hadoop      0 Apr  3 13:18 hadoop-hadoop-datanode-master.out.1</div><div class=\"line\">-rw-rw-r--. 1 hadoop hadoop   1434 Apr  3 12:52 hadoop-hadoop-datanode-master.out.2</div><div class=\"line\">-rwxrwxrwx. 1 hadoop hadoop   1434 Apr  3 12:45 hadoop-hadoop-datanode-master.out.3</div><div class=\"line\">-rwxrwxrwx. 1 hadoop hadoop   1434 Apr  3 12:42 hadoop-hadoop-datanode-master.out.4</div><div class=\"line\">-rwxrwxrwx. 1 hadoop hadoop   1434 Apr  3 12:35 hadoop-hadoop-datanode-master.out.5</div><div class=\"line\">-rwxrwxrwx. 1 hadoop hadoop 371773 Apr  3 13:26 hadoop-hadoop-namenode-master.log</div><div class=\"line\">-rw-rw-r--. 1 hadoop hadoop    717 Apr  3 13:18 hadoop-hadoop-namenode-master.out</div><div class=\"line\">-rw-rw-r--. 1 hadoop hadoop    717 Apr  3 13:09 hadoop-hadoop-namenode-master.out.1</div><div class=\"line\">-rw-rw-r--. 1 hadoop hadoop    717 Apr  3 12:52 hadoop-hadoop-namenode-master.out.2</div><div class=\"line\">-rwxrwxrwx. 1 hadoop hadoop    717 Apr  3 12:44 hadoop-hadoop-namenode-master.out.3</div><div class=\"line\">-rwxrwxrwx. 1 hadoop hadoop    717 Apr  3 12:42 hadoop-hadoop-namenode-master.out.4</div><div class=\"line\">-rwxrwxrwx. 1 hadoop hadoop    717 Apr  3 12:35 hadoop-hadoop-namenode-master.out.5</div><div class=\"line\">-rwxrwxrwx. 1 hadoop hadoop      0 Apr  3 01:43 SecurityAuth-hadoop.audit</div><div class=\"line\">-rwxrwxrwx. 1 hadoop hadoop 618506 Apr  3 13:19 yarn-hadoop-nodemanager-master.log</div><div class=\"line\">-rw-rw-r--. 1 hadoop hadoop   1402 Apr  3 13:19 yarn-hadoop-nodemanager-master.out</div><div class=\"line\">-rw-rw-r--. 1 hadoop hadoop      0 Apr  3 13:19 yarn-hadoop-nodemanager-master.out.1</div><div class=\"line\">-rw-rw-r--. 1 hadoop hadoop   1402 Apr  3 13:09 yarn-hadoop-nodemanager-master.out.2</div><div class=\"line\">-rw-rw-r--. 1 hadoop hadoop   1402 Apr  3 12:52 yarn-hadoop-nodemanager-master.out.3</div><div class=\"line\">-rwxrwxrwx. 1 hadoop hadoop   1402 Apr  3 12:36 yarn-hadoop-nodemanager-master.out.4</div><div class=\"line\">-rwxrwxrwx. 1 hadoop hadoop   1402 Apr  3 12:30 yarn-hadoop-nodemanager-master.out.5</div><div class=\"line\">-rwxrwxrwx. 1 hadoop hadoop 343209 Apr  3 13:19 yarn-hadoop-resourcemanager-master.log</div><div class=\"line\">-rw-rw-r--. 1 hadoop hadoop    701 Apr  3 13:19 yarn-hadoop-resourcemanager-master.out</div><div class=\"line\">-rw-rw-r--. 1 hadoop hadoop    701 Apr  3 13:09 yarn-hadoop-resourcemanager-master.out.1</div><div class=\"line\">-rw-rw-r--. 1 hadoop hadoop    701 Apr  3 12:52 yarn-hadoop-resourcemanager-master.out.2</div><div class=\"line\">-rwxrwxrwx. 1 hadoop hadoop    701 Apr  3 12:45 yarn-hadoop-resourcemanager-master.out.3</div><div class=\"line\">-rwxrwxrwx. 1 hadoop hadoop    701 Apr  3 12:36 yarn-hadoop-resourcemanager-master.out.4</div><div class=\"line\">-rwxrwxrwx. 1 hadoop hadoop    701 Apr  3 12:30 yarn-hadoop-resourcemanager-master.out.5</div></pre></td></tr></table></figure></p>"},{"title":"scala编程基础","date":"2017-04-28T13:25:21.000Z","author":"kaishun","id":"51","_content":"\n###  多行字符串的表示方法\n多行字符串用三个双引号来表示分隔符，格式为：\"\"\" ... \"\"\"。\n实例如下：\n```scala\nval foo = \"\"\"菜鸟教程\nwww.runoob.com\nwww.w3cschool.cc\nwww.runnoob.com\n以上三个地址都能访问\"\"\"\n```\n## 变量\n### 变量声明\n```\nvar VariableName : DataType [=  Initial Value]\n或\nval VariableName : DataType [=  Initial Value]\n\n变量声明不一定需要初始值，以下也是正确的：\nvar myVar :Int;\nval myVal :String;\n\n```  \n例如\n```\nvar myVar : String = \"Foo\"\nvar myVar : String = \"Too\"\n\nvar myVar :Int;\nval myVal :String;\n```\n\n### 变量类型引用\n在 Scala 中声明变量和常量不一定要指明数据类型，在没有指明数据类型的情况下，其数据类型是通过变量或常量的初始值推断出来的。\n所以，如果在没有指明数据类型的情况下声明变量或常量必须要给出其初始值，否则将会报错。  \n```\nvar myVar = 10;\nval myVal = \"Hello, Scala!\";\n``` \n\n## 访问修饰符\nScala 访问修饰符基本和Java的一样，分别有：private，protected，public。\n如果没有指定访问修饰符符，默认情况下，Scala对象的访问级别都是 public。\n**Scala 中的 private 限定符，比 Java 更严格，在嵌套类情况下，外层类甚至不能访问被嵌套类的私有成员。**\n```\nclass Outer{\n    class Inner{\n    private def f(){println(\"f\")}\n    class InnerMost{\n        f() // 正确\n        }\n    }\n    (new Inner).f() //错误\n}\n``` \n### 作用域保护\nScala中，访问修饰符可以通过使用限定词强调。格式为:\n```\nprivate[x] \n或 \nprotected[x]\n```\n这里的x指代某个所属的包、类或单例对象。如果写成private[x],读作\"这个成员除了对[…]中的类或[…]中的包中的类及它们的伴生对像可见外，对其它所有类都是private。\n这种技巧在横跨了若干包的大型项目中非常有用，它允许你定义一些在你项目的若干子包中可见但对于项目外部的客户却始终不可见的东西。\n```\npackage bobsrocckets{\n    package navigation{\n        private[bobsrockets] class Navigator{\n         protected[navigation] def useStarChart(){}\n         class LegOfJourney{\n             private[Navigator] val distance = 100\n             }\n            private[this] var speed = 200\n            }\n        }\n        package launch{\n        import navigation._\n        object Vehicle{\n        private[launch] val guide = new Navigator\n        }\n    }\n}\n``` \n上述例子中，类Navigator被标记为private[bobsrockets]就是说这个类对包含在bobsrockets包里的所有的类和对象可见。\n比如说，从Vehicle对象里对Navigator的访问是被允许的，因为对象Vehicle包含在包launch中，而launch包在bobsrockets中，相反，所有在包bobsrockets之外的代码都不能访问类Navigator。\n\n## 运算符\n运算符和java的基本类似\n### 位运算符\n```\nA = 0011 1100\n\nB = 0000 1101\n\n-------位运算----------\n\nA&B = 0000 1100\n\nA|B = 0011 1101\n\nA^B = 0011 0001\n\n~A  = 1100 0011\n```\n\n```\nobject Test {\n   def main(args: Array[String]) {\n      var a = 60;           /* 60 = 0011 1100 */  \n      var b = 13;           /* 13 = 0000 1101 */\n      var c = 0;\n\n      c = a & b;            /* 12 = 0000 1100 */ \n      println(\"a & b = \" + c );\n\n      c = a | b;            /* 61 = 0011 1101 */\n      println(\"a | b = \" + c );\n\n      c = a ^ b;            /* 49 = 0011 0001 */\n      println(\"a ^ b = \" + c );\n\n      c = ~a;               /* -61 = 1100 0011 */\n      println(\"~a = \" + c );\n\n      c = a << 2;           /* 240 = 1111 0000 */\n      println(\"a << 2 = \" + c );\n\n      c = a >> 2;           /* 215 = 1111 */\n      println(\"a >> 2  = \" + c );\n\n      c = a >>> 2;          /* 215 = 0000 1111 */\n      println(\"a >>> 2 = \" + c );\n   }\n} \n```\n执行以上代码结果为\n```\n$ scalac Test.scala \n$ scala Test\na & b = 12\na | b = 61\na ^ b = 49\n~a = -61\na << 2 = 240\na >> 2  = 15\na >>> 2 = 15\n\n```\n\n\n## 循环\n### for循环\n\nScala 语言中 for 循环的语法：\n```scala\nfor( var x <- Range ){\n   statement(s);\n}\n```\n以上语法中，Range 可以是一个数字区间表示 i to j ，或者 i until j。左箭头 <\\- 用于为变量 x 赋值。 \n\n**实例**\n\n```scala\nobject Test {\n   def main(args: Array[String]) {\n      var a = 0;\n      // for 循环\n      for( a <- 1 to 10){\n         println( \"Value of a: \" + a );\n      }\n   }\n}\n```\n输出\n```scala\nvalue of a: 1\nvalue of a: 2\nvalue of a: 3\nvalue of a: 4\nvalue of a: 5\nvalue of a: 6\nvalue of a: 7\nvalue of a: 8\nvalue of a: 9\nvalue of a: 10\n```\n以下是使用了 i until j \n```scala\nobject Test {\n   def main(args: Array[String]) {\n      var a = 0;\n      // for 循环\n      for( a <- 1 until 10){\n         println( \"Value of a: \" + a );\n      }\n   }\n}\n```\n输出\n```\nvalue of a: 1\nvalue of a: 2\nvalue of a: 3\nvalue of a: 4\nvalue of a: 5\nvalue of a: 6\nvalue of a: 7\nvalue of a: 8\nvalue of a: 9\n```\n在 for 循环 中你可以使用分号 (;) 来设置多个区间，它将迭代给定区间所有的可能值。以下实例演示了两个区间的循环实例：\n```\nobject Test {\n   def main(args: Array[String]) {\n      var a = 0;\n      var b = 0;\n      // for 循环\n      for( a <- 1 to 3; b <- 1 to 3){\n         println( \"Value of a: \" + a );\n         println( \"Value of b: \" + b );\n      }\n   }\n}\n```\n\n#### for 循环集合\n```\nobject Test {\n   def main(args: Array[String]) {\n      var a = 0;\n      val numList = List(1,2,3,4,5,6);\n\n      // for 循环\n      for( a <- numList ){\n         println( \"Value of a: \" + a );\n      }\n   }\n}\n------------------------输出\nvalue of a: 1\nvalue of a: 2\nvalue of a: 3\nvalue of a: 4\nvalue of a: 5\nvalue of a: 6\n\n```\n#### for 循环过滤\nScala 可以使用一个或多个 if 语句来过滤一些元素。\n以下是在 for 循环中使用过滤器的语法。 你可以使用分号(;)来为表达式添加一个或多个的过滤条件。\n```\nobject Test {\n   def main(args: Array[String]) {\n      var a = 0;\n      val numList = List(1,2,3,4,5,6,7,8,9,10);\n\n      // for 循环\n      for( a <- numList\n           if a != 3; if a < 8 ){\n         println( \"Value of a: \" + a );\n      }\n   }\n}\n------------------输出-----------\nvalue of a: 1\nvalue of a: 2\nvalue of a: 4\nvalue of a: 5\nvalue of a: 6\nvalue of a: 7\n```\n#### for 使用 yield\n你可以将 for 循环的返回值作为一个变量存储。语法格式如下：\n```\nvar retVal = for{ var x <- List\n     if condition1; if condition2...\n}yield x\n```\n实例\n以下实例演示了 for 循环中使用 yield：\n```\nobject Test {\n   def main(args: Array[String]) {\n      var a = 0;\n      val numList = List(1,2,3,4,5,6,7,8,9,10);\n\n      // for 循环\n      var retVal = for{ a <- numList \n                        if a != 3; if a < 8\n                      }yield a\n\n      // 输出返回值\n      for( a <- retVal){\n         println( \"Value of a: \" + a );\n      }\n   }\n}\n--------------------输出--------------\nvalue of a: 1\nvalue of a: 2\nvalue of a: 4\nvalue of a: 5\nvalue of a: 6\nvalue of a: 7\n\n```\n\n\n\n\n\n\n\n\n\n\n\n## Scala 函数\nScala 有函数和方法，二者在语义上的区别很小。Scala 方法是类的一部分，而函数是一个对象可以赋值给一个变量。换句话来说在类中定义的函数即是方法。\n我们可以在任何地方定义函数，甚至可以在函数内定义函数（内嵌函数）。更重要的一点是 Scala 函数名可以由以下特殊字符：+, ++, ~, &,-, -- , \\, /, : 等。\n\n### 函数声明\n```\ndef functionName ([参数列表]) : [return type]\n``` \n\n### 函数定义\n方法定义由一个def 关键字开始，紧接着是可选的参数列表，一个冒号\"：\" 和方法的返回类型，一个等于号\"=\"，最后是方法的主体。\nScala 函数定义格式如下：\n```\ndef functionName ([参数列表]) : [return type] = {\n   function body\n   return [expr]\n}\n``` \n以上代码中 return type 可以是任意合法的 Scala 数据类型。参数列表中的参数可以使用逗号分隔。\n以下函数的功能是将两个传入的参数相加并求和：\n```\nobject add{\n   def addInt( a:Int, b:Int ) : Int = {\n      var sum:Int = 0\n      sum = a + b\n\n      return sum\n   }\n}\n```  \n**如果函数没有返回值，可以返回为 Unit，这个类似于 Java 的 void**, 实例如下：\n```\nobject Hello{\n   def printMe( ) : Unit = {\n      println(\"Hello, Scala!\")\n   }\n}\n``` \n### 函数调用\n```\nfunctionName( 参数列表 )\n```\n如果函数使用了实例的对象来调用，我们可以使用类似java的格式 (使用 . 号)：\n```\n[instance.]functionName( 参数列表 )\n```\n以下实例演示了定义与调用函数的实例:\n```\nobject Test {\n   def main(args: Array[String]) {\n        println( \"Returned Value : \" + addInt(5,7) );\n   }\n   def addInt( a:Int, b:Int ) : Int = {\n      var sum:Int = 0\n      sum = a + b\n\n      return sum\n   }\n}\n``` \n**++Scala也是一种函数式语言++，所以函数是 Scala 语言的核心。以下一些函数概念有助于我们更好的理解 Scala 编程：**\n\n### 函数传名调用\n传值调用（call-by-value）：先计算参数表达式的值，再应用到函数内部；  \n传名调用（call-by-name）：将未计算的参数表达式直接应用到函数内部\n看例子 \n```scala\npackage com.doggie  \n  \nobject Add {  \n  def addByName(a: Int, b: => Int) = a + b   \n  def addByValue(a: Int, b: Int) = a + b   \n}  \n------------------------\naddByName(2, 2 + 2)  \n->2 + (2 + 2)  \n->2 + 4  \n->6  \n  \naddByValue(2, 2 + 2)  \n->addByValue(2, 4)  \n->2 + 4  \n->6  \n```\n**addByName是传名调用，addByValue是传值调用。语法上可以看出，使用传名调用时，在参数名称和参数类型中间有一个=>符号。**  \n例子： 酒鬼喝酒\n```scala\npackage first.example\n\n/**\n  * Created by Administrator on 2017/3/28.\n  */\nobject CallByName {\n  //最开始拥有的软妹币\n  var money = 10\n  //每天喝掉一个软妹币\n  def drink(): Unit = {\n    money -= 1\n  }\n  //数钱时要算上被喝掉的软妹币\n  def count(): Int = {\n    drink()\n    return money\n  }\n  //每天都数钱\n  def printByName(x: => Int): Unit = {\n    for(i <- 0 until 5)\n      println(\"每天算一算，酒鬼还剩\" + x + \"块钱！\")\n  }\n  //第一天数一下记墙上，以后每天看墙上的余额\n  def printByValue(x: Int): Unit = {\n    for(i <- 0 until 5)\n      println(\"只算第一天，酒鬼还剩\" + x + \"块钱！\")\n  }\n\n  def main(args: Array[String]) = {\n    printByName(count())\n    printByValue(count())\n  }\n}\n\n------------------输出-----------------\n每天算一算，酒鬼还剩9块钱！\n每天算一算，酒鬼还剩8块钱！\n每天算一算，酒鬼还剩7块钱！\n每天算一算，酒鬼还剩6块钱！\n每天算一算，酒鬼还剩5块钱！\n只算第一天，酒鬼还剩4块钱！\n只算第一天，酒鬼还剩4块钱！\n只算第一天，酒鬼还剩4块钱！\n只算第一天，酒鬼还剩4块钱！\n只算第一天，酒鬼还剩4块钱！\n```\n### Scala 指定函数参数名\n一般情况下函数调用参数，就按照函数定义时的参数顺序一个个传递。但是我们也可以通过指定函数参数名，并且不需要按照顺序向函数传递参数，实例如下\n```scala\nobject Test {\n   def main(args: Array[String]) {\n        printInt(b=5, a=7);\n   }\n   def printInt( a:Int, b:Int ) = {\n      println(\"Value of a : \" + a );\n      println(\"Value of b : \" + b );\n   }\n}\n-------------输出--------------------\nValue of a :  7\nValue of b :  5\n```\n### Scala 函数 - 可变参数\nScala 允许你指明函数的最后一个参数可以是重复的，即我们不需要指定函数参数的个数，可以向函数传入可变长度参数列表。\nScala 通过在参数的类型之后放一个星号来设置可变参数(可重复的参数)。例如：\n```scala\nobject Test {\n   def main(args: Array[String]) {\n        printStrings(\"Runoob\", \"Scala\", \"Python\");\n   }\n   def printStrings( args:String* ) = {\n      var i : Int = 0;\n      for( arg <- args ){\n         println(\"Arg value[\" + i + \"] = \" + arg );\n         i = i + 1;\n      }\n   }\n}\n-------------输出-------------\nArg value[0] = Runoob\nArg value[1] = Scala\nArg value[2] = Python\n```\n### Scala 递归函数\n例如计算阶乘\n```scala\nobject Test {\n   def main(args: Array[String]) {\n      for (i <- 1 to 10)\n         println(i + \" 的阶乘为: = \" + factorial(i) )\n   }\n   \n   def factorial(n: BigInt): BigInt = {  \n      if (n <= 1)\n         1  \n      else    \n      n * factorial(n - 1)\n   }\n}\n----------------输出-------------------\n1 的阶乘为: = 1\n2 的阶乘为: = 2\n3 的阶乘为: = 6\n4 的阶乘为: = 24\n5 的阶乘为: = 120\n6 的阶乘为: = 720\n7 的阶乘为: = 5040\n8 的阶乘为: = 40320\n9 的阶乘为: = 362880\n10 的阶乘为: = 3628800\n```\n### Scala 函数 - 默认参数值\nScala 可以为函数参数指定默认参数值，使用了默认参数，你在调用函数的过程中可以不需要传递参数，这时函数就会调用它的默认参数值，如果传递了参数，则传递值会取代默认值。实例如下：\n```scala\nobject Test {\n   def main(args: Array[String]) {\n        println( \"返回值 : \" + addInt() );\n   }\n   def addInt( a:Int=5, b:Int=7 ) : Int = {\n      var sum:Int = 0\n      sum = a + b\n\n      return sum\n   }\n}\n-----------------------------输出-----------------------\n$ scalac Test.scala\n$ scala Test\n返回值 : 12\n```\n\n### scala函数嵌套\nTODO\n\n\n### Scala 偏应用函数  \n\nScala 偏应用函数是一种表达式，你不需要提供函数需要的所有参数，只需要提供部分，或不提供所需参数。\n如下实例，我们打印日志信息：  \n\n实例中，log() 方法接收两个参数：date 和 message。我们在程序执行时调用了三次，参数 date 值都相同，message 不同。\n我们可以使用偏应用函数优化以上方法，绑定第一个 date 参数，第二个参数使用下划线(_)替换缺失的参数列表，并把这个新的函数值的索引的赋给变量。实例修改如下：\n```\n  def main(args: Array[String]) {\n    val date = new Date\n    val logWithDateBound = log(date, _ : String)\n\n    logWithDateBound(\"message1\" )\n    Thread.sleep(1000)\n    logWithDateBound(\"message2\" )\n    Thread.sleep(1000)\n    logWithDateBound(\"message3\" )\n  }\n\n  def log(date: Date, message: String)  = {\n    println(date + \"----\" + message)\n  } \n```\n### Scala匿名函数\nScala 中定义匿名函数的语法很简单，箭头左边是参数列表，右边是函数体。\n使用匿名函数后，我们的代码变得更简洁了。\n下面的表达式就定义了一个接受一个Int类型输入参数的匿名函数:  ,这里可能不好理解，其实可以先暂时放下，先看我另外一篇高阶函数的文章，再过来看匿名函数\n```scala\nvar inc = (x:Int) => x+1\n```  \n上述定义的匿名函数，其实是下面这种写法的简写：\n```scala\ndef add2 = new Function1[Int,Int]{  \n\tdef apply(x:Int):Int = x+1;  \n} \n----------更多参考http://www.runoob.com/scala/anonymous-functions.html----------\n```\n### Scala 高阶函数\n下一篇文章","source":"_posts/scala编程基础.md","raw":"---\ntitle: scala编程基础\ndate: 2017-04-28 21:25:21\ntags: [scala]\ncategories: [programme]\nauthor: kaishun\nid: 51\npermalink: scala-basic\n---\n\n###  多行字符串的表示方法\n多行字符串用三个双引号来表示分隔符，格式为：\"\"\" ... \"\"\"。\n实例如下：\n```scala\nval foo = \"\"\"菜鸟教程\nwww.runoob.com\nwww.w3cschool.cc\nwww.runnoob.com\n以上三个地址都能访问\"\"\"\n```\n## 变量\n### 变量声明\n```\nvar VariableName : DataType [=  Initial Value]\n或\nval VariableName : DataType [=  Initial Value]\n\n变量声明不一定需要初始值，以下也是正确的：\nvar myVar :Int;\nval myVal :String;\n\n```  \n例如\n```\nvar myVar : String = \"Foo\"\nvar myVar : String = \"Too\"\n\nvar myVar :Int;\nval myVal :String;\n```\n\n### 变量类型引用\n在 Scala 中声明变量和常量不一定要指明数据类型，在没有指明数据类型的情况下，其数据类型是通过变量或常量的初始值推断出来的。\n所以，如果在没有指明数据类型的情况下声明变量或常量必须要给出其初始值，否则将会报错。  \n```\nvar myVar = 10;\nval myVal = \"Hello, Scala!\";\n``` \n\n## 访问修饰符\nScala 访问修饰符基本和Java的一样，分别有：private，protected，public。\n如果没有指定访问修饰符符，默认情况下，Scala对象的访问级别都是 public。\n**Scala 中的 private 限定符，比 Java 更严格，在嵌套类情况下，外层类甚至不能访问被嵌套类的私有成员。**\n```\nclass Outer{\n    class Inner{\n    private def f(){println(\"f\")}\n    class InnerMost{\n        f() // 正确\n        }\n    }\n    (new Inner).f() //错误\n}\n``` \n### 作用域保护\nScala中，访问修饰符可以通过使用限定词强调。格式为:\n```\nprivate[x] \n或 \nprotected[x]\n```\n这里的x指代某个所属的包、类或单例对象。如果写成private[x],读作\"这个成员除了对[…]中的类或[…]中的包中的类及它们的伴生对像可见外，对其它所有类都是private。\n这种技巧在横跨了若干包的大型项目中非常有用，它允许你定义一些在你项目的若干子包中可见但对于项目外部的客户却始终不可见的东西。\n```\npackage bobsrocckets{\n    package navigation{\n        private[bobsrockets] class Navigator{\n         protected[navigation] def useStarChart(){}\n         class LegOfJourney{\n             private[Navigator] val distance = 100\n             }\n            private[this] var speed = 200\n            }\n        }\n        package launch{\n        import navigation._\n        object Vehicle{\n        private[launch] val guide = new Navigator\n        }\n    }\n}\n``` \n上述例子中，类Navigator被标记为private[bobsrockets]就是说这个类对包含在bobsrockets包里的所有的类和对象可见。\n比如说，从Vehicle对象里对Navigator的访问是被允许的，因为对象Vehicle包含在包launch中，而launch包在bobsrockets中，相反，所有在包bobsrockets之外的代码都不能访问类Navigator。\n\n## 运算符\n运算符和java的基本类似\n### 位运算符\n```\nA = 0011 1100\n\nB = 0000 1101\n\n-------位运算----------\n\nA&B = 0000 1100\n\nA|B = 0011 1101\n\nA^B = 0011 0001\n\n~A  = 1100 0011\n```\n\n```\nobject Test {\n   def main(args: Array[String]) {\n      var a = 60;           /* 60 = 0011 1100 */  \n      var b = 13;           /* 13 = 0000 1101 */\n      var c = 0;\n\n      c = a & b;            /* 12 = 0000 1100 */ \n      println(\"a & b = \" + c );\n\n      c = a | b;            /* 61 = 0011 1101 */\n      println(\"a | b = \" + c );\n\n      c = a ^ b;            /* 49 = 0011 0001 */\n      println(\"a ^ b = \" + c );\n\n      c = ~a;               /* -61 = 1100 0011 */\n      println(\"~a = \" + c );\n\n      c = a << 2;           /* 240 = 1111 0000 */\n      println(\"a << 2 = \" + c );\n\n      c = a >> 2;           /* 215 = 1111 */\n      println(\"a >> 2  = \" + c );\n\n      c = a >>> 2;          /* 215 = 0000 1111 */\n      println(\"a >>> 2 = \" + c );\n   }\n} \n```\n执行以上代码结果为\n```\n$ scalac Test.scala \n$ scala Test\na & b = 12\na | b = 61\na ^ b = 49\n~a = -61\na << 2 = 240\na >> 2  = 15\na >>> 2 = 15\n\n```\n\n\n## 循环\n### for循环\n\nScala 语言中 for 循环的语法：\n```scala\nfor( var x <- Range ){\n   statement(s);\n}\n```\n以上语法中，Range 可以是一个数字区间表示 i to j ，或者 i until j。左箭头 <\\- 用于为变量 x 赋值。 \n\n**实例**\n\n```scala\nobject Test {\n   def main(args: Array[String]) {\n      var a = 0;\n      // for 循环\n      for( a <- 1 to 10){\n         println( \"Value of a: \" + a );\n      }\n   }\n}\n```\n输出\n```scala\nvalue of a: 1\nvalue of a: 2\nvalue of a: 3\nvalue of a: 4\nvalue of a: 5\nvalue of a: 6\nvalue of a: 7\nvalue of a: 8\nvalue of a: 9\nvalue of a: 10\n```\n以下是使用了 i until j \n```scala\nobject Test {\n   def main(args: Array[String]) {\n      var a = 0;\n      // for 循环\n      for( a <- 1 until 10){\n         println( \"Value of a: \" + a );\n      }\n   }\n}\n```\n输出\n```\nvalue of a: 1\nvalue of a: 2\nvalue of a: 3\nvalue of a: 4\nvalue of a: 5\nvalue of a: 6\nvalue of a: 7\nvalue of a: 8\nvalue of a: 9\n```\n在 for 循环 中你可以使用分号 (;) 来设置多个区间，它将迭代给定区间所有的可能值。以下实例演示了两个区间的循环实例：\n```\nobject Test {\n   def main(args: Array[String]) {\n      var a = 0;\n      var b = 0;\n      // for 循环\n      for( a <- 1 to 3; b <- 1 to 3){\n         println( \"Value of a: \" + a );\n         println( \"Value of b: \" + b );\n      }\n   }\n}\n```\n\n#### for 循环集合\n```\nobject Test {\n   def main(args: Array[String]) {\n      var a = 0;\n      val numList = List(1,2,3,4,5,6);\n\n      // for 循环\n      for( a <- numList ){\n         println( \"Value of a: \" + a );\n      }\n   }\n}\n------------------------输出\nvalue of a: 1\nvalue of a: 2\nvalue of a: 3\nvalue of a: 4\nvalue of a: 5\nvalue of a: 6\n\n```\n#### for 循环过滤\nScala 可以使用一个或多个 if 语句来过滤一些元素。\n以下是在 for 循环中使用过滤器的语法。 你可以使用分号(;)来为表达式添加一个或多个的过滤条件。\n```\nobject Test {\n   def main(args: Array[String]) {\n      var a = 0;\n      val numList = List(1,2,3,4,5,6,7,8,9,10);\n\n      // for 循环\n      for( a <- numList\n           if a != 3; if a < 8 ){\n         println( \"Value of a: \" + a );\n      }\n   }\n}\n------------------输出-----------\nvalue of a: 1\nvalue of a: 2\nvalue of a: 4\nvalue of a: 5\nvalue of a: 6\nvalue of a: 7\n```\n#### for 使用 yield\n你可以将 for 循环的返回值作为一个变量存储。语法格式如下：\n```\nvar retVal = for{ var x <- List\n     if condition1; if condition2...\n}yield x\n```\n实例\n以下实例演示了 for 循环中使用 yield：\n```\nobject Test {\n   def main(args: Array[String]) {\n      var a = 0;\n      val numList = List(1,2,3,4,5,6,7,8,9,10);\n\n      // for 循环\n      var retVal = for{ a <- numList \n                        if a != 3; if a < 8\n                      }yield a\n\n      // 输出返回值\n      for( a <- retVal){\n         println( \"Value of a: \" + a );\n      }\n   }\n}\n--------------------输出--------------\nvalue of a: 1\nvalue of a: 2\nvalue of a: 4\nvalue of a: 5\nvalue of a: 6\nvalue of a: 7\n\n```\n\n\n\n\n\n\n\n\n\n\n\n## Scala 函数\nScala 有函数和方法，二者在语义上的区别很小。Scala 方法是类的一部分，而函数是一个对象可以赋值给一个变量。换句话来说在类中定义的函数即是方法。\n我们可以在任何地方定义函数，甚至可以在函数内定义函数（内嵌函数）。更重要的一点是 Scala 函数名可以由以下特殊字符：+, ++, ~, &,-, -- , \\, /, : 等。\n\n### 函数声明\n```\ndef functionName ([参数列表]) : [return type]\n``` \n\n### 函数定义\n方法定义由一个def 关键字开始，紧接着是可选的参数列表，一个冒号\"：\" 和方法的返回类型，一个等于号\"=\"，最后是方法的主体。\nScala 函数定义格式如下：\n```\ndef functionName ([参数列表]) : [return type] = {\n   function body\n   return [expr]\n}\n``` \n以上代码中 return type 可以是任意合法的 Scala 数据类型。参数列表中的参数可以使用逗号分隔。\n以下函数的功能是将两个传入的参数相加并求和：\n```\nobject add{\n   def addInt( a:Int, b:Int ) : Int = {\n      var sum:Int = 0\n      sum = a + b\n\n      return sum\n   }\n}\n```  \n**如果函数没有返回值，可以返回为 Unit，这个类似于 Java 的 void**, 实例如下：\n```\nobject Hello{\n   def printMe( ) : Unit = {\n      println(\"Hello, Scala!\")\n   }\n}\n``` \n### 函数调用\n```\nfunctionName( 参数列表 )\n```\n如果函数使用了实例的对象来调用，我们可以使用类似java的格式 (使用 . 号)：\n```\n[instance.]functionName( 参数列表 )\n```\n以下实例演示了定义与调用函数的实例:\n```\nobject Test {\n   def main(args: Array[String]) {\n        println( \"Returned Value : \" + addInt(5,7) );\n   }\n   def addInt( a:Int, b:Int ) : Int = {\n      var sum:Int = 0\n      sum = a + b\n\n      return sum\n   }\n}\n``` \n**++Scala也是一种函数式语言++，所以函数是 Scala 语言的核心。以下一些函数概念有助于我们更好的理解 Scala 编程：**\n\n### 函数传名调用\n传值调用（call-by-value）：先计算参数表达式的值，再应用到函数内部；  \n传名调用（call-by-name）：将未计算的参数表达式直接应用到函数内部\n看例子 \n```scala\npackage com.doggie  \n  \nobject Add {  \n  def addByName(a: Int, b: => Int) = a + b   \n  def addByValue(a: Int, b: Int) = a + b   \n}  \n------------------------\naddByName(2, 2 + 2)  \n->2 + (2 + 2)  \n->2 + 4  \n->6  \n  \naddByValue(2, 2 + 2)  \n->addByValue(2, 4)  \n->2 + 4  \n->6  \n```\n**addByName是传名调用，addByValue是传值调用。语法上可以看出，使用传名调用时，在参数名称和参数类型中间有一个=>符号。**  \n例子： 酒鬼喝酒\n```scala\npackage first.example\n\n/**\n  * Created by Administrator on 2017/3/28.\n  */\nobject CallByName {\n  //最开始拥有的软妹币\n  var money = 10\n  //每天喝掉一个软妹币\n  def drink(): Unit = {\n    money -= 1\n  }\n  //数钱时要算上被喝掉的软妹币\n  def count(): Int = {\n    drink()\n    return money\n  }\n  //每天都数钱\n  def printByName(x: => Int): Unit = {\n    for(i <- 0 until 5)\n      println(\"每天算一算，酒鬼还剩\" + x + \"块钱！\")\n  }\n  //第一天数一下记墙上，以后每天看墙上的余额\n  def printByValue(x: Int): Unit = {\n    for(i <- 0 until 5)\n      println(\"只算第一天，酒鬼还剩\" + x + \"块钱！\")\n  }\n\n  def main(args: Array[String]) = {\n    printByName(count())\n    printByValue(count())\n  }\n}\n\n------------------输出-----------------\n每天算一算，酒鬼还剩9块钱！\n每天算一算，酒鬼还剩8块钱！\n每天算一算，酒鬼还剩7块钱！\n每天算一算，酒鬼还剩6块钱！\n每天算一算，酒鬼还剩5块钱！\n只算第一天，酒鬼还剩4块钱！\n只算第一天，酒鬼还剩4块钱！\n只算第一天，酒鬼还剩4块钱！\n只算第一天，酒鬼还剩4块钱！\n只算第一天，酒鬼还剩4块钱！\n```\n### Scala 指定函数参数名\n一般情况下函数调用参数，就按照函数定义时的参数顺序一个个传递。但是我们也可以通过指定函数参数名，并且不需要按照顺序向函数传递参数，实例如下\n```scala\nobject Test {\n   def main(args: Array[String]) {\n        printInt(b=5, a=7);\n   }\n   def printInt( a:Int, b:Int ) = {\n      println(\"Value of a : \" + a );\n      println(\"Value of b : \" + b );\n   }\n}\n-------------输出--------------------\nValue of a :  7\nValue of b :  5\n```\n### Scala 函数 - 可变参数\nScala 允许你指明函数的最后一个参数可以是重复的，即我们不需要指定函数参数的个数，可以向函数传入可变长度参数列表。\nScala 通过在参数的类型之后放一个星号来设置可变参数(可重复的参数)。例如：\n```scala\nobject Test {\n   def main(args: Array[String]) {\n        printStrings(\"Runoob\", \"Scala\", \"Python\");\n   }\n   def printStrings( args:String* ) = {\n      var i : Int = 0;\n      for( arg <- args ){\n         println(\"Arg value[\" + i + \"] = \" + arg );\n         i = i + 1;\n      }\n   }\n}\n-------------输出-------------\nArg value[0] = Runoob\nArg value[1] = Scala\nArg value[2] = Python\n```\n### Scala 递归函数\n例如计算阶乘\n```scala\nobject Test {\n   def main(args: Array[String]) {\n      for (i <- 1 to 10)\n         println(i + \" 的阶乘为: = \" + factorial(i) )\n   }\n   \n   def factorial(n: BigInt): BigInt = {  \n      if (n <= 1)\n         1  \n      else    \n      n * factorial(n - 1)\n   }\n}\n----------------输出-------------------\n1 的阶乘为: = 1\n2 的阶乘为: = 2\n3 的阶乘为: = 6\n4 的阶乘为: = 24\n5 的阶乘为: = 120\n6 的阶乘为: = 720\n7 的阶乘为: = 5040\n8 的阶乘为: = 40320\n9 的阶乘为: = 362880\n10 的阶乘为: = 3628800\n```\n### Scala 函数 - 默认参数值\nScala 可以为函数参数指定默认参数值，使用了默认参数，你在调用函数的过程中可以不需要传递参数，这时函数就会调用它的默认参数值，如果传递了参数，则传递值会取代默认值。实例如下：\n```scala\nobject Test {\n   def main(args: Array[String]) {\n        println( \"返回值 : \" + addInt() );\n   }\n   def addInt( a:Int=5, b:Int=7 ) : Int = {\n      var sum:Int = 0\n      sum = a + b\n\n      return sum\n   }\n}\n-----------------------------输出-----------------------\n$ scalac Test.scala\n$ scala Test\n返回值 : 12\n```\n\n### scala函数嵌套\nTODO\n\n\n### Scala 偏应用函数  \n\nScala 偏应用函数是一种表达式，你不需要提供函数需要的所有参数，只需要提供部分，或不提供所需参数。\n如下实例，我们打印日志信息：  \n\n实例中，log() 方法接收两个参数：date 和 message。我们在程序执行时调用了三次，参数 date 值都相同，message 不同。\n我们可以使用偏应用函数优化以上方法，绑定第一个 date 参数，第二个参数使用下划线(_)替换缺失的参数列表，并把这个新的函数值的索引的赋给变量。实例修改如下：\n```\n  def main(args: Array[String]) {\n    val date = new Date\n    val logWithDateBound = log(date, _ : String)\n\n    logWithDateBound(\"message1\" )\n    Thread.sleep(1000)\n    logWithDateBound(\"message2\" )\n    Thread.sleep(1000)\n    logWithDateBound(\"message3\" )\n  }\n\n  def log(date: Date, message: String)  = {\n    println(date + \"----\" + message)\n  } \n```\n### Scala匿名函数\nScala 中定义匿名函数的语法很简单，箭头左边是参数列表，右边是函数体。\n使用匿名函数后，我们的代码变得更简洁了。\n下面的表达式就定义了一个接受一个Int类型输入参数的匿名函数:  ,这里可能不好理解，其实可以先暂时放下，先看我另外一篇高阶函数的文章，再过来看匿名函数\n```scala\nvar inc = (x:Int) => x+1\n```  \n上述定义的匿名函数，其实是下面这种写法的简写：\n```scala\ndef add2 = new Function1[Int,Int]{  \n\tdef apply(x:Int):Int = x+1;  \n} \n----------更多参考http://www.runoob.com/scala/anonymous-functions.html----------\n```\n### Scala 高阶函数\n下一篇文章","slug":"scala-basic","published":1,"updated":"2018-01-22T15:54:56.256Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcrpnzpt001y2wv3r9verdd9","content":"<h3 id=\"多行字符串的表示方法\"><a href=\"#多行字符串的表示方法\" class=\"headerlink\" title=\"多行字符串的表示方法\"></a>多行字符串的表示方法</h3><p>多行字符串用三个双引号来表示分隔符，格式为：””” … “””。<br>实例如下：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">val</span> foo = <span class=\"string\">\"\"</span><span class=\"string\">\"菜鸟教程</span></div><div class=\"line\"><span class=\"string\">www.runoob.com</span></div><div class=\"line\"><span class=\"string\">www.w3cschool.cc</span></div><div class=\"line\"><span class=\"string\">www.runnoob.com</span></div><div class=\"line\"><span class=\"string\">以上三个地址都能访问\"</span><span class=\"string\">\"\"</span></div></pre></td></tr></table></figure></p>\n<h2 id=\"变量\"><a href=\"#变量\" class=\"headerlink\" title=\"变量\"></a>变量</h2><h3 id=\"变量声明\"><a href=\"#变量声明\" class=\"headerlink\" title=\"变量声明\"></a>变量声明</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">var VariableName : DataType [=  Initial Value]</div><div class=\"line\">或</div><div class=\"line\">val VariableName : DataType [=  Initial Value]</div><div class=\"line\"></div><div class=\"line\">变量声明不一定需要初始值，以下也是正确的：</div><div class=\"line\">var myVar :Int;</div><div class=\"line\">val myVal :String;</div><div class=\"line\"></div><div class=\"line\">```  </div><div class=\"line\">例如</div></pre></td></tr></table></figure>\n<p>var myVar : String = “Foo”<br>var myVar : String = “Too”</p>\n<p>var myVar :Int;<br>val myVal :String;<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\">### 变量类型引用</div><div class=\"line\">在 Scala 中声明变量和常量不一定要指明数据类型，在没有指明数据类型的情况下，其数据类型是通过变量或常量的初始值推断出来的。</div><div class=\"line\">所以，如果在没有指明数据类型的情况下声明变量或常量必须要给出其初始值，否则将会报错。</div></pre></td></tr></table></figure></p>\n<p>var myVar = 10;<br>val myVal = “Hello, Scala!”;<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\">## 访问修饰符</div><div class=\"line\">Scala 访问修饰符基本和Java的一样，分别有：private，protected，public。</div><div class=\"line\">如果没有指定访问修饰符符，默认情况下，Scala对象的访问级别都是 public。</div><div class=\"line\">**Scala 中的 private 限定符，比 Java 更严格，在嵌套类情况下，外层类甚至不能访问被嵌套类的私有成员。**</div></pre></td></tr></table></figure></p>\n<p>class Outer{<br>    class Inner{<br>    private def f(){println(“f”)}<br>    class InnerMost{<br>        f() // 正确<br>        }<br>    }<br>    (new Inner).f() //错误<br>}<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">### 作用域保护</div><div class=\"line\">Scala中，访问修饰符可以通过使用限定词强调。格式为:</div></pre></td></tr></table></figure></p>\n<p>private[x]<br>或<br>protected[x]<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">这里的x指代某个所属的包、类或单例对象。如果写成private[x],读作&quot;这个成员除了对[…]中的类或[…]中的包中的类及它们的伴生对像可见外，对其它所有类都是private。</div><div class=\"line\">这种技巧在横跨了若干包的大型项目中非常有用，它允许你定义一些在你项目的若干子包中可见但对于项目外部的客户却始终不可见的东西。</div></pre></td></tr></table></figure></p>\n<p>package bobsrocckets{<br>    package navigation{<br>        private[bobsrockets] class Navigator{<br>         protected[navigation] def useStarChart(){}<br>         class LegOfJourney{<br>             private[Navigator] val distance = 100<br>             }<br>            private[this] var speed = 200<br>            }<br>        }<br>        package launch{<br>        import navigation._<br>        object Vehicle{<br>        private[launch] val guide = new Navigator<br>        }<br>    }<br>}<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">上述例子中，类Navigator被标记为private[bobsrockets]就是说这个类对包含在bobsrockets包里的所有的类和对象可见。</div><div class=\"line\">比如说，从Vehicle对象里对Navigator的访问是被允许的，因为对象Vehicle包含在包launch中，而launch包在bobsrockets中，相反，所有在包bobsrockets之外的代码都不能访问类Navigator。</div><div class=\"line\"></div><div class=\"line\">## 运算符</div><div class=\"line\">运算符和java的基本类似</div><div class=\"line\">### 位运算符</div></pre></td></tr></table></figure></p>\n<p>A = 0011 1100</p>\n<p>B = 0000 1101</p>\n<p>——-位运算———-</p>\n<p>A&amp;B = 0000 1100</p>\n<p>A|B = 0011 1101</p>\n<p>A^B = 0011 0001</p>\n<p>~A  = 1100 0011<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\"></div></pre></td></tr></table></figure></p>\n<p>object Test {<br>   def main(args: Array[String]) {<br>      var a = 60;           /<em> 60 = 0011 1100 </em>/<br>      var b = 13;           /<em> 13 = 0000 1101 </em>/<br>      var c = 0;</p>\n<pre><code>c = a &amp; b;            /* 12 = 0000 1100 */ \nprintln(&quot;a &amp; b = &quot; + c );\n\nc = a | b;            /* 61 = 0011 1101 */\nprintln(&quot;a | b = &quot; + c );\n\nc = a ^ b;            /* 49 = 0011 0001 */\nprintln(&quot;a ^ b = &quot; + c );\n\nc = ~a;               /* -61 = 1100 0011 */\nprintln(&quot;~a = &quot; + c );\n\nc = a &lt;&lt; 2;           /* 240 = 1111 0000 */\nprintln(&quot;a &lt;&lt; 2 = &quot; + c );\n\nc = a &gt;&gt; 2;           /* 215 = 1111 */\nprintln(&quot;a &gt;&gt; 2  = &quot; + c );\n\nc = a &gt;&gt;&gt; 2;          /* 215 = 0000 1111 */\nprintln(&quot;a &gt;&gt;&gt; 2 = &quot; + c );\n</code></pre><p>   }<br>}<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">执行以上代码结果为</div></pre></td></tr></table></figure></p>\n<p>$ scalac Test.scala<br>$ scala Test<br>a &amp; b = 12<br>a | b = 61<br>a ^ b = 49<br>~a = -61<br>a &lt;&lt; 2 = 240<br>a &gt;&gt; 2  = 15<br>a &gt;&gt;&gt; 2 = 15</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">## 循环</div><div class=\"line\">### for循环</div><div class=\"line\"></div><div class=\"line\">Scala 语言中 for 循环的语法：</div><div class=\"line\">```scala</div><div class=\"line\">for( var x &lt;- Range )&#123;</div><div class=\"line\">   statement(s);</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>以上语法中，Range 可以是一个数字区间表示 i to j ，或者 i until j。左箭头 &lt;- 用于为变量 x 赋值。 </p>\n<p><strong>实例</strong></p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Test</span> </span>&#123;</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">      <span class=\"keyword\">var</span> a = <span class=\"number\">0</span>;</div><div class=\"line\">      <span class=\"comment\">// for 循环</span></div><div class=\"line\">      <span class=\"keyword\">for</span>( a &lt;- <span class=\"number\">1</span> to <span class=\"number\">10</span>)&#123;</div><div class=\"line\">         println( <span class=\"string\">\"Value of a: \"</span> + a );</div><div class=\"line\">      &#125;</div><div class=\"line\">   &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>输出<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">value of a: <span class=\"number\">1</span></div><div class=\"line\">value of a: <span class=\"number\">2</span></div><div class=\"line\">value of a: <span class=\"number\">3</span></div><div class=\"line\">value of a: <span class=\"number\">4</span></div><div class=\"line\">value of a: <span class=\"number\">5</span></div><div class=\"line\">value of a: <span class=\"number\">6</span></div><div class=\"line\">value of a: <span class=\"number\">7</span></div><div class=\"line\">value of a: <span class=\"number\">8</span></div><div class=\"line\">value of a: <span class=\"number\">9</span></div><div class=\"line\">value of a: <span class=\"number\">10</span></div></pre></td></tr></table></figure></p>\n<p>以下是使用了 i until j<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Test</span> </span>&#123;</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">      <span class=\"keyword\">var</span> a = <span class=\"number\">0</span>;</div><div class=\"line\">      <span class=\"comment\">// for 循环</span></div><div class=\"line\">      <span class=\"keyword\">for</span>( a &lt;- <span class=\"number\">1</span> until <span class=\"number\">10</span>)&#123;</div><div class=\"line\">         println( <span class=\"string\">\"Value of a: \"</span> + a );</div><div class=\"line\">      &#125;</div><div class=\"line\">   &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>输出<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">value of a: 1</div><div class=\"line\">value of a: 2</div><div class=\"line\">value of a: 3</div><div class=\"line\">value of a: 4</div><div class=\"line\">value of a: 5</div><div class=\"line\">value of a: 6</div><div class=\"line\">value of a: 7</div><div class=\"line\">value of a: 8</div><div class=\"line\">value of a: 9</div></pre></td></tr></table></figure></p>\n<p>在 for 循环 中你可以使用分号 (;) 来设置多个区间，它将迭代给定区间所有的可能值。以下实例演示了两个区间的循环实例：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">object Test &#123;</div><div class=\"line\">   def main(args: Array[String]) &#123;</div><div class=\"line\">      var a = 0;</div><div class=\"line\">      var b = 0;</div><div class=\"line\">      // for 循环</div><div class=\"line\">      for( a &lt;- 1 to 3; b &lt;- 1 to 3)&#123;</div><div class=\"line\">         println( &quot;Value of a: &quot; + a );</div><div class=\"line\">         println( &quot;Value of b: &quot; + b );</div><div class=\"line\">      &#125;</div><div class=\"line\">   &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h4 id=\"for-循环集合\"><a href=\"#for-循环集合\" class=\"headerlink\" title=\"for 循环集合\"></a>for 循环集合</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div></pre></td><td class=\"code\"><pre><div class=\"line\">object Test &#123;</div><div class=\"line\">   def main(args: Array[String]) &#123;</div><div class=\"line\">      var a = 0;</div><div class=\"line\">      val numList = List(1,2,3,4,5,6);</div><div class=\"line\"></div><div class=\"line\">      // for 循环</div><div class=\"line\">      for( a &lt;- numList )&#123;</div><div class=\"line\">         println( &quot;Value of a: &quot; + a );</div><div class=\"line\">      &#125;</div><div class=\"line\">   &#125;</div><div class=\"line\">&#125;</div><div class=\"line\">------------------------输出</div><div class=\"line\">value of a: 1</div><div class=\"line\">value of a: 2</div><div class=\"line\">value of a: 3</div><div class=\"line\">value of a: 4</div><div class=\"line\">value of a: 5</div><div class=\"line\">value of a: 6</div></pre></td></tr></table></figure>\n<h4 id=\"for-循环过滤\"><a href=\"#for-循环过滤\" class=\"headerlink\" title=\"for 循环过滤\"></a>for 循环过滤</h4><p>Scala 可以使用一个或多个 if 语句来过滤一些元素。<br>以下是在 for 循环中使用过滤器的语法。 你可以使用分号(;)来为表达式添加一个或多个的过滤条件。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div></pre></td><td class=\"code\"><pre><div class=\"line\">object Test &#123;</div><div class=\"line\">   def main(args: Array[String]) &#123;</div><div class=\"line\">      var a = 0;</div><div class=\"line\">      val numList = List(1,2,3,4,5,6,7,8,9,10);</div><div class=\"line\"></div><div class=\"line\">      // for 循环</div><div class=\"line\">      for( a &lt;- numList</div><div class=\"line\">           if a != 3; if a &lt; 8 )&#123;</div><div class=\"line\">         println( &quot;Value of a: &quot; + a );</div><div class=\"line\">      &#125;</div><div class=\"line\">   &#125;</div><div class=\"line\">&#125;</div><div class=\"line\">------------------输出-----------</div><div class=\"line\">value of a: 1</div><div class=\"line\">value of a: 2</div><div class=\"line\">value of a: 4</div><div class=\"line\">value of a: 5</div><div class=\"line\">value of a: 6</div><div class=\"line\">value of a: 7</div></pre></td></tr></table></figure></p>\n<h4 id=\"for-使用-yield\"><a href=\"#for-使用-yield\" class=\"headerlink\" title=\"for 使用 yield\"></a>for 使用 yield</h4><p>你可以将 for 循环的返回值作为一个变量存储。语法格式如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">var retVal = for&#123; var x &lt;- List</div><div class=\"line\">     if condition1; if condition2...</div><div class=\"line\">&#125;yield x</div></pre></td></tr></table></figure></p>\n<p>实例<br>以下实例演示了 for 循环中使用 yield：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div></pre></td><td class=\"code\"><pre><div class=\"line\">object Test &#123;</div><div class=\"line\">   def main(args: Array[String]) &#123;</div><div class=\"line\">      var a = 0;</div><div class=\"line\">      val numList = List(1,2,3,4,5,6,7,8,9,10);</div><div class=\"line\"></div><div class=\"line\">      // for 循环</div><div class=\"line\">      var retVal = for&#123; a &lt;- numList </div><div class=\"line\">                        if a != 3; if a &lt; 8</div><div class=\"line\">                      &#125;yield a</div><div class=\"line\"></div><div class=\"line\">      // 输出返回值</div><div class=\"line\">      for( a &lt;- retVal)&#123;</div><div class=\"line\">         println( &quot;Value of a: &quot; + a );</div><div class=\"line\">      &#125;</div><div class=\"line\">   &#125;</div><div class=\"line\">&#125;</div><div class=\"line\">--------------------输出--------------</div><div class=\"line\">value of a: 1</div><div class=\"line\">value of a: 2</div><div class=\"line\">value of a: 4</div><div class=\"line\">value of a: 5</div><div class=\"line\">value of a: 6</div><div class=\"line\">value of a: 7</div></pre></td></tr></table></figure></p>\n<h2 id=\"Scala-函数\"><a href=\"#Scala-函数\" class=\"headerlink\" title=\"Scala 函数\"></a>Scala 函数</h2><p>Scala 有函数和方法，二者在语义上的区别很小。Scala 方法是类的一部分，而函数是一个对象可以赋值给一个变量。换句话来说在类中定义的函数即是方法。<br>我们可以在任何地方定义函数，甚至可以在函数内定义函数（内嵌函数）。更重要的一点是 Scala 函数名可以由以下特殊字符：+, ++, ~, &amp;,-, – , \\, /, : 等。</p>\n<h3 id=\"函数声明\"><a href=\"#函数声明\" class=\"headerlink\" title=\"函数声明\"></a>函数声明</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">def functionName ([参数列表]) : [return type]</div><div class=\"line\">``` </div><div class=\"line\"></div><div class=\"line\">### 函数定义</div><div class=\"line\">方法定义由一个def 关键字开始，紧接着是可选的参数列表，一个冒号&quot;：&quot; 和方法的返回类型，一个等于号&quot;=&quot;，最后是方法的主体。</div><div class=\"line\">Scala 函数定义格式如下：</div></pre></td></tr></table></figure>\n<p>def functionName ([参数列表]) : [return type] = {<br>   function body<br>   return [expr]<br>}<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">以上代码中 return type 可以是任意合法的 Scala 数据类型。参数列表中的参数可以使用逗号分隔。</div><div class=\"line\">以下函数的功能是将两个传入的参数相加并求和：</div></pre></td></tr></table></figure></p>\n<p>object add{<br>   def addInt( a:Int, b:Int ) : Int = {<br>      var sum:Int = 0<br>      sum = a + b</p>\n<pre><code>return sum\n</code></pre><p>   }<br>}<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">**如果函数没有返回值，可以返回为 Unit，这个类似于 Java 的 void**, 实例如下：</div></pre></td></tr></table></figure></p>\n<p>object Hello{<br>   def printMe( ) : Unit = {<br>      println(“Hello, Scala!”)<br>   }<br>}<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">### 函数调用</div></pre></td></tr></table></figure></p>\n<p>functionName( 参数列表 )<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">如果函数使用了实例的对象来调用，我们可以使用类似java的格式 (使用 . 号)：</div></pre></td></tr></table></figure></p>\n<p>[instance.]functionName( 参数列表 )<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">以下实例演示了定义与调用函数的实例:</div></pre></td></tr></table></figure></p>\n<p>object Test {<br>   def main(args: Array[String]) {<br>        println( “Returned Value : “ + addInt(5,7) );<br>   }<br>   def addInt( a:Int, b:Int ) : Int = {<br>      var sum:Int = 0<br>      sum = a + b</p>\n<pre><code>return sum\n</code></pre><p>   }<br>}<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div></pre></td><td class=\"code\"><pre><div class=\"line\">**++Scala也是一种函数式语言++，所以函数是 Scala 语言的核心。以下一些函数概念有助于我们更好的理解 Scala 编程：**</div><div class=\"line\"></div><div class=\"line\">### 函数传名调用</div><div class=\"line\">传值调用（call-by-value）：先计算参数表达式的值，再应用到函数内部；  </div><div class=\"line\">传名调用（call-by-name）：将未计算的参数表达式直接应用到函数内部</div><div class=\"line\">看例子 </div><div class=\"line\">```scala</div><div class=\"line\">package com.doggie  </div><div class=\"line\">  </div><div class=\"line\">object Add &#123;  </div><div class=\"line\">  def addByName(a: Int, b: =&gt; Int) = a + b   </div><div class=\"line\">  def addByValue(a: Int, b: Int) = a + b   </div><div class=\"line\">&#125;  </div><div class=\"line\">------------------------</div><div class=\"line\">addByName(2, 2 + 2)  </div><div class=\"line\">-&gt;2 + (2 + 2)  </div><div class=\"line\">-&gt;2 + 4  </div><div class=\"line\">-&gt;6  </div><div class=\"line\">  </div><div class=\"line\">addByValue(2, 2 + 2)  </div><div class=\"line\">-&gt;addByValue(2, 4)  </div><div class=\"line\">-&gt;2 + 4  </div><div class=\"line\">-&gt;6</div></pre></td></tr></table></figure></p>\n<p><strong>addByName是传名调用，addByValue是传值调用。语法上可以看出，使用传名调用时，在参数名称和参数类型中间有一个=&gt;符号。</strong><br>例子： 酒鬼喝酒<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">package</span> first.example</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\">  * Created by Administrator on 2017/3/28.</span></div><div class=\"line\"><span class=\"comment\">  */</span></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">CallByName</span> </span>&#123;</div><div class=\"line\">  <span class=\"comment\">//最开始拥有的软妹币</span></div><div class=\"line\">  <span class=\"keyword\">var</span> money = <span class=\"number\">10</span></div><div class=\"line\">  <span class=\"comment\">//每天喝掉一个软妹币</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">drink</span></span>(): <span class=\"type\">Unit</span> = &#123;</div><div class=\"line\">    money -= <span class=\"number\">1</span></div><div class=\"line\">  &#125;</div><div class=\"line\">  <span class=\"comment\">//数钱时要算上被喝掉的软妹币</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">count</span></span>(): <span class=\"type\">Int</span> = &#123;</div><div class=\"line\">    drink()</div><div class=\"line\">    <span class=\"keyword\">return</span> money</div><div class=\"line\">  &#125;</div><div class=\"line\">  <span class=\"comment\">//每天都数钱</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">printByName</span></span>(x: =&gt; <span class=\"type\">Int</span>): <span class=\"type\">Unit</span> = &#123;</div><div class=\"line\">    <span class=\"keyword\">for</span>(i &lt;- <span class=\"number\">0</span> until <span class=\"number\">5</span>)</div><div class=\"line\">      println(<span class=\"string\">\"每天算一算，酒鬼还剩\"</span> + x + <span class=\"string\">\"块钱！\"</span>)</div><div class=\"line\">  &#125;</div><div class=\"line\">  <span class=\"comment\">//第一天数一下记墙上，以后每天看墙上的余额</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">printByValue</span></span>(x: <span class=\"type\">Int</span>): <span class=\"type\">Unit</span> = &#123;</div><div class=\"line\">    <span class=\"keyword\">for</span>(i &lt;- <span class=\"number\">0</span> until <span class=\"number\">5</span>)</div><div class=\"line\">      println(<span class=\"string\">\"只算第一天，酒鬼还剩\"</span> + x + <span class=\"string\">\"块钱！\"</span>)</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) = &#123;</div><div class=\"line\">    printByName(count())</div><div class=\"line\">    printByValue(count())</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">------------------输出-----------------</div><div class=\"line\">每天算一算，酒鬼还剩<span class=\"number\">9</span>块钱！</div><div class=\"line\">每天算一算，酒鬼还剩<span class=\"number\">8</span>块钱！</div><div class=\"line\">每天算一算，酒鬼还剩<span class=\"number\">7</span>块钱！</div><div class=\"line\">每天算一算，酒鬼还剩<span class=\"number\">6</span>块钱！</div><div class=\"line\">每天算一算，酒鬼还剩<span class=\"number\">5</span>块钱！</div><div class=\"line\">只算第一天，酒鬼还剩<span class=\"number\">4</span>块钱！</div><div class=\"line\">只算第一天，酒鬼还剩<span class=\"number\">4</span>块钱！</div><div class=\"line\">只算第一天，酒鬼还剩<span class=\"number\">4</span>块钱！</div><div class=\"line\">只算第一天，酒鬼还剩<span class=\"number\">4</span>块钱！</div><div class=\"line\">只算第一天，酒鬼还剩<span class=\"number\">4</span>块钱！</div></pre></td></tr></table></figure></p>\n<h3 id=\"Scala-指定函数参数名\"><a href=\"#Scala-指定函数参数名\" class=\"headerlink\" title=\"Scala 指定函数参数名\"></a>Scala 指定函数参数名</h3><p>一般情况下函数调用参数，就按照函数定义时的参数顺序一个个传递。但是我们也可以通过指定函数参数名，并且不需要按照顺序向函数传递参数，实例如下<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Test</span> </span>&#123;</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">        printInt(b=<span class=\"number\">5</span>, a=<span class=\"number\">7</span>);</div><div class=\"line\">   &#125;</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">printInt</span></span>( a:<span class=\"type\">Int</span>, b:<span class=\"type\">Int</span> ) = &#123;</div><div class=\"line\">      println(<span class=\"string\">\"Value of a : \"</span> + a );</div><div class=\"line\">      println(<span class=\"string\">\"Value of b : \"</span> + b );</div><div class=\"line\">   &#125;</div><div class=\"line\">&#125;</div><div class=\"line\">-------------输出--------------------</div><div class=\"line\"><span class=\"type\">Value</span> of a :  <span class=\"number\">7</span></div><div class=\"line\"><span class=\"type\">Value</span> of b :  <span class=\"number\">5</span></div></pre></td></tr></table></figure></p>\n<h3 id=\"Scala-函数-可变参数\"><a href=\"#Scala-函数-可变参数\" class=\"headerlink\" title=\"Scala 函数 - 可变参数\"></a>Scala 函数 - 可变参数</h3><p>Scala 允许你指明函数的最后一个参数可以是重复的，即我们不需要指定函数参数的个数，可以向函数传入可变长度参数列表。<br>Scala 通过在参数的类型之后放一个星号来设置可变参数(可重复的参数)。例如：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Test</span> </span>&#123;</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">        printStrings(<span class=\"string\">\"Runoob\"</span>, <span class=\"string\">\"Scala\"</span>, <span class=\"string\">\"Python\"</span>);</div><div class=\"line\">   &#125;</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">printStrings</span></span>( args:<span class=\"type\">String</span>* ) = &#123;</div><div class=\"line\">      <span class=\"keyword\">var</span> i : <span class=\"type\">Int</span> = <span class=\"number\">0</span>;</div><div class=\"line\">      <span class=\"keyword\">for</span>( arg &lt;- args )&#123;</div><div class=\"line\">         println(<span class=\"string\">\"Arg value[\"</span> + i + <span class=\"string\">\"] = \"</span> + arg );</div><div class=\"line\">         i = i + <span class=\"number\">1</span>;</div><div class=\"line\">      &#125;</div><div class=\"line\">   &#125;</div><div class=\"line\">&#125;</div><div class=\"line\">-------------输出-------------</div><div class=\"line\"><span class=\"type\">Arg</span> value[<span class=\"number\">0</span>] = <span class=\"type\">Runoob</span></div><div class=\"line\"><span class=\"type\">Arg</span> value[<span class=\"number\">1</span>] = <span class=\"type\">Scala</span></div><div class=\"line\"><span class=\"type\">Arg</span> value[<span class=\"number\">2</span>] = <span class=\"type\">Python</span></div></pre></td></tr></table></figure></p>\n<h3 id=\"Scala-递归函数\"><a href=\"#Scala-递归函数\" class=\"headerlink\" title=\"Scala 递归函数\"></a>Scala 递归函数</h3><p>例如计算阶乘<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Test</span> </span>&#123;</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">      <span class=\"keyword\">for</span> (i &lt;- <span class=\"number\">1</span> to <span class=\"number\">10</span>)</div><div class=\"line\">         println(i + <span class=\"string\">\" 的阶乘为: = \"</span> + factorial(i) )</div><div class=\"line\">   &#125;</div><div class=\"line\">   </div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">factorial</span></span>(n: <span class=\"type\">BigInt</span>): <span class=\"type\">BigInt</span> = &#123;  </div><div class=\"line\">      <span class=\"keyword\">if</span> (n &lt;= <span class=\"number\">1</span>)</div><div class=\"line\">         <span class=\"number\">1</span>  </div><div class=\"line\">      <span class=\"keyword\">else</span>    </div><div class=\"line\">      n * factorial(n - <span class=\"number\">1</span>)</div><div class=\"line\">   &#125;</div><div class=\"line\">&#125;</div><div class=\"line\">----------------输出-------------------</div><div class=\"line\"><span class=\"number\">1</span> 的阶乘为: = <span class=\"number\">1</span></div><div class=\"line\"><span class=\"number\">2</span> 的阶乘为: = <span class=\"number\">2</span></div><div class=\"line\"><span class=\"number\">3</span> 的阶乘为: = <span class=\"number\">6</span></div><div class=\"line\"><span class=\"number\">4</span> 的阶乘为: = <span class=\"number\">24</span></div><div class=\"line\"><span class=\"number\">5</span> 的阶乘为: = <span class=\"number\">120</span></div><div class=\"line\"><span class=\"number\">6</span> 的阶乘为: = <span class=\"number\">720</span></div><div class=\"line\"><span class=\"number\">7</span> 的阶乘为: = <span class=\"number\">5040</span></div><div class=\"line\"><span class=\"number\">8</span> 的阶乘为: = <span class=\"number\">40320</span></div><div class=\"line\"><span class=\"number\">9</span> 的阶乘为: = <span class=\"number\">362880</span></div><div class=\"line\"><span class=\"number\">10</span> 的阶乘为: = <span class=\"number\">3628800</span></div></pre></td></tr></table></figure></p>\n<h3 id=\"Scala-函数-默认参数值\"><a href=\"#Scala-函数-默认参数值\" class=\"headerlink\" title=\"Scala 函数 - 默认参数值\"></a>Scala 函数 - 默认参数值</h3><p>Scala 可以为函数参数指定默认参数值，使用了默认参数，你在调用函数的过程中可以不需要传递参数，这时函数就会调用它的默认参数值，如果传递了参数，则传递值会取代默认值。实例如下：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Test</span> </span>&#123;</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">        println( <span class=\"string\">\"返回值 : \"</span> + addInt() );</div><div class=\"line\">   &#125;</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">addInt</span></span>( a:<span class=\"type\">Int</span>=<span class=\"number\">5</span>, b:<span class=\"type\">Int</span>=<span class=\"number\">7</span> ) : <span class=\"type\">Int</span> = &#123;</div><div class=\"line\">      <span class=\"keyword\">var</span> sum:<span class=\"type\">Int</span> = <span class=\"number\">0</span></div><div class=\"line\">      sum = a + b</div><div class=\"line\"></div><div class=\"line\">      <span class=\"keyword\">return</span> sum</div><div class=\"line\">   &#125;</div><div class=\"line\">&#125;</div><div class=\"line\">-----------------------------输出-----------------------</div><div class=\"line\">$ scalac <span class=\"type\">Test</span>.scala</div><div class=\"line\">$ scala <span class=\"type\">Test</span></div><div class=\"line\">返回值 : <span class=\"number\">12</span></div></pre></td></tr></table></figure></p>\n<h3 id=\"scala函数嵌套\"><a href=\"#scala函数嵌套\" class=\"headerlink\" title=\"scala函数嵌套\"></a>scala函数嵌套</h3><p>TODO</p>\n<h3 id=\"Scala-偏应用函数\"><a href=\"#Scala-偏应用函数\" class=\"headerlink\" title=\"Scala 偏应用函数\"></a>Scala 偏应用函数</h3><p>Scala 偏应用函数是一种表达式，你不需要提供函数需要的所有参数，只需要提供部分，或不提供所需参数。<br>如下实例，我们打印日志信息：  </p>\n<p>实例中，log() 方法接收两个参数：date 和 message。我们在程序执行时调用了三次，参数 date 值都相同，message 不同。<br>我们可以使用偏应用函数优化以上方法，绑定第一个 date 参数，第二个参数使用下划线(_)替换缺失的参数列表，并把这个新的函数值的索引的赋给变量。实例修改如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\">def main(args: Array[String]) &#123;</div><div class=\"line\">  val date = new Date</div><div class=\"line\">  val logWithDateBound = log(date, _ : String)</div><div class=\"line\"></div><div class=\"line\">  logWithDateBound(&quot;message1&quot; )</div><div class=\"line\">  Thread.sleep(1000)</div><div class=\"line\">  logWithDateBound(&quot;message2&quot; )</div><div class=\"line\">  Thread.sleep(1000)</div><div class=\"line\">  logWithDateBound(&quot;message3&quot; )</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">def log(date: Date, message: String)  = &#123;</div><div class=\"line\">  println(date + &quot;----&quot; + message)</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h3 id=\"Scala匿名函数\"><a href=\"#Scala匿名函数\" class=\"headerlink\" title=\"Scala匿名函数\"></a>Scala匿名函数</h3><p>Scala 中定义匿名函数的语法很简单，箭头左边是参数列表，右边是函数体。<br>使用匿名函数后，我们的代码变得更简洁了。<br>下面的表达式就定义了一个接受一个Int类型输入参数的匿名函数:  ,这里可能不好理解，其实可以先暂时放下，先看我另外一篇高阶函数的文章，再过来看匿名函数<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">var</span> inc = (x:<span class=\"type\">Int</span>) =&gt; x+<span class=\"number\">1</span></div><div class=\"line\">```  </div><div class=\"line\">上述定义的匿名函数，其实是下面这种写法的简写：</div><div class=\"line\">```scala</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">add2</span> </span>= <span class=\"keyword\">new</span> <span class=\"type\">Function1</span>[<span class=\"type\">Int</span>,<span class=\"type\">Int</span>]&#123;  </div><div class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">apply</span></span>(x:<span class=\"type\">Int</span>):<span class=\"type\">Int</span> = x+<span class=\"number\">1</span>;  </div><div class=\"line\">&#125; </div><div class=\"line\">----------更多参考http:<span class=\"comment\">//www.runoob.com/scala/anonymous-functions.html----------</span></div></pre></td></tr></table></figure></p>\n<h3 id=\"Scala-高阶函数\"><a href=\"#Scala-高阶函数\" class=\"headerlink\" title=\"Scala 高阶函数\"></a>Scala 高阶函数</h3><p>下一篇文章</p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"多行字符串的表示方法\"><a href=\"#多行字符串的表示方法\" class=\"headerlink\" title=\"多行字符串的表示方法\"></a>多行字符串的表示方法</h3><p>多行字符串用三个双引号来表示分隔符，格式为：””” … “””。<br>实例如下：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">val</span> foo = <span class=\"string\">\"\"</span><span class=\"string\">\"菜鸟教程</span></div><div class=\"line\"><span class=\"string\">www.runoob.com</span></div><div class=\"line\"><span class=\"string\">www.w3cschool.cc</span></div><div class=\"line\"><span class=\"string\">www.runnoob.com</span></div><div class=\"line\"><span class=\"string\">以上三个地址都能访问\"</span><span class=\"string\">\"\"</span></div></pre></td></tr></table></figure></p>\n<h2 id=\"变量\"><a href=\"#变量\" class=\"headerlink\" title=\"变量\"></a>变量</h2><h3 id=\"变量声明\"><a href=\"#变量声明\" class=\"headerlink\" title=\"变量声明\"></a>变量声明</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">var VariableName : DataType [=  Initial Value]</div><div class=\"line\">或</div><div class=\"line\">val VariableName : DataType [=  Initial Value]</div><div class=\"line\"></div><div class=\"line\">变量声明不一定需要初始值，以下也是正确的：</div><div class=\"line\">var myVar :Int;</div><div class=\"line\">val myVal :String;</div><div class=\"line\"></div><div class=\"line\">```  </div><div class=\"line\">例如</div></pre></td></tr></table></figure>\n<p>var myVar : String = “Foo”<br>var myVar : String = “Too”</p>\n<p>var myVar :Int;<br>val myVal :String;<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\">### 变量类型引用</div><div class=\"line\">在 Scala 中声明变量和常量不一定要指明数据类型，在没有指明数据类型的情况下，其数据类型是通过变量或常量的初始值推断出来的。</div><div class=\"line\">所以，如果在没有指明数据类型的情况下声明变量或常量必须要给出其初始值，否则将会报错。</div></pre></td></tr></table></figure></p>\n<p>var myVar = 10;<br>val myVal = “Hello, Scala!”;<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\">## 访问修饰符</div><div class=\"line\">Scala 访问修饰符基本和Java的一样，分别有：private，protected，public。</div><div class=\"line\">如果没有指定访问修饰符符，默认情况下，Scala对象的访问级别都是 public。</div><div class=\"line\">**Scala 中的 private 限定符，比 Java 更严格，在嵌套类情况下，外层类甚至不能访问被嵌套类的私有成员。**</div></pre></td></tr></table></figure></p>\n<p>class Outer{<br>    class Inner{<br>    private def f(){println(“f”)}<br>    class InnerMost{<br>        f() // 正确<br>        }<br>    }<br>    (new Inner).f() //错误<br>}<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">### 作用域保护</div><div class=\"line\">Scala中，访问修饰符可以通过使用限定词强调。格式为:</div></pre></td></tr></table></figure></p>\n<p>private[x]<br>或<br>protected[x]<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">这里的x指代某个所属的包、类或单例对象。如果写成private[x],读作&quot;这个成员除了对[…]中的类或[…]中的包中的类及它们的伴生对像可见外，对其它所有类都是private。</div><div class=\"line\">这种技巧在横跨了若干包的大型项目中非常有用，它允许你定义一些在你项目的若干子包中可见但对于项目外部的客户却始终不可见的东西。</div></pre></td></tr></table></figure></p>\n<p>package bobsrocckets{<br>    package navigation{<br>        private[bobsrockets] class Navigator{<br>         protected[navigation] def useStarChart(){}<br>         class LegOfJourney{<br>             private[Navigator] val distance = 100<br>             }<br>            private[this] var speed = 200<br>            }<br>        }<br>        package launch{<br>        import navigation._<br>        object Vehicle{<br>        private[launch] val guide = new Navigator<br>        }<br>    }<br>}<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">上述例子中，类Navigator被标记为private[bobsrockets]就是说这个类对包含在bobsrockets包里的所有的类和对象可见。</div><div class=\"line\">比如说，从Vehicle对象里对Navigator的访问是被允许的，因为对象Vehicle包含在包launch中，而launch包在bobsrockets中，相反，所有在包bobsrockets之外的代码都不能访问类Navigator。</div><div class=\"line\"></div><div class=\"line\">## 运算符</div><div class=\"line\">运算符和java的基本类似</div><div class=\"line\">### 位运算符</div></pre></td></tr></table></figure></p>\n<p>A = 0011 1100</p>\n<p>B = 0000 1101</p>\n<p>——-位运算———-</p>\n<p>A&amp;B = 0000 1100</p>\n<p>A|B = 0011 1101</p>\n<p>A^B = 0011 0001</p>\n<p>~A  = 1100 0011<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\"></div></pre></td></tr></table></figure></p>\n<p>object Test {<br>   def main(args: Array[String]) {<br>      var a = 60;           /<em> 60 = 0011 1100 </em>/<br>      var b = 13;           /<em> 13 = 0000 1101 </em>/<br>      var c = 0;</p>\n<pre><code>c = a &amp; b;            /* 12 = 0000 1100 */ \nprintln(&quot;a &amp; b = &quot; + c );\n\nc = a | b;            /* 61 = 0011 1101 */\nprintln(&quot;a | b = &quot; + c );\n\nc = a ^ b;            /* 49 = 0011 0001 */\nprintln(&quot;a ^ b = &quot; + c );\n\nc = ~a;               /* -61 = 1100 0011 */\nprintln(&quot;~a = &quot; + c );\n\nc = a &lt;&lt; 2;           /* 240 = 1111 0000 */\nprintln(&quot;a &lt;&lt; 2 = &quot; + c );\n\nc = a &gt;&gt; 2;           /* 215 = 1111 */\nprintln(&quot;a &gt;&gt; 2  = &quot; + c );\n\nc = a &gt;&gt;&gt; 2;          /* 215 = 0000 1111 */\nprintln(&quot;a &gt;&gt;&gt; 2 = &quot; + c );\n</code></pre><p>   }<br>}<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">执行以上代码结果为</div></pre></td></tr></table></figure></p>\n<p>$ scalac Test.scala<br>$ scala Test<br>a &amp; b = 12<br>a | b = 61<br>a ^ b = 49<br>~a = -61<br>a &lt;&lt; 2 = 240<br>a &gt;&gt; 2  = 15<br>a &gt;&gt;&gt; 2 = 15</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">## 循环</div><div class=\"line\">### for循环</div><div class=\"line\"></div><div class=\"line\">Scala 语言中 for 循环的语法：</div><div class=\"line\">```scala</div><div class=\"line\">for( var x &lt;- Range )&#123;</div><div class=\"line\">   statement(s);</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>以上语法中，Range 可以是一个数字区间表示 i to j ，或者 i until j。左箭头 &lt;- 用于为变量 x 赋值。 </p>\n<p><strong>实例</strong></p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Test</span> </span>&#123;</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">      <span class=\"keyword\">var</span> a = <span class=\"number\">0</span>;</div><div class=\"line\">      <span class=\"comment\">// for 循环</span></div><div class=\"line\">      <span class=\"keyword\">for</span>( a &lt;- <span class=\"number\">1</span> to <span class=\"number\">10</span>)&#123;</div><div class=\"line\">         println( <span class=\"string\">\"Value of a: \"</span> + a );</div><div class=\"line\">      &#125;</div><div class=\"line\">   &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>输出<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">value of a: <span class=\"number\">1</span></div><div class=\"line\">value of a: <span class=\"number\">2</span></div><div class=\"line\">value of a: <span class=\"number\">3</span></div><div class=\"line\">value of a: <span class=\"number\">4</span></div><div class=\"line\">value of a: <span class=\"number\">5</span></div><div class=\"line\">value of a: <span class=\"number\">6</span></div><div class=\"line\">value of a: <span class=\"number\">7</span></div><div class=\"line\">value of a: <span class=\"number\">8</span></div><div class=\"line\">value of a: <span class=\"number\">9</span></div><div class=\"line\">value of a: <span class=\"number\">10</span></div></pre></td></tr></table></figure></p>\n<p>以下是使用了 i until j<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Test</span> </span>&#123;</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">      <span class=\"keyword\">var</span> a = <span class=\"number\">0</span>;</div><div class=\"line\">      <span class=\"comment\">// for 循环</span></div><div class=\"line\">      <span class=\"keyword\">for</span>( a &lt;- <span class=\"number\">1</span> until <span class=\"number\">10</span>)&#123;</div><div class=\"line\">         println( <span class=\"string\">\"Value of a: \"</span> + a );</div><div class=\"line\">      &#125;</div><div class=\"line\">   &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>输出<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">value of a: 1</div><div class=\"line\">value of a: 2</div><div class=\"line\">value of a: 3</div><div class=\"line\">value of a: 4</div><div class=\"line\">value of a: 5</div><div class=\"line\">value of a: 6</div><div class=\"line\">value of a: 7</div><div class=\"line\">value of a: 8</div><div class=\"line\">value of a: 9</div></pre></td></tr></table></figure></p>\n<p>在 for 循环 中你可以使用分号 (;) 来设置多个区间，它将迭代给定区间所有的可能值。以下实例演示了两个区间的循环实例：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">object Test &#123;</div><div class=\"line\">   def main(args: Array[String]) &#123;</div><div class=\"line\">      var a = 0;</div><div class=\"line\">      var b = 0;</div><div class=\"line\">      // for 循环</div><div class=\"line\">      for( a &lt;- 1 to 3; b &lt;- 1 to 3)&#123;</div><div class=\"line\">         println( &quot;Value of a: &quot; + a );</div><div class=\"line\">         println( &quot;Value of b: &quot; + b );</div><div class=\"line\">      &#125;</div><div class=\"line\">   &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h4 id=\"for-循环集合\"><a href=\"#for-循环集合\" class=\"headerlink\" title=\"for 循环集合\"></a>for 循环集合</h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div></pre></td><td class=\"code\"><pre><div class=\"line\">object Test &#123;</div><div class=\"line\">   def main(args: Array[String]) &#123;</div><div class=\"line\">      var a = 0;</div><div class=\"line\">      val numList = List(1,2,3,4,5,6);</div><div class=\"line\"></div><div class=\"line\">      // for 循环</div><div class=\"line\">      for( a &lt;- numList )&#123;</div><div class=\"line\">         println( &quot;Value of a: &quot; + a );</div><div class=\"line\">      &#125;</div><div class=\"line\">   &#125;</div><div class=\"line\">&#125;</div><div class=\"line\">------------------------输出</div><div class=\"line\">value of a: 1</div><div class=\"line\">value of a: 2</div><div class=\"line\">value of a: 3</div><div class=\"line\">value of a: 4</div><div class=\"line\">value of a: 5</div><div class=\"line\">value of a: 6</div></pre></td></tr></table></figure>\n<h4 id=\"for-循环过滤\"><a href=\"#for-循环过滤\" class=\"headerlink\" title=\"for 循环过滤\"></a>for 循环过滤</h4><p>Scala 可以使用一个或多个 if 语句来过滤一些元素。<br>以下是在 for 循环中使用过滤器的语法。 你可以使用分号(;)来为表达式添加一个或多个的过滤条件。<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div></pre></td><td class=\"code\"><pre><div class=\"line\">object Test &#123;</div><div class=\"line\">   def main(args: Array[String]) &#123;</div><div class=\"line\">      var a = 0;</div><div class=\"line\">      val numList = List(1,2,3,4,5,6,7,8,9,10);</div><div class=\"line\"></div><div class=\"line\">      // for 循环</div><div class=\"line\">      for( a &lt;- numList</div><div class=\"line\">           if a != 3; if a &lt; 8 )&#123;</div><div class=\"line\">         println( &quot;Value of a: &quot; + a );</div><div class=\"line\">      &#125;</div><div class=\"line\">   &#125;</div><div class=\"line\">&#125;</div><div class=\"line\">------------------输出-----------</div><div class=\"line\">value of a: 1</div><div class=\"line\">value of a: 2</div><div class=\"line\">value of a: 4</div><div class=\"line\">value of a: 5</div><div class=\"line\">value of a: 6</div><div class=\"line\">value of a: 7</div></pre></td></tr></table></figure></p>\n<h4 id=\"for-使用-yield\"><a href=\"#for-使用-yield\" class=\"headerlink\" title=\"for 使用 yield\"></a>for 使用 yield</h4><p>你可以将 for 循环的返回值作为一个变量存储。语法格式如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">var retVal = for&#123; var x &lt;- List</div><div class=\"line\">     if condition1; if condition2...</div><div class=\"line\">&#125;yield x</div></pre></td></tr></table></figure></p>\n<p>实例<br>以下实例演示了 for 循环中使用 yield：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div></pre></td><td class=\"code\"><pre><div class=\"line\">object Test &#123;</div><div class=\"line\">   def main(args: Array[String]) &#123;</div><div class=\"line\">      var a = 0;</div><div class=\"line\">      val numList = List(1,2,3,4,5,6,7,8,9,10);</div><div class=\"line\"></div><div class=\"line\">      // for 循环</div><div class=\"line\">      var retVal = for&#123; a &lt;- numList </div><div class=\"line\">                        if a != 3; if a &lt; 8</div><div class=\"line\">                      &#125;yield a</div><div class=\"line\"></div><div class=\"line\">      // 输出返回值</div><div class=\"line\">      for( a &lt;- retVal)&#123;</div><div class=\"line\">         println( &quot;Value of a: &quot; + a );</div><div class=\"line\">      &#125;</div><div class=\"line\">   &#125;</div><div class=\"line\">&#125;</div><div class=\"line\">--------------------输出--------------</div><div class=\"line\">value of a: 1</div><div class=\"line\">value of a: 2</div><div class=\"line\">value of a: 4</div><div class=\"line\">value of a: 5</div><div class=\"line\">value of a: 6</div><div class=\"line\">value of a: 7</div></pre></td></tr></table></figure></p>\n<h2 id=\"Scala-函数\"><a href=\"#Scala-函数\" class=\"headerlink\" title=\"Scala 函数\"></a>Scala 函数</h2><p>Scala 有函数和方法，二者在语义上的区别很小。Scala 方法是类的一部分，而函数是一个对象可以赋值给一个变量。换句话来说在类中定义的函数即是方法。<br>我们可以在任何地方定义函数，甚至可以在函数内定义函数（内嵌函数）。更重要的一点是 Scala 函数名可以由以下特殊字符：+, ++, ~, &amp;,-, – , \\, /, : 等。</p>\n<h3 id=\"函数声明\"><a href=\"#函数声明\" class=\"headerlink\" title=\"函数声明\"></a>函数声明</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">def functionName ([参数列表]) : [return type]</div><div class=\"line\">``` </div><div class=\"line\"></div><div class=\"line\">### 函数定义</div><div class=\"line\">方法定义由一个def 关键字开始，紧接着是可选的参数列表，一个冒号&quot;：&quot; 和方法的返回类型，一个等于号&quot;=&quot;，最后是方法的主体。</div><div class=\"line\">Scala 函数定义格式如下：</div></pre></td></tr></table></figure>\n<p>def functionName ([参数列表]) : [return type] = {<br>   function body<br>   return [expr]<br>}<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">以上代码中 return type 可以是任意合法的 Scala 数据类型。参数列表中的参数可以使用逗号分隔。</div><div class=\"line\">以下函数的功能是将两个传入的参数相加并求和：</div></pre></td></tr></table></figure></p>\n<p>object add{<br>   def addInt( a:Int, b:Int ) : Int = {<br>      var sum:Int = 0<br>      sum = a + b</p>\n<pre><code>return sum\n</code></pre><p>   }<br>}<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">**如果函数没有返回值，可以返回为 Unit，这个类似于 Java 的 void**, 实例如下：</div></pre></td></tr></table></figure></p>\n<p>object Hello{<br>   def printMe( ) : Unit = {<br>      println(“Hello, Scala!”)<br>   }<br>}<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">### 函数调用</div></pre></td></tr></table></figure></p>\n<p>functionName( 参数列表 )<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">如果函数使用了实例的对象来调用，我们可以使用类似java的格式 (使用 . 号)：</div></pre></td></tr></table></figure></p>\n<p>[instance.]functionName( 参数列表 )<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">以下实例演示了定义与调用函数的实例:</div></pre></td></tr></table></figure></p>\n<p>object Test {<br>   def main(args: Array[String]) {<br>        println( “Returned Value : “ + addInt(5,7) );<br>   }<br>   def addInt( a:Int, b:Int ) : Int = {<br>      var sum:Int = 0<br>      sum = a + b</p>\n<pre><code>return sum\n</code></pre><p>   }<br>}<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div></pre></td><td class=\"code\"><pre><div class=\"line\">**++Scala也是一种函数式语言++，所以函数是 Scala 语言的核心。以下一些函数概念有助于我们更好的理解 Scala 编程：**</div><div class=\"line\"></div><div class=\"line\">### 函数传名调用</div><div class=\"line\">传值调用（call-by-value）：先计算参数表达式的值，再应用到函数内部；  </div><div class=\"line\">传名调用（call-by-name）：将未计算的参数表达式直接应用到函数内部</div><div class=\"line\">看例子 </div><div class=\"line\">```scala</div><div class=\"line\">package com.doggie  </div><div class=\"line\">  </div><div class=\"line\">object Add &#123;  </div><div class=\"line\">  def addByName(a: Int, b: =&gt; Int) = a + b   </div><div class=\"line\">  def addByValue(a: Int, b: Int) = a + b   </div><div class=\"line\">&#125;  </div><div class=\"line\">------------------------</div><div class=\"line\">addByName(2, 2 + 2)  </div><div class=\"line\">-&gt;2 + (2 + 2)  </div><div class=\"line\">-&gt;2 + 4  </div><div class=\"line\">-&gt;6  </div><div class=\"line\">  </div><div class=\"line\">addByValue(2, 2 + 2)  </div><div class=\"line\">-&gt;addByValue(2, 4)  </div><div class=\"line\">-&gt;2 + 4  </div><div class=\"line\">-&gt;6</div></pre></td></tr></table></figure></p>\n<p><strong>addByName是传名调用，addByValue是传值调用。语法上可以看出，使用传名调用时，在参数名称和参数类型中间有一个=&gt;符号。</strong><br>例子： 酒鬼喝酒<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">package</span> first.example</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">/**</span></div><div class=\"line\"><span class=\"comment\">  * Created by Administrator on 2017/3/28.</span></div><div class=\"line\"><span class=\"comment\">  */</span></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">CallByName</span> </span>&#123;</div><div class=\"line\">  <span class=\"comment\">//最开始拥有的软妹币</span></div><div class=\"line\">  <span class=\"keyword\">var</span> money = <span class=\"number\">10</span></div><div class=\"line\">  <span class=\"comment\">//每天喝掉一个软妹币</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">drink</span></span>(): <span class=\"type\">Unit</span> = &#123;</div><div class=\"line\">    money -= <span class=\"number\">1</span></div><div class=\"line\">  &#125;</div><div class=\"line\">  <span class=\"comment\">//数钱时要算上被喝掉的软妹币</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">count</span></span>(): <span class=\"type\">Int</span> = &#123;</div><div class=\"line\">    drink()</div><div class=\"line\">    <span class=\"keyword\">return</span> money</div><div class=\"line\">  &#125;</div><div class=\"line\">  <span class=\"comment\">//每天都数钱</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">printByName</span></span>(x: =&gt; <span class=\"type\">Int</span>): <span class=\"type\">Unit</span> = &#123;</div><div class=\"line\">    <span class=\"keyword\">for</span>(i &lt;- <span class=\"number\">0</span> until <span class=\"number\">5</span>)</div><div class=\"line\">      println(<span class=\"string\">\"每天算一算，酒鬼还剩\"</span> + x + <span class=\"string\">\"块钱！\"</span>)</div><div class=\"line\">  &#125;</div><div class=\"line\">  <span class=\"comment\">//第一天数一下记墙上，以后每天看墙上的余额</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">printByValue</span></span>(x: <span class=\"type\">Int</span>): <span class=\"type\">Unit</span> = &#123;</div><div class=\"line\">    <span class=\"keyword\">for</span>(i &lt;- <span class=\"number\">0</span> until <span class=\"number\">5</span>)</div><div class=\"line\">      println(<span class=\"string\">\"只算第一天，酒鬼还剩\"</span> + x + <span class=\"string\">\"块钱！\"</span>)</div><div class=\"line\">  &#125;</div><div class=\"line\"></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) = &#123;</div><div class=\"line\">    printByName(count())</div><div class=\"line\">    printByValue(count())</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">------------------输出-----------------</div><div class=\"line\">每天算一算，酒鬼还剩<span class=\"number\">9</span>块钱！</div><div class=\"line\">每天算一算，酒鬼还剩<span class=\"number\">8</span>块钱！</div><div class=\"line\">每天算一算，酒鬼还剩<span class=\"number\">7</span>块钱！</div><div class=\"line\">每天算一算，酒鬼还剩<span class=\"number\">6</span>块钱！</div><div class=\"line\">每天算一算，酒鬼还剩<span class=\"number\">5</span>块钱！</div><div class=\"line\">只算第一天，酒鬼还剩<span class=\"number\">4</span>块钱！</div><div class=\"line\">只算第一天，酒鬼还剩<span class=\"number\">4</span>块钱！</div><div class=\"line\">只算第一天，酒鬼还剩<span class=\"number\">4</span>块钱！</div><div class=\"line\">只算第一天，酒鬼还剩<span class=\"number\">4</span>块钱！</div><div class=\"line\">只算第一天，酒鬼还剩<span class=\"number\">4</span>块钱！</div></pre></td></tr></table></figure></p>\n<h3 id=\"Scala-指定函数参数名\"><a href=\"#Scala-指定函数参数名\" class=\"headerlink\" title=\"Scala 指定函数参数名\"></a>Scala 指定函数参数名</h3><p>一般情况下函数调用参数，就按照函数定义时的参数顺序一个个传递。但是我们也可以通过指定函数参数名，并且不需要按照顺序向函数传递参数，实例如下<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Test</span> </span>&#123;</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">        printInt(b=<span class=\"number\">5</span>, a=<span class=\"number\">7</span>);</div><div class=\"line\">   &#125;</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">printInt</span></span>( a:<span class=\"type\">Int</span>, b:<span class=\"type\">Int</span> ) = &#123;</div><div class=\"line\">      println(<span class=\"string\">\"Value of a : \"</span> + a );</div><div class=\"line\">      println(<span class=\"string\">\"Value of b : \"</span> + b );</div><div class=\"line\">   &#125;</div><div class=\"line\">&#125;</div><div class=\"line\">-------------输出--------------------</div><div class=\"line\"><span class=\"type\">Value</span> of a :  <span class=\"number\">7</span></div><div class=\"line\"><span class=\"type\">Value</span> of b :  <span class=\"number\">5</span></div></pre></td></tr></table></figure></p>\n<h3 id=\"Scala-函数-可变参数\"><a href=\"#Scala-函数-可变参数\" class=\"headerlink\" title=\"Scala 函数 - 可变参数\"></a>Scala 函数 - 可变参数</h3><p>Scala 允许你指明函数的最后一个参数可以是重复的，即我们不需要指定函数参数的个数，可以向函数传入可变长度参数列表。<br>Scala 通过在参数的类型之后放一个星号来设置可变参数(可重复的参数)。例如：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Test</span> </span>&#123;</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">        printStrings(<span class=\"string\">\"Runoob\"</span>, <span class=\"string\">\"Scala\"</span>, <span class=\"string\">\"Python\"</span>);</div><div class=\"line\">   &#125;</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">printStrings</span></span>( args:<span class=\"type\">String</span>* ) = &#123;</div><div class=\"line\">      <span class=\"keyword\">var</span> i : <span class=\"type\">Int</span> = <span class=\"number\">0</span>;</div><div class=\"line\">      <span class=\"keyword\">for</span>( arg &lt;- args )&#123;</div><div class=\"line\">         println(<span class=\"string\">\"Arg value[\"</span> + i + <span class=\"string\">\"] = \"</span> + arg );</div><div class=\"line\">         i = i + <span class=\"number\">1</span>;</div><div class=\"line\">      &#125;</div><div class=\"line\">   &#125;</div><div class=\"line\">&#125;</div><div class=\"line\">-------------输出-------------</div><div class=\"line\"><span class=\"type\">Arg</span> value[<span class=\"number\">0</span>] = <span class=\"type\">Runoob</span></div><div class=\"line\"><span class=\"type\">Arg</span> value[<span class=\"number\">1</span>] = <span class=\"type\">Scala</span></div><div class=\"line\"><span class=\"type\">Arg</span> value[<span class=\"number\">2</span>] = <span class=\"type\">Python</span></div></pre></td></tr></table></figure></p>\n<h3 id=\"Scala-递归函数\"><a href=\"#Scala-递归函数\" class=\"headerlink\" title=\"Scala 递归函数\"></a>Scala 递归函数</h3><p>例如计算阶乘<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Test</span> </span>&#123;</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">      <span class=\"keyword\">for</span> (i &lt;- <span class=\"number\">1</span> to <span class=\"number\">10</span>)</div><div class=\"line\">         println(i + <span class=\"string\">\" 的阶乘为: = \"</span> + factorial(i) )</div><div class=\"line\">   &#125;</div><div class=\"line\">   </div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">factorial</span></span>(n: <span class=\"type\">BigInt</span>): <span class=\"type\">BigInt</span> = &#123;  </div><div class=\"line\">      <span class=\"keyword\">if</span> (n &lt;= <span class=\"number\">1</span>)</div><div class=\"line\">         <span class=\"number\">1</span>  </div><div class=\"line\">      <span class=\"keyword\">else</span>    </div><div class=\"line\">      n * factorial(n - <span class=\"number\">1</span>)</div><div class=\"line\">   &#125;</div><div class=\"line\">&#125;</div><div class=\"line\">----------------输出-------------------</div><div class=\"line\"><span class=\"number\">1</span> 的阶乘为: = <span class=\"number\">1</span></div><div class=\"line\"><span class=\"number\">2</span> 的阶乘为: = <span class=\"number\">2</span></div><div class=\"line\"><span class=\"number\">3</span> 的阶乘为: = <span class=\"number\">6</span></div><div class=\"line\"><span class=\"number\">4</span> 的阶乘为: = <span class=\"number\">24</span></div><div class=\"line\"><span class=\"number\">5</span> 的阶乘为: = <span class=\"number\">120</span></div><div class=\"line\"><span class=\"number\">6</span> 的阶乘为: = <span class=\"number\">720</span></div><div class=\"line\"><span class=\"number\">7</span> 的阶乘为: = <span class=\"number\">5040</span></div><div class=\"line\"><span class=\"number\">8</span> 的阶乘为: = <span class=\"number\">40320</span></div><div class=\"line\"><span class=\"number\">9</span> 的阶乘为: = <span class=\"number\">362880</span></div><div class=\"line\"><span class=\"number\">10</span> 的阶乘为: = <span class=\"number\">3628800</span></div></pre></td></tr></table></figure></p>\n<h3 id=\"Scala-函数-默认参数值\"><a href=\"#Scala-函数-默认参数值\" class=\"headerlink\" title=\"Scala 函数 - 默认参数值\"></a>Scala 函数 - 默认参数值</h3><p>Scala 可以为函数参数指定默认参数值，使用了默认参数，你在调用函数的过程中可以不需要传递参数，这时函数就会调用它的默认参数值，如果传递了参数，则传递值会取代默认值。实例如下：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Test</span> </span>&#123;</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">        println( <span class=\"string\">\"返回值 : \"</span> + addInt() );</div><div class=\"line\">   &#125;</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">addInt</span></span>( a:<span class=\"type\">Int</span>=<span class=\"number\">5</span>, b:<span class=\"type\">Int</span>=<span class=\"number\">7</span> ) : <span class=\"type\">Int</span> = &#123;</div><div class=\"line\">      <span class=\"keyword\">var</span> sum:<span class=\"type\">Int</span> = <span class=\"number\">0</span></div><div class=\"line\">      sum = a + b</div><div class=\"line\"></div><div class=\"line\">      <span class=\"keyword\">return</span> sum</div><div class=\"line\">   &#125;</div><div class=\"line\">&#125;</div><div class=\"line\">-----------------------------输出-----------------------</div><div class=\"line\">$ scalac <span class=\"type\">Test</span>.scala</div><div class=\"line\">$ scala <span class=\"type\">Test</span></div><div class=\"line\">返回值 : <span class=\"number\">12</span></div></pre></td></tr></table></figure></p>\n<h3 id=\"scala函数嵌套\"><a href=\"#scala函数嵌套\" class=\"headerlink\" title=\"scala函数嵌套\"></a>scala函数嵌套</h3><p>TODO</p>\n<h3 id=\"Scala-偏应用函数\"><a href=\"#Scala-偏应用函数\" class=\"headerlink\" title=\"Scala 偏应用函数\"></a>Scala 偏应用函数</h3><p>Scala 偏应用函数是一种表达式，你不需要提供函数需要的所有参数，只需要提供部分，或不提供所需参数。<br>如下实例，我们打印日志信息：  </p>\n<p>实例中，log() 方法接收两个参数：date 和 message。我们在程序执行时调用了三次，参数 date 值都相同，message 不同。<br>我们可以使用偏应用函数优化以上方法，绑定第一个 date 参数，第二个参数使用下划线(_)替换缺失的参数列表，并把这个新的函数值的索引的赋给变量。实例修改如下：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\">def main(args: Array[String]) &#123;</div><div class=\"line\">  val date = new Date</div><div class=\"line\">  val logWithDateBound = log(date, _ : String)</div><div class=\"line\"></div><div class=\"line\">  logWithDateBound(&quot;message1&quot; )</div><div class=\"line\">  Thread.sleep(1000)</div><div class=\"line\">  logWithDateBound(&quot;message2&quot; )</div><div class=\"line\">  Thread.sleep(1000)</div><div class=\"line\">  logWithDateBound(&quot;message3&quot; )</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">def log(date: Date, message: String)  = &#123;</div><div class=\"line\">  println(date + &quot;----&quot; + message)</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h3 id=\"Scala匿名函数\"><a href=\"#Scala匿名函数\" class=\"headerlink\" title=\"Scala匿名函数\"></a>Scala匿名函数</h3><p>Scala 中定义匿名函数的语法很简单，箭头左边是参数列表，右边是函数体。<br>使用匿名函数后，我们的代码变得更简洁了。<br>下面的表达式就定义了一个接受一个Int类型输入参数的匿名函数:  ,这里可能不好理解，其实可以先暂时放下，先看我另外一篇高阶函数的文章，再过来看匿名函数<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">var</span> inc = (x:<span class=\"type\">Int</span>) =&gt; x+<span class=\"number\">1</span></div><div class=\"line\">```  </div><div class=\"line\">上述定义的匿名函数，其实是下面这种写法的简写：</div><div class=\"line\">```scala</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">add2</span> </span>= <span class=\"keyword\">new</span> <span class=\"type\">Function1</span>[<span class=\"type\">Int</span>,<span class=\"type\">Int</span>]&#123;  </div><div class=\"line\">\t<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">apply</span></span>(x:<span class=\"type\">Int</span>):<span class=\"type\">Int</span> = x+<span class=\"number\">1</span>;  </div><div class=\"line\">&#125; </div><div class=\"line\">----------更多参考http:<span class=\"comment\">//www.runoob.com/scala/anonymous-functions.html----------</span></div></pre></td></tr></table></figure></p>\n<h3 id=\"Scala-高阶函数\"><a href=\"#Scala-高阶函数\" class=\"headerlink\" title=\"Scala 高阶函数\"></a>Scala 高阶函数</h3><p>下一篇文章</p>\n"},{"title":"java曲线拟合commons-math3-3.6.1函数","date":"2017-04-24T13:25:21.000Z","author":"kaishun","id":"50","_content":"\n需要的jar 包 commons-math3-3.6.1.jar  \n[jar包下载地址\n](http://commons.apache.org/proper/commons-math/download_math.cgi)  \n[API参考地址](http://commons.apache.org/proper/commons-math/javadocs/api-3.4/org/apache/commons/math3/fitting/WeightedObservedPoint.html)\n\n示例代码如下\n\n```java\npackage cn.mingtong.testdistance;\n\nimport org.apache.commons.math3.fitting.PolynomialCurveFitter;\nimport org.apache.commons.math3.fitting.WeightedObservedPoint;\n\nimport java.awt.*;\nimport java.util.ArrayList;\nimport java.util.Collection;\n\nimport static java.lang.Math.toRadians;\n\n/**\n * Created by Administrator on 2017/3/24.\n */\npublic class TestInstance {\n    public static void main(String[] args) {\n\n\n\n        double[] doubles = trainPolyFit(3, 10000);\n\n        //下面是测试\n        System.out.println(\"a1: \"+doubles[0]);\n        System.out.println(\"a2: \"+doubles[1]);\n        System.out.println(\"a3: \"+doubles[2]);\n        System.out.println(\"a4: \"+doubles[3]);\n        System.out.println(\"double的个数\"+doubles.length);\n        double hehe = Math.cos(toRadians(40));\n\n        double fo = doubles[3]*40*40*40+doubles[2]*40*40+doubles[1]*40+doubles[0];\n        System.out.println(\"hehe: \"+hehe);\n        System.out.println(\"fo: \"+fo);\n        double sub = hehe-fo;\n        System.out.println(sub*300000);\n    }\n\n    /**\n     *\n     * @param degree 代表你用几阶去拟合\n     * @param Length  把10 --60 分成多少个点去拟合，越大应该越精确\n     * @return\n     */\n    public static double[] trainPolyFit(int degree, int Length){\n\n        PolynomialCurveFitter polynomialCurveFitter = PolynomialCurveFitter.create(degree);\n\n        double minLat = 10.0; //中国最低纬度\n\n        double maxLat = 60.0; //中国最高纬度\n\n        double interv = (maxLat - minLat) / (double)Length;\n\n        ArrayList weightedObservedPoints = new ArrayList();\n\n        for(int i = 0; i < Length; i++) {\n\n            WeightedObservedPoint weightedObservedPoint = new WeightedObservedPoint(1,  minLat + (double)i*interv, Math.cos(toRadians(minLat + (double)i*interv)));\n\n            weightedObservedPoints.add(weightedObservedPoint);\n\n        }\n\n        return polynomialCurveFitter.fit(weightedObservedPoints);\n\n    }\n\n\n\n    public static double distanceSimplifyMore(double lat1, double lng1, double lat2, double lng2, double[] a) {\n\n        //1) 计算三个参数\n\n        double dx = lng1 - lng2; // 经度差值\n\n        double dy = lat1 - lat2; // 纬度差值\n\n        double b = (lat1 + lat2) / 2.0; // 平均纬度\n\n        //2) 计算东西方向距离和南北方向距离(单位：米)，东西距离采用三阶多项式\n\n        double Lx = (a[3] * b*b*b  + a[2]* b*b  +a[1] * b + a[0] ) * toRadians(dx) * 6367000.0; // 东西距离\n\n        double Ly = 6367000.0 * toRadians(dy); // 南北距离\n\n        //3) 用平面的矩形对角距离公式计算总距离\n\n        return Math.sqrt(Lx * Lx + Ly * Ly);\n\n    }\n\n}\n\n\n```  \n","source":"_posts/java曲线拟合commons-math3-3.6.1函数.md","raw":"---\ntitle: java曲线拟合commons-math3-3.6.1函数\ndate: 2017-04-24 21:25:21\ntags: [java,曲线拟合]\ncategories: [programme]\nauthor: kaishun\nid: 50\npermalink: java-commons-math\n---\n\n需要的jar 包 commons-math3-3.6.1.jar  \n[jar包下载地址\n](http://commons.apache.org/proper/commons-math/download_math.cgi)  \n[API参考地址](http://commons.apache.org/proper/commons-math/javadocs/api-3.4/org/apache/commons/math3/fitting/WeightedObservedPoint.html)\n\n示例代码如下\n\n```java\npackage cn.mingtong.testdistance;\n\nimport org.apache.commons.math3.fitting.PolynomialCurveFitter;\nimport org.apache.commons.math3.fitting.WeightedObservedPoint;\n\nimport java.awt.*;\nimport java.util.ArrayList;\nimport java.util.Collection;\n\nimport static java.lang.Math.toRadians;\n\n/**\n * Created by Administrator on 2017/3/24.\n */\npublic class TestInstance {\n    public static void main(String[] args) {\n\n\n\n        double[] doubles = trainPolyFit(3, 10000);\n\n        //下面是测试\n        System.out.println(\"a1: \"+doubles[0]);\n        System.out.println(\"a2: \"+doubles[1]);\n        System.out.println(\"a3: \"+doubles[2]);\n        System.out.println(\"a4: \"+doubles[3]);\n        System.out.println(\"double的个数\"+doubles.length);\n        double hehe = Math.cos(toRadians(40));\n\n        double fo = doubles[3]*40*40*40+doubles[2]*40*40+doubles[1]*40+doubles[0];\n        System.out.println(\"hehe: \"+hehe);\n        System.out.println(\"fo: \"+fo);\n        double sub = hehe-fo;\n        System.out.println(sub*300000);\n    }\n\n    /**\n     *\n     * @param degree 代表你用几阶去拟合\n     * @param Length  把10 --60 分成多少个点去拟合，越大应该越精确\n     * @return\n     */\n    public static double[] trainPolyFit(int degree, int Length){\n\n        PolynomialCurveFitter polynomialCurveFitter = PolynomialCurveFitter.create(degree);\n\n        double minLat = 10.0; //中国最低纬度\n\n        double maxLat = 60.0; //中国最高纬度\n\n        double interv = (maxLat - minLat) / (double)Length;\n\n        ArrayList weightedObservedPoints = new ArrayList();\n\n        for(int i = 0; i < Length; i++) {\n\n            WeightedObservedPoint weightedObservedPoint = new WeightedObservedPoint(1,  minLat + (double)i*interv, Math.cos(toRadians(minLat + (double)i*interv)));\n\n            weightedObservedPoints.add(weightedObservedPoint);\n\n        }\n\n        return polynomialCurveFitter.fit(weightedObservedPoints);\n\n    }\n\n\n\n    public static double distanceSimplifyMore(double lat1, double lng1, double lat2, double lng2, double[] a) {\n\n        //1) 计算三个参数\n\n        double dx = lng1 - lng2; // 经度差值\n\n        double dy = lat1 - lat2; // 纬度差值\n\n        double b = (lat1 + lat2) / 2.0; // 平均纬度\n\n        //2) 计算东西方向距离和南北方向距离(单位：米)，东西距离采用三阶多项式\n\n        double Lx = (a[3] * b*b*b  + a[2]* b*b  +a[1] * b + a[0] ) * toRadians(dx) * 6367000.0; // 东西距离\n\n        double Ly = 6367000.0 * toRadians(dy); // 南北距离\n\n        //3) 用平面的矩形对角距离公式计算总距离\n\n        return Math.sqrt(Lx * Lx + Ly * Ly);\n\n    }\n\n}\n\n\n```  \n","slug":"java-commons-math","published":1,"updated":"2018-01-22T15:53:16.807Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcrpnzq900212wv3t0trcu0c","content":"<p>需要的jar 包 commons-math3-3.6.1.jar<br><a href=\"http://commons.apache.org/proper/commons-math/download_math.cgi\" target=\"_blank\" rel=\"external\">jar包下载地址\n</a><br><a href=\"http://commons.apache.org/proper/commons-math/javadocs/api-3.4/org/apache/commons/math3/fitting/WeightedObservedPoint.html\" target=\"_blank\" rel=\"external\">API参考地址</a></p>\n<p>示例代码如下</p>\n<pre><code class=\"java\"><span class=\"keyword\">package</span> cn.mingtong.testdistance;\n\n<span class=\"keyword\">import</span> org.apache.commons.math3.fitting.PolynomialCurveFitter;\n<span class=\"keyword\">import</span> org.apache.commons.math3.fitting.WeightedObservedPoint;\n\n<span class=\"keyword\">import</span> java.awt.*;\n<span class=\"keyword\">import</span> java.util.ArrayList;\n<span class=\"keyword\">import</span> java.util.Collection;\n\n<span class=\"keyword\">import</span> <span class=\"keyword\">static</span> java.lang.Math.toRadians;\n\n<span class=\"comment\">/**</span>\n<span class=\"comment\"> * Created by Administrator on 2017/3/24.</span>\n<span class=\"comment\"> */</span>\n<span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TestInstance</span> </span>{\n    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> </span>{\n\n\n\n        <span class=\"keyword\">double</span>[] doubles = trainPolyFit(<span class=\"number\">3</span>, <span class=\"number\">10000</span>);\n\n        <span class=\"comment\">//下面是测试</span>\n        System.out.println(<span class=\"string\">\"a1: \"</span>+doubles[<span class=\"number\">0</span>]);\n        System.out.println(<span class=\"string\">\"a2: \"</span>+doubles[<span class=\"number\">1</span>]);\n        System.out.println(<span class=\"string\">\"a3: \"</span>+doubles[<span class=\"number\">2</span>]);\n        System.out.println(<span class=\"string\">\"a4: \"</span>+doubles[<span class=\"number\">3</span>]);\n        System.out.println(<span class=\"string\">\"double的个数\"</span>+doubles.length);\n        <span class=\"keyword\">double</span> hehe = Math.cos(toRadians(<span class=\"number\">40</span>));\n\n        <span class=\"keyword\">double</span> fo = doubles[<span class=\"number\">3</span>]*<span class=\"number\">40</span>*<span class=\"number\">40</span>*<span class=\"number\">40</span>+doubles[<span class=\"number\">2</span>]*<span class=\"number\">40</span>*<span class=\"number\">40</span>+doubles[<span class=\"number\">1</span>]*<span class=\"number\">40</span>+doubles[<span class=\"number\">0</span>];\n        System.out.println(<span class=\"string\">\"hehe: \"</span>+hehe);\n        System.out.println(<span class=\"string\">\"fo: \"</span>+fo);\n        <span class=\"keyword\">double</span> sub = hehe-fo;\n        System.out.println(sub*<span class=\"number\">300000</span>);\n    }\n\n    <span class=\"comment\">/**</span>\n<span class=\"comment\">     *</span>\n<span class=\"comment\">     * <span class=\"doctag\">@param</span> degree 代表你用几阶去拟合</span>\n<span class=\"comment\">     * <span class=\"doctag\">@param</span> Length  把10 --60 分成多少个点去拟合，越大应该越精确</span>\n<span class=\"comment\">     * <span class=\"doctag\">@return</span></span>\n<span class=\"comment\">     */</span>\n    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">double</span>[] trainPolyFit(<span class=\"keyword\">int</span> degree, <span class=\"keyword\">int</span> Length){\n\n        PolynomialCurveFitter polynomialCurveFitter = PolynomialCurveFitter.create(degree);\n\n        <span class=\"keyword\">double</span> minLat = <span class=\"number\">10.0</span>; <span class=\"comment\">//中国最低纬度</span>\n\n        <span class=\"keyword\">double</span> maxLat = <span class=\"number\">60.0</span>; <span class=\"comment\">//中国最高纬度</span>\n\n        <span class=\"keyword\">double</span> interv = (maxLat - minLat) / (<span class=\"keyword\">double</span>)Length;\n\n        ArrayList weightedObservedPoints = <span class=\"keyword\">new</span> ArrayList();\n\n        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; Length; i++) {\n\n            WeightedObservedPoint weightedObservedPoint = <span class=\"keyword\">new</span> WeightedObservedPoint(<span class=\"number\">1</span>,  minLat + (<span class=\"keyword\">double</span>)i*interv, Math.cos(toRadians(minLat + (<span class=\"keyword\">double</span>)i*interv)));\n\n            weightedObservedPoints.add(weightedObservedPoint);\n\n        }\n\n        <span class=\"keyword\">return</span> polynomialCurveFitter.fit(weightedObservedPoints);\n\n    }\n\n\n\n    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">double</span> <span class=\"title\">distanceSimplifyMore</span><span class=\"params\">(<span class=\"keyword\">double</span> lat1, <span class=\"keyword\">double</span> lng1, <span class=\"keyword\">double</span> lat2, <span class=\"keyword\">double</span> lng2, <span class=\"keyword\">double</span>[] a)</span> </span>{\n\n        <span class=\"comment\">//1) 计算三个参数</span>\n\n        <span class=\"keyword\">double</span> dx = lng1 - lng2; <span class=\"comment\">// 经度差值</span>\n\n        <span class=\"keyword\">double</span> dy = lat1 - lat2; <span class=\"comment\">// 纬度差值</span>\n\n        <span class=\"keyword\">double</span> b = (lat1 + lat2) / <span class=\"number\">2.0</span>; <span class=\"comment\">// 平均纬度</span>\n\n        <span class=\"comment\">//2) 计算东西方向距离和南北方向距离(单位：米)，东西距离采用三阶多项式</span>\n\n        <span class=\"keyword\">double</span> Lx = (a[<span class=\"number\">3</span>] * b*b*b  + a[<span class=\"number\">2</span>]* b*b  +a[<span class=\"number\">1</span>] * b + a[<span class=\"number\">0</span>] ) * toRadians(dx) * <span class=\"number\">6367000.0</span>; <span class=\"comment\">// 东西距离</span>\n\n        <span class=\"keyword\">double</span> Ly = <span class=\"number\">6367000.0</span> * toRadians(dy); <span class=\"comment\">// 南北距离</span>\n\n        <span class=\"comment\">//3) 用平面的矩形对角距离公式计算总距离</span>\n\n        <span class=\"keyword\">return</span> Math.sqrt(Lx * Lx + Ly * Ly);\n\n    }\n\n}\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p>需要的jar 包 commons-math3-3.6.1.jar<br><a href=\"http://commons.apache.org/proper/commons-math/download_math.cgi\" target=\"_blank\" rel=\"external\">jar包下载地址\n</a><br><a href=\"http://commons.apache.org/proper/commons-math/javadocs/api-3.4/org/apache/commons/math3/fitting/WeightedObservedPoint.html\" target=\"_blank\" rel=\"external\">API参考地址</a></p>\n<p>示例代码如下</p>\n<pre><code class=\"java\"><span class=\"keyword\">package</span> cn.mingtong.testdistance;\n\n<span class=\"keyword\">import</span> org.apache.commons.math3.fitting.PolynomialCurveFitter;\n<span class=\"keyword\">import</span> org.apache.commons.math3.fitting.WeightedObservedPoint;\n\n<span class=\"keyword\">import</span> java.awt.*;\n<span class=\"keyword\">import</span> java.util.ArrayList;\n<span class=\"keyword\">import</span> java.util.Collection;\n\n<span class=\"keyword\">import</span> <span class=\"keyword\">static</span> java.lang.Math.toRadians;\n\n<span class=\"comment\">/**</span>\n<span class=\"comment\"> * Created by Administrator on 2017/3/24.</span>\n<span class=\"comment\"> */</span>\n<span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">TestInstance</span> </span>{\n    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> </span>{\n\n\n\n        <span class=\"keyword\">double</span>[] doubles = trainPolyFit(<span class=\"number\">3</span>, <span class=\"number\">10000</span>);\n\n        <span class=\"comment\">//下面是测试</span>\n        System.out.println(<span class=\"string\">\"a1: \"</span>+doubles[<span class=\"number\">0</span>]);\n        System.out.println(<span class=\"string\">\"a2: \"</span>+doubles[<span class=\"number\">1</span>]);\n        System.out.println(<span class=\"string\">\"a3: \"</span>+doubles[<span class=\"number\">2</span>]);\n        System.out.println(<span class=\"string\">\"a4: \"</span>+doubles[<span class=\"number\">3</span>]);\n        System.out.println(<span class=\"string\">\"double的个数\"</span>+doubles.length);\n        <span class=\"keyword\">double</span> hehe = Math.cos(toRadians(<span class=\"number\">40</span>));\n\n        <span class=\"keyword\">double</span> fo = doubles[<span class=\"number\">3</span>]*<span class=\"number\">40</span>*<span class=\"number\">40</span>*<span class=\"number\">40</span>+doubles[<span class=\"number\">2</span>]*<span class=\"number\">40</span>*<span class=\"number\">40</span>+doubles[<span class=\"number\">1</span>]*<span class=\"number\">40</span>+doubles[<span class=\"number\">0</span>];\n        System.out.println(<span class=\"string\">\"hehe: \"</span>+hehe);\n        System.out.println(<span class=\"string\">\"fo: \"</span>+fo);\n        <span class=\"keyword\">double</span> sub = hehe-fo;\n        System.out.println(sub*<span class=\"number\">300000</span>);\n    }\n\n    <span class=\"comment\">/**</span>\n<span class=\"comment\">     *</span>\n<span class=\"comment\">     * <span class=\"doctag\">@param</span> degree 代表你用几阶去拟合</span>\n<span class=\"comment\">     * <span class=\"doctag\">@param</span> Length  把10 --60 分成多少个点去拟合，越大应该越精确</span>\n<span class=\"comment\">     * <span class=\"doctag\">@return</span></span>\n<span class=\"comment\">     */</span>\n    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">double</span>[] trainPolyFit(<span class=\"keyword\">int</span> degree, <span class=\"keyword\">int</span> Length){\n\n        PolynomialCurveFitter polynomialCurveFitter = PolynomialCurveFitter.create(degree);\n\n        <span class=\"keyword\">double</span> minLat = <span class=\"number\">10.0</span>; <span class=\"comment\">//中国最低纬度</span>\n\n        <span class=\"keyword\">double</span> maxLat = <span class=\"number\">60.0</span>; <span class=\"comment\">//中国最高纬度</span>\n\n        <span class=\"keyword\">double</span> interv = (maxLat - minLat) / (<span class=\"keyword\">double</span>)Length;\n\n        ArrayList weightedObservedPoints = <span class=\"keyword\">new</span> ArrayList();\n\n        <span class=\"keyword\">for</span>(<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt; Length; i++) {\n\n            WeightedObservedPoint weightedObservedPoint = <span class=\"keyword\">new</span> WeightedObservedPoint(<span class=\"number\">1</span>,  minLat + (<span class=\"keyword\">double</span>)i*interv, Math.cos(toRadians(minLat + (<span class=\"keyword\">double</span>)i*interv)));\n\n            weightedObservedPoints.add(weightedObservedPoint);\n\n        }\n\n        <span class=\"keyword\">return</span> polynomialCurveFitter.fit(weightedObservedPoints);\n\n    }\n\n\n\n    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">double</span> <span class=\"title\">distanceSimplifyMore</span><span class=\"params\">(<span class=\"keyword\">double</span> lat1, <span class=\"keyword\">double</span> lng1, <span class=\"keyword\">double</span> lat2, <span class=\"keyword\">double</span> lng2, <span class=\"keyword\">double</span>[] a)</span> </span>{\n\n        <span class=\"comment\">//1) 计算三个参数</span>\n\n        <span class=\"keyword\">double</span> dx = lng1 - lng2; <span class=\"comment\">// 经度差值</span>\n\n        <span class=\"keyword\">double</span> dy = lat1 - lat2; <span class=\"comment\">// 纬度差值</span>\n\n        <span class=\"keyword\">double</span> b = (lat1 + lat2) / <span class=\"number\">2.0</span>; <span class=\"comment\">// 平均纬度</span>\n\n        <span class=\"comment\">//2) 计算东西方向距离和南北方向距离(单位：米)，东西距离采用三阶多项式</span>\n\n        <span class=\"keyword\">double</span> Lx = (a[<span class=\"number\">3</span>] * b*b*b  + a[<span class=\"number\">2</span>]* b*b  +a[<span class=\"number\">1</span>] * b + a[<span class=\"number\">0</span>] ) * toRadians(dx) * <span class=\"number\">6367000.0</span>; <span class=\"comment\">// 东西距离</span>\n\n        <span class=\"keyword\">double</span> Ly = <span class=\"number\">6367000.0</span> * toRadians(dy); <span class=\"comment\">// 南北距离</span>\n\n        <span class=\"comment\">//3) 用平面的矩形对角距离公式计算总距离</span>\n\n        <span class=\"keyword\">return</span> Math.sqrt(Lx * Lx + Ly * Ly);\n\n    }\n\n}\n</code></pre>\n"},{"title":"Hadoop入门案例（二） 单词去重","date":"2016-07-22T13:25:21.000Z","author":"kaishun","id":"15","blogexcerpt":"前言:单词去重在很多地方都会进行，其实这个就类似于wordcount。 需求说明:对指定的一个或者多个文本进行数据去重,需求输入一个或者多个文本，测试文本内容:输出的内容中单词没有重复,代码如下","_content":"\n\n# **前言**\n单词去重在很多地方都会进行，其实这个就类似于wordcount\n# **1. 需求说明**\n对指定的一个或者多个文本进行数据去重\n## **1.1 需求输入**\n一个或者多个文本，测试文本内容:  \n```\naa bb cc aa aa aa dd dd ee ee ee ee \nff aa bb zks\nee kks\nee  zz zks\n```\n## **1.2 需求输出**\n输出的内容中单词没有重复\n# **2. 代码如下**  \n```java\npackage com.myhadoop.mapreduce.test;\n\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.input.TextInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;\nimport java.io.IOException;\nimport java.util.StringTokenizer;\n\n\npublic class Dedup{\n\n\tpublic static class Map extends Mapper<LongWritable, Text, Text, Text>\n\t{\n\t\tprivate final static IntWritable one = new IntWritable(1);\n\t\tprivate Text word = new Text();\n\t\tpublic void map(LongWritable key,Text value,Context context) throws IOException,InterruptedException\n\t\t{\n\t\t\tString lines = value.toString();\n\t\t\tStringTokenizer tokenizer = new StringTokenizer(lines,\" \");\n\t\t\twhile(tokenizer.hasMoreElements())\n\t\t\t{\n\t\t\t\tword.set(tokenizer.nextToken());\n\t\t\t\tcontext.write(word, new Text(\"\"));\n\t\t\t}\n\t\t}\n\t}\n\n\tpublic static class Reduce extends Reducer<Text, Text, Text, Text>\n\t{\n\t\tpublic void reduce(Text key,Iterable<Text> values,Context context) throws IOException,InterruptedException\n\t\t{\n\t\t\tcontext.write(key, new Text(\"\"));\n\t\t}\n\t}\n\n\tpublic static void main(String[] args) throws Exception {\n\t\tJob job = Job.getInstance();\n\t\tjob.setJarByClass(Dedup.class);\n\t\tjob.setJobName(\"Dedup\");\n\n\t\tjob.setOutputKeyClass(Text.class);\n\t\tjob.setOutputValueClass(Text.class);\n\n\t\tjob.setMapperClass(Map.class);\n\t\tjob.setReducerClass(Reduce.class);\n\n\t\tjob.setInputFormatClass(TextInputFormat.class);\n\t\tjob.setOutputFormatClass(TextOutputFormat.class);\n\t\tFileInputFormat.setInputPaths(job, new Path(args[0]));\n\t\tFileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n\t\tSystem.exit(job.waitForCompletion(true) ? 0 : 1);\n\t}\n\n\n}\n\n```  \n# **3. 代码输出**  \n```\naa\t\nbb\t\ncc\t\ndd\t\nee\t\nff\t\nkks\t\nzks\t\nzz\n```\n# 4. **代码解析**  \n整体非常类似于wordcount，先将文本每行内容进行分割，每行都得到一些单词，将单词都转变成map，并且key设置成单词，value设置成空值，经过shuffle，把相同key的放在一组，在reduce中把  \n相同key中的value变成一个空值，然后输出(word,\"\")的形式\n**Map类：**  \n输入： LongWritable, Text\n输出： Text, Text  \n**Reduce类：**  \n输入：Text, Text ---> Text, Interable<Text>\n输出：Text, Text  ","source":"_posts/Hadoop入门案例（二） 单词去重.md","raw":"---\ntitle: Hadoop入门案例（二） 单词去重\ndate: 2016-07-22 21:25:21\ntags: [hadoop]\ncategories: [大数据,hadoop]\nauthor: kaishun\nid: 15\npermalink: hadoop-example-2\nblogexcerpt: 前言:单词去重在很多地方都会进行，其实这个就类似于wordcount。 需求说明:对指定的一个或者多个文本进行数据去重,需求输入一个或者多个文本，测试文本内容:输出的内容中单词没有重复,代码如下\n---\n\n\n# **前言**\n单词去重在很多地方都会进行，其实这个就类似于wordcount\n# **1. 需求说明**\n对指定的一个或者多个文本进行数据去重\n## **1.1 需求输入**\n一个或者多个文本，测试文本内容:  \n```\naa bb cc aa aa aa dd dd ee ee ee ee \nff aa bb zks\nee kks\nee  zz zks\n```\n## **1.2 需求输出**\n输出的内容中单词没有重复\n# **2. 代码如下**  \n```java\npackage com.myhadoop.mapreduce.test;\n\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.input.TextInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;\nimport java.io.IOException;\nimport java.util.StringTokenizer;\n\n\npublic class Dedup{\n\n\tpublic static class Map extends Mapper<LongWritable, Text, Text, Text>\n\t{\n\t\tprivate final static IntWritable one = new IntWritable(1);\n\t\tprivate Text word = new Text();\n\t\tpublic void map(LongWritable key,Text value,Context context) throws IOException,InterruptedException\n\t\t{\n\t\t\tString lines = value.toString();\n\t\t\tStringTokenizer tokenizer = new StringTokenizer(lines,\" \");\n\t\t\twhile(tokenizer.hasMoreElements())\n\t\t\t{\n\t\t\t\tword.set(tokenizer.nextToken());\n\t\t\t\tcontext.write(word, new Text(\"\"));\n\t\t\t}\n\t\t}\n\t}\n\n\tpublic static class Reduce extends Reducer<Text, Text, Text, Text>\n\t{\n\t\tpublic void reduce(Text key,Iterable<Text> values,Context context) throws IOException,InterruptedException\n\t\t{\n\t\t\tcontext.write(key, new Text(\"\"));\n\t\t}\n\t}\n\n\tpublic static void main(String[] args) throws Exception {\n\t\tJob job = Job.getInstance();\n\t\tjob.setJarByClass(Dedup.class);\n\t\tjob.setJobName(\"Dedup\");\n\n\t\tjob.setOutputKeyClass(Text.class);\n\t\tjob.setOutputValueClass(Text.class);\n\n\t\tjob.setMapperClass(Map.class);\n\t\tjob.setReducerClass(Reduce.class);\n\n\t\tjob.setInputFormatClass(TextInputFormat.class);\n\t\tjob.setOutputFormatClass(TextOutputFormat.class);\n\t\tFileInputFormat.setInputPaths(job, new Path(args[0]));\n\t\tFileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n\t\tSystem.exit(job.waitForCompletion(true) ? 0 : 1);\n\t}\n\n\n}\n\n```  \n# **3. 代码输出**  \n```\naa\t\nbb\t\ncc\t\ndd\t\nee\t\nff\t\nkks\t\nzks\t\nzz\n```\n# 4. **代码解析**  \n整体非常类似于wordcount，先将文本每行内容进行分割，每行都得到一些单词，将单词都转变成map，并且key设置成单词，value设置成空值，经过shuffle，把相同key的放在一组，在reduce中把  \n相同key中的value变成一个空值，然后输出(word,\"\")的形式\n**Map类：**  \n输入： LongWritable, Text\n输出： Text, Text  \n**Reduce类：**  \n输入：Text, Text ---> Text, Interable<Text>\n输出：Text, Text  ","slug":"hadoop-example-2","published":1,"updated":"2018-01-23T14:16:13.408Z","_id":"cjcrpnzq900252wv3iyji4c5t","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a><strong>前言</strong></h1><p>单词去重在很多地方都会进行，其实这个就类似于wordcount</p>\n<h1 id=\"1-需求说明\"><a href=\"#1-需求说明\" class=\"headerlink\" title=\"1. 需求说明\"></a><strong>1. 需求说明</strong></h1><p>对指定的一个或者多个文本进行数据去重</p>\n<h2 id=\"1-1-需求输入\"><a href=\"#1-1-需求输入\" class=\"headerlink\" title=\"1.1 需求输入\"></a><strong>1.1 需求输入</strong></h2><p>一个或者多个文本，测试文本内容:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">aa bb cc aa aa aa dd dd ee ee ee ee </div><div class=\"line\">ff aa bb zks</div><div class=\"line\">ee kks</div><div class=\"line\">ee  zz zks</div></pre></td></tr></table></figure></p>\n<h2 id=\"1-2-需求输出\"><a href=\"#1-2-需求输出\" class=\"headerlink\" title=\"1.2 需求输出\"></a><strong>1.2 需求输出</strong></h2><p>输出的内容中单词没有重复</p>\n<h1 id=\"2-代码如下\"><a href=\"#2-代码如下\" class=\"headerlink\" title=\"2. 代码如下\"></a><strong>2. 代码如下</strong></h1><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">package</span> com.myhadoop.mapreduce.test;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.fs.Path;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.IntWritable;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.LongWritable;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.Text;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Job;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Mapper;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Reducer;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</div><div class=\"line\"><span class=\"keyword\">import</span> java.io.IOException;</div><div class=\"line\"><span class=\"keyword\">import</span> java.util.StringTokenizer;</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Dedup</span></span>&#123;</div><div class=\"line\"></div><div class=\"line\">\t<span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Map</span> <span class=\"keyword\">extends</span> <span class=\"title\">Mapper</span>&lt;<span class=\"title\">LongWritable</span>, <span class=\"title\">Text</span>, <span class=\"title\">Text</span>, <span class=\"title\">Text</span>&gt;</span></div><div class=\"line\"><span class=\"class\">\t</span>&#123;</div><div class=\"line\">\t\t<span class=\"keyword\">private</span> <span class=\"keyword\">final</span> <span class=\"keyword\">static</span> IntWritable one = <span class=\"keyword\">new</span> IntWritable(<span class=\"number\">1</span>);</div><div class=\"line\">\t\t<span class=\"keyword\">private</span> Text word = <span class=\"keyword\">new</span> Text();</div><div class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">map</span><span class=\"params\">(LongWritable key,Text value,Context context)</span> <span class=\"keyword\">throws</span> IOException,InterruptedException</span></div><div class=\"line\"><span class=\"function\">\t\t</span>&#123;</div><div class=\"line\">\t\t\tString lines = value.toString();</div><div class=\"line\">\t\t\tStringTokenizer tokenizer = <span class=\"keyword\">new</span> StringTokenizer(lines,<span class=\"string\">\" \"</span>);</div><div class=\"line\">\t\t\t<span class=\"keyword\">while</span>(tokenizer.hasMoreElements())</div><div class=\"line\">\t\t\t&#123;</div><div class=\"line\">\t\t\t\tword.set(tokenizer.nextToken());</div><div class=\"line\">\t\t\t\tcontext.write(word, <span class=\"keyword\">new</span> Text(<span class=\"string\">\"\"</span>));</div><div class=\"line\">\t\t\t&#125;</div><div class=\"line\">\t\t&#125;</div><div class=\"line\">\t&#125;</div><div class=\"line\"></div><div class=\"line\">\t<span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Reduce</span> <span class=\"keyword\">extends</span> <span class=\"title\">Reducer</span>&lt;<span class=\"title\">Text</span>, <span class=\"title\">Text</span>, <span class=\"title\">Text</span>, <span class=\"title\">Text</span>&gt;</span></div><div class=\"line\"><span class=\"class\">\t</span>&#123;</div><div class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">reduce</span><span class=\"params\">(Text key,Iterable&lt;Text&gt; values,Context context)</span> <span class=\"keyword\">throws</span> IOException,InterruptedException</span></div><div class=\"line\"><span class=\"function\">\t\t</span>&#123;</div><div class=\"line\">\t\t\tcontext.write(key, <span class=\"keyword\">new</span> Text(<span class=\"string\">\"\"</span>));</div><div class=\"line\">\t\t&#125;</div><div class=\"line\">\t&#125;</div><div class=\"line\"></div><div class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">\t\tJob job = Job.getInstance();</div><div class=\"line\">\t\tjob.setJarByClass(Dedup.class);</div><div class=\"line\">\t\tjob.setJobName(<span class=\"string\">\"Dedup\"</span>);</div><div class=\"line\"></div><div class=\"line\">\t\tjob.setOutputKeyClass(Text.class);</div><div class=\"line\">\t\tjob.setOutputValueClass(Text.class);</div><div class=\"line\"></div><div class=\"line\">\t\tjob.setMapperClass(Map.class);</div><div class=\"line\">\t\tjob.setReducerClass(Reduce.class);</div><div class=\"line\"></div><div class=\"line\">\t\tjob.setInputFormatClass(TextInputFormat.class);</div><div class=\"line\">\t\tjob.setOutputFormatClass(TextOutputFormat.class);</div><div class=\"line\">\t\tFileInputFormat.setInputPaths(job, <span class=\"keyword\">new</span> Path(args[<span class=\"number\">0</span>]));</div><div class=\"line\">\t\tFileOutputFormat.setOutputPath(job, <span class=\"keyword\">new</span> Path(args[<span class=\"number\">1</span>]));</div><div class=\"line\"></div><div class=\"line\">\t\tSystem.exit(job.waitForCompletion(<span class=\"keyword\">true</span>) ? <span class=\"number\">0</span> : <span class=\"number\">1</span>);</div><div class=\"line\">\t&#125;</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">```  </div><div class=\"line\"># **3. 代码输出**</div></pre></td></tr></table></figure>\n<p>aa<br>bb<br>cc<br>dd<br>ee<br>ff<br>kks<br>zks<br>zz<br>```</p>\n<h1 id=\"4-代码解析\"><a href=\"#4-代码解析\" class=\"headerlink\" title=\"4. 代码解析\"></a>4. <strong>代码解析</strong></h1><p>整体非常类似于wordcount，先将文本每行内容进行分割，每行都得到一些单词，将单词都转变成map，并且key设置成单词，value设置成空值，经过shuffle，把相同key的放在一组，在reduce中把<br>相同key中的value变成一个空值，然后输出(word,””)的形式<br><strong>Map类：</strong><br>输入： LongWritable, Text<br>输出： Text, Text<br><strong>Reduce类：</strong><br>输入：Text, Text —&gt; Text, Interable<text><br>输出：Text, Text  </text></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a><strong>前言</strong></h1><p>单词去重在很多地方都会进行，其实这个就类似于wordcount</p>\n<h1 id=\"1-需求说明\"><a href=\"#1-需求说明\" class=\"headerlink\" title=\"1. 需求说明\"></a><strong>1. 需求说明</strong></h1><p>对指定的一个或者多个文本进行数据去重</p>\n<h2 id=\"1-1-需求输入\"><a href=\"#1-1-需求输入\" class=\"headerlink\" title=\"1.1 需求输入\"></a><strong>1.1 需求输入</strong></h2><p>一个或者多个文本，测试文本内容:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">aa bb cc aa aa aa dd dd ee ee ee ee </div><div class=\"line\">ff aa bb zks</div><div class=\"line\">ee kks</div><div class=\"line\">ee  zz zks</div></pre></td></tr></table></figure></p>\n<h2 id=\"1-2-需求输出\"><a href=\"#1-2-需求输出\" class=\"headerlink\" title=\"1.2 需求输出\"></a><strong>1.2 需求输出</strong></h2><p>输出的内容中单词没有重复</p>\n<h1 id=\"2-代码如下\"><a href=\"#2-代码如下\" class=\"headerlink\" title=\"2. 代码如下\"></a><strong>2. 代码如下</strong></h1><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">package</span> com.myhadoop.mapreduce.test;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.fs.Path;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.IntWritable;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.LongWritable;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.Text;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Job;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Mapper;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.Reducer;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</div><div class=\"line\"><span class=\"keyword\">import</span> java.io.IOException;</div><div class=\"line\"><span class=\"keyword\">import</span> java.util.StringTokenizer;</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Dedup</span></span>&#123;</div><div class=\"line\"></div><div class=\"line\">\t<span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Map</span> <span class=\"keyword\">extends</span> <span class=\"title\">Mapper</span>&lt;<span class=\"title\">LongWritable</span>, <span class=\"title\">Text</span>, <span class=\"title\">Text</span>, <span class=\"title\">Text</span>&gt;</span></div><div class=\"line\"><span class=\"class\">\t</span>&#123;</div><div class=\"line\">\t\t<span class=\"keyword\">private</span> <span class=\"keyword\">final</span> <span class=\"keyword\">static</span> IntWritable one = <span class=\"keyword\">new</span> IntWritable(<span class=\"number\">1</span>);</div><div class=\"line\">\t\t<span class=\"keyword\">private</span> Text word = <span class=\"keyword\">new</span> Text();</div><div class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">map</span><span class=\"params\">(LongWritable key,Text value,Context context)</span> <span class=\"keyword\">throws</span> IOException,InterruptedException</span></div><div class=\"line\"><span class=\"function\">\t\t</span>&#123;</div><div class=\"line\">\t\t\tString lines = value.toString();</div><div class=\"line\">\t\t\tStringTokenizer tokenizer = <span class=\"keyword\">new</span> StringTokenizer(lines,<span class=\"string\">\" \"</span>);</div><div class=\"line\">\t\t\t<span class=\"keyword\">while</span>(tokenizer.hasMoreElements())</div><div class=\"line\">\t\t\t&#123;</div><div class=\"line\">\t\t\t\tword.set(tokenizer.nextToken());</div><div class=\"line\">\t\t\t\tcontext.write(word, <span class=\"keyword\">new</span> Text(<span class=\"string\">\"\"</span>));</div><div class=\"line\">\t\t\t&#125;</div><div class=\"line\">\t\t&#125;</div><div class=\"line\">\t&#125;</div><div class=\"line\"></div><div class=\"line\">\t<span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Reduce</span> <span class=\"keyword\">extends</span> <span class=\"title\">Reducer</span>&lt;<span class=\"title\">Text</span>, <span class=\"title\">Text</span>, <span class=\"title\">Text</span>, <span class=\"title\">Text</span>&gt;</span></div><div class=\"line\"><span class=\"class\">\t</span>&#123;</div><div class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">reduce</span><span class=\"params\">(Text key,Iterable&lt;Text&gt; values,Context context)</span> <span class=\"keyword\">throws</span> IOException,InterruptedException</span></div><div class=\"line\"><span class=\"function\">\t\t</span>&#123;</div><div class=\"line\">\t\t\tcontext.write(key, <span class=\"keyword\">new</span> Text(<span class=\"string\">\"\"</span>));</div><div class=\"line\">\t\t&#125;</div><div class=\"line\">\t&#125;</div><div class=\"line\"></div><div class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">\t\tJob job = Job.getInstance();</div><div class=\"line\">\t\tjob.setJarByClass(Dedup.class);</div><div class=\"line\">\t\tjob.setJobName(<span class=\"string\">\"Dedup\"</span>);</div><div class=\"line\"></div><div class=\"line\">\t\tjob.setOutputKeyClass(Text.class);</div><div class=\"line\">\t\tjob.setOutputValueClass(Text.class);</div><div class=\"line\"></div><div class=\"line\">\t\tjob.setMapperClass(Map.class);</div><div class=\"line\">\t\tjob.setReducerClass(Reduce.class);</div><div class=\"line\"></div><div class=\"line\">\t\tjob.setInputFormatClass(TextInputFormat.class);</div><div class=\"line\">\t\tjob.setOutputFormatClass(TextOutputFormat.class);</div><div class=\"line\">\t\tFileInputFormat.setInputPaths(job, <span class=\"keyword\">new</span> Path(args[<span class=\"number\">0</span>]));</div><div class=\"line\">\t\tFileOutputFormat.setOutputPath(job, <span class=\"keyword\">new</span> Path(args[<span class=\"number\">1</span>]));</div><div class=\"line\"></div><div class=\"line\">\t\tSystem.exit(job.waitForCompletion(<span class=\"keyword\">true</span>) ? <span class=\"number\">0</span> : <span class=\"number\">1</span>);</div><div class=\"line\">\t&#125;</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">```  </div><div class=\"line\"># **3. 代码输出**</div></pre></td></tr></table></figure>\n<p>aa<br>bb<br>cc<br>dd<br>ee<br>ff<br>kks<br>zks<br>zz<br>```</p>\n<h1 id=\"4-代码解析\"><a href=\"#4-代码解析\" class=\"headerlink\" title=\"4. 代码解析\"></a>4. <strong>代码解析</strong></h1><p>整体非常类似于wordcount，先将文本每行内容进行分割，每行都得到一些单词，将单词都转变成map，并且key设置成单词，value设置成空值，经过shuffle，把相同key的放在一组，在reduce中把<br>相同key中的value变成一个空值，然后输出(word,””)的形式<br><strong>Map类：</strong><br>输入： LongWritable, Text<br>输出： Text, Text<br><strong>Reduce类：</strong><br>输入：Text, Text —&gt; Text, Interable<text><br>输出：Text, Text  </text></p>\n"},{"title":"spark RDD算子（一）  parallelize，makeRDD，textFile","date":"2017-03-01T13:25:21.000Z","author":"kaishun","id":"34","_content":"\n\n## parallelize  \n调用SparkContext 的 parallelize()，将一个存在的集合，变成一个RDD，这种方式试用于学习spark和做一些spark的测试\n**scala版本**    \ndef parallelize[T](seq: Seq[T], numSlices: Int = defaultParallelism)(implicit arg0: ClassTag[T]): RDD[T]  \n- 第一个参数一是一个 Seq集合\n- 第二个参数是分区数\n- 返回的是RDD[T]\n```scala\nscala> sc.parallelize(List(\"shenzhen\", \"is a beautiful city\"))\nres1: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[1] at parallelize at <console>:22\n```  \n\n**java版本**     \ndef parallelize[T](list : java.util.List[T], numSlices : scala.Int) : org.apache.spark.api.java.JavaRDD[T] = { /* compiled code */ } \n- 第一个参数是一个List集合\n- 第二个参数是一个分区，可以默认\n- 返回的是一个JavaRDD[T]\njava版本只能接收List的集合\n```java\nJavaRDD<String> javaStringRDD = sc.parallelize(Arrays.asList(\"shenzhen\", \"is a beautiful city\"));\n```  \n\n## makeRDD\n只有scala版本的才有makeRDD  \ndef makeRDD[T](seq : scala.Seq[T], numSlices : scala.Int = { /* compiled code */ })  \n跟parallelize类似\n```scala\nsc.makeRDD(List(\"shenzhen\", \"is a beautiful city\"))\n```\n\n## textFile  \n调用SparkContext.textFile()方法，从外部存储中读取数据来创建 RDD  \n例如在我本地F:\\dataexample\\wordcount\\input下有个sample.txt文件，文件随便写了点内容，我需要将里面的内容读取出来创建RDD  \n**scala版本**\n```scala\nvar lines = sc.textFile(\"F:\\\\dataexample\\\\wordcount\\\\input\") \n\n```\n**java版本**\n```java\n JavaRDD<String> lines = sc.textFile(\"F:\\\\dataexample\\\\wordcount\\\\input\");\n```  \n注: textFile支持分区，支持模式匹配，例如把F:\\\\dataexample\\\\wordcount\\\\目录下inp开头的给转换成RDD\n```scala\nvar lines = sc.textFile(\"F:\\\\dataexample\\\\wordcount\\\\inp*\")\n```\n多个路径可以使用逗号分隔，例如\n```scala\nvar lines = sc.textFile(\"dir1,dir2\",3)\n```","source":"_posts/spark RDD算子（一）  parallelize，makeRDD，textFile.md","raw":"---\ntitle: spark RDD算子（一）  parallelize，makeRDD，textFile\ndate: 2017-03-01 21:25:21\ntags: [spark]\ncategories: [大数据,spark]\nauthor: kaishun\nid: 34\npermalink: spark-rdd-1\n---\n\n\n## parallelize  \n调用SparkContext 的 parallelize()，将一个存在的集合，变成一个RDD，这种方式试用于学习spark和做一些spark的测试\n**scala版本**    \ndef parallelize[T](seq: Seq[T], numSlices: Int = defaultParallelism)(implicit arg0: ClassTag[T]): RDD[T]  \n- 第一个参数一是一个 Seq集合\n- 第二个参数是分区数\n- 返回的是RDD[T]\n```scala\nscala> sc.parallelize(List(\"shenzhen\", \"is a beautiful city\"))\nres1: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[1] at parallelize at <console>:22\n```  \n\n**java版本**     \ndef parallelize[T](list : java.util.List[T], numSlices : scala.Int) : org.apache.spark.api.java.JavaRDD[T] = { /* compiled code */ } \n- 第一个参数是一个List集合\n- 第二个参数是一个分区，可以默认\n- 返回的是一个JavaRDD[T]\njava版本只能接收List的集合\n```java\nJavaRDD<String> javaStringRDD = sc.parallelize(Arrays.asList(\"shenzhen\", \"is a beautiful city\"));\n```  \n\n## makeRDD\n只有scala版本的才有makeRDD  \ndef makeRDD[T](seq : scala.Seq[T], numSlices : scala.Int = { /* compiled code */ })  \n跟parallelize类似\n```scala\nsc.makeRDD(List(\"shenzhen\", \"is a beautiful city\"))\n```\n\n## textFile  \n调用SparkContext.textFile()方法，从外部存储中读取数据来创建 RDD  \n例如在我本地F:\\dataexample\\wordcount\\input下有个sample.txt文件，文件随便写了点内容，我需要将里面的内容读取出来创建RDD  \n**scala版本**\n```scala\nvar lines = sc.textFile(\"F:\\\\dataexample\\\\wordcount\\\\input\") \n\n```\n**java版本**\n```java\n JavaRDD<String> lines = sc.textFile(\"F:\\\\dataexample\\\\wordcount\\\\input\");\n```  \n注: textFile支持分区，支持模式匹配，例如把F:\\\\dataexample\\\\wordcount\\\\目录下inp开头的给转换成RDD\n```scala\nvar lines = sc.textFile(\"F:\\\\dataexample\\\\wordcount\\\\inp*\")\n```\n多个路径可以使用逗号分隔，例如\n```scala\nvar lines = sc.textFile(\"dir1,dir2\",3)\n```","slug":"spark-rdd-1","published":1,"updated":"2018-01-22T15:22:39.900Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcrpnzq900272wv3dnci1z2c","content":"<h2 id=\"parallelize\"><a href=\"#parallelize\" class=\"headerlink\" title=\"parallelize\"></a>parallelize</h2><p>调用SparkContext 的 parallelize()，将一个存在的集合，变成一个RDD，这种方式试用于学习spark和做一些spark的测试<br><strong>scala版本</strong><br>def parallelize<a href=\"seq: Seq[T], numSlices: Int = defaultParallelism\" target=\"_blank\" rel=\"external\">T</a>(implicit arg0: ClassTag[T]): RDD[T]  </p>\n<ul>\n<li>第一个参数一是一个 Seq集合</li>\n<li>第二个参数是分区数</li>\n<li>返回的是RDD[T]<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; sc.parallelize(<span class=\"type\">List</span>(<span class=\"string\">\"shenzhen\"</span>, <span class=\"string\">\"is a beautiful city\"</span>))</div><div class=\"line\">res1: org.apache.spark.rdd.<span class=\"type\">RDD</span>[<span class=\"type\">String</span>] = <span class=\"type\">ParallelCollectionRDD</span>[<span class=\"number\">1</span>] at parallelize at &lt;console&gt;:<span class=\"number\">22</span></div><div class=\"line\">```  </div><div class=\"line\"></div><div class=\"line\">**java版本**     </div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">parallelize</span></span>[<span class=\"type\">T</span>](list : java.util.<span class=\"type\">List</span>[<span class=\"type\">T</span>], numSlices : scala.<span class=\"type\">Int</span>) : org.apache.spark.api.java.<span class=\"type\">JavaRDD</span>[<span class=\"type\">T</span>] = &#123; <span class=\"comment\">/* compiled code */</span> &#125; </div><div class=\"line\">- 第一个参数是一个<span class=\"type\">List</span>集合</div><div class=\"line\">- 第二个参数是一个分区，可以默认</div><div class=\"line\">- 返回的是一个<span class=\"type\">JavaRDD</span>[<span class=\"type\">T</span>]</div><div class=\"line\">java版本只能接收<span class=\"type\">List</span>的集合</div><div class=\"line\">```java</div><div class=\"line\"><span class=\"type\">JavaRDD</span>&lt;<span class=\"type\">String</span>&gt; javaStringRDD = sc.parallelize(<span class=\"type\">Arrays</span>.asList(<span class=\"string\">\"shenzhen\"</span>, <span class=\"string\">\"is a beautiful city\"</span>));</div><div class=\"line\">```  </div><div class=\"line\"></div><div class=\"line\">## makeRDD</div><div class=\"line\">只有scala版本的才有makeRDD  </div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">makeRDD</span></span>[<span class=\"type\">T</span>](seq : scala.<span class=\"type\">Seq</span>[<span class=\"type\">T</span>], numSlices : scala.<span class=\"type\">Int</span> = &#123; <span class=\"comment\">/* compiled code */</span> &#125;)  </div><div class=\"line\">跟parallelize类似</div><div class=\"line\">```scala</div><div class=\"line\">sc.makeRDD(<span class=\"type\">List</span>(<span class=\"string\">\"shenzhen\"</span>, <span class=\"string\">\"is a beautiful city\"</span>))</div></pre></td></tr></table></figure>\n</li>\n</ul>\n<h2 id=\"textFile\"><a href=\"#textFile\" class=\"headerlink\" title=\"textFile\"></a>textFile</h2><p>调用SparkContext.textFile()方法，从外部存储中读取数据来创建 RDD<br>例如在我本地F:\\dataexample\\wordcount\\input下有个sample.txt文件，文件随便写了点内容，我需要将里面的内容读取出来创建RDD<br><strong>scala版本</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">var</span> lines = sc.textFile(<span class=\"string\">\"F:\\\\dataexample\\\\wordcount\\\\input\"</span>)</div></pre></td></tr></table></figure></p>\n<p><strong>java版本</strong><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"> JavaRDD&lt;String&gt; lines = sc.textFile(<span class=\"string\">\"F:\\\\dataexample\\\\wordcount\\\\input\"</span>);</div><div class=\"line\">```  </div><div class=\"line\">注: textFile支持分区，支持模式匹配，例如把F:\\\\dataexample\\\\wordcount\\\\目录下inp开头的给转换成RDD</div><div class=\"line\">```scala</div><div class=\"line\">var lines = sc.textFile(<span class=\"string\">\"F:\\\\dataexample\\\\wordcount\\\\inp*\"</span>)</div></pre></td></tr></table></figure></p>\n<p>多个路径可以使用逗号分隔，例如<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">var</span> lines = sc.textFile(<span class=\"string\">\"dir1,dir2\"</span>,<span class=\"number\">3</span>)</div></pre></td></tr></table></figure></p>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"parallelize\"><a href=\"#parallelize\" class=\"headerlink\" title=\"parallelize\"></a>parallelize</h2><p>调用SparkContext 的 parallelize()，将一个存在的集合，变成一个RDD，这种方式试用于学习spark和做一些spark的测试<br><strong>scala版本</strong><br>def parallelize<a href=\"seq: Seq[T], numSlices: Int = defaultParallelism\" target=\"_blank\" rel=\"external\">T</a>(implicit arg0: ClassTag[T]): RDD[T]  </p>\n<ul>\n<li>第一个参数一是一个 Seq集合</li>\n<li>第二个参数是分区数</li>\n<li>返回的是RDD[T]<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; sc.parallelize(<span class=\"type\">List</span>(<span class=\"string\">\"shenzhen\"</span>, <span class=\"string\">\"is a beautiful city\"</span>))</div><div class=\"line\">res1: org.apache.spark.rdd.<span class=\"type\">RDD</span>[<span class=\"type\">String</span>] = <span class=\"type\">ParallelCollectionRDD</span>[<span class=\"number\">1</span>] at parallelize at &lt;console&gt;:<span class=\"number\">22</span></div><div class=\"line\">```  </div><div class=\"line\"></div><div class=\"line\">**java版本**     </div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">parallelize</span></span>[<span class=\"type\">T</span>](list : java.util.<span class=\"type\">List</span>[<span class=\"type\">T</span>], numSlices : scala.<span class=\"type\">Int</span>) : org.apache.spark.api.java.<span class=\"type\">JavaRDD</span>[<span class=\"type\">T</span>] = &#123; <span class=\"comment\">/* compiled code */</span> &#125; </div><div class=\"line\">- 第一个参数是一个<span class=\"type\">List</span>集合</div><div class=\"line\">- 第二个参数是一个分区，可以默认</div><div class=\"line\">- 返回的是一个<span class=\"type\">JavaRDD</span>[<span class=\"type\">T</span>]</div><div class=\"line\">java版本只能接收<span class=\"type\">List</span>的集合</div><div class=\"line\">```java</div><div class=\"line\"><span class=\"type\">JavaRDD</span>&lt;<span class=\"type\">String</span>&gt; javaStringRDD = sc.parallelize(<span class=\"type\">Arrays</span>.asList(<span class=\"string\">\"shenzhen\"</span>, <span class=\"string\">\"is a beautiful city\"</span>));</div><div class=\"line\">```  </div><div class=\"line\"></div><div class=\"line\">## makeRDD</div><div class=\"line\">只有scala版本的才有makeRDD  </div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">makeRDD</span></span>[<span class=\"type\">T</span>](seq : scala.<span class=\"type\">Seq</span>[<span class=\"type\">T</span>], numSlices : scala.<span class=\"type\">Int</span> = &#123; <span class=\"comment\">/* compiled code */</span> &#125;)  </div><div class=\"line\">跟parallelize类似</div><div class=\"line\">```scala</div><div class=\"line\">sc.makeRDD(<span class=\"type\">List</span>(<span class=\"string\">\"shenzhen\"</span>, <span class=\"string\">\"is a beautiful city\"</span>))</div></pre></td></tr></table></figure>\n</li>\n</ul>\n<h2 id=\"textFile\"><a href=\"#textFile\" class=\"headerlink\" title=\"textFile\"></a>textFile</h2><p>调用SparkContext.textFile()方法，从外部存储中读取数据来创建 RDD<br>例如在我本地F:\\dataexample\\wordcount\\input下有个sample.txt文件，文件随便写了点内容，我需要将里面的内容读取出来创建RDD<br><strong>scala版本</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">var</span> lines = sc.textFile(<span class=\"string\">\"F:\\\\dataexample\\\\wordcount\\\\input\"</span>)</div></pre></td></tr></table></figure></p>\n<p><strong>java版本</strong><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"> JavaRDD&lt;String&gt; lines = sc.textFile(<span class=\"string\">\"F:\\\\dataexample\\\\wordcount\\\\input\"</span>);</div><div class=\"line\">```  </div><div class=\"line\">注: textFile支持分区，支持模式匹配，例如把F:\\\\dataexample\\\\wordcount\\\\目录下inp开头的给转换成RDD</div><div class=\"line\">```scala</div><div class=\"line\">var lines = sc.textFile(<span class=\"string\">\"F:\\\\dataexample\\\\wordcount\\\\inp*\"</span>)</div></pre></td></tr></table></figure></p>\n<p>多个路径可以使用逗号分隔，例如<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">var</span> lines = sc.textFile(<span class=\"string\">\"dir1,dir2\"</span>,<span class=\"number\">3</span>)</div></pre></td></tr></table></figure></p>\n"},{"title":"scala集合","date":"2017-04-29T13:25:21.000Z","author":"kaishun","id":"52","_content":"\n本文参考至scala编程，菜鸟教程，然后将自己的判断以及重要方法的提取，解释，合并\n# **字符串**\n在 Scala 中，字符串的类型实际上是 Java String，它本身没有 String 类。在 Scala 中，String 是一个不可变的对象，所以该对象不可被修改。这就意味着你如果修改字符串就会产生一个新的字符串对象。  \n\n String 对象是不可变的，如果你需要创建一个可以修改的字符串，可以使用 String Builder 类，如下实例:  \n```scala\n object Test {\n   def main(args: Array[String]) {\n      val buf = new StringBuilder;\n      buf += 'a'\n      buf ++= \"bcdef\"\n      println( \"buf is : \" + buf.toString );\n   }\n}\n```\n \n同java一样，scala的字符串用过length()方法得到长度，String 类中使用string1.concat(string2); 方法来连接两个字符串\n也可以直接用+号连接字符串\njava.lang.String的所有方法，在scala中也可以使用, 这里不仔细介绍\n\n# **数组**\n## **声明数组**\n```\nvar z:Array[String] = new Array[String](3)\n\n或\n\nvar z = new Array[String](3)\n```\n## **赋值**\n```scala\nz(0) = \"Runoob\"; z(1) = \"Baidu\"; z(4/2) = \"Google\"\n```\n也可以这样定义一个数组\n```scala\nvar z = Array(\"Runoob\", \"Baidu\", \"Google\")\n```\n## **处理数组**\n```scala\nobject Test {\n   def main(args: Array[String]) {\n      var myList = Array(1.9, 2.9, 3.4, 3.5)\n      \n      // 输出所有数组元素\n      for ( x <- myList ) {\n         println( x )\n      }\n\n      // 计算数组所有元素的总和\n      var total = 0.0;\n      for ( i <- 0 to (myList.length - 1)) {\n         total += myList(i);\n      }\n      println(\"总和为 \" + total);\n\n      // 查找数组中的最大元素\n      var max = myList(0);\n      for ( i <- 1 to (myList.length - 1) ) {\n         if (myList(i) > max) max = myList(i);\n      }\n      println(\"最大值为 \" + max);\n    \n   }\n}\n\n-------输出-------\n1.9\n2.9\n3.4\n3.5\n总和为 11.7\n最大值为 3.5\n```\n## **多维数组**\n```\ndef main(args: Array[String]) {\n      var myMatrix = ofDim[Int](3,3)\n      \n      // 创建矩阵\n      for (i <- 0 to 2) {\n         for ( j <- 0 to 2) {\n            myMatrix(i)(j) = j;\n         }\n      }\n      \n      // 打印二维阵列\n      for (i <- 0 to 2) {\n         for ( j <- 0 to 2) {\n            print(\" \" + myMatrix(i)(j));\n         }\n         println();\n      }\n    \n   }\n```\n## **合并数组**\n使用concat() 方法来合并两个数组\n```scala\ndef main(args: Array[String]) {\n      var myList1 = Array(1, 2, 3, 4,)\n      var myList2 = Array(5, 6, 7, 8)\n\n      var myList3 =  concat( myList1, myList2)\n      \n      // 输出所有数组元素\n      for ( x <- myList3 ) {\n         println( x )\n      }\n   }\n   \n----------输出-----------\n1\n2\n3\n4\n5\n6\n7\n8\n\n```\n## **创建区间数组**\n以下实例中，我们使用了 range() 方法来生成一个区间范围内的数组。range() 方法最后一个参数为步长，默认为 1：\n```scala\ndef main(args: Array[String]) {\n      var myList1 = range(10, 20, 2)\n      var myList2 = range(10,20)\n\n      // 输出所有数组元素\n      for ( x <- myList1 ) {\n         print( \" \" + x )\n      }\n      println()\n      for ( x <- myList2 ) {\n         print( \" \" + x )\n      }\n   }\n   -----------------输出---------------\n10 12 14 16 18\n10 11 12 13 14 15 16 17 18 19\n```\n\n# **Scala 集合List**\nScala 列表类似于数组，它们所有元素的类型都相同，但是它们也有所不同：列表是不可变的，值一旦被定义了就不能改变，其次列表 具有递归的结构（也就是链接表结构）而数组不是。。\n列表的元素类型 T 可以写成 List[T]。例如，以下列出了多种类型的列表：\n## **列表构造方式**\n### **构造列表方式一**\n```scala\n// 字符串列表\nval site: List[String] = List(\"Runoob\", \"Google\", \"Baidu\")\n\n// 整型列表\nval nums: List[Int] = List(1, 2, 3, 4)\n\n// 空列表\nval empty: List[Nothing] = List()\n\n// 二维列表\nval dim: List[List[Int]] =\n   List(\n      List(1, 0, 0),\n      List(0, 1, 0),\n      List(0, 0, 1)\n   )\n```\n### **构造列表方式二**\n构造列表的两个基本单位是 Nil 和 ::(发音为cons) Nil代表空列表，中缀符号:: 表示列表从前端扩展。也就是说x::xs代表了一个元素为x，后面紧贴着xs的列表，因此以上实例我们可以写成\n```scala\n\n// 字符串列表\nval site = \"Runoob\" :: (\"Google\" :: (\"Baidu\" :: Nil))\n\n// 整型列表\nval nums = 1 :: (2 :: (3 :: (4 :: Nil)))\n\n// 空列表\nval empty = Nil\n\n// 二维列表\nval dim = (1 :: (0 :: (0 :: Nil))) ::\n          (0 :: (1 :: (0 :: Nil))) ::\n          (0 :: (0 :: (1 :: Nil))) :: Nil\n```\n### **方式二的简化**\n由于以::结尾，::遵循右结合的规则，A::B::C 等价于A::(B::C), 因此，前面定义用到的括号可以去掉\n```scala\nval names = 1::2::3::4::Nil\n```\n与前面的names定义一致\n\n## **列表的基本操作**\n- Scala列表有三个基本操作：\n- head 返回列表第一个元素\n- tail 返回一个列表，包含除了第一元素之外的其他元素\n- isEmpty 在列表为空时返回true  \n对于Scala列表的任何操作都可以使用这三个基本操作来表达。实例如下:\n```scala\nobject Test {\n   def main(args: Array[String]) {\n      val site = \"Runoob\" :: (\"Google\" :: (\"Baidu\" :: Nil))\n      val nums = Nil\n\n      println( \"第一网站是 : \" + site.head )\n      println( \"最后一个网站是 : \" + site.tail )\n      println( \"查看列表 site 是否为空 : \" + site.isEmpty )\n      println( \"查看 nums 是否为空 : \" + nums.isEmpty )\n   }\n}\n-----------输出--------\n第一网站是 : Runoob\n最后一个网站是 : List(Google, Baidu)\n查看列表 site 是否为空 : false\n查看 nums 是否为空 : true\n```\n## **List类的一阶方法**\n### **连接列表**\n你可以使用 ::: 运算符或 List.:::() 方法或 List.concat() 方法来连接两个或多个列表。实例如下:\n```scala\nobject Test {\n   def main(args: Array[String]) {\n      val site1 = \"Runoob\" :: (\"Google\" :: (\"Baidu\" :: Nil))\n      val site2 = \"Facebook\" :: (\"Taobao\" :: Nil)\n\n      // 使用 ::: 运算符\n      var fruit = site1 ::: site2\n      println( \"site1 ::: site2 : \" + fruit )\n      \n      // 使用 Set.:::() 方法\n      fruit = site1.:::(site2)\n      println( \"site1.:::(site2) : \" + fruit )\n\n      // 使用 concat 方法\n      fruit = List.concat(site1, site2)\n      println( \"List.concat(site1, site2) : \" + fruit  )\n      \n\n   }\n}\n```\n### **List.fill()**\n我们可以使用 List.fill() 方法来创建一个指定重复数量的元素列表：\n```scala\nobject Test {\n   def main(args: Array[String]) {\n      val site = List.fill(3)(\"Runoob\") // 重复 Runoob 3次\n      println( \"site : \" + site  )\n\n      val num = List.fill(10)(2)         // 重复元素 2, 10 次\n      println( \"num : \" + num  )\n   }\n}\n---------输出--------\nsite : List(Runoob, Runoob, Runoob)\nnum : List(2, 2, 2, 2, 2, 2, 2, 2, 2, 2)\n```\n### **List.tabulate()**\nList.tabulate() 方法是通过给定的函数来创建列表。\n方法的第一个参数为元素的数量，可以是二维的，第二个参数为指定的函数，我们通过指定的函数计算结果并返回值插入到列表中，起始值为 0，实例如下：\n```scala\nobject Test {\n   def main(args: Array[String]) {\n      // 通过给定的函数创建 5 个元素\n      val squares = List.tabulate(6)(n => n * n)\n      println( \"一维 : \" + squares  )\n\n      // 创建二维列表\n      val mul = List.tabulate( 4,5 )( _ * _ )      \n      println( \"多维 : \" + mul  )\n   }\n}\n----------------输出---------\n一维 : List(0, 1, 4, 9, 16, 25)\n多维 : List(List(0, 0, 0, 0, 0), List(0, 1, 2, 3, 4), List(0, 2, 4, 6, 8), List(0, 3, 6, 9, 12))\n```\n\n### **列表反转**\nList.reverse 用于将列表的顺序反转，实例如下：\n```scala\nobject Test {\n   def main(args: Array[String]) {\n      val site = \"Runoob\" :: (\"Google\" :: (\"Baidu\" :: Nil))\n      println( \"site 反转前 : \" + site )\n\n      println( \"site 反转前 : \" + site.reverse )\n   }\n}\n------------输出---------------\n$ vim Test.scala \n$ scala Test.scala \nsite 反转前 : List(Runoob, Google, Baidu)\nsite 反转前 : List(Baidu, Google, Runoob)\n```\n\n### **前缀与后缀 drop take splitAt**\n```scala\ndef main(args: Array[String]) {\n    var ls = \"google\"::\"baidu\"::\"tenxun\"::\"alibaba\"::\"apples\"::Nil\n    var takeTest =ls.take(2)\n    var dropTest = ls.drop(2)\n    var splitTest = ls.splitAt(3)\n    println(\"takeTest: \"+takeTest)\n    println(\"dropTest: \"+dropTest)\n    println(\"splitTest: \"+splitTest)\n  }\n  \n--------------输出-------------\ntakeTest: List(google, baidu)\ndropTest: List(tenxun, alibaba, apples)\nsplitTest: (List(google, baidu, tenxun),List(alibaba, apples))\n```\n### **元素选择 apply方法和indices方法**\napply方法是获取第几个值，list.apply(2)和list(2)是一样的  \nindices是获取所有的列表的Range,暂时不知道有多大用处\n```scala\ndef main(args: Array[String]) {\n    var ls = \"google\"::\"baidu\"::\"tenxun\"::\"alibaba\"::\"apples\"::\"stackoverflow\"::Nil\n    var applyTest = ls.apply(2)\n    var indicesTest =  ls.indices\n    println(\"applyTest: \"+applyTest)\n    println(\"applyTest2: \"+ls(2))\n    println(\"indicesTest: \"+indicesTest)\n    for(i<-indicesTest){\n      println(ls(i))\n    }\n  }\n-------------输出--------\napplyTest: tenxun\napplyTest2: tenxun\nindicesTest: Range(0, 1, 2, 3, 4, 5)\ngoogle\nbaidu\ntenxun\nalibaba\napples\nstackoverflow\n\n\n```\n### **toString和mkString**\ntoString和java的一样\nmkString  mkString(start: String,sep: String,end: String): String  \n第一个参数是以什么开始，第二个参数是以什么分割，第三个参数是以什么结尾，函数返回一个String  \n```scala\ndef main(args: Array[String]) {\n    var ls = \"google\" :: \"baidu\" :: \"tenxun\" :: \"alibaba\" :: \"apples\" :: \"stackoverflow\" :: Nil\n    println(\"toString: \"+ls.toString())\n    println(\"mkString: \"+ls.mkString(\"{\",\";\",\"}\"))\n  }\n---------输出----------\ntoString: List(google, baidu, tenxun, alibaba, apples, stackoverflow)\nmkString: {google;baidu;tenxun;alibaba;apples;stackoverflow{\n```\n### **转换列表 toArray element iterator**\n想要在数组Array和列表list之间转换，可以使用List的toArray和Array的toList  \n例子\n```\nscala> var ls = \"google\" :: \"baidu\" :: \"tenxun\" :: \"alibaba\" :: \"apples\" :: \"stackoverflow\" :: Nil\nls: List[String] = List(google, baidu, tenxun, alibaba, apples, stackoverflow)\n\nscala> var arrays = ls.toArray\narrays: Array[String] = Array(google, baidu, tenxun, alibaba, apples, stackoverflow)\n\nscala> var lists = arrays.toList\nlists: List[String] = List(google, baidu, tenxun, alibaba, apples, stackoverflow)\n```\nelement在很旧的版本有，现在已经过时不用了。如果要用枚举器访问列表元素，可以使用iterator \n```scala\ndef main(args: Array[String]) {\n    var ls = \"google\" :: \"baidu\" :: \"tenxun\" :: \"alibaba\" :: \"apples\" :: \"stackoverflow\" :: Nil\n     val it =ls.iterator\n      while(it.hasNext){\n        print(it.next()+\",\")\n      }\n  }\n--------------输出------------\ngoogle, baidu, tenxun, alibaba, apples, stackoverflow, \n```\n## **list类的高阶用法**\n在java中，若要提取出满足特点条件的元素，或者检查所有元素是否满足某种性质，或者用某种方式转变列表的所有元素，这样的需求一般都需要使用for或者while循环的固定表达式，在scala中，可以通过使用List的一些高阶方法（函数）来更为简介的实现  \n### **列表间映射：map、flatMap和foreach**\n1. xs map f 操作返回**把函数f应用在xs的每个列表元素之后**由此**组成的新列表**。如：\n```\nscala> List(1,2,3).map(_ +1)\nres1: List[Int] = List(2, 3, 4)\n```\n```scala\nscala> val words = List(\"zks\",\"zhaikaishun\",\"kaishun\",\"kai\",\"xiaozhai\")\nwords: List[String] = List(zks, zhaikaishun, kaishun, kai, xiaozhai)\n\nscala> words.map(_.length)\nres2: List[Int] = List(3, 11, 7, 3, 8)\n```\n2. flatMap操作符与map类似，不过它的右操作元是能够返回元素列表的函数。它对列表的每个元素调用该方法，**然后连接所有方法**的结果并返回。map与flatMap的差异举例说明如下：\n```scala\nscala> words.map(_.toList)\nres3: List[List[Char]] = List(List(z, k, s), List(z, h, a, i, k, a, i, s, h, u, n), List(k, a, i, s, h, u, n), List(k, a, i), List(x, i, a, o, z, h, a, i))\n\nscala> words.flatMap(_.toList)\nres4: List[Char] = List(z, k, s, z, h, a, i, k, a, i, s, h, u, n, k, a, i, s, h, u, n, k, a, i, x, i, a, o, z, h, a, i)\n```\nmap与flatMap的差异和协作可以用下面的例子体会\n```scala\nscala> List.range(1, 5).flatMap(i => List.range(1, i).map(j => (i, j)))\nres9: List[(Int, Int)] = List((2,1), (3,1), (3,2), (4,1), (4,2), (4,3))\n```\n解释 : List.range(1,5)生成了List(1,2,3,4),注意没有5  \n.flatMap对内部的每一个元素进行操作, 后面有个.map是对内部List.range(1, i)的每一个元素进行操作，最后flatMap返回的还是一个List  \n上述例子也可以用for循环+yield来完成\n```scala\nscala>  for (i <- List.range(1, 5); j <- List.range(1, i)) yield (i,j)\nres10: List[(Int, Int)] = List((2,1), (3,1), (3,2), (4,1), (4,2), (4,3))\n```\n3. foreach是第三种与映射类似的操作。它的右操作元是过程（返回Unit的函数）。它只是对每个列表元素都调用一遍过程。操作的结果仍然是Unit，不会产生结果列表。例如：\n```scala\ndef main(args: Array[String]) {\n    var sum =0\n    List(1, 2, 3, 4, 5) foreach (sum += _)\n    println(sum)\n  }\n-----输出----\n15\n```\n### **列表过滤：filter、partition、find、takeWhile、dropWhile和span**\n1.xs filter p操作产生xs中符合p（x）为true的所有元素组成的列表。如：\n```scala\nscala> List (1, 2, 3, 4, 5) filter (_ % 2 == 0)\nres10: List[Int] = List(2, 4)\n\nscala> words filter (_.length == 3)\nres11: List[String] = List(the, fox)\n```\n2.partition方法与filter类似，不过返回的是列表对。其中一个包含所有论断为真的元素，另一个包含所有论断为假的元素。\n**xs partition p  等价于 (xs filter p, xs filter (!p()))  **\n举例如下：\n```scala\nscala> List(1, 2, 3, 4, 5) partition (_ % 2 ==0)\nres12: (List[Int], List[Int]) = (List(2, 4),List(1, 3, 5))\n```\n3.find方法同样与filter方法类似，不过返回的是第一个满足给定论断的元素，而并不是全部。xs find p 操作以列表xs和论断p为操作元。返回可选值。如果xs中存在元素x使得p（x）为真，Some（x）将返回。否则，若p对所有元素都不成立，None将返回。举例如下：\n```scala\nscala> List(1, 2, 3, 4, 5) find (_ % 2 == 0)\nres13: Option[Int] = Some(2)\n\nscala> List(1, 2, 3, 4, 5) find (_  <= 0)\nres15: Option[Int] = None\n```\n4. xs takeWhile p操作返回列表xs中最长的能够满足p的前缀。例如：\n```scala\nscala> List(1, 2, 3, -4, 5) takeWhile (_ > 0)\nres16: List[Int] = List(1, 2, 3)\n```\n5.xs dropWhile p操作移除最长能够满足p的前缀。举例如下：\n```scala\nscala> val words = List(\"the\", \"quick\", \"brown\", \"fox\")\nwords: List[String] = List(the, quick, brown, fox)\n\nscala> words dropWhile (_ startsWith \"t\")\nres11: List[String] = List(quick, brown, fox)\n```\n6.span方法把takeWhile和dropWhile组合成一个操作。它返回一对列表，定义与下列等式一致：\nxs span p 等价于 （xs takeWhile p， xs dropWhile p） \n```scala\nscala> List(1, 2, 3, -4, 5) span (_ >0)\nres18: (List[Int], List[Int]) = (List(1, 2, 3),List(-4, 5))\n```\n###  **列表的论断：forall和exists**\n1. xs forall p 如果列表的所有元素满足p则返回true\n2. xs exists p 如果列表中有一个值满足p就返回true\n```scala\n  def hasZeroRow(m: List[List[Int]]) = m.exists(row => row forall (_ == 0))\n  def main(args: Array[String]) {\n    val m= List(List(3,0,0), List(0,3,0), List(0,0,3))\n    var flag :Boolean= hasZeroRow(m)\n    println(flag)\n  }\n----------输出--------\nfalse\n```\n### **折叠操作**\n如果我们把集合看成是一张纸条，每一小段代表一个元素，那么reduceLeft就将这张纸条从左向右”折叠”，最前面的两个元素会首先“重叠”在一起，这时会使用传给reduceLeft的参数函数进行计算，返回的结果就像是已经折叠在一起的两段纸条，它们已经是一个叠加的状态了，所以它，也就是上次重叠的结果会继续做为一个单一的值和下一个元素继续“叠加”，直到折叠到集合的最后一个元素\n1. **reduceLeft**\n```scala\nscala>  List.range(1, 5).reduceLeft(_+_)\nres12: Int = 10\n```\n2. **reduceRight**\n和reduceLeft相似，但是是从右向左折叠,**注意**:==它的操作方向是从右到左，但是参数的顺序却并不是，而是依然第一参数是左边的元素，第二参数是右边的元素==\n```scala\nscala> List.range(1, 4) reduceRight(_ - _)\nres15: Int = 2\n// 2-3 = -1\n// 1-(-1)=2\n```\n3. **foldLeft**\n类似于reduceLeft, 不过开始折叠的第一个元素不是集合中的第一个元素，而是传入的一个元素，有点类似于先将这个参数放入的集合中的首位，然后在reduceLeft\n```scala\nscala> List.range(1, 5).foldLeft(1)(_+_)\nres1: Int = 11\n```\n4. **foldRight**\n类似于foldLeft，不过是从右向左，不再举例\n```scala\nscala> List.range(1, 4).foldRight(3)(_-_)\nres1: Int = -1\n```\n# **scala 集合Set**\nSet和List基本相似，只是所有的元素都是唯一的\n默认的Set是不可变的,默认引用的是 scala.collection.immutable.Set\n```scala\nval set = Set(1,2,3)\nprintln(set.getClass.getName) // \n\nprintln(set.exists(_ % 2 == 0)) //true\nprintln(set.drop(1)) //Set(2,3)\n```\n如果需要使用可变集合需要引入 scala.collection.mutable.Set：\n```scala\nimport scala.collection.mutable.Set // 可以在任何地方引入 可变集合\n\nval mutableSet = Set(1,2,3)\nprintln(mutableSet.getClass.getName) // scala.collection.mutable.HashSet\n\nmutableSet.add(4)\nmutableSet.remove(1)\nmutableSet += 5\nmutableSet -= 2\n\nprintln(mutableSet) // Set(5, 3, 4)\n\nval another = mutableSet.toSet\nprintln(another.getClass.getName) // scala.collection.immutable.Set\n```\n\n## **连接集合**\n```scala\n var site = site1 ++ site2\n```\n## **查找集合中最大与最小元素**\nSet.min  \nSet.max\n\n## **交集**\nSet.intersect\n```scala\n set1.intersect(set2)\n```\n\n# **map**\nap(映射)是一种可迭代的键值对（key/value）结构。\n所有的值都可以通过键来获取。\nMap 中的键都是唯一的。\nMap 也叫哈希表（Hash tables）。\nMap 有两种类型，可变与不可变，区别在于可变对象可以修改它，而不可变对象不可以。\n默认情况下 Scala 使用不可变 Map。如果你需要使用可变集合，你需要显式的引入 import scala.collection.mutable.Map 类\n在 Scala 中 你可以同时使用可变与不可变 Map，不可变的直接使用 Map，可变的使用 mutable.Map。以下实例演示了不可变 Map 的应用：\n不可变map\n```scala\nscala> val colors = Map(\"red\" -> \"#FF0000\", \"azure\" -> \"#F0FFFF\")\ncolors: scala.collection.immutable.Map[String,String] = Map(red -> #FF0000, azure -> #F0FFFF)\n```  \n## **Map的赋值**  \n如果需要添加 key-value 对，可以使用 + 号，如下所示：\n```scala\n// 空哈希表，键为字符串，值为整型\nvar A:Map[Char,Int] = Map()\nA += ('I' -> 1)\nA += ('J' -> 5)\nA += ('K' -> 10)\nA += ('L' -> 100)\nprintln（A）\n-------输出--------\nMap(I -> 1, J -> 5, K -> 10, L -> 100)\n```\n## **Map的基本操作**\n- keys      返回 Map 所有的键(key)\n- values\t返回 Map 所有的值(value)\n- isEmpty\t在 Map 为空时返回true  \n例子\n```\nobject Test {\n   def main(args: Array[String]) {\n      val colors = Map(\"red\" -> \"#FF0000\",\n                       \"azure\" -> \"#F0FFFF\",\n                       \"peru\" -> \"#CD853F\")\n\n      val nums: Map[Int, Int] = Map()\n\n      println( \"colors 中的键为 : \" + colors.keys )\n      println( \"colors 中的值为 : \" + colors.values )\n      println( \"检测 colors 是否为空 : \" + colors.isEmpty )\n      println( \"检测 nums 是否为空 : \" + nums.isEmpty )\n   }\n}\n---------输出---------\ncolors 中的键为 : Set(red, azure, peru)\ncolors 中的值为 : MapLike(#FF0000, #F0FFFF, #CD853F)\n检测 colors 是否为空 : false\n检测 nums 是否为空 : true\n```\n## **Map 合并**\n++ 运算符或 Map.++() 方法来连接两个 Map, Map 合并时会移除重复的 key。\n```scala\nval colors1 = Map(\"red\" -> \"#FF0000\",\n      \"azure\" -> \"#F0FFFF\",\n      \"peru\" -> \"#CD853F\")\n    val colors2 = Map(\"blue\" -> \"#0033FF\",\n      \"yellow\" -> \"#FFFF00\",\n      \"red\" -> \"#FF0001\")\n\n    //  ++ 作为运算符\n    var colors = colors1 ++ colors2\n    println( \"colors1 ++ colors2 : \" + colors )\n\n    //  ++ 作为方法\n    colors = colors1.++(colors2)\n    println( \"colors1.++(colors2)) : \" + colors )\n--------输出--------------------\ncolors1 ++ colors2 : Map(blue -> #0033FF, azure -> #F0FFFF, peru -> #CD853F, yellow -> #FFFF00, red -> #FF0001)\ncolors1.++(colors2)) : Map(blue -> #0033FF, azure -> #F0FFFF, peru -> #CD853F, yellow -> #FFFF00, red -> #FF0001)\n```  \n## **输出map和key**\n```scala\ndef main(args: Array[String]): Unit = {\n    val sites = Map(\"runoob\" -> \"http://www.runoob.com\",\n      \"baidu\" -> \"http://www.baidu.com\",\n      \"taobao\" -> \"http://www.taobao.com\")\n    sites.keys.foreach{\n      i=>print(\"key: \"+i)\n        println(\"value: \"+sites(i))\n    }\n  }\n----------输出-----\nkey: runoobvalue: http://www.runoob.com\nkey: baiduvalue: http://www.baidu.com\nkey: taobaovalue: http://www.taobao.com\n```  \n##  **Map.contains查看是否存在指定的key**\n\n## **元组**\n元组可以把固定数量的条目组合在一起以便于整体的传送，不像数组或者列表，元祖可以保存不同类型的对象    \n元组的值是通过将单个的值包含在圆括号中构成的。例如\n```scala\nval t = (1, 3.14, \"Fred\") \n```\n以上实例在元组中定义了三个元素，对应的类型分别为[Int, Double, java.lang.String]。\n此外我们也可以使用以下方式来定义：\n```scala\nval t = new Tuple3(1, 3.14, \"Fred\")\n```  \n### **定义与取值**\n元组的实际类型取决于它的元素的类型，比如 (99, \"runoob\") 是 Tuple2[Int, String]。 ('u', 'r', \"the\", 1, 4, \"me\") 为 Tuple6[Char, Char, String, Int, Int, String]。  \n目前 Scala 支持的元组最大长度为 22。对于更大长度你可以使用集合，或者扩展元组。\n访问元组的元素可以通过数字索引，如下一个元组：  \n我们可以使用 t.\\_1 访问第一个元素， t.\\_2 访问第二个元素，如下所示：  \n```scala\n  def main(args: Array[String]) {\n    val t = (4,3,2,1)\n\n    val sum = t._1 + t._2 + t._3 + t._4\n    var secondTuple=t._2\n    println(\"第二个元素为: \"+secondTuple)\n    println( \"元素之和为: \"  + sum )\n  }\n----输出----------\n第二个元素为: 3\n元素之和为: 10\n\n```\n### **迭代元组**\n你可以使用 Tuple.productIterator() 方法来迭代输出元组的所有元素：\n```scala\n  def main(args: Array[String]) {\n    val t = (4,3,2,1)\n    t.productIterator.foreach(i=>println(\"value: \"+i))\n  }\n--------输出--------------\nvalue: 4\nvalue: 3\nvalue: 2\nvalue: 1\n```  \n### **元组转为字符串**\nTuple.toString()  \n### **元素交换**\nTuple.swap 方法来交换元组的元素  \n```scala\n  def main(args: Array[String]) {\n      val t = new Tuple2(\"www.google.com\", \"http://blog.csdn.net/t1dmzks\")\n      println(\"交换后的元组: \" + t )\n    }\n-------输出-----------\n交换后的元组: (www.google.com,www.runoob.com)\n```  \n# **Scala Option**\nTODO\n\n# **Scala Iterator**\nScala Iterator（迭代器）不是一个集合，它是一种用于访问集合的方法。\n迭代器 it 的两个基本操作是 next 和 hasNext。\n调用 it.next() 会返回迭代器的下一个元素，并且更新迭代器的状态。\n调用 it.hasNext() 用于检测集合中是否还有元素。\n让迭代器 it 逐个返回所有元素最简单的方法是使用 while 循环：\n```scala\nobject Test {\n   def main(args: Array[String]) {\n      val it = Iterator(\"Baidu\", \"Google\", \"Runoob\", \"Taobao\")\n      \n      while (it.hasNext){\n         println(it.next())\n      }\n   }\n}\n```\n## **查找最大与最小元素**\n it.min 和 it.max 方法\n\n## **获取迭代器的长度**\nit.size 或 it.length \n","source":"_posts/scala集合.md","raw":"---\ntitle: scala集合\ndate: 2017-04-29 21:25:21\ntags: [scala]\ncategories: [programme]\nauthor: kaishun\nid: 52\npermalink: scala-collect\n---\n\n本文参考至scala编程，菜鸟教程，然后将自己的判断以及重要方法的提取，解释，合并\n# **字符串**\n在 Scala 中，字符串的类型实际上是 Java String，它本身没有 String 类。在 Scala 中，String 是一个不可变的对象，所以该对象不可被修改。这就意味着你如果修改字符串就会产生一个新的字符串对象。  \n\n String 对象是不可变的，如果你需要创建一个可以修改的字符串，可以使用 String Builder 类，如下实例:  \n```scala\n object Test {\n   def main(args: Array[String]) {\n      val buf = new StringBuilder;\n      buf += 'a'\n      buf ++= \"bcdef\"\n      println( \"buf is : \" + buf.toString );\n   }\n}\n```\n \n同java一样，scala的字符串用过length()方法得到长度，String 类中使用string1.concat(string2); 方法来连接两个字符串\n也可以直接用+号连接字符串\njava.lang.String的所有方法，在scala中也可以使用, 这里不仔细介绍\n\n# **数组**\n## **声明数组**\n```\nvar z:Array[String] = new Array[String](3)\n\n或\n\nvar z = new Array[String](3)\n```\n## **赋值**\n```scala\nz(0) = \"Runoob\"; z(1) = \"Baidu\"; z(4/2) = \"Google\"\n```\n也可以这样定义一个数组\n```scala\nvar z = Array(\"Runoob\", \"Baidu\", \"Google\")\n```\n## **处理数组**\n```scala\nobject Test {\n   def main(args: Array[String]) {\n      var myList = Array(1.9, 2.9, 3.4, 3.5)\n      \n      // 输出所有数组元素\n      for ( x <- myList ) {\n         println( x )\n      }\n\n      // 计算数组所有元素的总和\n      var total = 0.0;\n      for ( i <- 0 to (myList.length - 1)) {\n         total += myList(i);\n      }\n      println(\"总和为 \" + total);\n\n      // 查找数组中的最大元素\n      var max = myList(0);\n      for ( i <- 1 to (myList.length - 1) ) {\n         if (myList(i) > max) max = myList(i);\n      }\n      println(\"最大值为 \" + max);\n    \n   }\n}\n\n-------输出-------\n1.9\n2.9\n3.4\n3.5\n总和为 11.7\n最大值为 3.5\n```\n## **多维数组**\n```\ndef main(args: Array[String]) {\n      var myMatrix = ofDim[Int](3,3)\n      \n      // 创建矩阵\n      for (i <- 0 to 2) {\n         for ( j <- 0 to 2) {\n            myMatrix(i)(j) = j;\n         }\n      }\n      \n      // 打印二维阵列\n      for (i <- 0 to 2) {\n         for ( j <- 0 to 2) {\n            print(\" \" + myMatrix(i)(j));\n         }\n         println();\n      }\n    \n   }\n```\n## **合并数组**\n使用concat() 方法来合并两个数组\n```scala\ndef main(args: Array[String]) {\n      var myList1 = Array(1, 2, 3, 4,)\n      var myList2 = Array(5, 6, 7, 8)\n\n      var myList3 =  concat( myList1, myList2)\n      \n      // 输出所有数组元素\n      for ( x <- myList3 ) {\n         println( x )\n      }\n   }\n   \n----------输出-----------\n1\n2\n3\n4\n5\n6\n7\n8\n\n```\n## **创建区间数组**\n以下实例中，我们使用了 range() 方法来生成一个区间范围内的数组。range() 方法最后一个参数为步长，默认为 1：\n```scala\ndef main(args: Array[String]) {\n      var myList1 = range(10, 20, 2)\n      var myList2 = range(10,20)\n\n      // 输出所有数组元素\n      for ( x <- myList1 ) {\n         print( \" \" + x )\n      }\n      println()\n      for ( x <- myList2 ) {\n         print( \" \" + x )\n      }\n   }\n   -----------------输出---------------\n10 12 14 16 18\n10 11 12 13 14 15 16 17 18 19\n```\n\n# **Scala 集合List**\nScala 列表类似于数组，它们所有元素的类型都相同，但是它们也有所不同：列表是不可变的，值一旦被定义了就不能改变，其次列表 具有递归的结构（也就是链接表结构）而数组不是。。\n列表的元素类型 T 可以写成 List[T]。例如，以下列出了多种类型的列表：\n## **列表构造方式**\n### **构造列表方式一**\n```scala\n// 字符串列表\nval site: List[String] = List(\"Runoob\", \"Google\", \"Baidu\")\n\n// 整型列表\nval nums: List[Int] = List(1, 2, 3, 4)\n\n// 空列表\nval empty: List[Nothing] = List()\n\n// 二维列表\nval dim: List[List[Int]] =\n   List(\n      List(1, 0, 0),\n      List(0, 1, 0),\n      List(0, 0, 1)\n   )\n```\n### **构造列表方式二**\n构造列表的两个基本单位是 Nil 和 ::(发音为cons) Nil代表空列表，中缀符号:: 表示列表从前端扩展。也就是说x::xs代表了一个元素为x，后面紧贴着xs的列表，因此以上实例我们可以写成\n```scala\n\n// 字符串列表\nval site = \"Runoob\" :: (\"Google\" :: (\"Baidu\" :: Nil))\n\n// 整型列表\nval nums = 1 :: (2 :: (3 :: (4 :: Nil)))\n\n// 空列表\nval empty = Nil\n\n// 二维列表\nval dim = (1 :: (0 :: (0 :: Nil))) ::\n          (0 :: (1 :: (0 :: Nil))) ::\n          (0 :: (0 :: (1 :: Nil))) :: Nil\n```\n### **方式二的简化**\n由于以::结尾，::遵循右结合的规则，A::B::C 等价于A::(B::C), 因此，前面定义用到的括号可以去掉\n```scala\nval names = 1::2::3::4::Nil\n```\n与前面的names定义一致\n\n## **列表的基本操作**\n- Scala列表有三个基本操作：\n- head 返回列表第一个元素\n- tail 返回一个列表，包含除了第一元素之外的其他元素\n- isEmpty 在列表为空时返回true  \n对于Scala列表的任何操作都可以使用这三个基本操作来表达。实例如下:\n```scala\nobject Test {\n   def main(args: Array[String]) {\n      val site = \"Runoob\" :: (\"Google\" :: (\"Baidu\" :: Nil))\n      val nums = Nil\n\n      println( \"第一网站是 : \" + site.head )\n      println( \"最后一个网站是 : \" + site.tail )\n      println( \"查看列表 site 是否为空 : \" + site.isEmpty )\n      println( \"查看 nums 是否为空 : \" + nums.isEmpty )\n   }\n}\n-----------输出--------\n第一网站是 : Runoob\n最后一个网站是 : List(Google, Baidu)\n查看列表 site 是否为空 : false\n查看 nums 是否为空 : true\n```\n## **List类的一阶方法**\n### **连接列表**\n你可以使用 ::: 运算符或 List.:::() 方法或 List.concat() 方法来连接两个或多个列表。实例如下:\n```scala\nobject Test {\n   def main(args: Array[String]) {\n      val site1 = \"Runoob\" :: (\"Google\" :: (\"Baidu\" :: Nil))\n      val site2 = \"Facebook\" :: (\"Taobao\" :: Nil)\n\n      // 使用 ::: 运算符\n      var fruit = site1 ::: site2\n      println( \"site1 ::: site2 : \" + fruit )\n      \n      // 使用 Set.:::() 方法\n      fruit = site1.:::(site2)\n      println( \"site1.:::(site2) : \" + fruit )\n\n      // 使用 concat 方法\n      fruit = List.concat(site1, site2)\n      println( \"List.concat(site1, site2) : \" + fruit  )\n      \n\n   }\n}\n```\n### **List.fill()**\n我们可以使用 List.fill() 方法来创建一个指定重复数量的元素列表：\n```scala\nobject Test {\n   def main(args: Array[String]) {\n      val site = List.fill(3)(\"Runoob\") // 重复 Runoob 3次\n      println( \"site : \" + site  )\n\n      val num = List.fill(10)(2)         // 重复元素 2, 10 次\n      println( \"num : \" + num  )\n   }\n}\n---------输出--------\nsite : List(Runoob, Runoob, Runoob)\nnum : List(2, 2, 2, 2, 2, 2, 2, 2, 2, 2)\n```\n### **List.tabulate()**\nList.tabulate() 方法是通过给定的函数来创建列表。\n方法的第一个参数为元素的数量，可以是二维的，第二个参数为指定的函数，我们通过指定的函数计算结果并返回值插入到列表中，起始值为 0，实例如下：\n```scala\nobject Test {\n   def main(args: Array[String]) {\n      // 通过给定的函数创建 5 个元素\n      val squares = List.tabulate(6)(n => n * n)\n      println( \"一维 : \" + squares  )\n\n      // 创建二维列表\n      val mul = List.tabulate( 4,5 )( _ * _ )      \n      println( \"多维 : \" + mul  )\n   }\n}\n----------------输出---------\n一维 : List(0, 1, 4, 9, 16, 25)\n多维 : List(List(0, 0, 0, 0, 0), List(0, 1, 2, 3, 4), List(0, 2, 4, 6, 8), List(0, 3, 6, 9, 12))\n```\n\n### **列表反转**\nList.reverse 用于将列表的顺序反转，实例如下：\n```scala\nobject Test {\n   def main(args: Array[String]) {\n      val site = \"Runoob\" :: (\"Google\" :: (\"Baidu\" :: Nil))\n      println( \"site 反转前 : \" + site )\n\n      println( \"site 反转前 : \" + site.reverse )\n   }\n}\n------------输出---------------\n$ vim Test.scala \n$ scala Test.scala \nsite 反转前 : List(Runoob, Google, Baidu)\nsite 反转前 : List(Baidu, Google, Runoob)\n```\n\n### **前缀与后缀 drop take splitAt**\n```scala\ndef main(args: Array[String]) {\n    var ls = \"google\"::\"baidu\"::\"tenxun\"::\"alibaba\"::\"apples\"::Nil\n    var takeTest =ls.take(2)\n    var dropTest = ls.drop(2)\n    var splitTest = ls.splitAt(3)\n    println(\"takeTest: \"+takeTest)\n    println(\"dropTest: \"+dropTest)\n    println(\"splitTest: \"+splitTest)\n  }\n  \n--------------输出-------------\ntakeTest: List(google, baidu)\ndropTest: List(tenxun, alibaba, apples)\nsplitTest: (List(google, baidu, tenxun),List(alibaba, apples))\n```\n### **元素选择 apply方法和indices方法**\napply方法是获取第几个值，list.apply(2)和list(2)是一样的  \nindices是获取所有的列表的Range,暂时不知道有多大用处\n```scala\ndef main(args: Array[String]) {\n    var ls = \"google\"::\"baidu\"::\"tenxun\"::\"alibaba\"::\"apples\"::\"stackoverflow\"::Nil\n    var applyTest = ls.apply(2)\n    var indicesTest =  ls.indices\n    println(\"applyTest: \"+applyTest)\n    println(\"applyTest2: \"+ls(2))\n    println(\"indicesTest: \"+indicesTest)\n    for(i<-indicesTest){\n      println(ls(i))\n    }\n  }\n-------------输出--------\napplyTest: tenxun\napplyTest2: tenxun\nindicesTest: Range(0, 1, 2, 3, 4, 5)\ngoogle\nbaidu\ntenxun\nalibaba\napples\nstackoverflow\n\n\n```\n### **toString和mkString**\ntoString和java的一样\nmkString  mkString(start: String,sep: String,end: String): String  \n第一个参数是以什么开始，第二个参数是以什么分割，第三个参数是以什么结尾，函数返回一个String  \n```scala\ndef main(args: Array[String]) {\n    var ls = \"google\" :: \"baidu\" :: \"tenxun\" :: \"alibaba\" :: \"apples\" :: \"stackoverflow\" :: Nil\n    println(\"toString: \"+ls.toString())\n    println(\"mkString: \"+ls.mkString(\"{\",\";\",\"}\"))\n  }\n---------输出----------\ntoString: List(google, baidu, tenxun, alibaba, apples, stackoverflow)\nmkString: {google;baidu;tenxun;alibaba;apples;stackoverflow{\n```\n### **转换列表 toArray element iterator**\n想要在数组Array和列表list之间转换，可以使用List的toArray和Array的toList  \n例子\n```\nscala> var ls = \"google\" :: \"baidu\" :: \"tenxun\" :: \"alibaba\" :: \"apples\" :: \"stackoverflow\" :: Nil\nls: List[String] = List(google, baidu, tenxun, alibaba, apples, stackoverflow)\n\nscala> var arrays = ls.toArray\narrays: Array[String] = Array(google, baidu, tenxun, alibaba, apples, stackoverflow)\n\nscala> var lists = arrays.toList\nlists: List[String] = List(google, baidu, tenxun, alibaba, apples, stackoverflow)\n```\nelement在很旧的版本有，现在已经过时不用了。如果要用枚举器访问列表元素，可以使用iterator \n```scala\ndef main(args: Array[String]) {\n    var ls = \"google\" :: \"baidu\" :: \"tenxun\" :: \"alibaba\" :: \"apples\" :: \"stackoverflow\" :: Nil\n     val it =ls.iterator\n      while(it.hasNext){\n        print(it.next()+\",\")\n      }\n  }\n--------------输出------------\ngoogle, baidu, tenxun, alibaba, apples, stackoverflow, \n```\n## **list类的高阶用法**\n在java中，若要提取出满足特点条件的元素，或者检查所有元素是否满足某种性质，或者用某种方式转变列表的所有元素，这样的需求一般都需要使用for或者while循环的固定表达式，在scala中，可以通过使用List的一些高阶方法（函数）来更为简介的实现  \n### **列表间映射：map、flatMap和foreach**\n1. xs map f 操作返回**把函数f应用在xs的每个列表元素之后**由此**组成的新列表**。如：\n```\nscala> List(1,2,3).map(_ +1)\nres1: List[Int] = List(2, 3, 4)\n```\n```scala\nscala> val words = List(\"zks\",\"zhaikaishun\",\"kaishun\",\"kai\",\"xiaozhai\")\nwords: List[String] = List(zks, zhaikaishun, kaishun, kai, xiaozhai)\n\nscala> words.map(_.length)\nres2: List[Int] = List(3, 11, 7, 3, 8)\n```\n2. flatMap操作符与map类似，不过它的右操作元是能够返回元素列表的函数。它对列表的每个元素调用该方法，**然后连接所有方法**的结果并返回。map与flatMap的差异举例说明如下：\n```scala\nscala> words.map(_.toList)\nres3: List[List[Char]] = List(List(z, k, s), List(z, h, a, i, k, a, i, s, h, u, n), List(k, a, i, s, h, u, n), List(k, a, i), List(x, i, a, o, z, h, a, i))\n\nscala> words.flatMap(_.toList)\nres4: List[Char] = List(z, k, s, z, h, a, i, k, a, i, s, h, u, n, k, a, i, s, h, u, n, k, a, i, x, i, a, o, z, h, a, i)\n```\nmap与flatMap的差异和协作可以用下面的例子体会\n```scala\nscala> List.range(1, 5).flatMap(i => List.range(1, i).map(j => (i, j)))\nres9: List[(Int, Int)] = List((2,1), (3,1), (3,2), (4,1), (4,2), (4,3))\n```\n解释 : List.range(1,5)生成了List(1,2,3,4),注意没有5  \n.flatMap对内部的每一个元素进行操作, 后面有个.map是对内部List.range(1, i)的每一个元素进行操作，最后flatMap返回的还是一个List  \n上述例子也可以用for循环+yield来完成\n```scala\nscala>  for (i <- List.range(1, 5); j <- List.range(1, i)) yield (i,j)\nres10: List[(Int, Int)] = List((2,1), (3,1), (3,2), (4,1), (4,2), (4,3))\n```\n3. foreach是第三种与映射类似的操作。它的右操作元是过程（返回Unit的函数）。它只是对每个列表元素都调用一遍过程。操作的结果仍然是Unit，不会产生结果列表。例如：\n```scala\ndef main(args: Array[String]) {\n    var sum =0\n    List(1, 2, 3, 4, 5) foreach (sum += _)\n    println(sum)\n  }\n-----输出----\n15\n```\n### **列表过滤：filter、partition、find、takeWhile、dropWhile和span**\n1.xs filter p操作产生xs中符合p（x）为true的所有元素组成的列表。如：\n```scala\nscala> List (1, 2, 3, 4, 5) filter (_ % 2 == 0)\nres10: List[Int] = List(2, 4)\n\nscala> words filter (_.length == 3)\nres11: List[String] = List(the, fox)\n```\n2.partition方法与filter类似，不过返回的是列表对。其中一个包含所有论断为真的元素，另一个包含所有论断为假的元素。\n**xs partition p  等价于 (xs filter p, xs filter (!p()))  **\n举例如下：\n```scala\nscala> List(1, 2, 3, 4, 5) partition (_ % 2 ==0)\nres12: (List[Int], List[Int]) = (List(2, 4),List(1, 3, 5))\n```\n3.find方法同样与filter方法类似，不过返回的是第一个满足给定论断的元素，而并不是全部。xs find p 操作以列表xs和论断p为操作元。返回可选值。如果xs中存在元素x使得p（x）为真，Some（x）将返回。否则，若p对所有元素都不成立，None将返回。举例如下：\n```scala\nscala> List(1, 2, 3, 4, 5) find (_ % 2 == 0)\nres13: Option[Int] = Some(2)\n\nscala> List(1, 2, 3, 4, 5) find (_  <= 0)\nres15: Option[Int] = None\n```\n4. xs takeWhile p操作返回列表xs中最长的能够满足p的前缀。例如：\n```scala\nscala> List(1, 2, 3, -4, 5) takeWhile (_ > 0)\nres16: List[Int] = List(1, 2, 3)\n```\n5.xs dropWhile p操作移除最长能够满足p的前缀。举例如下：\n```scala\nscala> val words = List(\"the\", \"quick\", \"brown\", \"fox\")\nwords: List[String] = List(the, quick, brown, fox)\n\nscala> words dropWhile (_ startsWith \"t\")\nres11: List[String] = List(quick, brown, fox)\n```\n6.span方法把takeWhile和dropWhile组合成一个操作。它返回一对列表，定义与下列等式一致：\nxs span p 等价于 （xs takeWhile p， xs dropWhile p） \n```scala\nscala> List(1, 2, 3, -4, 5) span (_ >0)\nres18: (List[Int], List[Int]) = (List(1, 2, 3),List(-4, 5))\n```\n###  **列表的论断：forall和exists**\n1. xs forall p 如果列表的所有元素满足p则返回true\n2. xs exists p 如果列表中有一个值满足p就返回true\n```scala\n  def hasZeroRow(m: List[List[Int]]) = m.exists(row => row forall (_ == 0))\n  def main(args: Array[String]) {\n    val m= List(List(3,0,0), List(0,3,0), List(0,0,3))\n    var flag :Boolean= hasZeroRow(m)\n    println(flag)\n  }\n----------输出--------\nfalse\n```\n### **折叠操作**\n如果我们把集合看成是一张纸条，每一小段代表一个元素，那么reduceLeft就将这张纸条从左向右”折叠”，最前面的两个元素会首先“重叠”在一起，这时会使用传给reduceLeft的参数函数进行计算，返回的结果就像是已经折叠在一起的两段纸条，它们已经是一个叠加的状态了，所以它，也就是上次重叠的结果会继续做为一个单一的值和下一个元素继续“叠加”，直到折叠到集合的最后一个元素\n1. **reduceLeft**\n```scala\nscala>  List.range(1, 5).reduceLeft(_+_)\nres12: Int = 10\n```\n2. **reduceRight**\n和reduceLeft相似，但是是从右向左折叠,**注意**:==它的操作方向是从右到左，但是参数的顺序却并不是，而是依然第一参数是左边的元素，第二参数是右边的元素==\n```scala\nscala> List.range(1, 4) reduceRight(_ - _)\nres15: Int = 2\n// 2-3 = -1\n// 1-(-1)=2\n```\n3. **foldLeft**\n类似于reduceLeft, 不过开始折叠的第一个元素不是集合中的第一个元素，而是传入的一个元素，有点类似于先将这个参数放入的集合中的首位，然后在reduceLeft\n```scala\nscala> List.range(1, 5).foldLeft(1)(_+_)\nres1: Int = 11\n```\n4. **foldRight**\n类似于foldLeft，不过是从右向左，不再举例\n```scala\nscala> List.range(1, 4).foldRight(3)(_-_)\nres1: Int = -1\n```\n# **scala 集合Set**\nSet和List基本相似，只是所有的元素都是唯一的\n默认的Set是不可变的,默认引用的是 scala.collection.immutable.Set\n```scala\nval set = Set(1,2,3)\nprintln(set.getClass.getName) // \n\nprintln(set.exists(_ % 2 == 0)) //true\nprintln(set.drop(1)) //Set(2,3)\n```\n如果需要使用可变集合需要引入 scala.collection.mutable.Set：\n```scala\nimport scala.collection.mutable.Set // 可以在任何地方引入 可变集合\n\nval mutableSet = Set(1,2,3)\nprintln(mutableSet.getClass.getName) // scala.collection.mutable.HashSet\n\nmutableSet.add(4)\nmutableSet.remove(1)\nmutableSet += 5\nmutableSet -= 2\n\nprintln(mutableSet) // Set(5, 3, 4)\n\nval another = mutableSet.toSet\nprintln(another.getClass.getName) // scala.collection.immutable.Set\n```\n\n## **连接集合**\n```scala\n var site = site1 ++ site2\n```\n## **查找集合中最大与最小元素**\nSet.min  \nSet.max\n\n## **交集**\nSet.intersect\n```scala\n set1.intersect(set2)\n```\n\n# **map**\nap(映射)是一种可迭代的键值对（key/value）结构。\n所有的值都可以通过键来获取。\nMap 中的键都是唯一的。\nMap 也叫哈希表（Hash tables）。\nMap 有两种类型，可变与不可变，区别在于可变对象可以修改它，而不可变对象不可以。\n默认情况下 Scala 使用不可变 Map。如果你需要使用可变集合，你需要显式的引入 import scala.collection.mutable.Map 类\n在 Scala 中 你可以同时使用可变与不可变 Map，不可变的直接使用 Map，可变的使用 mutable.Map。以下实例演示了不可变 Map 的应用：\n不可变map\n```scala\nscala> val colors = Map(\"red\" -> \"#FF0000\", \"azure\" -> \"#F0FFFF\")\ncolors: scala.collection.immutable.Map[String,String] = Map(red -> #FF0000, azure -> #F0FFFF)\n```  \n## **Map的赋值**  \n如果需要添加 key-value 对，可以使用 + 号，如下所示：\n```scala\n// 空哈希表，键为字符串，值为整型\nvar A:Map[Char,Int] = Map()\nA += ('I' -> 1)\nA += ('J' -> 5)\nA += ('K' -> 10)\nA += ('L' -> 100)\nprintln（A）\n-------输出--------\nMap(I -> 1, J -> 5, K -> 10, L -> 100)\n```\n## **Map的基本操作**\n- keys      返回 Map 所有的键(key)\n- values\t返回 Map 所有的值(value)\n- isEmpty\t在 Map 为空时返回true  \n例子\n```\nobject Test {\n   def main(args: Array[String]) {\n      val colors = Map(\"red\" -> \"#FF0000\",\n                       \"azure\" -> \"#F0FFFF\",\n                       \"peru\" -> \"#CD853F\")\n\n      val nums: Map[Int, Int] = Map()\n\n      println( \"colors 中的键为 : \" + colors.keys )\n      println( \"colors 中的值为 : \" + colors.values )\n      println( \"检测 colors 是否为空 : \" + colors.isEmpty )\n      println( \"检测 nums 是否为空 : \" + nums.isEmpty )\n   }\n}\n---------输出---------\ncolors 中的键为 : Set(red, azure, peru)\ncolors 中的值为 : MapLike(#FF0000, #F0FFFF, #CD853F)\n检测 colors 是否为空 : false\n检测 nums 是否为空 : true\n```\n## **Map 合并**\n++ 运算符或 Map.++() 方法来连接两个 Map, Map 合并时会移除重复的 key。\n```scala\nval colors1 = Map(\"red\" -> \"#FF0000\",\n      \"azure\" -> \"#F0FFFF\",\n      \"peru\" -> \"#CD853F\")\n    val colors2 = Map(\"blue\" -> \"#0033FF\",\n      \"yellow\" -> \"#FFFF00\",\n      \"red\" -> \"#FF0001\")\n\n    //  ++ 作为运算符\n    var colors = colors1 ++ colors2\n    println( \"colors1 ++ colors2 : \" + colors )\n\n    //  ++ 作为方法\n    colors = colors1.++(colors2)\n    println( \"colors1.++(colors2)) : \" + colors )\n--------输出--------------------\ncolors1 ++ colors2 : Map(blue -> #0033FF, azure -> #F0FFFF, peru -> #CD853F, yellow -> #FFFF00, red -> #FF0001)\ncolors1.++(colors2)) : Map(blue -> #0033FF, azure -> #F0FFFF, peru -> #CD853F, yellow -> #FFFF00, red -> #FF0001)\n```  \n## **输出map和key**\n```scala\ndef main(args: Array[String]): Unit = {\n    val sites = Map(\"runoob\" -> \"http://www.runoob.com\",\n      \"baidu\" -> \"http://www.baidu.com\",\n      \"taobao\" -> \"http://www.taobao.com\")\n    sites.keys.foreach{\n      i=>print(\"key: \"+i)\n        println(\"value: \"+sites(i))\n    }\n  }\n----------输出-----\nkey: runoobvalue: http://www.runoob.com\nkey: baiduvalue: http://www.baidu.com\nkey: taobaovalue: http://www.taobao.com\n```  \n##  **Map.contains查看是否存在指定的key**\n\n## **元组**\n元组可以把固定数量的条目组合在一起以便于整体的传送，不像数组或者列表，元祖可以保存不同类型的对象    \n元组的值是通过将单个的值包含在圆括号中构成的。例如\n```scala\nval t = (1, 3.14, \"Fred\") \n```\n以上实例在元组中定义了三个元素，对应的类型分别为[Int, Double, java.lang.String]。\n此外我们也可以使用以下方式来定义：\n```scala\nval t = new Tuple3(1, 3.14, \"Fred\")\n```  \n### **定义与取值**\n元组的实际类型取决于它的元素的类型，比如 (99, \"runoob\") 是 Tuple2[Int, String]。 ('u', 'r', \"the\", 1, 4, \"me\") 为 Tuple6[Char, Char, String, Int, Int, String]。  \n目前 Scala 支持的元组最大长度为 22。对于更大长度你可以使用集合，或者扩展元组。\n访问元组的元素可以通过数字索引，如下一个元组：  \n我们可以使用 t.\\_1 访问第一个元素， t.\\_2 访问第二个元素，如下所示：  \n```scala\n  def main(args: Array[String]) {\n    val t = (4,3,2,1)\n\n    val sum = t._1 + t._2 + t._3 + t._4\n    var secondTuple=t._2\n    println(\"第二个元素为: \"+secondTuple)\n    println( \"元素之和为: \"  + sum )\n  }\n----输出----------\n第二个元素为: 3\n元素之和为: 10\n\n```\n### **迭代元组**\n你可以使用 Tuple.productIterator() 方法来迭代输出元组的所有元素：\n```scala\n  def main(args: Array[String]) {\n    val t = (4,3,2,1)\n    t.productIterator.foreach(i=>println(\"value: \"+i))\n  }\n--------输出--------------\nvalue: 4\nvalue: 3\nvalue: 2\nvalue: 1\n```  \n### **元组转为字符串**\nTuple.toString()  \n### **元素交换**\nTuple.swap 方法来交换元组的元素  \n```scala\n  def main(args: Array[String]) {\n      val t = new Tuple2(\"www.google.com\", \"http://blog.csdn.net/t1dmzks\")\n      println(\"交换后的元组: \" + t )\n    }\n-------输出-----------\n交换后的元组: (www.google.com,www.runoob.com)\n```  \n# **Scala Option**\nTODO\n\n# **Scala Iterator**\nScala Iterator（迭代器）不是一个集合，它是一种用于访问集合的方法。\n迭代器 it 的两个基本操作是 next 和 hasNext。\n调用 it.next() 会返回迭代器的下一个元素，并且更新迭代器的状态。\n调用 it.hasNext() 用于检测集合中是否还有元素。\n让迭代器 it 逐个返回所有元素最简单的方法是使用 while 循环：\n```scala\nobject Test {\n   def main(args: Array[String]) {\n      val it = Iterator(\"Baidu\", \"Google\", \"Runoob\", \"Taobao\")\n      \n      while (it.hasNext){\n         println(it.next())\n      }\n   }\n}\n```\n## **查找最大与最小元素**\n it.min 和 it.max 方法\n\n## **获取迭代器的长度**\nit.size 或 it.length \n","slug":"scala-collect","published":1,"updated":"2018-01-22T15:55:31.087Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcrpnzq9002b2wv3jey3i00t","content":"<p>本文参考至scala编程，菜鸟教程，然后将自己的判断以及重要方法的提取，解释，合并</p>\n<h1 id=\"字符串\"><a href=\"#字符串\" class=\"headerlink\" title=\"字符串\"></a><strong>字符串</strong></h1><p>在 Scala 中，字符串的类型实际上是 Java String，它本身没有 String 类。在 Scala 中，String 是一个不可变的对象，所以该对象不可被修改。这就意味着你如果修改字符串就会产生一个新的字符串对象。  </p>\n<p> String 对象是不可变的，如果你需要创建一个可以修改的字符串，可以使用 String Builder 类，如下实例:<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\"> <span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Test</span> </span>&#123;</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">      <span class=\"keyword\">val</span> buf = <span class=\"keyword\">new</span> <span class=\"type\">StringBuilder</span>;</div><div class=\"line\">      buf += 'a'</div><div class=\"line\">      buf ++= <span class=\"string\">\"bcdef\"</span></div><div class=\"line\">      println( <span class=\"string\">\"buf is : \"</span> + buf.toString );</div><div class=\"line\">   &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>同java一样，scala的字符串用过length()方法得到长度，String 类中使用string1.concat(string2); 方法来连接两个字符串<br>也可以直接用+号连接字符串<br>java.lang.String的所有方法，在scala中也可以使用, 这里不仔细介绍</p>\n<h1 id=\"数组\"><a href=\"#数组\" class=\"headerlink\" title=\"数组\"></a><strong>数组</strong></h1><h2 id=\"声明数组\"><a href=\"#声明数组\" class=\"headerlink\" title=\"声明数组\"></a><strong>声明数组</strong></h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">var z:Array[String] = new Array[String](3)</div><div class=\"line\"></div><div class=\"line\">或</div><div class=\"line\"></div><div class=\"line\">var z = new Array[String](3)</div></pre></td></tr></table></figure>\n<h2 id=\"赋值\"><a href=\"#赋值\" class=\"headerlink\" title=\"赋值\"></a><strong>赋值</strong></h2><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">z(<span class=\"number\">0</span>) = <span class=\"string\">\"Runoob\"</span>; z(<span class=\"number\">1</span>) = <span class=\"string\">\"Baidu\"</span>; z(<span class=\"number\">4</span>/<span class=\"number\">2</span>) = <span class=\"string\">\"Google\"</span></div></pre></td></tr></table></figure>\n<p>也可以这样定义一个数组<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">var</span> z = <span class=\"type\">Array</span>(<span class=\"string\">\"Runoob\"</span>, <span class=\"string\">\"Baidu\"</span>, <span class=\"string\">\"Google\"</span>)</div></pre></td></tr></table></figure></p>\n<h2 id=\"处理数组\"><a href=\"#处理数组\" class=\"headerlink\" title=\"处理数组\"></a><strong>处理数组</strong></h2><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Test</span> </span>&#123;</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">      <span class=\"keyword\">var</span> myList = <span class=\"type\">Array</span>(<span class=\"number\">1.9</span>, <span class=\"number\">2.9</span>, <span class=\"number\">3.4</span>, <span class=\"number\">3.5</span>)</div><div class=\"line\">      </div><div class=\"line\">      <span class=\"comment\">// 输出所有数组元素</span></div><div class=\"line\">      <span class=\"keyword\">for</span> ( x &lt;- myList ) &#123;</div><div class=\"line\">         println( x )</div><div class=\"line\">      &#125;</div><div class=\"line\"></div><div class=\"line\">      <span class=\"comment\">// 计算数组所有元素的总和</span></div><div class=\"line\">      <span class=\"keyword\">var</span> total = <span class=\"number\">0.0</span>;</div><div class=\"line\">      <span class=\"keyword\">for</span> ( i &lt;- <span class=\"number\">0</span> to (myList.length - <span class=\"number\">1</span>)) &#123;</div><div class=\"line\">         total += myList(i);</div><div class=\"line\">      &#125;</div><div class=\"line\">      println(<span class=\"string\">\"总和为 \"</span> + total);</div><div class=\"line\"></div><div class=\"line\">      <span class=\"comment\">// 查找数组中的最大元素</span></div><div class=\"line\">      <span class=\"keyword\">var</span> max = myList(<span class=\"number\">0</span>);</div><div class=\"line\">      <span class=\"keyword\">for</span> ( i &lt;- <span class=\"number\">1</span> to (myList.length - <span class=\"number\">1</span>) ) &#123;</div><div class=\"line\">         <span class=\"keyword\">if</span> (myList(i) &gt; max) max = myList(i);</div><div class=\"line\">      &#125;</div><div class=\"line\">      println(<span class=\"string\">\"最大值为 \"</span> + max);</div><div class=\"line\">    </div><div class=\"line\">   &#125;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">-------输出-------</div><div class=\"line\"><span class=\"number\">1.9</span></div><div class=\"line\"><span class=\"number\">2.9</span></div><div class=\"line\"><span class=\"number\">3.4</span></div><div class=\"line\"><span class=\"number\">3.5</span></div><div class=\"line\">总和为 <span class=\"number\">11.7</span></div><div class=\"line\">最大值为 <span class=\"number\">3.5</span></div></pre></td></tr></table></figure>\n<h2 id=\"多维数组\"><a href=\"#多维数组\" class=\"headerlink\" title=\"多维数组\"></a><strong>多维数组</strong></h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div></pre></td><td class=\"code\"><pre><div class=\"line\">def main(args: Array[String]) &#123;</div><div class=\"line\">      var myMatrix = ofDim[Int](3,3)</div><div class=\"line\">      </div><div class=\"line\">      // 创建矩阵</div><div class=\"line\">      for (i &lt;- 0 to 2) &#123;</div><div class=\"line\">         for ( j &lt;- 0 to 2) &#123;</div><div class=\"line\">            myMatrix(i)(j) = j;</div><div class=\"line\">         &#125;</div><div class=\"line\">      &#125;</div><div class=\"line\">      </div><div class=\"line\">      // 打印二维阵列</div><div class=\"line\">      for (i &lt;- 0 to 2) &#123;</div><div class=\"line\">         for ( j &lt;- 0 to 2) &#123;</div><div class=\"line\">            print(&quot; &quot; + myMatrix(i)(j));</div><div class=\"line\">         &#125;</div><div class=\"line\">         println();</div><div class=\"line\">      &#125;</div><div class=\"line\">    </div><div class=\"line\">   &#125;</div></pre></td></tr></table></figure>\n<h2 id=\"合并数组\"><a href=\"#合并数组\" class=\"headerlink\" title=\"合并数组\"></a><strong>合并数组</strong></h2><p>使用concat() 方法来合并两个数组<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">      <span class=\"keyword\">var</span> myList1 = <span class=\"type\">Array</span>(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>,)</div><div class=\"line\">      <span class=\"keyword\">var</span> myList2 = <span class=\"type\">Array</span>(<span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>)</div><div class=\"line\"></div><div class=\"line\">      <span class=\"keyword\">var</span> myList3 =  concat( myList1, myList2)</div><div class=\"line\">      </div><div class=\"line\">      <span class=\"comment\">// 输出所有数组元素</span></div><div class=\"line\">      <span class=\"keyword\">for</span> ( x &lt;- myList3 ) &#123;</div><div class=\"line\">         println( x )</div><div class=\"line\">      &#125;</div><div class=\"line\">   &#125;</div><div class=\"line\">   </div><div class=\"line\">----------输出-----------</div><div class=\"line\"><span class=\"number\">1</span></div><div class=\"line\"><span class=\"number\">2</span></div><div class=\"line\"><span class=\"number\">3</span></div><div class=\"line\"><span class=\"number\">4</span></div><div class=\"line\"><span class=\"number\">5</span></div><div class=\"line\"><span class=\"number\">6</span></div><div class=\"line\"><span class=\"number\">7</span></div><div class=\"line\"><span class=\"number\">8</span></div></pre></td></tr></table></figure></p>\n<h2 id=\"创建区间数组\"><a href=\"#创建区间数组\" class=\"headerlink\" title=\"创建区间数组\"></a><strong>创建区间数组</strong></h2><p>以下实例中，我们使用了 range() 方法来生成一个区间范围内的数组。range() 方法最后一个参数为步长，默认为 1：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">      <span class=\"keyword\">var</span> myList1 = range(<span class=\"number\">10</span>, <span class=\"number\">20</span>, <span class=\"number\">2</span>)</div><div class=\"line\">      <span class=\"keyword\">var</span> myList2 = range(<span class=\"number\">10</span>,<span class=\"number\">20</span>)</div><div class=\"line\"></div><div class=\"line\">      <span class=\"comment\">// 输出所有数组元素</span></div><div class=\"line\">      <span class=\"keyword\">for</span> ( x &lt;- myList1 ) &#123;</div><div class=\"line\">         print( <span class=\"string\">\" \"</span> + x )</div><div class=\"line\">      &#125;</div><div class=\"line\">      println()</div><div class=\"line\">      <span class=\"keyword\">for</span> ( x &lt;- myList2 ) &#123;</div><div class=\"line\">         print( <span class=\"string\">\" \"</span> + x )</div><div class=\"line\">      &#125;</div><div class=\"line\">   &#125;</div><div class=\"line\">   -----------------输出---------------</div><div class=\"line\"><span class=\"number\">10</span> <span class=\"number\">12</span> <span class=\"number\">14</span> <span class=\"number\">16</span> <span class=\"number\">18</span></div><div class=\"line\"><span class=\"number\">10</span> <span class=\"number\">11</span> <span class=\"number\">12</span> <span class=\"number\">13</span> <span class=\"number\">14</span> <span class=\"number\">15</span> <span class=\"number\">16</span> <span class=\"number\">17</span> <span class=\"number\">18</span> <span class=\"number\">19</span></div></pre></td></tr></table></figure></p>\n<h1 id=\"Scala-集合List\"><a href=\"#Scala-集合List\" class=\"headerlink\" title=\"Scala 集合List\"></a><strong>Scala 集合List</strong></h1><p>Scala 列表类似于数组，它们所有元素的类型都相同，但是它们也有所不同：列表是不可变的，值一旦被定义了就不能改变，其次列表 具有递归的结构（也就是链接表结构）而数组不是。。<br>列表的元素类型 T 可以写成 List[T]。例如，以下列出了多种类型的列表：</p>\n<h2 id=\"列表构造方式\"><a href=\"#列表构造方式\" class=\"headerlink\" title=\"列表构造方式\"></a><strong>列表构造方式</strong></h2><h3 id=\"构造列表方式一\"><a href=\"#构造列表方式一\" class=\"headerlink\" title=\"构造列表方式一\"></a><strong>构造列表方式一</strong></h3><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// 字符串列表</span></div><div class=\"line\"><span class=\"keyword\">val</span> site: <span class=\"type\">List</span>[<span class=\"type\">String</span>] = <span class=\"type\">List</span>(<span class=\"string\">\"Runoob\"</span>, <span class=\"string\">\"Google\"</span>, <span class=\"string\">\"Baidu\"</span>)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 整型列表</span></div><div class=\"line\"><span class=\"keyword\">val</span> nums: <span class=\"type\">List</span>[<span class=\"type\">Int</span>] = <span class=\"type\">List</span>(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 空列表</span></div><div class=\"line\"><span class=\"keyword\">val</span> empty: <span class=\"type\">List</span>[<span class=\"type\">Nothing</span>] = <span class=\"type\">List</span>()</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 二维列表</span></div><div class=\"line\"><span class=\"keyword\">val</span> dim: <span class=\"type\">List</span>[<span class=\"type\">List</span>[<span class=\"type\">Int</span>]] =</div><div class=\"line\">   <span class=\"type\">List</span>(</div><div class=\"line\">      <span class=\"type\">List</span>(<span class=\"number\">1</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>),</div><div class=\"line\">      <span class=\"type\">List</span>(<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">0</span>),</div><div class=\"line\">      <span class=\"type\">List</span>(<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>)</div><div class=\"line\">   )</div></pre></td></tr></table></figure>\n<h3 id=\"构造列表方式二\"><a href=\"#构造列表方式二\" class=\"headerlink\" title=\"构造列表方式二\"></a><strong>构造列表方式二</strong></h3><p>构造列表的两个基本单位是 Nil 和 ::(发音为cons) Nil代表空列表，中缀符号:: 表示列表从前端扩展。也就是说x::xs代表了一个元素为x，后面紧贴着xs的列表，因此以上实例我们可以写成<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 字符串列表</span></div><div class=\"line\"><span class=\"keyword\">val</span> site = <span class=\"string\">\"Runoob\"</span> :: (<span class=\"string\">\"Google\"</span> :: (<span class=\"string\">\"Baidu\"</span> :: <span class=\"type\">Nil</span>))</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 整型列表</span></div><div class=\"line\"><span class=\"keyword\">val</span> nums = <span class=\"number\">1</span> :: (<span class=\"number\">2</span> :: (<span class=\"number\">3</span> :: (<span class=\"number\">4</span> :: <span class=\"type\">Nil</span>)))</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 空列表</span></div><div class=\"line\"><span class=\"keyword\">val</span> empty = <span class=\"type\">Nil</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 二维列表</span></div><div class=\"line\"><span class=\"keyword\">val</span> dim = (<span class=\"number\">1</span> :: (<span class=\"number\">0</span> :: (<span class=\"number\">0</span> :: <span class=\"type\">Nil</span>))) ::</div><div class=\"line\">          (<span class=\"number\">0</span> :: (<span class=\"number\">1</span> :: (<span class=\"number\">0</span> :: <span class=\"type\">Nil</span>))) ::</div><div class=\"line\">          (<span class=\"number\">0</span> :: (<span class=\"number\">0</span> :: (<span class=\"number\">1</span> :: <span class=\"type\">Nil</span>))) :: <span class=\"type\">Nil</span></div></pre></td></tr></table></figure></p>\n<h3 id=\"方式二的简化\"><a href=\"#方式二的简化\" class=\"headerlink\" title=\"方式二的简化\"></a><strong>方式二的简化</strong></h3><p>由于以::结尾，::遵循右结合的规则，A::B::C 等价于A::(B::C), 因此，前面定义用到的括号可以去掉<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">val</span> names = <span class=\"number\">1</span>::<span class=\"number\">2</span>::<span class=\"number\">3</span>::<span class=\"number\">4</span>::<span class=\"type\">Nil</span></div></pre></td></tr></table></figure></p>\n<p>与前面的names定义一致</p>\n<h2 id=\"列表的基本操作\"><a href=\"#列表的基本操作\" class=\"headerlink\" title=\"列表的基本操作\"></a><strong>列表的基本操作</strong></h2><ul>\n<li>Scala列表有三个基本操作：</li>\n<li>head 返回列表第一个元素</li>\n<li>tail 返回一个列表，包含除了第一元素之外的其他元素</li>\n<li>isEmpty 在列表为空时返回true<br>对于Scala列表的任何操作都可以使用这三个基本操作来表达。实例如下:<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Test</span> </span>&#123;</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">      <span class=\"keyword\">val</span> site = <span class=\"string\">\"Runoob\"</span> :: (<span class=\"string\">\"Google\"</span> :: (<span class=\"string\">\"Baidu\"</span> :: <span class=\"type\">Nil</span>))</div><div class=\"line\">      <span class=\"keyword\">val</span> nums = <span class=\"type\">Nil</span></div><div class=\"line\"></div><div class=\"line\">      println( <span class=\"string\">\"第一网站是 : \"</span> + site.head )</div><div class=\"line\">      println( <span class=\"string\">\"最后一个网站是 : \"</span> + site.tail )</div><div class=\"line\">      println( <span class=\"string\">\"查看列表 site 是否为空 : \"</span> + site.isEmpty )</div><div class=\"line\">      println( <span class=\"string\">\"查看 nums 是否为空 : \"</span> + nums.isEmpty )</div><div class=\"line\">   &#125;</div><div class=\"line\">&#125;</div><div class=\"line\">-----------输出--------</div><div class=\"line\">第一网站是 : <span class=\"type\">Runoob</span></div><div class=\"line\">最后一个网站是 : <span class=\"type\">List</span>(<span class=\"type\">Google</span>, <span class=\"type\">Baidu</span>)</div><div class=\"line\">查看列表 site 是否为空 : <span class=\"literal\">false</span></div><div class=\"line\">查看 nums 是否为空 : <span class=\"literal\">true</span></div></pre></td></tr></table></figure>\n</li>\n</ul>\n<h2 id=\"List类的一阶方法\"><a href=\"#List类的一阶方法\" class=\"headerlink\" title=\"List类的一阶方法\"></a><strong>List类的一阶方法</strong></h2><h3 id=\"连接列表\"><a href=\"#连接列表\" class=\"headerlink\" title=\"连接列表\"></a><strong>连接列表</strong></h3><p>你可以使用 ::: 运算符或 List.:::() 方法或 List.concat() 方法来连接两个或多个列表。实例如下:<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Test</span> </span>&#123;</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">      <span class=\"keyword\">val</span> site1 = <span class=\"string\">\"Runoob\"</span> :: (<span class=\"string\">\"Google\"</span> :: (<span class=\"string\">\"Baidu\"</span> :: <span class=\"type\">Nil</span>))</div><div class=\"line\">      <span class=\"keyword\">val</span> site2 = <span class=\"string\">\"Facebook\"</span> :: (<span class=\"string\">\"Taobao\"</span> :: <span class=\"type\">Nil</span>)</div><div class=\"line\"></div><div class=\"line\">      <span class=\"comment\">// 使用 ::: 运算符</span></div><div class=\"line\">      <span class=\"keyword\">var</span> fruit = site1 ::: site2</div><div class=\"line\">      println( <span class=\"string\">\"site1 ::: site2 : \"</span> + fruit )</div><div class=\"line\">      </div><div class=\"line\">      <span class=\"comment\">// 使用 Set.:::() 方法</span></div><div class=\"line\">      fruit = site1.:::(site2)</div><div class=\"line\">      println( <span class=\"string\">\"site1.:::(site2) : \"</span> + fruit )</div><div class=\"line\"></div><div class=\"line\">      <span class=\"comment\">// 使用 concat 方法</span></div><div class=\"line\">      fruit = <span class=\"type\">List</span>.concat(site1, site2)</div><div class=\"line\">      println( <span class=\"string\">\"List.concat(site1, site2) : \"</span> + fruit  )</div><div class=\"line\">      </div><div class=\"line\"></div><div class=\"line\">   &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h3 id=\"List-fill\"><a href=\"#List-fill\" class=\"headerlink\" title=\"List.fill()\"></a><strong>List.fill()</strong></h3><p>我们可以使用 List.fill() 方法来创建一个指定重复数量的元素列表：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Test</span> </span>&#123;</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">      <span class=\"keyword\">val</span> site = <span class=\"type\">List</span>.fill(<span class=\"number\">3</span>)(<span class=\"string\">\"Runoob\"</span>) <span class=\"comment\">// 重复 Runoob 3次</span></div><div class=\"line\">      println( <span class=\"string\">\"site : \"</span> + site  )</div><div class=\"line\"></div><div class=\"line\">      <span class=\"keyword\">val</span> num = <span class=\"type\">List</span>.fill(<span class=\"number\">10</span>)(<span class=\"number\">2</span>)         <span class=\"comment\">// 重复元素 2, 10 次</span></div><div class=\"line\">      println( <span class=\"string\">\"num : \"</span> + num  )</div><div class=\"line\">   &#125;</div><div class=\"line\">&#125;</div><div class=\"line\">---------输出--------</div><div class=\"line\">site : <span class=\"type\">List</span>(<span class=\"type\">Runoob</span>, <span class=\"type\">Runoob</span>, <span class=\"type\">Runoob</span>)</div><div class=\"line\">num : <span class=\"type\">List</span>(<span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>)</div></pre></td></tr></table></figure></p>\n<h3 id=\"List-tabulate\"><a href=\"#List-tabulate\" class=\"headerlink\" title=\"List.tabulate()\"></a><strong>List.tabulate()</strong></h3><p>List.tabulate() 方法是通过给定的函数来创建列表。<br>方法的第一个参数为元素的数量，可以是二维的，第二个参数为指定的函数，我们通过指定的函数计算结果并返回值插入到列表中，起始值为 0，实例如下：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Test</span> </span>&#123;</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">      <span class=\"comment\">// 通过给定的函数创建 5 个元素</span></div><div class=\"line\">      <span class=\"keyword\">val</span> squares = <span class=\"type\">List</span>.tabulate(<span class=\"number\">6</span>)(n =&gt; n * n)</div><div class=\"line\">      println( <span class=\"string\">\"一维 : \"</span> + squares  )</div><div class=\"line\"></div><div class=\"line\">      <span class=\"comment\">// 创建二维列表</span></div><div class=\"line\">      <span class=\"keyword\">val</span> mul = <span class=\"type\">List</span>.tabulate( <span class=\"number\">4</span>,<span class=\"number\">5</span> )( _ * _ )      </div><div class=\"line\">      println( <span class=\"string\">\"多维 : \"</span> + mul  )</div><div class=\"line\">   &#125;</div><div class=\"line\">&#125;</div><div class=\"line\">----------------输出---------</div><div class=\"line\">一维 : <span class=\"type\">List</span>(<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">4</span>, <span class=\"number\">9</span>, <span class=\"number\">16</span>, <span class=\"number\">25</span>)</div><div class=\"line\">多维 : <span class=\"type\">List</span>(<span class=\"type\">List</span>(<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>), <span class=\"type\">List</span>(<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>), <span class=\"type\">List</span>(<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">4</span>, <span class=\"number\">6</span>, <span class=\"number\">8</span>), <span class=\"type\">List</span>(<span class=\"number\">0</span>, <span class=\"number\">3</span>, <span class=\"number\">6</span>, <span class=\"number\">9</span>, <span class=\"number\">12</span>))</div></pre></td></tr></table></figure></p>\n<h3 id=\"列表反转\"><a href=\"#列表反转\" class=\"headerlink\" title=\"列表反转\"></a><strong>列表反转</strong></h3><p>List.reverse 用于将列表的顺序反转，实例如下：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Test</span> </span>&#123;</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">      <span class=\"keyword\">val</span> site = <span class=\"string\">\"Runoob\"</span> :: (<span class=\"string\">\"Google\"</span> :: (<span class=\"string\">\"Baidu\"</span> :: <span class=\"type\">Nil</span>))</div><div class=\"line\">      println( <span class=\"string\">\"site 反转前 : \"</span> + site )</div><div class=\"line\"></div><div class=\"line\">      println( <span class=\"string\">\"site 反转前 : \"</span> + site.reverse )</div><div class=\"line\">   &#125;</div><div class=\"line\">&#125;</div><div class=\"line\">------------输出---------------</div><div class=\"line\">$ vim <span class=\"type\">Test</span>.scala </div><div class=\"line\">$ scala <span class=\"type\">Test</span>.scala </div><div class=\"line\">site 反转前 : <span class=\"type\">List</span>(<span class=\"type\">Runoob</span>, <span class=\"type\">Google</span>, <span class=\"type\">Baidu</span>)</div><div class=\"line\">site 反转前 : <span class=\"type\">List</span>(<span class=\"type\">Baidu</span>, <span class=\"type\">Google</span>, <span class=\"type\">Runoob</span>)</div></pre></td></tr></table></figure></p>\n<h3 id=\"前缀与后缀-drop-take-splitAt\"><a href=\"#前缀与后缀-drop-take-splitAt\" class=\"headerlink\" title=\"前缀与后缀 drop take splitAt\"></a><strong>前缀与后缀 drop take splitAt</strong></h3><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">    <span class=\"keyword\">var</span> ls = <span class=\"string\">\"google\"</span>::<span class=\"string\">\"baidu\"</span>::<span class=\"string\">\"tenxun\"</span>::<span class=\"string\">\"alibaba\"</span>::<span class=\"string\">\"apples\"</span>::<span class=\"type\">Nil</span></div><div class=\"line\">    <span class=\"keyword\">var</span> takeTest =ls.take(<span class=\"number\">2</span>)</div><div class=\"line\">    <span class=\"keyword\">var</span> dropTest = ls.drop(<span class=\"number\">2</span>)</div><div class=\"line\">    <span class=\"keyword\">var</span> splitTest = ls.splitAt(<span class=\"number\">3</span>)</div><div class=\"line\">    println(<span class=\"string\">\"takeTest: \"</span>+takeTest)</div><div class=\"line\">    println(<span class=\"string\">\"dropTest: \"</span>+dropTest)</div><div class=\"line\">    println(<span class=\"string\">\"splitTest: \"</span>+splitTest)</div><div class=\"line\">  &#125;</div><div class=\"line\">  </div><div class=\"line\">--------------输出-------------</div><div class=\"line\">takeTest: <span class=\"type\">List</span>(google, baidu)</div><div class=\"line\">dropTest: <span class=\"type\">List</span>(tenxun, alibaba, apples)</div><div class=\"line\">splitTest: (<span class=\"type\">List</span>(google, baidu, tenxun),<span class=\"type\">List</span>(alibaba, apples))</div></pre></td></tr></table></figure>\n<h3 id=\"元素选择-apply方法和indices方法\"><a href=\"#元素选择-apply方法和indices方法\" class=\"headerlink\" title=\"元素选择 apply方法和indices方法\"></a><strong>元素选择 apply方法和indices方法</strong></h3><p>apply方法是获取第几个值，list.apply(2)和list(2)是一样的<br>indices是获取所有的列表的Range,暂时不知道有多大用处<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">    <span class=\"keyword\">var</span> ls = <span class=\"string\">\"google\"</span>::<span class=\"string\">\"baidu\"</span>::<span class=\"string\">\"tenxun\"</span>::<span class=\"string\">\"alibaba\"</span>::<span class=\"string\">\"apples\"</span>::<span class=\"string\">\"stackoverflow\"</span>::<span class=\"type\">Nil</span></div><div class=\"line\">    <span class=\"keyword\">var</span> applyTest = ls.apply(<span class=\"number\">2</span>)</div><div class=\"line\">    <span class=\"keyword\">var</span> indicesTest =  ls.indices</div><div class=\"line\">    println(<span class=\"string\">\"applyTest: \"</span>+applyTest)</div><div class=\"line\">    println(<span class=\"string\">\"applyTest2: \"</span>+ls(<span class=\"number\">2</span>))</div><div class=\"line\">    println(<span class=\"string\">\"indicesTest: \"</span>+indicesTest)</div><div class=\"line\">    <span class=\"keyword\">for</span>(i&lt;-indicesTest)&#123;</div><div class=\"line\">      println(ls(i))</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">-------------输出--------</div><div class=\"line\">applyTest: tenxun</div><div class=\"line\">applyTest2: tenxun</div><div class=\"line\">indicesTest: <span class=\"type\">Range</span>(<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>)</div><div class=\"line\">google</div><div class=\"line\">baidu</div><div class=\"line\">tenxun</div><div class=\"line\">alibaba</div><div class=\"line\">apples</div><div class=\"line\">stackoverflow</div></pre></td></tr></table></figure></p>\n<h3 id=\"toString和mkString\"><a href=\"#toString和mkString\" class=\"headerlink\" title=\"toString和mkString\"></a><strong>toString和mkString</strong></h3><p>toString和java的一样<br>mkString  mkString(start: String,sep: String,end: String): String<br>第一个参数是以什么开始，第二个参数是以什么分割，第三个参数是以什么结尾，函数返回一个String<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">    <span class=\"keyword\">var</span> ls = <span class=\"string\">\"google\"</span> :: <span class=\"string\">\"baidu\"</span> :: <span class=\"string\">\"tenxun\"</span> :: <span class=\"string\">\"alibaba\"</span> :: <span class=\"string\">\"apples\"</span> :: <span class=\"string\">\"stackoverflow\"</span> :: <span class=\"type\">Nil</span></div><div class=\"line\">    println(<span class=\"string\">\"toString: \"</span>+ls.toString())</div><div class=\"line\">    println(<span class=\"string\">\"mkString: \"</span>+ls.mkString(<span class=\"string\">\"&#123;\"</span>,<span class=\"string\">\";\"</span>,<span class=\"string\">\"&#125;\"</span>))</div><div class=\"line\">  &#125;</div><div class=\"line\">---------输出----------</div><div class=\"line\">toString: <span class=\"type\">List</span>(google, baidu, tenxun, alibaba, apples, stackoverflow)</div><div class=\"line\">mkString: &#123;google;baidu;tenxun;alibaba;apples;stackoverflow&#123;</div></pre></td></tr></table></figure></p>\n<h3 id=\"转换列表-toArray-element-iterator\"><a href=\"#转换列表-toArray-element-iterator\" class=\"headerlink\" title=\"转换列表 toArray element iterator\"></a><strong>转换列表 toArray element iterator</strong></h3><p>想要在数组Array和列表list之间转换，可以使用List的toArray和Array的toList<br>例子<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; var ls = &quot;google&quot; :: &quot;baidu&quot; :: &quot;tenxun&quot; :: &quot;alibaba&quot; :: &quot;apples&quot; :: &quot;stackoverflow&quot; :: Nil</div><div class=\"line\">ls: List[String] = List(google, baidu, tenxun, alibaba, apples, stackoverflow)</div><div class=\"line\"></div><div class=\"line\">scala&gt; var arrays = ls.toArray</div><div class=\"line\">arrays: Array[String] = Array(google, baidu, tenxun, alibaba, apples, stackoverflow)</div><div class=\"line\"></div><div class=\"line\">scala&gt; var lists = arrays.toList</div><div class=\"line\">lists: List[String] = List(google, baidu, tenxun, alibaba, apples, stackoverflow)</div></pre></td></tr></table></figure></p>\n<p>element在很旧的版本有，现在已经过时不用了。如果要用枚举器访问列表元素，可以使用iterator<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">    <span class=\"keyword\">var</span> ls = <span class=\"string\">\"google\"</span> :: <span class=\"string\">\"baidu\"</span> :: <span class=\"string\">\"tenxun\"</span> :: <span class=\"string\">\"alibaba\"</span> :: <span class=\"string\">\"apples\"</span> :: <span class=\"string\">\"stackoverflow\"</span> :: <span class=\"type\">Nil</span></div><div class=\"line\">     <span class=\"keyword\">val</span> it =ls.iterator</div><div class=\"line\">      <span class=\"keyword\">while</span>(it.hasNext)&#123;</div><div class=\"line\">        print(it.next()+<span class=\"string\">\",\"</span>)</div><div class=\"line\">      &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">--------------输出------------</div><div class=\"line\">google, baidu, tenxun, alibaba, apples, stackoverflow,</div></pre></td></tr></table></figure></p>\n<h2 id=\"list类的高阶用法\"><a href=\"#list类的高阶用法\" class=\"headerlink\" title=\"list类的高阶用法\"></a><strong>list类的高阶用法</strong></h2><p>在java中，若要提取出满足特点条件的元素，或者检查所有元素是否满足某种性质，或者用某种方式转变列表的所有元素，这样的需求一般都需要使用for或者while循环的固定表达式，在scala中，可以通过使用List的一些高阶方法（函数）来更为简介的实现  </p>\n<h3 id=\"列表间映射：map、flatMap和foreach\"><a href=\"#列表间映射：map、flatMap和foreach\" class=\"headerlink\" title=\"列表间映射：map、flatMap和foreach\"></a><strong>列表间映射：map、flatMap和foreach</strong></h3><ol>\n<li>xs map f 操作返回<strong>把函数f应用在xs的每个列表元素之后</strong>由此<strong>组成的新列表</strong>。如：<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; List(1,2,3).map(_ +1)</div><div class=\"line\">res1: List[Int] = List(2, 3, 4)</div></pre></td></tr></table></figure>\n</li>\n</ol>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> words = <span class=\"type\">List</span>(<span class=\"string\">\"zks\"</span>,<span class=\"string\">\"zhaikaishun\"</span>,<span class=\"string\">\"kaishun\"</span>,<span class=\"string\">\"kai\"</span>,<span class=\"string\">\"xiaozhai\"</span>)</div><div class=\"line\">words: <span class=\"type\">List</span>[<span class=\"type\">String</span>] = <span class=\"type\">List</span>(zks, zhaikaishun, kaishun, kai, xiaozhai)</div><div class=\"line\"></div><div class=\"line\">scala&gt; words.map(_.length)</div><div class=\"line\">res2: <span class=\"type\">List</span>[<span class=\"type\">Int</span>] = <span class=\"type\">List</span>(<span class=\"number\">3</span>, <span class=\"number\">11</span>, <span class=\"number\">7</span>, <span class=\"number\">3</span>, <span class=\"number\">8</span>)</div></pre></td></tr></table></figure>\n<ol>\n<li>flatMap操作符与map类似，不过它的右操作元是能够返回元素列表的函数。它对列表的每个元素调用该方法，<strong>然后连接所有方法</strong>的结果并返回。map与flatMap的差异举例说明如下：<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; words.map(_.toList)</div><div class=\"line\">res3: <span class=\"type\">List</span>[<span class=\"type\">List</span>[<span class=\"type\">Char</span>]] = <span class=\"type\">List</span>(<span class=\"type\">List</span>(z, k, s), <span class=\"type\">List</span>(z, h, a, i, k, a, i, s, h, u, n), <span class=\"type\">List</span>(k, a, i, s, h, u, n), <span class=\"type\">List</span>(k, a, i), <span class=\"type\">List</span>(x, i, a, o, z, h, a, i))</div><div class=\"line\"></div><div class=\"line\">scala&gt; words.flatMap(_.toList)</div><div class=\"line\">res4: <span class=\"type\">List</span>[<span class=\"type\">Char</span>] = <span class=\"type\">List</span>(z, k, s, z, h, a, i, k, a, i, s, h, u, n, k, a, i, s, h, u, n, k, a, i, x, i, a, o, z, h, a, i)</div></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>map与flatMap的差异和协作可以用下面的例子体会<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"type\">List</span>.range(<span class=\"number\">1</span>, <span class=\"number\">5</span>).flatMap(i =&gt; <span class=\"type\">List</span>.range(<span class=\"number\">1</span>, i).map(j =&gt; (i, j)))</div><div class=\"line\">res9: <span class=\"type\">List</span>[(<span class=\"type\">Int</span>, <span class=\"type\">Int</span>)] = <span class=\"type\">List</span>((<span class=\"number\">2</span>,<span class=\"number\">1</span>), (<span class=\"number\">3</span>,<span class=\"number\">1</span>), (<span class=\"number\">3</span>,<span class=\"number\">2</span>), (<span class=\"number\">4</span>,<span class=\"number\">1</span>), (<span class=\"number\">4</span>,<span class=\"number\">2</span>), (<span class=\"number\">4</span>,<span class=\"number\">3</span>))</div></pre></td></tr></table></figure></p>\n<p>解释 : List.range(1,5)生成了List(1,2,3,4),注意没有5<br>.flatMap对内部的每一个元素进行操作, 后面有个.map是对内部List.range(1, i)的每一个元素进行操作，最后flatMap返回的还是一个List<br>上述例子也可以用for循环+yield来完成<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt;  <span class=\"keyword\">for</span> (i &lt;- <span class=\"type\">List</span>.range(<span class=\"number\">1</span>, <span class=\"number\">5</span>); j &lt;- <span class=\"type\">List</span>.range(<span class=\"number\">1</span>, i)) <span class=\"keyword\">yield</span> (i,j)</div><div class=\"line\">res10: <span class=\"type\">List</span>[(<span class=\"type\">Int</span>, <span class=\"type\">Int</span>)] = <span class=\"type\">List</span>((<span class=\"number\">2</span>,<span class=\"number\">1</span>), (<span class=\"number\">3</span>,<span class=\"number\">1</span>), (<span class=\"number\">3</span>,<span class=\"number\">2</span>), (<span class=\"number\">4</span>,<span class=\"number\">1</span>), (<span class=\"number\">4</span>,<span class=\"number\">2</span>), (<span class=\"number\">4</span>,<span class=\"number\">3</span>))</div></pre></td></tr></table></figure></p>\n<ol>\n<li>foreach是第三种与映射类似的操作。它的右操作元是过程（返回Unit的函数）。它只是对每个列表元素都调用一遍过程。操作的结果仍然是Unit，不会产生结果列表。例如：<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">    <span class=\"keyword\">var</span> sum =<span class=\"number\">0</span></div><div class=\"line\">    <span class=\"type\">List</span>(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>) foreach (sum += _)</div><div class=\"line\">    println(sum)</div><div class=\"line\">  &#125;</div><div class=\"line\">-----输出----</div><div class=\"line\"><span class=\"number\">15</span></div></pre></td></tr></table></figure>\n</li>\n</ol>\n<h3 id=\"列表过滤：filter、partition、find、takeWhile、dropWhile和span\"><a href=\"#列表过滤：filter、partition、find、takeWhile、dropWhile和span\" class=\"headerlink\" title=\"列表过滤：filter、partition、find、takeWhile、dropWhile和span\"></a><strong>列表过滤：filter、partition、find、takeWhile、dropWhile和span</strong></h3><p>1.xs filter p操作产生xs中符合p（x）为true的所有元素组成的列表。如：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"type\">List</span> (<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>) filter (_ % <span class=\"number\">2</span> == <span class=\"number\">0</span>)</div><div class=\"line\">res10: <span class=\"type\">List</span>[<span class=\"type\">Int</span>] = <span class=\"type\">List</span>(<span class=\"number\">2</span>, <span class=\"number\">4</span>)</div><div class=\"line\"></div><div class=\"line\">scala&gt; words filter (_.length == <span class=\"number\">3</span>)</div><div class=\"line\">res11: <span class=\"type\">List</span>[<span class=\"type\">String</span>] = <span class=\"type\">List</span>(the, fox)</div></pre></td></tr></table></figure></p>\n<p>2.partition方法与filter类似，不过返回的是列表对。其中一个包含所有论断为真的元素，另一个包含所有论断为假的元素。<br><strong>xs partition p  等价于 (xs filter p, xs filter (!p()))  </strong><br>举例如下：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"type\">List</span>(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>) partition (_ % <span class=\"number\">2</span> ==<span class=\"number\">0</span>)</div><div class=\"line\">res12: (<span class=\"type\">List</span>[<span class=\"type\">Int</span>], <span class=\"type\">List</span>[<span class=\"type\">Int</span>]) = (<span class=\"type\">List</span>(<span class=\"number\">2</span>, <span class=\"number\">4</span>),<span class=\"type\">List</span>(<span class=\"number\">1</span>, <span class=\"number\">3</span>, <span class=\"number\">5</span>))</div></pre></td></tr></table></figure></p>\n<p>3.find方法同样与filter方法类似，不过返回的是第一个满足给定论断的元素，而并不是全部。xs find p 操作以列表xs和论断p为操作元。返回可选值。如果xs中存在元素x使得p（x）为真，Some（x）将返回。否则，若p对所有元素都不成立，None将返回。举例如下：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"type\">List</span>(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>) find (_ % <span class=\"number\">2</span> == <span class=\"number\">0</span>)</div><div class=\"line\">res13: <span class=\"type\">Option</span>[<span class=\"type\">Int</span>] = <span class=\"type\">Some</span>(<span class=\"number\">2</span>)</div><div class=\"line\"></div><div class=\"line\">scala&gt; <span class=\"type\">List</span>(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>) find (_  &lt;= <span class=\"number\">0</span>)</div><div class=\"line\">res15: <span class=\"type\">Option</span>[<span class=\"type\">Int</span>] = <span class=\"type\">None</span></div></pre></td></tr></table></figure></p>\n<ol>\n<li>xs takeWhile p操作返回列表xs中最长的能够满足p的前缀。例如：<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"type\">List</span>(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">-4</span>, <span class=\"number\">5</span>) takeWhile (_ &gt; <span class=\"number\">0</span>)</div><div class=\"line\">res16: <span class=\"type\">List</span>[<span class=\"type\">Int</span>] = <span class=\"type\">List</span>(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>)</div></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>5.xs dropWhile p操作移除最长能够满足p的前缀。举例如下：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> words = <span class=\"type\">List</span>(<span class=\"string\">\"the\"</span>, <span class=\"string\">\"quick\"</span>, <span class=\"string\">\"brown\"</span>, <span class=\"string\">\"fox\"</span>)</div><div class=\"line\">words: <span class=\"type\">List</span>[<span class=\"type\">String</span>] = <span class=\"type\">List</span>(the, quick, brown, fox)</div><div class=\"line\"></div><div class=\"line\">scala&gt; words dropWhile (_ startsWith <span class=\"string\">\"t\"</span>)</div><div class=\"line\">res11: <span class=\"type\">List</span>[<span class=\"type\">String</span>] = <span class=\"type\">List</span>(quick, brown, fox)</div></pre></td></tr></table></figure></p>\n<p>6.span方法把takeWhile和dropWhile组合成一个操作。它返回一对列表，定义与下列等式一致：<br>xs span p 等价于 （xs takeWhile p， xs dropWhile p）<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"type\">List</span>(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">-4</span>, <span class=\"number\">5</span>) span (_ &gt;<span class=\"number\">0</span>)</div><div class=\"line\">res18: (<span class=\"type\">List</span>[<span class=\"type\">Int</span>], <span class=\"type\">List</span>[<span class=\"type\">Int</span>]) = (<span class=\"type\">List</span>(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>),<span class=\"type\">List</span>(<span class=\"number\">-4</span>, <span class=\"number\">5</span>))</div></pre></td></tr></table></figure></p>\n<h3 id=\"列表的论断：forall和exists\"><a href=\"#列表的论断：forall和exists\" class=\"headerlink\" title=\"列表的论断：forall和exists\"></a><strong>列表的论断：forall和exists</strong></h3><ol>\n<li>xs forall p 如果列表的所有元素满足p则返回true</li>\n<li>xs exists p 如果列表中有一个值满足p就返回true<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hasZeroRow</span></span>(m: <span class=\"type\">List</span>[<span class=\"type\">List</span>[<span class=\"type\">Int</span>]]) = m.exists(row =&gt; row forall (_ == <span class=\"number\">0</span>))</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">    <span class=\"keyword\">val</span> m= <span class=\"type\">List</span>(<span class=\"type\">List</span>(<span class=\"number\">3</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>), <span class=\"type\">List</span>(<span class=\"number\">0</span>,<span class=\"number\">3</span>,<span class=\"number\">0</span>), <span class=\"type\">List</span>(<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">3</span>))</div><div class=\"line\">    <span class=\"keyword\">var</span> flag :<span class=\"type\">Boolean</span>= hasZeroRow(m)</div><div class=\"line\">    println(flag)</div><div class=\"line\">  &#125;</div><div class=\"line\">----------输出--------</div><div class=\"line\"><span class=\"literal\">false</span></div></pre></td></tr></table></figure>\n</li>\n</ol>\n<h3 id=\"折叠操作\"><a href=\"#折叠操作\" class=\"headerlink\" title=\"折叠操作\"></a><strong>折叠操作</strong></h3><p>如果我们把集合看成是一张纸条，每一小段代表一个元素，那么reduceLeft就将这张纸条从左向右”折叠”，最前面的两个元素会首先“重叠”在一起，这时会使用传给reduceLeft的参数函数进行计算，返回的结果就像是已经折叠在一起的两段纸条，它们已经是一个叠加的状态了，所以它，也就是上次重叠的结果会继续做为一个单一的值和下一个元素继续“叠加”，直到折叠到集合的最后一个元素</p>\n<ol>\n<li><p><strong>reduceLeft</strong></p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt;  <span class=\"type\">List</span>.range(<span class=\"number\">1</span>, <span class=\"number\">5</span>).reduceLeft(_+_)</div><div class=\"line\">res12: <span class=\"type\">Int</span> = <span class=\"number\">10</span></div></pre></td></tr></table></figure>\n</li>\n<li><p><strong>reduceRight</strong><br>和reduceLeft相似，但是是从右向左折叠,<strong>注意</strong>:==它的操作方向是从右到左，但是参数的顺序却并不是，而是依然第一参数是左边的元素，第二参数是右边的元素==</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"type\">List</span>.range(<span class=\"number\">1</span>, <span class=\"number\">4</span>) reduceRight(_ - _)</div><div class=\"line\">res15: <span class=\"type\">Int</span> = <span class=\"number\">2</span></div><div class=\"line\"><span class=\"comment\">// 2-3 = -1</span></div><div class=\"line\"><span class=\"comment\">// 1-(-1)=2</span></div></pre></td></tr></table></figure>\n</li>\n<li><p><strong>foldLeft</strong><br>类似于reduceLeft, 不过开始折叠的第一个元素不是集合中的第一个元素，而是传入的一个元素，有点类似于先将这个参数放入的集合中的首位，然后在reduceLeft</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"type\">List</span>.range(<span class=\"number\">1</span>, <span class=\"number\">5</span>).foldLeft(<span class=\"number\">1</span>)(_+_)</div><div class=\"line\">res1: <span class=\"type\">Int</span> = <span class=\"number\">11</span></div></pre></td></tr></table></figure>\n</li>\n<li><p><strong>foldRight</strong><br>类似于foldLeft，不过是从右向左，不再举例</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"type\">List</span>.range(<span class=\"number\">1</span>, <span class=\"number\">4</span>).foldRight(<span class=\"number\">3</span>)(_-_)</div><div class=\"line\">res1: <span class=\"type\">Int</span> = <span class=\"number\">-1</span></div></pre></td></tr></table></figure>\n</li>\n</ol>\n<h1 id=\"scala-集合Set\"><a href=\"#scala-集合Set\" class=\"headerlink\" title=\"scala 集合Set\"></a><strong>scala 集合Set</strong></h1><p>Set和List基本相似，只是所有的元素都是唯一的<br>默认的Set是不可变的,默认引用的是 scala.collection.immutable.Set<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">val</span> set = <span class=\"type\">Set</span>(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>)</div><div class=\"line\">println(set.getClass.getName) <span class=\"comment\">// </span></div><div class=\"line\"></div><div class=\"line\">println(set.exists(_ % <span class=\"number\">2</span> == <span class=\"number\">0</span>)) <span class=\"comment\">//true</span></div><div class=\"line\">println(set.drop(<span class=\"number\">1</span>)) <span class=\"comment\">//Set(2,3)</span></div></pre></td></tr></table></figure></p>\n<p>如果需要使用可变集合需要引入 scala.collection.mutable.Set：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> scala.collection.mutable.<span class=\"type\">Set</span> <span class=\"comment\">// 可以在任何地方引入 可变集合</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">val</span> mutableSet = <span class=\"type\">Set</span>(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>)</div><div class=\"line\">println(mutableSet.getClass.getName) <span class=\"comment\">// scala.collection.mutable.HashSet</span></div><div class=\"line\"></div><div class=\"line\">mutableSet.add(<span class=\"number\">4</span>)</div><div class=\"line\">mutableSet.remove(<span class=\"number\">1</span>)</div><div class=\"line\">mutableSet += <span class=\"number\">5</span></div><div class=\"line\">mutableSet -= <span class=\"number\">2</span></div><div class=\"line\"></div><div class=\"line\">println(mutableSet) <span class=\"comment\">// Set(5, 3, 4)</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">val</span> another = mutableSet.toSet</div><div class=\"line\">println(another.getClass.getName) <span class=\"comment\">// scala.collection.immutable.Set</span></div></pre></td></tr></table></figure></p>\n<h2 id=\"连接集合\"><a href=\"#连接集合\" class=\"headerlink\" title=\"连接集合\"></a><strong>连接集合</strong></h2><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">var</span> site = site1 ++ site2</div></pre></td></tr></table></figure>\n<h2 id=\"查找集合中最大与最小元素\"><a href=\"#查找集合中最大与最小元素\" class=\"headerlink\" title=\"查找集合中最大与最小元素\"></a><strong>查找集合中最大与最小元素</strong></h2><p>Set.min<br>Set.max</p>\n<h2 id=\"交集\"><a href=\"#交集\" class=\"headerlink\" title=\"交集\"></a><strong>交集</strong></h2><p>Set.intersect<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">set1.intersect(set2)</div></pre></td></tr></table></figure></p>\n<h1 id=\"map\"><a href=\"#map\" class=\"headerlink\" title=\"map\"></a><strong>map</strong></h1><p>ap(映射)是一种可迭代的键值对（key/value）结构。<br>所有的值都可以通过键来获取。<br>Map 中的键都是唯一的。<br>Map 也叫哈希表（Hash tables）。<br>Map 有两种类型，可变与不可变，区别在于可变对象可以修改它，而不可变对象不可以。<br>默认情况下 Scala 使用不可变 Map。如果你需要使用可变集合，你需要显式的引入 import scala.collection.mutable.Map 类<br>在 Scala 中 你可以同时使用可变与不可变 Map，不可变的直接使用 Map，可变的使用 mutable.Map。以下实例演示了不可变 Map 的应用：<br>不可变map<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> colors = <span class=\"type\">Map</span>(<span class=\"string\">\"red\"</span> -&gt; <span class=\"string\">\"#FF0000\"</span>, <span class=\"string\">\"azure\"</span> -&gt; <span class=\"string\">\"#F0FFFF\"</span>)</div><div class=\"line\">colors: scala.collection.immutable.<span class=\"type\">Map</span>[<span class=\"type\">String</span>,<span class=\"type\">String</span>] = <span class=\"type\">Map</span>(red -&gt; #<span class=\"type\">FF0000</span>, azure -&gt; #<span class=\"type\">F0FFFF</span>)</div><div class=\"line\">```  </div><div class=\"line\">## **<span class=\"type\">Map</span>的赋值**  </div><div class=\"line\">如果需要添加 key-value 对，可以使用 + 号，如下所示：</div><div class=\"line\">```scala</div><div class=\"line\"><span class=\"comment\">// 空哈希表，键为字符串，值为整型</span></div><div class=\"line\"><span class=\"keyword\">var</span> <span class=\"type\">A</span>:<span class=\"type\">Map</span>[<span class=\"type\">Char</span>,<span class=\"type\">Int</span>] = <span class=\"type\">Map</span>()</div><div class=\"line\"><span class=\"type\">A</span> += ('<span class=\"type\">I</span>' -&gt; <span class=\"number\">1</span>)</div><div class=\"line\"><span class=\"type\">A</span> += ('<span class=\"type\">J</span>' -&gt; <span class=\"number\">5</span>)</div><div class=\"line\"><span class=\"type\">A</span> += ('<span class=\"type\">K</span>' -&gt; <span class=\"number\">10</span>)</div><div class=\"line\"><span class=\"type\">A</span> += ('<span class=\"type\">L</span>' -&gt; <span class=\"number\">100</span>)</div><div class=\"line\">println（<span class=\"type\">A</span>）</div><div class=\"line\">-------输出--------</div><div class=\"line\"><span class=\"type\">Map</span>(<span class=\"type\">I</span> -&gt; <span class=\"number\">1</span>, <span class=\"type\">J</span> -&gt; <span class=\"number\">5</span>, <span class=\"type\">K</span> -&gt; <span class=\"number\">10</span>, <span class=\"type\">L</span> -&gt; <span class=\"number\">100</span>)</div></pre></td></tr></table></figure></p>\n<h2 id=\"Map的基本操作\"><a href=\"#Map的基本操作\" class=\"headerlink\" title=\"Map的基本操作\"></a><strong>Map的基本操作</strong></h2><ul>\n<li>keys      返回 Map 所有的键(key)</li>\n<li>values    返回 Map 所有的值(value)</li>\n<li>isEmpty    在 Map 为空时返回true<br>例子<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div></pre></td><td class=\"code\"><pre><div class=\"line\">object Test &#123;</div><div class=\"line\">   def main(args: Array[String]) &#123;</div><div class=\"line\">      val colors = Map(&quot;red&quot; -&gt; &quot;#FF0000&quot;,</div><div class=\"line\">                       &quot;azure&quot; -&gt; &quot;#F0FFFF&quot;,</div><div class=\"line\">                       &quot;peru&quot; -&gt; &quot;#CD853F&quot;)</div><div class=\"line\"></div><div class=\"line\">      val nums: Map[Int, Int] = Map()</div><div class=\"line\"></div><div class=\"line\">      println( &quot;colors 中的键为 : &quot; + colors.keys )</div><div class=\"line\">      println( &quot;colors 中的值为 : &quot; + colors.values )</div><div class=\"line\">      println( &quot;检测 colors 是否为空 : &quot; + colors.isEmpty )</div><div class=\"line\">      println( &quot;检测 nums 是否为空 : &quot; + nums.isEmpty )</div><div class=\"line\">   &#125;</div><div class=\"line\">&#125;</div><div class=\"line\">---------输出---------</div><div class=\"line\">colors 中的键为 : Set(red, azure, peru)</div><div class=\"line\">colors 中的值为 : MapLike(#FF0000, #F0FFFF, #CD853F)</div><div class=\"line\">检测 colors 是否为空 : false</div><div class=\"line\">检测 nums 是否为空 : true</div></pre></td></tr></table></figure>\n</li>\n</ul>\n<h2 id=\"Map-合并\"><a href=\"#Map-合并\" class=\"headerlink\" title=\"Map 合并\"></a><strong>Map 合并</strong></h2><p>++ 运算符或 Map.++() 方法来连接两个 Map, Map 合并时会移除重复的 key。<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">val</span> colors1 = <span class=\"type\">Map</span>(<span class=\"string\">\"red\"</span> -&gt; <span class=\"string\">\"#FF0000\"</span>,</div><div class=\"line\">      <span class=\"string\">\"azure\"</span> -&gt; <span class=\"string\">\"#F0FFFF\"</span>,</div><div class=\"line\">      <span class=\"string\">\"peru\"</span> -&gt; <span class=\"string\">\"#CD853F\"</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> colors2 = <span class=\"type\">Map</span>(<span class=\"string\">\"blue\"</span> -&gt; <span class=\"string\">\"#0033FF\"</span>,</div><div class=\"line\">      <span class=\"string\">\"yellow\"</span> -&gt; <span class=\"string\">\"#FFFF00\"</span>,</div><div class=\"line\">      <span class=\"string\">\"red\"</span> -&gt; <span class=\"string\">\"#FF0001\"</span>)</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">//  ++ 作为运算符</span></div><div class=\"line\">    <span class=\"keyword\">var</span> colors = colors1 ++ colors2</div><div class=\"line\">    println( <span class=\"string\">\"colors1 ++ colors2 : \"</span> + colors )</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">//  ++ 作为方法</span></div><div class=\"line\">    colors = colors1.++(colors2)</div><div class=\"line\">    println( <span class=\"string\">\"colors1.++(colors2)) : \"</span> + colors )</div><div class=\"line\">--------输出--------------------</div><div class=\"line\">colors1 ++ colors2 : <span class=\"type\">Map</span>(blue -&gt; #<span class=\"number\">0033</span>FF, azure -&gt; #<span class=\"type\">F0FFFF</span>, peru -&gt; #<span class=\"type\">CD853F</span>, yellow -&gt; #<span class=\"type\">FFFF00</span>, red -&gt; #<span class=\"type\">FF0001</span>)</div><div class=\"line\">colors1.++(colors2)) : <span class=\"type\">Map</span>(blue -&gt; #<span class=\"number\">0033</span>FF, azure -&gt; #<span class=\"type\">F0FFFF</span>, peru -&gt; #<span class=\"type\">CD853F</span>, yellow -&gt; #<span class=\"type\">FFFF00</span>, red -&gt; #<span class=\"type\">FF0001</span>)</div><div class=\"line\">```  </div><div class=\"line\">## **输出map和key**</div><div class=\"line\">```scala</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]): <span class=\"type\">Unit</span> = &#123;</div><div class=\"line\">    <span class=\"keyword\">val</span> sites = <span class=\"type\">Map</span>(<span class=\"string\">\"runoob\"</span> -&gt; <span class=\"string\">\"http://www.runoob.com\"</span>,</div><div class=\"line\">      <span class=\"string\">\"baidu\"</span> -&gt; <span class=\"string\">\"http://www.baidu.com\"</span>,</div><div class=\"line\">      <span class=\"string\">\"taobao\"</span> -&gt; <span class=\"string\">\"http://www.taobao.com\"</span>)</div><div class=\"line\">    sites.keys.foreach&#123;</div><div class=\"line\">      i=&gt;print(<span class=\"string\">\"key: \"</span>+i)</div><div class=\"line\">        println(<span class=\"string\">\"value: \"</span>+sites(i))</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">----------输出-----</div><div class=\"line\">key: runoobvalue: http:<span class=\"comment\">//www.runoob.com</span></div><div class=\"line\">key: baiduvalue: http:<span class=\"comment\">//www.baidu.com</span></div><div class=\"line\">key: taobaovalue: http:<span class=\"comment\">//www.taobao.com</span></div><div class=\"line\">```  </div><div class=\"line\">##  **<span class=\"type\">Map</span>.contains查看是否存在指定的key**</div><div class=\"line\"></div><div class=\"line\">## **元组**</div><div class=\"line\">元组可以把固定数量的条目组合在一起以便于整体的传送，不像数组或者列表，元祖可以保存不同类型的对象    </div><div class=\"line\">元组的值是通过将单个的值包含在圆括号中构成的。例如</div><div class=\"line\">```scala</div><div class=\"line\"><span class=\"keyword\">val</span> t = (<span class=\"number\">1</span>, <span class=\"number\">3.14</span>, <span class=\"string\">\"Fred\"</span>)</div></pre></td></tr></table></figure></p>\n<p>以上实例在元组中定义了三个元素，对应的类型分别为[Int, Double, java.lang.String]。<br>此外我们也可以使用以下方式来定义：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">val</span> t = <span class=\"keyword\">new</span> <span class=\"type\">Tuple3</span>(<span class=\"number\">1</span>, <span class=\"number\">3.14</span>, <span class=\"string\">\"Fred\"</span>)</div><div class=\"line\">```  </div><div class=\"line\">### **定义与取值**</div><div class=\"line\">元组的实际类型取决于它的元素的类型，比如 (<span class=\"number\">99</span>, <span class=\"string\">\"runoob\"</span>) 是 <span class=\"type\">Tuple2</span>[<span class=\"type\">Int</span>, <span class=\"type\">String</span>]。 ('u', 'r', <span class=\"string\">\"the\"</span>, <span class=\"number\">1</span>, <span class=\"number\">4</span>, <span class=\"string\">\"me\"</span>) 为 <span class=\"type\">Tuple6</span>[<span class=\"type\">Char</span>, <span class=\"type\">Char</span>, <span class=\"type\">String</span>, <span class=\"type\">Int</span>, <span class=\"type\">Int</span>, <span class=\"type\">String</span>]。  </div><div class=\"line\">目前 <span class=\"type\">Scala</span> 支持的元组最大长度为 <span class=\"number\">22</span>。对于更大长度你可以使用集合，或者扩展元组。</div><div class=\"line\">访问元组的元素可以通过数字索引，如下一个元组：  </div><div class=\"line\">我们可以使用 t.\\_1 访问第一个元素， t.\\_2 访问第二个元素，如下所示：  </div><div class=\"line\">```scala</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">    <span class=\"keyword\">val</span> t = (<span class=\"number\">4</span>,<span class=\"number\">3</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>)</div><div class=\"line\"></div><div class=\"line\">    <span class=\"keyword\">val</span> sum = t._1 + t._2 + t._3 + t._4</div><div class=\"line\">    <span class=\"keyword\">var</span> secondTuple=t._2</div><div class=\"line\">    println(<span class=\"string\">\"第二个元素为: \"</span>+secondTuple)</div><div class=\"line\">    println( <span class=\"string\">\"元素之和为: \"</span>  + sum )</div><div class=\"line\">  &#125;</div><div class=\"line\">----输出----------</div><div class=\"line\">第二个元素为: <span class=\"number\">3</span></div><div class=\"line\">元素之和为: <span class=\"number\">10</span></div></pre></td></tr></table></figure></p>\n<h3 id=\"迭代元组\"><a href=\"#迭代元组\" class=\"headerlink\" title=\"迭代元组\"></a><strong>迭代元组</strong></h3><p>你可以使用 Tuple.productIterator() 方法来迭代输出元组的所有元素：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div></pre></td><td class=\"code\"><pre><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">    <span class=\"keyword\">val</span> t = (<span class=\"number\">4</span>,<span class=\"number\">3</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>)</div><div class=\"line\">    t.productIterator.foreach(i=&gt;println(<span class=\"string\">\"value: \"</span>+i))</div><div class=\"line\">  &#125;</div><div class=\"line\">--------输出--------------</div><div class=\"line\">value: <span class=\"number\">4</span></div><div class=\"line\">value: <span class=\"number\">3</span></div><div class=\"line\">value: <span class=\"number\">2</span></div><div class=\"line\">value: <span class=\"number\">1</span></div><div class=\"line\">```  </div><div class=\"line\">### **元组转为字符串**</div><div class=\"line\"><span class=\"type\">Tuple</span>.toString()  </div><div class=\"line\">### **元素交换**</div><div class=\"line\"><span class=\"type\">Tuple</span>.swap 方法来交换元组的元素  </div><div class=\"line\">```scala</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">      <span class=\"keyword\">val</span> t = <span class=\"keyword\">new</span> <span class=\"type\">Tuple2</span>(<span class=\"string\">\"www.google.com\"</span>, <span class=\"string\">\"http://blog.csdn.net/t1dmzks\"</span>)</div><div class=\"line\">      println(<span class=\"string\">\"交换后的元组: \"</span> + t )</div><div class=\"line\">    &#125;</div><div class=\"line\">-------输出-----------</div><div class=\"line\">交换后的元组: (www.google.com,www.runoob.com)</div><div class=\"line\">```  </div><div class=\"line\"># **<span class=\"type\">Scala</span> <span class=\"type\">Option</span>**</div><div class=\"line\"><span class=\"type\">TODO</span></div><div class=\"line\"></div><div class=\"line\"># **<span class=\"type\">Scala</span> <span class=\"type\">Iterator</span>**</div><div class=\"line\"><span class=\"type\">Scala</span> <span class=\"type\">Iterator</span>（迭代器）不是一个集合，它是一种用于访问集合的方法。</div><div class=\"line\">迭代器 it 的两个基本操作是 next 和 hasNext。</div><div class=\"line\">调用 it.next() 会返回迭代器的下一个元素，并且更新迭代器的状态。</div><div class=\"line\">调用 it.hasNext() 用于检测集合中是否还有元素。</div><div class=\"line\">让迭代器 it 逐个返回所有元素最简单的方法是使用 <span class=\"keyword\">while</span> 循环：</div><div class=\"line\">```scala</div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Test</span> </span>&#123;</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">      <span class=\"keyword\">val</span> it = <span class=\"type\">Iterator</span>(<span class=\"string\">\"Baidu\"</span>, <span class=\"string\">\"Google\"</span>, <span class=\"string\">\"Runoob\"</span>, <span class=\"string\">\"Taobao\"</span>)</div><div class=\"line\">      </div><div class=\"line\">      <span class=\"keyword\">while</span> (it.hasNext)&#123;</div><div class=\"line\">         println(it.next())</div><div class=\"line\">      &#125;</div><div class=\"line\">   &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h2 id=\"查找最大与最小元素\"><a href=\"#查找最大与最小元素\" class=\"headerlink\" title=\"查找最大与最小元素\"></a><strong>查找最大与最小元素</strong></h2><p> it.min 和 it.max 方法</p>\n<h2 id=\"获取迭代器的长度\"><a href=\"#获取迭代器的长度\" class=\"headerlink\" title=\"获取迭代器的长度\"></a><strong>获取迭代器的长度</strong></h2><p>it.size 或 it.length </p>\n","site":{"data":{}},"excerpt":"","more":"<p>本文参考至scala编程，菜鸟教程，然后将自己的判断以及重要方法的提取，解释，合并</p>\n<h1 id=\"字符串\"><a href=\"#字符串\" class=\"headerlink\" title=\"字符串\"></a><strong>字符串</strong></h1><p>在 Scala 中，字符串的类型实际上是 Java String，它本身没有 String 类。在 Scala 中，String 是一个不可变的对象，所以该对象不可被修改。这就意味着你如果修改字符串就会产生一个新的字符串对象。  </p>\n<p> String 对象是不可变的，如果你需要创建一个可以修改的字符串，可以使用 String Builder 类，如下实例:<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\"> <span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Test</span> </span>&#123;</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">      <span class=\"keyword\">val</span> buf = <span class=\"keyword\">new</span> <span class=\"type\">StringBuilder</span>;</div><div class=\"line\">      buf += 'a'</div><div class=\"line\">      buf ++= <span class=\"string\">\"bcdef\"</span></div><div class=\"line\">      println( <span class=\"string\">\"buf is : \"</span> + buf.toString );</div><div class=\"line\">   &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>同java一样，scala的字符串用过length()方法得到长度，String 类中使用string1.concat(string2); 方法来连接两个字符串<br>也可以直接用+号连接字符串<br>java.lang.String的所有方法，在scala中也可以使用, 这里不仔细介绍</p>\n<h1 id=\"数组\"><a href=\"#数组\" class=\"headerlink\" title=\"数组\"></a><strong>数组</strong></h1><h2 id=\"声明数组\"><a href=\"#声明数组\" class=\"headerlink\" title=\"声明数组\"></a><strong>声明数组</strong></h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">var z:Array[String] = new Array[String](3)</div><div class=\"line\"></div><div class=\"line\">或</div><div class=\"line\"></div><div class=\"line\">var z = new Array[String](3)</div></pre></td></tr></table></figure>\n<h2 id=\"赋值\"><a href=\"#赋值\" class=\"headerlink\" title=\"赋值\"></a><strong>赋值</strong></h2><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">z(<span class=\"number\">0</span>) = <span class=\"string\">\"Runoob\"</span>; z(<span class=\"number\">1</span>) = <span class=\"string\">\"Baidu\"</span>; z(<span class=\"number\">4</span>/<span class=\"number\">2</span>) = <span class=\"string\">\"Google\"</span></div></pre></td></tr></table></figure>\n<p>也可以这样定义一个数组<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">var</span> z = <span class=\"type\">Array</span>(<span class=\"string\">\"Runoob\"</span>, <span class=\"string\">\"Baidu\"</span>, <span class=\"string\">\"Google\"</span>)</div></pre></td></tr></table></figure></p>\n<h2 id=\"处理数组\"><a href=\"#处理数组\" class=\"headerlink\" title=\"处理数组\"></a><strong>处理数组</strong></h2><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Test</span> </span>&#123;</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">      <span class=\"keyword\">var</span> myList = <span class=\"type\">Array</span>(<span class=\"number\">1.9</span>, <span class=\"number\">2.9</span>, <span class=\"number\">3.4</span>, <span class=\"number\">3.5</span>)</div><div class=\"line\">      </div><div class=\"line\">      <span class=\"comment\">// 输出所有数组元素</span></div><div class=\"line\">      <span class=\"keyword\">for</span> ( x &lt;- myList ) &#123;</div><div class=\"line\">         println( x )</div><div class=\"line\">      &#125;</div><div class=\"line\"></div><div class=\"line\">      <span class=\"comment\">// 计算数组所有元素的总和</span></div><div class=\"line\">      <span class=\"keyword\">var</span> total = <span class=\"number\">0.0</span>;</div><div class=\"line\">      <span class=\"keyword\">for</span> ( i &lt;- <span class=\"number\">0</span> to (myList.length - <span class=\"number\">1</span>)) &#123;</div><div class=\"line\">         total += myList(i);</div><div class=\"line\">      &#125;</div><div class=\"line\">      println(<span class=\"string\">\"总和为 \"</span> + total);</div><div class=\"line\"></div><div class=\"line\">      <span class=\"comment\">// 查找数组中的最大元素</span></div><div class=\"line\">      <span class=\"keyword\">var</span> max = myList(<span class=\"number\">0</span>);</div><div class=\"line\">      <span class=\"keyword\">for</span> ( i &lt;- <span class=\"number\">1</span> to (myList.length - <span class=\"number\">1</span>) ) &#123;</div><div class=\"line\">         <span class=\"keyword\">if</span> (myList(i) &gt; max) max = myList(i);</div><div class=\"line\">      &#125;</div><div class=\"line\">      println(<span class=\"string\">\"最大值为 \"</span> + max);</div><div class=\"line\">    </div><div class=\"line\">   &#125;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\">-------输出-------</div><div class=\"line\"><span class=\"number\">1.9</span></div><div class=\"line\"><span class=\"number\">2.9</span></div><div class=\"line\"><span class=\"number\">3.4</span></div><div class=\"line\"><span class=\"number\">3.5</span></div><div class=\"line\">总和为 <span class=\"number\">11.7</span></div><div class=\"line\">最大值为 <span class=\"number\">3.5</span></div></pre></td></tr></table></figure>\n<h2 id=\"多维数组\"><a href=\"#多维数组\" class=\"headerlink\" title=\"多维数组\"></a><strong>多维数组</strong></h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div></pre></td><td class=\"code\"><pre><div class=\"line\">def main(args: Array[String]) &#123;</div><div class=\"line\">      var myMatrix = ofDim[Int](3,3)</div><div class=\"line\">      </div><div class=\"line\">      // 创建矩阵</div><div class=\"line\">      for (i &lt;- 0 to 2) &#123;</div><div class=\"line\">         for ( j &lt;- 0 to 2) &#123;</div><div class=\"line\">            myMatrix(i)(j) = j;</div><div class=\"line\">         &#125;</div><div class=\"line\">      &#125;</div><div class=\"line\">      </div><div class=\"line\">      // 打印二维阵列</div><div class=\"line\">      for (i &lt;- 0 to 2) &#123;</div><div class=\"line\">         for ( j &lt;- 0 to 2) &#123;</div><div class=\"line\">            print(&quot; &quot; + myMatrix(i)(j));</div><div class=\"line\">         &#125;</div><div class=\"line\">         println();</div><div class=\"line\">      &#125;</div><div class=\"line\">    </div><div class=\"line\">   &#125;</div></pre></td></tr></table></figure>\n<h2 id=\"合并数组\"><a href=\"#合并数组\" class=\"headerlink\" title=\"合并数组\"></a><strong>合并数组</strong></h2><p>使用concat() 方法来合并两个数组<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">      <span class=\"keyword\">var</span> myList1 = <span class=\"type\">Array</span>(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>,)</div><div class=\"line\">      <span class=\"keyword\">var</span> myList2 = <span class=\"type\">Array</span>(<span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>)</div><div class=\"line\"></div><div class=\"line\">      <span class=\"keyword\">var</span> myList3 =  concat( myList1, myList2)</div><div class=\"line\">      </div><div class=\"line\">      <span class=\"comment\">// 输出所有数组元素</span></div><div class=\"line\">      <span class=\"keyword\">for</span> ( x &lt;- myList3 ) &#123;</div><div class=\"line\">         println( x )</div><div class=\"line\">      &#125;</div><div class=\"line\">   &#125;</div><div class=\"line\">   </div><div class=\"line\">----------输出-----------</div><div class=\"line\"><span class=\"number\">1</span></div><div class=\"line\"><span class=\"number\">2</span></div><div class=\"line\"><span class=\"number\">3</span></div><div class=\"line\"><span class=\"number\">4</span></div><div class=\"line\"><span class=\"number\">5</span></div><div class=\"line\"><span class=\"number\">6</span></div><div class=\"line\"><span class=\"number\">7</span></div><div class=\"line\"><span class=\"number\">8</span></div></pre></td></tr></table></figure></p>\n<h2 id=\"创建区间数组\"><a href=\"#创建区间数组\" class=\"headerlink\" title=\"创建区间数组\"></a><strong>创建区间数组</strong></h2><p>以下实例中，我们使用了 range() 方法来生成一个区间范围内的数组。range() 方法最后一个参数为步长，默认为 1：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">      <span class=\"keyword\">var</span> myList1 = range(<span class=\"number\">10</span>, <span class=\"number\">20</span>, <span class=\"number\">2</span>)</div><div class=\"line\">      <span class=\"keyword\">var</span> myList2 = range(<span class=\"number\">10</span>,<span class=\"number\">20</span>)</div><div class=\"line\"></div><div class=\"line\">      <span class=\"comment\">// 输出所有数组元素</span></div><div class=\"line\">      <span class=\"keyword\">for</span> ( x &lt;- myList1 ) &#123;</div><div class=\"line\">         print( <span class=\"string\">\" \"</span> + x )</div><div class=\"line\">      &#125;</div><div class=\"line\">      println()</div><div class=\"line\">      <span class=\"keyword\">for</span> ( x &lt;- myList2 ) &#123;</div><div class=\"line\">         print( <span class=\"string\">\" \"</span> + x )</div><div class=\"line\">      &#125;</div><div class=\"line\">   &#125;</div><div class=\"line\">   -----------------输出---------------</div><div class=\"line\"><span class=\"number\">10</span> <span class=\"number\">12</span> <span class=\"number\">14</span> <span class=\"number\">16</span> <span class=\"number\">18</span></div><div class=\"line\"><span class=\"number\">10</span> <span class=\"number\">11</span> <span class=\"number\">12</span> <span class=\"number\">13</span> <span class=\"number\">14</span> <span class=\"number\">15</span> <span class=\"number\">16</span> <span class=\"number\">17</span> <span class=\"number\">18</span> <span class=\"number\">19</span></div></pre></td></tr></table></figure></p>\n<h1 id=\"Scala-集合List\"><a href=\"#Scala-集合List\" class=\"headerlink\" title=\"Scala 集合List\"></a><strong>Scala 集合List</strong></h1><p>Scala 列表类似于数组，它们所有元素的类型都相同，但是它们也有所不同：列表是不可变的，值一旦被定义了就不能改变，其次列表 具有递归的结构（也就是链接表结构）而数组不是。。<br>列表的元素类型 T 可以写成 List[T]。例如，以下列出了多种类型的列表：</p>\n<h2 id=\"列表构造方式\"><a href=\"#列表构造方式\" class=\"headerlink\" title=\"列表构造方式\"></a><strong>列表构造方式</strong></h2><h3 id=\"构造列表方式一\"><a href=\"#构造列表方式一\" class=\"headerlink\" title=\"构造列表方式一\"></a><strong>构造列表方式一</strong></h3><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// 字符串列表</span></div><div class=\"line\"><span class=\"keyword\">val</span> site: <span class=\"type\">List</span>[<span class=\"type\">String</span>] = <span class=\"type\">List</span>(<span class=\"string\">\"Runoob\"</span>, <span class=\"string\">\"Google\"</span>, <span class=\"string\">\"Baidu\"</span>)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 整型列表</span></div><div class=\"line\"><span class=\"keyword\">val</span> nums: <span class=\"type\">List</span>[<span class=\"type\">Int</span>] = <span class=\"type\">List</span>(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 空列表</span></div><div class=\"line\"><span class=\"keyword\">val</span> empty: <span class=\"type\">List</span>[<span class=\"type\">Nothing</span>] = <span class=\"type\">List</span>()</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 二维列表</span></div><div class=\"line\"><span class=\"keyword\">val</span> dim: <span class=\"type\">List</span>[<span class=\"type\">List</span>[<span class=\"type\">Int</span>]] =</div><div class=\"line\">   <span class=\"type\">List</span>(</div><div class=\"line\">      <span class=\"type\">List</span>(<span class=\"number\">1</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>),</div><div class=\"line\">      <span class=\"type\">List</span>(<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">0</span>),</div><div class=\"line\">      <span class=\"type\">List</span>(<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>)</div><div class=\"line\">   )</div></pre></td></tr></table></figure>\n<h3 id=\"构造列表方式二\"><a href=\"#构造列表方式二\" class=\"headerlink\" title=\"构造列表方式二\"></a><strong>构造列表方式二</strong></h3><p>构造列表的两个基本单位是 Nil 和 ::(发音为cons) Nil代表空列表，中缀符号:: 表示列表从前端扩展。也就是说x::xs代表了一个元素为x，后面紧贴着xs的列表，因此以上实例我们可以写成<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 字符串列表</span></div><div class=\"line\"><span class=\"keyword\">val</span> site = <span class=\"string\">\"Runoob\"</span> :: (<span class=\"string\">\"Google\"</span> :: (<span class=\"string\">\"Baidu\"</span> :: <span class=\"type\">Nil</span>))</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 整型列表</span></div><div class=\"line\"><span class=\"keyword\">val</span> nums = <span class=\"number\">1</span> :: (<span class=\"number\">2</span> :: (<span class=\"number\">3</span> :: (<span class=\"number\">4</span> :: <span class=\"type\">Nil</span>)))</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 空列表</span></div><div class=\"line\"><span class=\"keyword\">val</span> empty = <span class=\"type\">Nil</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">// 二维列表</span></div><div class=\"line\"><span class=\"keyword\">val</span> dim = (<span class=\"number\">1</span> :: (<span class=\"number\">0</span> :: (<span class=\"number\">0</span> :: <span class=\"type\">Nil</span>))) ::</div><div class=\"line\">          (<span class=\"number\">0</span> :: (<span class=\"number\">1</span> :: (<span class=\"number\">0</span> :: <span class=\"type\">Nil</span>))) ::</div><div class=\"line\">          (<span class=\"number\">0</span> :: (<span class=\"number\">0</span> :: (<span class=\"number\">1</span> :: <span class=\"type\">Nil</span>))) :: <span class=\"type\">Nil</span></div></pre></td></tr></table></figure></p>\n<h3 id=\"方式二的简化\"><a href=\"#方式二的简化\" class=\"headerlink\" title=\"方式二的简化\"></a><strong>方式二的简化</strong></h3><p>由于以::结尾，::遵循右结合的规则，A::B::C 等价于A::(B::C), 因此，前面定义用到的括号可以去掉<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">val</span> names = <span class=\"number\">1</span>::<span class=\"number\">2</span>::<span class=\"number\">3</span>::<span class=\"number\">4</span>::<span class=\"type\">Nil</span></div></pre></td></tr></table></figure></p>\n<p>与前面的names定义一致</p>\n<h2 id=\"列表的基本操作\"><a href=\"#列表的基本操作\" class=\"headerlink\" title=\"列表的基本操作\"></a><strong>列表的基本操作</strong></h2><ul>\n<li>Scala列表有三个基本操作：</li>\n<li>head 返回列表第一个元素</li>\n<li>tail 返回一个列表，包含除了第一元素之外的其他元素</li>\n<li>isEmpty 在列表为空时返回true<br>对于Scala列表的任何操作都可以使用这三个基本操作来表达。实例如下:<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Test</span> </span>&#123;</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">      <span class=\"keyword\">val</span> site = <span class=\"string\">\"Runoob\"</span> :: (<span class=\"string\">\"Google\"</span> :: (<span class=\"string\">\"Baidu\"</span> :: <span class=\"type\">Nil</span>))</div><div class=\"line\">      <span class=\"keyword\">val</span> nums = <span class=\"type\">Nil</span></div><div class=\"line\"></div><div class=\"line\">      println( <span class=\"string\">\"第一网站是 : \"</span> + site.head )</div><div class=\"line\">      println( <span class=\"string\">\"最后一个网站是 : \"</span> + site.tail )</div><div class=\"line\">      println( <span class=\"string\">\"查看列表 site 是否为空 : \"</span> + site.isEmpty )</div><div class=\"line\">      println( <span class=\"string\">\"查看 nums 是否为空 : \"</span> + nums.isEmpty )</div><div class=\"line\">   &#125;</div><div class=\"line\">&#125;</div><div class=\"line\">-----------输出--------</div><div class=\"line\">第一网站是 : <span class=\"type\">Runoob</span></div><div class=\"line\">最后一个网站是 : <span class=\"type\">List</span>(<span class=\"type\">Google</span>, <span class=\"type\">Baidu</span>)</div><div class=\"line\">查看列表 site 是否为空 : <span class=\"literal\">false</span></div><div class=\"line\">查看 nums 是否为空 : <span class=\"literal\">true</span></div></pre></td></tr></table></figure>\n</li>\n</ul>\n<h2 id=\"List类的一阶方法\"><a href=\"#List类的一阶方法\" class=\"headerlink\" title=\"List类的一阶方法\"></a><strong>List类的一阶方法</strong></h2><h3 id=\"连接列表\"><a href=\"#连接列表\" class=\"headerlink\" title=\"连接列表\"></a><strong>连接列表</strong></h3><p>你可以使用 ::: 运算符或 List.:::() 方法或 List.concat() 方法来连接两个或多个列表。实例如下:<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Test</span> </span>&#123;</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">      <span class=\"keyword\">val</span> site1 = <span class=\"string\">\"Runoob\"</span> :: (<span class=\"string\">\"Google\"</span> :: (<span class=\"string\">\"Baidu\"</span> :: <span class=\"type\">Nil</span>))</div><div class=\"line\">      <span class=\"keyword\">val</span> site2 = <span class=\"string\">\"Facebook\"</span> :: (<span class=\"string\">\"Taobao\"</span> :: <span class=\"type\">Nil</span>)</div><div class=\"line\"></div><div class=\"line\">      <span class=\"comment\">// 使用 ::: 运算符</span></div><div class=\"line\">      <span class=\"keyword\">var</span> fruit = site1 ::: site2</div><div class=\"line\">      println( <span class=\"string\">\"site1 ::: site2 : \"</span> + fruit )</div><div class=\"line\">      </div><div class=\"line\">      <span class=\"comment\">// 使用 Set.:::() 方法</span></div><div class=\"line\">      fruit = site1.:::(site2)</div><div class=\"line\">      println( <span class=\"string\">\"site1.:::(site2) : \"</span> + fruit )</div><div class=\"line\"></div><div class=\"line\">      <span class=\"comment\">// 使用 concat 方法</span></div><div class=\"line\">      fruit = <span class=\"type\">List</span>.concat(site1, site2)</div><div class=\"line\">      println( <span class=\"string\">\"List.concat(site1, site2) : \"</span> + fruit  )</div><div class=\"line\">      </div><div class=\"line\"></div><div class=\"line\">   &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h3 id=\"List-fill\"><a href=\"#List-fill\" class=\"headerlink\" title=\"List.fill()\"></a><strong>List.fill()</strong></h3><p>我们可以使用 List.fill() 方法来创建一个指定重复数量的元素列表：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Test</span> </span>&#123;</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">      <span class=\"keyword\">val</span> site = <span class=\"type\">List</span>.fill(<span class=\"number\">3</span>)(<span class=\"string\">\"Runoob\"</span>) <span class=\"comment\">// 重复 Runoob 3次</span></div><div class=\"line\">      println( <span class=\"string\">\"site : \"</span> + site  )</div><div class=\"line\"></div><div class=\"line\">      <span class=\"keyword\">val</span> num = <span class=\"type\">List</span>.fill(<span class=\"number\">10</span>)(<span class=\"number\">2</span>)         <span class=\"comment\">// 重复元素 2, 10 次</span></div><div class=\"line\">      println( <span class=\"string\">\"num : \"</span> + num  )</div><div class=\"line\">   &#125;</div><div class=\"line\">&#125;</div><div class=\"line\">---------输出--------</div><div class=\"line\">site : <span class=\"type\">List</span>(<span class=\"type\">Runoob</span>, <span class=\"type\">Runoob</span>, <span class=\"type\">Runoob</span>)</div><div class=\"line\">num : <span class=\"type\">List</span>(<span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>, <span class=\"number\">2</span>)</div></pre></td></tr></table></figure></p>\n<h3 id=\"List-tabulate\"><a href=\"#List-tabulate\" class=\"headerlink\" title=\"List.tabulate()\"></a><strong>List.tabulate()</strong></h3><p>List.tabulate() 方法是通过给定的函数来创建列表。<br>方法的第一个参数为元素的数量，可以是二维的，第二个参数为指定的函数，我们通过指定的函数计算结果并返回值插入到列表中，起始值为 0，实例如下：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Test</span> </span>&#123;</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">      <span class=\"comment\">// 通过给定的函数创建 5 个元素</span></div><div class=\"line\">      <span class=\"keyword\">val</span> squares = <span class=\"type\">List</span>.tabulate(<span class=\"number\">6</span>)(n =&gt; n * n)</div><div class=\"line\">      println( <span class=\"string\">\"一维 : \"</span> + squares  )</div><div class=\"line\"></div><div class=\"line\">      <span class=\"comment\">// 创建二维列表</span></div><div class=\"line\">      <span class=\"keyword\">val</span> mul = <span class=\"type\">List</span>.tabulate( <span class=\"number\">4</span>,<span class=\"number\">5</span> )( _ * _ )      </div><div class=\"line\">      println( <span class=\"string\">\"多维 : \"</span> + mul  )</div><div class=\"line\">   &#125;</div><div class=\"line\">&#125;</div><div class=\"line\">----------------输出---------</div><div class=\"line\">一维 : <span class=\"type\">List</span>(<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">4</span>, <span class=\"number\">9</span>, <span class=\"number\">16</span>, <span class=\"number\">25</span>)</div><div class=\"line\">多维 : <span class=\"type\">List</span>(<span class=\"type\">List</span>(<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span>), <span class=\"type\">List</span>(<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>), <span class=\"type\">List</span>(<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">4</span>, <span class=\"number\">6</span>, <span class=\"number\">8</span>), <span class=\"type\">List</span>(<span class=\"number\">0</span>, <span class=\"number\">3</span>, <span class=\"number\">6</span>, <span class=\"number\">9</span>, <span class=\"number\">12</span>))</div></pre></td></tr></table></figure></p>\n<h3 id=\"列表反转\"><a href=\"#列表反转\" class=\"headerlink\" title=\"列表反转\"></a><strong>列表反转</strong></h3><p>List.reverse 用于将列表的顺序反转，实例如下：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Test</span> </span>&#123;</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">      <span class=\"keyword\">val</span> site = <span class=\"string\">\"Runoob\"</span> :: (<span class=\"string\">\"Google\"</span> :: (<span class=\"string\">\"Baidu\"</span> :: <span class=\"type\">Nil</span>))</div><div class=\"line\">      println( <span class=\"string\">\"site 反转前 : \"</span> + site )</div><div class=\"line\"></div><div class=\"line\">      println( <span class=\"string\">\"site 反转前 : \"</span> + site.reverse )</div><div class=\"line\">   &#125;</div><div class=\"line\">&#125;</div><div class=\"line\">------------输出---------------</div><div class=\"line\">$ vim <span class=\"type\">Test</span>.scala </div><div class=\"line\">$ scala <span class=\"type\">Test</span>.scala </div><div class=\"line\">site 反转前 : <span class=\"type\">List</span>(<span class=\"type\">Runoob</span>, <span class=\"type\">Google</span>, <span class=\"type\">Baidu</span>)</div><div class=\"line\">site 反转前 : <span class=\"type\">List</span>(<span class=\"type\">Baidu</span>, <span class=\"type\">Google</span>, <span class=\"type\">Runoob</span>)</div></pre></td></tr></table></figure></p>\n<h3 id=\"前缀与后缀-drop-take-splitAt\"><a href=\"#前缀与后缀-drop-take-splitAt\" class=\"headerlink\" title=\"前缀与后缀 drop take splitAt\"></a><strong>前缀与后缀 drop take splitAt</strong></h3><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">    <span class=\"keyword\">var</span> ls = <span class=\"string\">\"google\"</span>::<span class=\"string\">\"baidu\"</span>::<span class=\"string\">\"tenxun\"</span>::<span class=\"string\">\"alibaba\"</span>::<span class=\"string\">\"apples\"</span>::<span class=\"type\">Nil</span></div><div class=\"line\">    <span class=\"keyword\">var</span> takeTest =ls.take(<span class=\"number\">2</span>)</div><div class=\"line\">    <span class=\"keyword\">var</span> dropTest = ls.drop(<span class=\"number\">2</span>)</div><div class=\"line\">    <span class=\"keyword\">var</span> splitTest = ls.splitAt(<span class=\"number\">3</span>)</div><div class=\"line\">    println(<span class=\"string\">\"takeTest: \"</span>+takeTest)</div><div class=\"line\">    println(<span class=\"string\">\"dropTest: \"</span>+dropTest)</div><div class=\"line\">    println(<span class=\"string\">\"splitTest: \"</span>+splitTest)</div><div class=\"line\">  &#125;</div><div class=\"line\">  </div><div class=\"line\">--------------输出-------------</div><div class=\"line\">takeTest: <span class=\"type\">List</span>(google, baidu)</div><div class=\"line\">dropTest: <span class=\"type\">List</span>(tenxun, alibaba, apples)</div><div class=\"line\">splitTest: (<span class=\"type\">List</span>(google, baidu, tenxun),<span class=\"type\">List</span>(alibaba, apples))</div></pre></td></tr></table></figure>\n<h3 id=\"元素选择-apply方法和indices方法\"><a href=\"#元素选择-apply方法和indices方法\" class=\"headerlink\" title=\"元素选择 apply方法和indices方法\"></a><strong>元素选择 apply方法和indices方法</strong></h3><p>apply方法是获取第几个值，list.apply(2)和list(2)是一样的<br>indices是获取所有的列表的Range,暂时不知道有多大用处<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">    <span class=\"keyword\">var</span> ls = <span class=\"string\">\"google\"</span>::<span class=\"string\">\"baidu\"</span>::<span class=\"string\">\"tenxun\"</span>::<span class=\"string\">\"alibaba\"</span>::<span class=\"string\">\"apples\"</span>::<span class=\"string\">\"stackoverflow\"</span>::<span class=\"type\">Nil</span></div><div class=\"line\">    <span class=\"keyword\">var</span> applyTest = ls.apply(<span class=\"number\">2</span>)</div><div class=\"line\">    <span class=\"keyword\">var</span> indicesTest =  ls.indices</div><div class=\"line\">    println(<span class=\"string\">\"applyTest: \"</span>+applyTest)</div><div class=\"line\">    println(<span class=\"string\">\"applyTest2: \"</span>+ls(<span class=\"number\">2</span>))</div><div class=\"line\">    println(<span class=\"string\">\"indicesTest: \"</span>+indicesTest)</div><div class=\"line\">    <span class=\"keyword\">for</span>(i&lt;-indicesTest)&#123;</div><div class=\"line\">      println(ls(i))</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">-------------输出--------</div><div class=\"line\">applyTest: tenxun</div><div class=\"line\">applyTest2: tenxun</div><div class=\"line\">indicesTest: <span class=\"type\">Range</span>(<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>)</div><div class=\"line\">google</div><div class=\"line\">baidu</div><div class=\"line\">tenxun</div><div class=\"line\">alibaba</div><div class=\"line\">apples</div><div class=\"line\">stackoverflow</div></pre></td></tr></table></figure></p>\n<h3 id=\"toString和mkString\"><a href=\"#toString和mkString\" class=\"headerlink\" title=\"toString和mkString\"></a><strong>toString和mkString</strong></h3><p>toString和java的一样<br>mkString  mkString(start: String,sep: String,end: String): String<br>第一个参数是以什么开始，第二个参数是以什么分割，第三个参数是以什么结尾，函数返回一个String<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">    <span class=\"keyword\">var</span> ls = <span class=\"string\">\"google\"</span> :: <span class=\"string\">\"baidu\"</span> :: <span class=\"string\">\"tenxun\"</span> :: <span class=\"string\">\"alibaba\"</span> :: <span class=\"string\">\"apples\"</span> :: <span class=\"string\">\"stackoverflow\"</span> :: <span class=\"type\">Nil</span></div><div class=\"line\">    println(<span class=\"string\">\"toString: \"</span>+ls.toString())</div><div class=\"line\">    println(<span class=\"string\">\"mkString: \"</span>+ls.mkString(<span class=\"string\">\"&#123;\"</span>,<span class=\"string\">\";\"</span>,<span class=\"string\">\"&#125;\"</span>))</div><div class=\"line\">  &#125;</div><div class=\"line\">---------输出----------</div><div class=\"line\">toString: <span class=\"type\">List</span>(google, baidu, tenxun, alibaba, apples, stackoverflow)</div><div class=\"line\">mkString: &#123;google;baidu;tenxun;alibaba;apples;stackoverflow&#123;</div></pre></td></tr></table></figure></p>\n<h3 id=\"转换列表-toArray-element-iterator\"><a href=\"#转换列表-toArray-element-iterator\" class=\"headerlink\" title=\"转换列表 toArray element iterator\"></a><strong>转换列表 toArray element iterator</strong></h3><p>想要在数组Array和列表list之间转换，可以使用List的toArray和Array的toList<br>例子<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; var ls = &quot;google&quot; :: &quot;baidu&quot; :: &quot;tenxun&quot; :: &quot;alibaba&quot; :: &quot;apples&quot; :: &quot;stackoverflow&quot; :: Nil</div><div class=\"line\">ls: List[String] = List(google, baidu, tenxun, alibaba, apples, stackoverflow)</div><div class=\"line\"></div><div class=\"line\">scala&gt; var arrays = ls.toArray</div><div class=\"line\">arrays: Array[String] = Array(google, baidu, tenxun, alibaba, apples, stackoverflow)</div><div class=\"line\"></div><div class=\"line\">scala&gt; var lists = arrays.toList</div><div class=\"line\">lists: List[String] = List(google, baidu, tenxun, alibaba, apples, stackoverflow)</div></pre></td></tr></table></figure></p>\n<p>element在很旧的版本有，现在已经过时不用了。如果要用枚举器访问列表元素，可以使用iterator<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">    <span class=\"keyword\">var</span> ls = <span class=\"string\">\"google\"</span> :: <span class=\"string\">\"baidu\"</span> :: <span class=\"string\">\"tenxun\"</span> :: <span class=\"string\">\"alibaba\"</span> :: <span class=\"string\">\"apples\"</span> :: <span class=\"string\">\"stackoverflow\"</span> :: <span class=\"type\">Nil</span></div><div class=\"line\">     <span class=\"keyword\">val</span> it =ls.iterator</div><div class=\"line\">      <span class=\"keyword\">while</span>(it.hasNext)&#123;</div><div class=\"line\">        print(it.next()+<span class=\"string\">\",\"</span>)</div><div class=\"line\">      &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">--------------输出------------</div><div class=\"line\">google, baidu, tenxun, alibaba, apples, stackoverflow,</div></pre></td></tr></table></figure></p>\n<h2 id=\"list类的高阶用法\"><a href=\"#list类的高阶用法\" class=\"headerlink\" title=\"list类的高阶用法\"></a><strong>list类的高阶用法</strong></h2><p>在java中，若要提取出满足特点条件的元素，或者检查所有元素是否满足某种性质，或者用某种方式转变列表的所有元素，这样的需求一般都需要使用for或者while循环的固定表达式，在scala中，可以通过使用List的一些高阶方法（函数）来更为简介的实现  </p>\n<h3 id=\"列表间映射：map、flatMap和foreach\"><a href=\"#列表间映射：map、flatMap和foreach\" class=\"headerlink\" title=\"列表间映射：map、flatMap和foreach\"></a><strong>列表间映射：map、flatMap和foreach</strong></h3><ol>\n<li>xs map f 操作返回<strong>把函数f应用在xs的每个列表元素之后</strong>由此<strong>组成的新列表</strong>。如：<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; List(1,2,3).map(_ +1)</div><div class=\"line\">res1: List[Int] = List(2, 3, 4)</div></pre></td></tr></table></figure>\n</li>\n</ol>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> words = <span class=\"type\">List</span>(<span class=\"string\">\"zks\"</span>,<span class=\"string\">\"zhaikaishun\"</span>,<span class=\"string\">\"kaishun\"</span>,<span class=\"string\">\"kai\"</span>,<span class=\"string\">\"xiaozhai\"</span>)</div><div class=\"line\">words: <span class=\"type\">List</span>[<span class=\"type\">String</span>] = <span class=\"type\">List</span>(zks, zhaikaishun, kaishun, kai, xiaozhai)</div><div class=\"line\"></div><div class=\"line\">scala&gt; words.map(_.length)</div><div class=\"line\">res2: <span class=\"type\">List</span>[<span class=\"type\">Int</span>] = <span class=\"type\">List</span>(<span class=\"number\">3</span>, <span class=\"number\">11</span>, <span class=\"number\">7</span>, <span class=\"number\">3</span>, <span class=\"number\">8</span>)</div></pre></td></tr></table></figure>\n<ol>\n<li>flatMap操作符与map类似，不过它的右操作元是能够返回元素列表的函数。它对列表的每个元素调用该方法，<strong>然后连接所有方法</strong>的结果并返回。map与flatMap的差异举例说明如下：<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; words.map(_.toList)</div><div class=\"line\">res3: <span class=\"type\">List</span>[<span class=\"type\">List</span>[<span class=\"type\">Char</span>]] = <span class=\"type\">List</span>(<span class=\"type\">List</span>(z, k, s), <span class=\"type\">List</span>(z, h, a, i, k, a, i, s, h, u, n), <span class=\"type\">List</span>(k, a, i, s, h, u, n), <span class=\"type\">List</span>(k, a, i), <span class=\"type\">List</span>(x, i, a, o, z, h, a, i))</div><div class=\"line\"></div><div class=\"line\">scala&gt; words.flatMap(_.toList)</div><div class=\"line\">res4: <span class=\"type\">List</span>[<span class=\"type\">Char</span>] = <span class=\"type\">List</span>(z, k, s, z, h, a, i, k, a, i, s, h, u, n, k, a, i, s, h, u, n, k, a, i, x, i, a, o, z, h, a, i)</div></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>map与flatMap的差异和协作可以用下面的例子体会<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"type\">List</span>.range(<span class=\"number\">1</span>, <span class=\"number\">5</span>).flatMap(i =&gt; <span class=\"type\">List</span>.range(<span class=\"number\">1</span>, i).map(j =&gt; (i, j)))</div><div class=\"line\">res9: <span class=\"type\">List</span>[(<span class=\"type\">Int</span>, <span class=\"type\">Int</span>)] = <span class=\"type\">List</span>((<span class=\"number\">2</span>,<span class=\"number\">1</span>), (<span class=\"number\">3</span>,<span class=\"number\">1</span>), (<span class=\"number\">3</span>,<span class=\"number\">2</span>), (<span class=\"number\">4</span>,<span class=\"number\">1</span>), (<span class=\"number\">4</span>,<span class=\"number\">2</span>), (<span class=\"number\">4</span>,<span class=\"number\">3</span>))</div></pre></td></tr></table></figure></p>\n<p>解释 : List.range(1,5)生成了List(1,2,3,4),注意没有5<br>.flatMap对内部的每一个元素进行操作, 后面有个.map是对内部List.range(1, i)的每一个元素进行操作，最后flatMap返回的还是一个List<br>上述例子也可以用for循环+yield来完成<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt;  <span class=\"keyword\">for</span> (i &lt;- <span class=\"type\">List</span>.range(<span class=\"number\">1</span>, <span class=\"number\">5</span>); j &lt;- <span class=\"type\">List</span>.range(<span class=\"number\">1</span>, i)) <span class=\"keyword\">yield</span> (i,j)</div><div class=\"line\">res10: <span class=\"type\">List</span>[(<span class=\"type\">Int</span>, <span class=\"type\">Int</span>)] = <span class=\"type\">List</span>((<span class=\"number\">2</span>,<span class=\"number\">1</span>), (<span class=\"number\">3</span>,<span class=\"number\">1</span>), (<span class=\"number\">3</span>,<span class=\"number\">2</span>), (<span class=\"number\">4</span>,<span class=\"number\">1</span>), (<span class=\"number\">4</span>,<span class=\"number\">2</span>), (<span class=\"number\">4</span>,<span class=\"number\">3</span>))</div></pre></td></tr></table></figure></p>\n<ol>\n<li>foreach是第三种与映射类似的操作。它的右操作元是过程（返回Unit的函数）。它只是对每个列表元素都调用一遍过程。操作的结果仍然是Unit，不会产生结果列表。例如：<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">    <span class=\"keyword\">var</span> sum =<span class=\"number\">0</span></div><div class=\"line\">    <span class=\"type\">List</span>(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>) foreach (sum += _)</div><div class=\"line\">    println(sum)</div><div class=\"line\">  &#125;</div><div class=\"line\">-----输出----</div><div class=\"line\"><span class=\"number\">15</span></div></pre></td></tr></table></figure>\n</li>\n</ol>\n<h3 id=\"列表过滤：filter、partition、find、takeWhile、dropWhile和span\"><a href=\"#列表过滤：filter、partition、find、takeWhile、dropWhile和span\" class=\"headerlink\" title=\"列表过滤：filter、partition、find、takeWhile、dropWhile和span\"></a><strong>列表过滤：filter、partition、find、takeWhile、dropWhile和span</strong></h3><p>1.xs filter p操作产生xs中符合p（x）为true的所有元素组成的列表。如：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"type\">List</span> (<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>) filter (_ % <span class=\"number\">2</span> == <span class=\"number\">0</span>)</div><div class=\"line\">res10: <span class=\"type\">List</span>[<span class=\"type\">Int</span>] = <span class=\"type\">List</span>(<span class=\"number\">2</span>, <span class=\"number\">4</span>)</div><div class=\"line\"></div><div class=\"line\">scala&gt; words filter (_.length == <span class=\"number\">3</span>)</div><div class=\"line\">res11: <span class=\"type\">List</span>[<span class=\"type\">String</span>] = <span class=\"type\">List</span>(the, fox)</div></pre></td></tr></table></figure></p>\n<p>2.partition方法与filter类似，不过返回的是列表对。其中一个包含所有论断为真的元素，另一个包含所有论断为假的元素。<br><strong>xs partition p  等价于 (xs filter p, xs filter (!p()))  </strong><br>举例如下：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"type\">List</span>(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>) partition (_ % <span class=\"number\">2</span> ==<span class=\"number\">0</span>)</div><div class=\"line\">res12: (<span class=\"type\">List</span>[<span class=\"type\">Int</span>], <span class=\"type\">List</span>[<span class=\"type\">Int</span>]) = (<span class=\"type\">List</span>(<span class=\"number\">2</span>, <span class=\"number\">4</span>),<span class=\"type\">List</span>(<span class=\"number\">1</span>, <span class=\"number\">3</span>, <span class=\"number\">5</span>))</div></pre></td></tr></table></figure></p>\n<p>3.find方法同样与filter方法类似，不过返回的是第一个满足给定论断的元素，而并不是全部。xs find p 操作以列表xs和论断p为操作元。返回可选值。如果xs中存在元素x使得p（x）为真，Some（x）将返回。否则，若p对所有元素都不成立，None将返回。举例如下：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"type\">List</span>(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>) find (_ % <span class=\"number\">2</span> == <span class=\"number\">0</span>)</div><div class=\"line\">res13: <span class=\"type\">Option</span>[<span class=\"type\">Int</span>] = <span class=\"type\">Some</span>(<span class=\"number\">2</span>)</div><div class=\"line\"></div><div class=\"line\">scala&gt; <span class=\"type\">List</span>(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>) find (_  &lt;= <span class=\"number\">0</span>)</div><div class=\"line\">res15: <span class=\"type\">Option</span>[<span class=\"type\">Int</span>] = <span class=\"type\">None</span></div></pre></td></tr></table></figure></p>\n<ol>\n<li>xs takeWhile p操作返回列表xs中最长的能够满足p的前缀。例如：<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"type\">List</span>(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">-4</span>, <span class=\"number\">5</span>) takeWhile (_ &gt; <span class=\"number\">0</span>)</div><div class=\"line\">res16: <span class=\"type\">List</span>[<span class=\"type\">Int</span>] = <span class=\"type\">List</span>(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>)</div></pre></td></tr></table></figure>\n</li>\n</ol>\n<p>5.xs dropWhile p操作移除最长能够满足p的前缀。举例如下：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> words = <span class=\"type\">List</span>(<span class=\"string\">\"the\"</span>, <span class=\"string\">\"quick\"</span>, <span class=\"string\">\"brown\"</span>, <span class=\"string\">\"fox\"</span>)</div><div class=\"line\">words: <span class=\"type\">List</span>[<span class=\"type\">String</span>] = <span class=\"type\">List</span>(the, quick, brown, fox)</div><div class=\"line\"></div><div class=\"line\">scala&gt; words dropWhile (_ startsWith <span class=\"string\">\"t\"</span>)</div><div class=\"line\">res11: <span class=\"type\">List</span>[<span class=\"type\">String</span>] = <span class=\"type\">List</span>(quick, brown, fox)</div></pre></td></tr></table></figure></p>\n<p>6.span方法把takeWhile和dropWhile组合成一个操作。它返回一对列表，定义与下列等式一致：<br>xs span p 等价于 （xs takeWhile p， xs dropWhile p）<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"type\">List</span>(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">-4</span>, <span class=\"number\">5</span>) span (_ &gt;<span class=\"number\">0</span>)</div><div class=\"line\">res18: (<span class=\"type\">List</span>[<span class=\"type\">Int</span>], <span class=\"type\">List</span>[<span class=\"type\">Int</span>]) = (<span class=\"type\">List</span>(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>),<span class=\"type\">List</span>(<span class=\"number\">-4</span>, <span class=\"number\">5</span>))</div></pre></td></tr></table></figure></p>\n<h3 id=\"列表的论断：forall和exists\"><a href=\"#列表的论断：forall和exists\" class=\"headerlink\" title=\"列表的论断：forall和exists\"></a><strong>列表的论断：forall和exists</strong></h3><ol>\n<li>xs forall p 如果列表的所有元素满足p则返回true</li>\n<li>xs exists p 如果列表中有一个值满足p就返回true<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">hasZeroRow</span></span>(m: <span class=\"type\">List</span>[<span class=\"type\">List</span>[<span class=\"type\">Int</span>]]) = m.exists(row =&gt; row forall (_ == <span class=\"number\">0</span>))</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">    <span class=\"keyword\">val</span> m= <span class=\"type\">List</span>(<span class=\"type\">List</span>(<span class=\"number\">3</span>,<span class=\"number\">0</span>,<span class=\"number\">0</span>), <span class=\"type\">List</span>(<span class=\"number\">0</span>,<span class=\"number\">3</span>,<span class=\"number\">0</span>), <span class=\"type\">List</span>(<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">3</span>))</div><div class=\"line\">    <span class=\"keyword\">var</span> flag :<span class=\"type\">Boolean</span>= hasZeroRow(m)</div><div class=\"line\">    println(flag)</div><div class=\"line\">  &#125;</div><div class=\"line\">----------输出--------</div><div class=\"line\"><span class=\"literal\">false</span></div></pre></td></tr></table></figure>\n</li>\n</ol>\n<h3 id=\"折叠操作\"><a href=\"#折叠操作\" class=\"headerlink\" title=\"折叠操作\"></a><strong>折叠操作</strong></h3><p>如果我们把集合看成是一张纸条，每一小段代表一个元素，那么reduceLeft就将这张纸条从左向右”折叠”，最前面的两个元素会首先“重叠”在一起，这时会使用传给reduceLeft的参数函数进行计算，返回的结果就像是已经折叠在一起的两段纸条，它们已经是一个叠加的状态了，所以它，也就是上次重叠的结果会继续做为一个单一的值和下一个元素继续“叠加”，直到折叠到集合的最后一个元素</p>\n<ol>\n<li><p><strong>reduceLeft</strong></p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt;  <span class=\"type\">List</span>.range(<span class=\"number\">1</span>, <span class=\"number\">5</span>).reduceLeft(_+_)</div><div class=\"line\">res12: <span class=\"type\">Int</span> = <span class=\"number\">10</span></div></pre></td></tr></table></figure>\n</li>\n<li><p><strong>reduceRight</strong><br>和reduceLeft相似，但是是从右向左折叠,<strong>注意</strong>:==它的操作方向是从右到左，但是参数的顺序却并不是，而是依然第一参数是左边的元素，第二参数是右边的元素==</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"type\">List</span>.range(<span class=\"number\">1</span>, <span class=\"number\">4</span>) reduceRight(_ - _)</div><div class=\"line\">res15: <span class=\"type\">Int</span> = <span class=\"number\">2</span></div><div class=\"line\"><span class=\"comment\">// 2-3 = -1</span></div><div class=\"line\"><span class=\"comment\">// 1-(-1)=2</span></div></pre></td></tr></table></figure>\n</li>\n<li><p><strong>foldLeft</strong><br>类似于reduceLeft, 不过开始折叠的第一个元素不是集合中的第一个元素，而是传入的一个元素，有点类似于先将这个参数放入的集合中的首位，然后在reduceLeft</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"type\">List</span>.range(<span class=\"number\">1</span>, <span class=\"number\">5</span>).foldLeft(<span class=\"number\">1</span>)(_+_)</div><div class=\"line\">res1: <span class=\"type\">Int</span> = <span class=\"number\">11</span></div></pre></td></tr></table></figure>\n</li>\n<li><p><strong>foldRight</strong><br>类似于foldLeft，不过是从右向左，不再举例</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"type\">List</span>.range(<span class=\"number\">1</span>, <span class=\"number\">4</span>).foldRight(<span class=\"number\">3</span>)(_-_)</div><div class=\"line\">res1: <span class=\"type\">Int</span> = <span class=\"number\">-1</span></div></pre></td></tr></table></figure>\n</li>\n</ol>\n<h1 id=\"scala-集合Set\"><a href=\"#scala-集合Set\" class=\"headerlink\" title=\"scala 集合Set\"></a><strong>scala 集合Set</strong></h1><p>Set和List基本相似，只是所有的元素都是唯一的<br>默认的Set是不可变的,默认引用的是 scala.collection.immutable.Set<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">val</span> set = <span class=\"type\">Set</span>(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>)</div><div class=\"line\">println(set.getClass.getName) <span class=\"comment\">// </span></div><div class=\"line\"></div><div class=\"line\">println(set.exists(_ % <span class=\"number\">2</span> == <span class=\"number\">0</span>)) <span class=\"comment\">//true</span></div><div class=\"line\">println(set.drop(<span class=\"number\">1</span>)) <span class=\"comment\">//Set(2,3)</span></div></pre></td></tr></table></figure></p>\n<p>如果需要使用可变集合需要引入 scala.collection.mutable.Set：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> scala.collection.mutable.<span class=\"type\">Set</span> <span class=\"comment\">// 可以在任何地方引入 可变集合</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">val</span> mutableSet = <span class=\"type\">Set</span>(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>)</div><div class=\"line\">println(mutableSet.getClass.getName) <span class=\"comment\">// scala.collection.mutable.HashSet</span></div><div class=\"line\"></div><div class=\"line\">mutableSet.add(<span class=\"number\">4</span>)</div><div class=\"line\">mutableSet.remove(<span class=\"number\">1</span>)</div><div class=\"line\">mutableSet += <span class=\"number\">5</span></div><div class=\"line\">mutableSet -= <span class=\"number\">2</span></div><div class=\"line\"></div><div class=\"line\">println(mutableSet) <span class=\"comment\">// Set(5, 3, 4)</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">val</span> another = mutableSet.toSet</div><div class=\"line\">println(another.getClass.getName) <span class=\"comment\">// scala.collection.immutable.Set</span></div></pre></td></tr></table></figure></p>\n<h2 id=\"连接集合\"><a href=\"#连接集合\" class=\"headerlink\" title=\"连接集合\"></a><strong>连接集合</strong></h2><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">var</span> site = site1 ++ site2</div></pre></td></tr></table></figure>\n<h2 id=\"查找集合中最大与最小元素\"><a href=\"#查找集合中最大与最小元素\" class=\"headerlink\" title=\"查找集合中最大与最小元素\"></a><strong>查找集合中最大与最小元素</strong></h2><p>Set.min<br>Set.max</p>\n<h2 id=\"交集\"><a href=\"#交集\" class=\"headerlink\" title=\"交集\"></a><strong>交集</strong></h2><p>Set.intersect<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">set1.intersect(set2)</div></pre></td></tr></table></figure></p>\n<h1 id=\"map\"><a href=\"#map\" class=\"headerlink\" title=\"map\"></a><strong>map</strong></h1><p>ap(映射)是一种可迭代的键值对（key/value）结构。<br>所有的值都可以通过键来获取。<br>Map 中的键都是唯一的。<br>Map 也叫哈希表（Hash tables）。<br>Map 有两种类型，可变与不可变，区别在于可变对象可以修改它，而不可变对象不可以。<br>默认情况下 Scala 使用不可变 Map。如果你需要使用可变集合，你需要显式的引入 import scala.collection.mutable.Map 类<br>在 Scala 中 你可以同时使用可变与不可变 Map，不可变的直接使用 Map，可变的使用 mutable.Map。以下实例演示了不可变 Map 的应用：<br>不可变map<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> colors = <span class=\"type\">Map</span>(<span class=\"string\">\"red\"</span> -&gt; <span class=\"string\">\"#FF0000\"</span>, <span class=\"string\">\"azure\"</span> -&gt; <span class=\"string\">\"#F0FFFF\"</span>)</div><div class=\"line\">colors: scala.collection.immutable.<span class=\"type\">Map</span>[<span class=\"type\">String</span>,<span class=\"type\">String</span>] = <span class=\"type\">Map</span>(red -&gt; #<span class=\"type\">FF0000</span>, azure -&gt; #<span class=\"type\">F0FFFF</span>)</div><div class=\"line\">```  </div><div class=\"line\">## **<span class=\"type\">Map</span>的赋值**  </div><div class=\"line\">如果需要添加 key-value 对，可以使用 + 号，如下所示：</div><div class=\"line\">```scala</div><div class=\"line\"><span class=\"comment\">// 空哈希表，键为字符串，值为整型</span></div><div class=\"line\"><span class=\"keyword\">var</span> <span class=\"type\">A</span>:<span class=\"type\">Map</span>[<span class=\"type\">Char</span>,<span class=\"type\">Int</span>] = <span class=\"type\">Map</span>()</div><div class=\"line\"><span class=\"type\">A</span> += ('<span class=\"type\">I</span>' -&gt; <span class=\"number\">1</span>)</div><div class=\"line\"><span class=\"type\">A</span> += ('<span class=\"type\">J</span>' -&gt; <span class=\"number\">5</span>)</div><div class=\"line\"><span class=\"type\">A</span> += ('<span class=\"type\">K</span>' -&gt; <span class=\"number\">10</span>)</div><div class=\"line\"><span class=\"type\">A</span> += ('<span class=\"type\">L</span>' -&gt; <span class=\"number\">100</span>)</div><div class=\"line\">println（<span class=\"type\">A</span>）</div><div class=\"line\">-------输出--------</div><div class=\"line\"><span class=\"type\">Map</span>(<span class=\"type\">I</span> -&gt; <span class=\"number\">1</span>, <span class=\"type\">J</span> -&gt; <span class=\"number\">5</span>, <span class=\"type\">K</span> -&gt; <span class=\"number\">10</span>, <span class=\"type\">L</span> -&gt; <span class=\"number\">100</span>)</div></pre></td></tr></table></figure></p>\n<h2 id=\"Map的基本操作\"><a href=\"#Map的基本操作\" class=\"headerlink\" title=\"Map的基本操作\"></a><strong>Map的基本操作</strong></h2><ul>\n<li>keys      返回 Map 所有的键(key)</li>\n<li>values    返回 Map 所有的值(value)</li>\n<li>isEmpty    在 Map 为空时返回true<br>例子<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div></pre></td><td class=\"code\"><pre><div class=\"line\">object Test &#123;</div><div class=\"line\">   def main(args: Array[String]) &#123;</div><div class=\"line\">      val colors = Map(&quot;red&quot; -&gt; &quot;#FF0000&quot;,</div><div class=\"line\">                       &quot;azure&quot; -&gt; &quot;#F0FFFF&quot;,</div><div class=\"line\">                       &quot;peru&quot; -&gt; &quot;#CD853F&quot;)</div><div class=\"line\"></div><div class=\"line\">      val nums: Map[Int, Int] = Map()</div><div class=\"line\"></div><div class=\"line\">      println( &quot;colors 中的键为 : &quot; + colors.keys )</div><div class=\"line\">      println( &quot;colors 中的值为 : &quot; + colors.values )</div><div class=\"line\">      println( &quot;检测 colors 是否为空 : &quot; + colors.isEmpty )</div><div class=\"line\">      println( &quot;检测 nums 是否为空 : &quot; + nums.isEmpty )</div><div class=\"line\">   &#125;</div><div class=\"line\">&#125;</div><div class=\"line\">---------输出---------</div><div class=\"line\">colors 中的键为 : Set(red, azure, peru)</div><div class=\"line\">colors 中的值为 : MapLike(#FF0000, #F0FFFF, #CD853F)</div><div class=\"line\">检测 colors 是否为空 : false</div><div class=\"line\">检测 nums 是否为空 : true</div></pre></td></tr></table></figure>\n</li>\n</ul>\n<h2 id=\"Map-合并\"><a href=\"#Map-合并\" class=\"headerlink\" title=\"Map 合并\"></a><strong>Map 合并</strong></h2><p>++ 运算符或 Map.++() 方法来连接两个 Map, Map 合并时会移除重复的 key。<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">val</span> colors1 = <span class=\"type\">Map</span>(<span class=\"string\">\"red\"</span> -&gt; <span class=\"string\">\"#FF0000\"</span>,</div><div class=\"line\">      <span class=\"string\">\"azure\"</span> -&gt; <span class=\"string\">\"#F0FFFF\"</span>,</div><div class=\"line\">      <span class=\"string\">\"peru\"</span> -&gt; <span class=\"string\">\"#CD853F\"</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> colors2 = <span class=\"type\">Map</span>(<span class=\"string\">\"blue\"</span> -&gt; <span class=\"string\">\"#0033FF\"</span>,</div><div class=\"line\">      <span class=\"string\">\"yellow\"</span> -&gt; <span class=\"string\">\"#FFFF00\"</span>,</div><div class=\"line\">      <span class=\"string\">\"red\"</span> -&gt; <span class=\"string\">\"#FF0001\"</span>)</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">//  ++ 作为运算符</span></div><div class=\"line\">    <span class=\"keyword\">var</span> colors = colors1 ++ colors2</div><div class=\"line\">    println( <span class=\"string\">\"colors1 ++ colors2 : \"</span> + colors )</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">//  ++ 作为方法</span></div><div class=\"line\">    colors = colors1.++(colors2)</div><div class=\"line\">    println( <span class=\"string\">\"colors1.++(colors2)) : \"</span> + colors )</div><div class=\"line\">--------输出--------------------</div><div class=\"line\">colors1 ++ colors2 : <span class=\"type\">Map</span>(blue -&gt; #<span class=\"number\">0033</span>FF, azure -&gt; #<span class=\"type\">F0FFFF</span>, peru -&gt; #<span class=\"type\">CD853F</span>, yellow -&gt; #<span class=\"type\">FFFF00</span>, red -&gt; #<span class=\"type\">FF0001</span>)</div><div class=\"line\">colors1.++(colors2)) : <span class=\"type\">Map</span>(blue -&gt; #<span class=\"number\">0033</span>FF, azure -&gt; #<span class=\"type\">F0FFFF</span>, peru -&gt; #<span class=\"type\">CD853F</span>, yellow -&gt; #<span class=\"type\">FFFF00</span>, red -&gt; #<span class=\"type\">FF0001</span>)</div><div class=\"line\">```  </div><div class=\"line\">## **输出map和key**</div><div class=\"line\">```scala</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]): <span class=\"type\">Unit</span> = &#123;</div><div class=\"line\">    <span class=\"keyword\">val</span> sites = <span class=\"type\">Map</span>(<span class=\"string\">\"runoob\"</span> -&gt; <span class=\"string\">\"http://www.runoob.com\"</span>,</div><div class=\"line\">      <span class=\"string\">\"baidu\"</span> -&gt; <span class=\"string\">\"http://www.baidu.com\"</span>,</div><div class=\"line\">      <span class=\"string\">\"taobao\"</span> -&gt; <span class=\"string\">\"http://www.taobao.com\"</span>)</div><div class=\"line\">    sites.keys.foreach&#123;</div><div class=\"line\">      i=&gt;print(<span class=\"string\">\"key: \"</span>+i)</div><div class=\"line\">        println(<span class=\"string\">\"value: \"</span>+sites(i))</div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">----------输出-----</div><div class=\"line\">key: runoobvalue: http:<span class=\"comment\">//www.runoob.com</span></div><div class=\"line\">key: baiduvalue: http:<span class=\"comment\">//www.baidu.com</span></div><div class=\"line\">key: taobaovalue: http:<span class=\"comment\">//www.taobao.com</span></div><div class=\"line\">```  </div><div class=\"line\">##  **<span class=\"type\">Map</span>.contains查看是否存在指定的key**</div><div class=\"line\"></div><div class=\"line\">## **元组**</div><div class=\"line\">元组可以把固定数量的条目组合在一起以便于整体的传送，不像数组或者列表，元祖可以保存不同类型的对象    </div><div class=\"line\">元组的值是通过将单个的值包含在圆括号中构成的。例如</div><div class=\"line\">```scala</div><div class=\"line\"><span class=\"keyword\">val</span> t = (<span class=\"number\">1</span>, <span class=\"number\">3.14</span>, <span class=\"string\">\"Fred\"</span>)</div></pre></td></tr></table></figure></p>\n<p>以上实例在元组中定义了三个元素，对应的类型分别为[Int, Double, java.lang.String]。<br>此外我们也可以使用以下方式来定义：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">val</span> t = <span class=\"keyword\">new</span> <span class=\"type\">Tuple3</span>(<span class=\"number\">1</span>, <span class=\"number\">3.14</span>, <span class=\"string\">\"Fred\"</span>)</div><div class=\"line\">```  </div><div class=\"line\">### **定义与取值**</div><div class=\"line\">元组的实际类型取决于它的元素的类型，比如 (<span class=\"number\">99</span>, <span class=\"string\">\"runoob\"</span>) 是 <span class=\"type\">Tuple2</span>[<span class=\"type\">Int</span>, <span class=\"type\">String</span>]。 ('u', 'r', <span class=\"string\">\"the\"</span>, <span class=\"number\">1</span>, <span class=\"number\">4</span>, <span class=\"string\">\"me\"</span>) 为 <span class=\"type\">Tuple6</span>[<span class=\"type\">Char</span>, <span class=\"type\">Char</span>, <span class=\"type\">String</span>, <span class=\"type\">Int</span>, <span class=\"type\">Int</span>, <span class=\"type\">String</span>]。  </div><div class=\"line\">目前 <span class=\"type\">Scala</span> 支持的元组最大长度为 <span class=\"number\">22</span>。对于更大长度你可以使用集合，或者扩展元组。</div><div class=\"line\">访问元组的元素可以通过数字索引，如下一个元组：  </div><div class=\"line\">我们可以使用 t.\\_1 访问第一个元素， t.\\_2 访问第二个元素，如下所示：  </div><div class=\"line\">```scala</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">    <span class=\"keyword\">val</span> t = (<span class=\"number\">4</span>,<span class=\"number\">3</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>)</div><div class=\"line\"></div><div class=\"line\">    <span class=\"keyword\">val</span> sum = t._1 + t._2 + t._3 + t._4</div><div class=\"line\">    <span class=\"keyword\">var</span> secondTuple=t._2</div><div class=\"line\">    println(<span class=\"string\">\"第二个元素为: \"</span>+secondTuple)</div><div class=\"line\">    println( <span class=\"string\">\"元素之和为: \"</span>  + sum )</div><div class=\"line\">  &#125;</div><div class=\"line\">----输出----------</div><div class=\"line\">第二个元素为: <span class=\"number\">3</span></div><div class=\"line\">元素之和为: <span class=\"number\">10</span></div></pre></td></tr></table></figure></p>\n<h3 id=\"迭代元组\"><a href=\"#迭代元组\" class=\"headerlink\" title=\"迭代元组\"></a><strong>迭代元组</strong></h3><p>你可以使用 Tuple.productIterator() 方法来迭代输出元组的所有元素：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div></pre></td><td class=\"code\"><pre><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">    <span class=\"keyword\">val</span> t = (<span class=\"number\">4</span>,<span class=\"number\">3</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>)</div><div class=\"line\">    t.productIterator.foreach(i=&gt;println(<span class=\"string\">\"value: \"</span>+i))</div><div class=\"line\">  &#125;</div><div class=\"line\">--------输出--------------</div><div class=\"line\">value: <span class=\"number\">4</span></div><div class=\"line\">value: <span class=\"number\">3</span></div><div class=\"line\">value: <span class=\"number\">2</span></div><div class=\"line\">value: <span class=\"number\">1</span></div><div class=\"line\">```  </div><div class=\"line\">### **元组转为字符串**</div><div class=\"line\"><span class=\"type\">Tuple</span>.toString()  </div><div class=\"line\">### **元素交换**</div><div class=\"line\"><span class=\"type\">Tuple</span>.swap 方法来交换元组的元素  </div><div class=\"line\">```scala</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">      <span class=\"keyword\">val</span> t = <span class=\"keyword\">new</span> <span class=\"type\">Tuple2</span>(<span class=\"string\">\"www.google.com\"</span>, <span class=\"string\">\"http://blog.csdn.net/t1dmzks\"</span>)</div><div class=\"line\">      println(<span class=\"string\">\"交换后的元组: \"</span> + t )</div><div class=\"line\">    &#125;</div><div class=\"line\">-------输出-----------</div><div class=\"line\">交换后的元组: (www.google.com,www.runoob.com)</div><div class=\"line\">```  </div><div class=\"line\"># **<span class=\"type\">Scala</span> <span class=\"type\">Option</span>**</div><div class=\"line\"><span class=\"type\">TODO</span></div><div class=\"line\"></div><div class=\"line\"># **<span class=\"type\">Scala</span> <span class=\"type\">Iterator</span>**</div><div class=\"line\"><span class=\"type\">Scala</span> <span class=\"type\">Iterator</span>（迭代器）不是一个集合，它是一种用于访问集合的方法。</div><div class=\"line\">迭代器 it 的两个基本操作是 next 和 hasNext。</div><div class=\"line\">调用 it.next() 会返回迭代器的下一个元素，并且更新迭代器的状态。</div><div class=\"line\">调用 it.hasNext() 用于检测集合中是否还有元素。</div><div class=\"line\">让迭代器 it 逐个返回所有元素最简单的方法是使用 <span class=\"keyword\">while</span> 循环：</div><div class=\"line\">```scala</div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Test</span> </span>&#123;</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args: <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">      <span class=\"keyword\">val</span> it = <span class=\"type\">Iterator</span>(<span class=\"string\">\"Baidu\"</span>, <span class=\"string\">\"Google\"</span>, <span class=\"string\">\"Runoob\"</span>, <span class=\"string\">\"Taobao\"</span>)</div><div class=\"line\">      </div><div class=\"line\">      <span class=\"keyword\">while</span> (it.hasNext)&#123;</div><div class=\"line\">         println(it.next())</div><div class=\"line\">      &#125;</div><div class=\"line\">   &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h2 id=\"查找最大与最小元素\"><a href=\"#查找最大与最小元素\" class=\"headerlink\" title=\"查找最大与最小元素\"></a><strong>查找最大与最小元素</strong></h2><p> it.min 和 it.max 方法</p>\n<h2 id=\"获取迭代器的长度\"><a href=\"#获取迭代器的长度\" class=\"headerlink\" title=\"获取迭代器的长度\"></a><strong>获取迭代器的长度</strong></h2><p>it.size 或 it.length </p>\n"},{"title":"spark RDD算子（七）之键值对分组操作 groupByKey，cogroup","date":"2017-03-07T13:25:21.000Z","author":"kaishun","id":"41","_content":"\n# **groupByKey**\n```\ndef groupByKey(): RDD[(K, Iterable[V])]\n\ndef groupByKey(numPartitions: Int): RDD[(K, Iterable[V])]\n\ndef groupByKey(partitioner: Partitioner): RDD[(K, Iterable[V])]\n```\ngroupByKey会将RDD[key,value] 按照相同的key进行分组，形成RDD[key,Iterable[value]]的形式， 有点类似于sql中的groupby，例如类似于mysql中的group_concat  \n例如这个例子， 我们对学生的成绩进行分组\n**scala版本**\n```scala\n    val scoreDetail = sc.parallelize(List((\"xiaoming\",75),(\"xiaoming\",90),(\"lihua\",95),(\"lihua\",100),(\"xiaofeng\",85)))\n    scoreDetail.groupByKey().collect().foreach(println(_));\n    /*输出\n(lihua,CompactBuffer(95, 100))\n(xiaoming,CompactBuffer(75, 90))\n(xiaofeng,CompactBuffer(85))\n    */\n```\n**java版本**\n```java\n        JavaRDD<Tuple2<String,Float>> scoreDetails = sc.parallelize(Arrays.asList(new Tuple2(\"xiaoming\", 75)\n                , new Tuple2(\"xiaoming\", 90)\n                , new Tuple2(\"lihua\", 95)\n                , new Tuple2(\"lihua\", 188)));\n        //将JavaRDD<Tuple2<String,Float>> 类型转换为 JavaPairRDD<String, Float>\n        JavaPairRDD<String, Float> scoreMapRDD = JavaPairRDD.fromJavaRDD(scoreDetails);\n        Map<String, Iterable<Float>> resultMap = scoreMapRDD.groupByKey().collectAsMap();\n        for (String key:resultMap.keySet()) {\n            System.out.println(\"(\"+key+\", \"+resultMap.get(key)+\")\");\n        }\n```\n\n# **cogroup**\ngroupByKey是对单个 RDD 的数据进行分组，还可以使用一个叫作 cogroup() 的函数对多个共享同一个键的 RDD 进行分组\n例如  \nRDD1.cogroup(RDD2) 会将RDD1和RDD2按照相同的key进行分组，得到(key,RDD[key,Iterable[value1],Iterable[value2]])的形式\ncogroup也可以多个进行分组  \n例如RDD1.cogroup(RDD2,RDD3,...RDDN), 可以得到(key,Iterable[value1],Iterable[value2],Iterable[value3],...,Iterable[valueN])  \n案例,scoreDetail存放的是学生的优秀学科的分数，scoreDetai2存放的是刚刚及格的分数，scoreDetai3存放的是没有及格的科目的分数，我们要对每一个学生的优秀学科，刚及格和不及格的分数给分组统计出来     \n**scala版本**\n```scala\nscala> val scoreDetail = sc.parallelize(List((\"xiaoming\",95),(\"xiaoming\",90),(\"lihua\",95),(\"lihua\",98),(\"xiaofeng\",97)))\nscala> val scoreDetai2 = sc.parallelize(List((\"xiaoming\",65),(\"lihua\",63),(\"lihua\",62),(\"xiaofeng\",67)))\nscala> val scoreDetai3 = sc.parallelize(List((\"xiaoming\",25),(\"xiaoming\",15),(\"lihua\",35),(\"lihua\",28),(\"xiaofeng\",36)))\nscala> scoreDetail.cogroup(scoreDetai2,scoreDetai3)\n\n//输出\nres1: Array[(String, (Iterable[Int], Iterable[Int], Iterable[Int]))] = Array((xiaoming,(CompactBuffer(95, 90),CompactBuffer(65),CompactBuffer(25, 15))), (lihua,(CompactBuffer(95, 98),CompactBuffer(63, 62),CompactBuffer(35, 28))), (xiaofeng,(CompactBuffer(97),CompactBuffer(67),CompactBuffer(36))))\n\n```  \n**java版本**  \n```java\n        JavaRDD<Tuple2<String,Float>> scoreDetails1 = sc.parallelize(Arrays.asList(new Tuple2(\"xiaoming\", 75)\n                , new Tuple2(\"xiaoming\", 90)\n                , new Tuple2(\"lihua\", 95)\n                , new Tuple2(\"lihua\", 96)));\n        JavaRDD<Tuple2<String,Float>> scoreDetails2 = sc.parallelize(Arrays.asList(new Tuple2(\"xiaoming\", 75)\n                , new Tuple2(\"lihua\", 60)\n                , new Tuple2(\"lihua\", 62)));\n        JavaRDD<Tuple2<String,Float>> scoreDetails3 = sc.parallelize(Arrays.asList(new Tuple2(\"xiaoming\", 75)\n                , new Tuple2(\"xiaoming\", 45)\n                , new Tuple2(\"lihua\", 24)\n                , new Tuple2(\"lihua\", 57)));\n        \n        JavaPairRDD<String, Float> scoreMapRDD1 = JavaPairRDD.fromJavaRDD(scoreDetails1);\n        JavaPairRDD<String, Float> scoreMapRDD2 = JavaPairRDD.fromJavaRDD(scoreDetails2);\n        JavaPairRDD<String, Float> scoreMapRDD3 = JavaPairRDD.fromJavaRDD(scoreDetails2);\n        \n        JavaPairRDD<String, Tuple3<Iterable<Float>, Iterable<Float>, Iterable<Float>>> cogroupRDD = (JavaPairRDD<String, Tuple3<Iterable<Float>, Iterable<Float>, Iterable<Float>>>) scoreMapRDD1.cogroup(scoreMapRDD2, scoreMapRDD3);\n        Map<String, Tuple3<Iterable<Float>, Iterable<Float>, Iterable<Float>>> tuple3 = cogroupRDD.collectAsMap();\n        for (String key:tuple3.keySet()) {\n            System.out.println(\"(\"+key+\", \"+tuple3.get(key)+\")\");\n        }\n        \n-----输出----------\n(lihua, ([95, 96],[60, 62],[60, 62]))\n(xiaoming, ([75, 90],[75],[75]))\n\n```","source":"_posts/spark RDD算子（七）之键值对分组操作 groupByKey，cogroup.md","raw":"---\ntitle: spark RDD算子（七）之键值对分组操作 groupByKey，cogroup\ndate: 2017-03-07 21:25:21\ntags: [spark]\ncategories: [大数据,spark]\nauthor: kaishun\nid: 41\npermalink: spark-rdd-7\n---\n\n# **groupByKey**\n```\ndef groupByKey(): RDD[(K, Iterable[V])]\n\ndef groupByKey(numPartitions: Int): RDD[(K, Iterable[V])]\n\ndef groupByKey(partitioner: Partitioner): RDD[(K, Iterable[V])]\n```\ngroupByKey会将RDD[key,value] 按照相同的key进行分组，形成RDD[key,Iterable[value]]的形式， 有点类似于sql中的groupby，例如类似于mysql中的group_concat  \n例如这个例子， 我们对学生的成绩进行分组\n**scala版本**\n```scala\n    val scoreDetail = sc.parallelize(List((\"xiaoming\",75),(\"xiaoming\",90),(\"lihua\",95),(\"lihua\",100),(\"xiaofeng\",85)))\n    scoreDetail.groupByKey().collect().foreach(println(_));\n    /*输出\n(lihua,CompactBuffer(95, 100))\n(xiaoming,CompactBuffer(75, 90))\n(xiaofeng,CompactBuffer(85))\n    */\n```\n**java版本**\n```java\n        JavaRDD<Tuple2<String,Float>> scoreDetails = sc.parallelize(Arrays.asList(new Tuple2(\"xiaoming\", 75)\n                , new Tuple2(\"xiaoming\", 90)\n                , new Tuple2(\"lihua\", 95)\n                , new Tuple2(\"lihua\", 188)));\n        //将JavaRDD<Tuple2<String,Float>> 类型转换为 JavaPairRDD<String, Float>\n        JavaPairRDD<String, Float> scoreMapRDD = JavaPairRDD.fromJavaRDD(scoreDetails);\n        Map<String, Iterable<Float>> resultMap = scoreMapRDD.groupByKey().collectAsMap();\n        for (String key:resultMap.keySet()) {\n            System.out.println(\"(\"+key+\", \"+resultMap.get(key)+\")\");\n        }\n```\n\n# **cogroup**\ngroupByKey是对单个 RDD 的数据进行分组，还可以使用一个叫作 cogroup() 的函数对多个共享同一个键的 RDD 进行分组\n例如  \nRDD1.cogroup(RDD2) 会将RDD1和RDD2按照相同的key进行分组，得到(key,RDD[key,Iterable[value1],Iterable[value2]])的形式\ncogroup也可以多个进行分组  \n例如RDD1.cogroup(RDD2,RDD3,...RDDN), 可以得到(key,Iterable[value1],Iterable[value2],Iterable[value3],...,Iterable[valueN])  \n案例,scoreDetail存放的是学生的优秀学科的分数，scoreDetai2存放的是刚刚及格的分数，scoreDetai3存放的是没有及格的科目的分数，我们要对每一个学生的优秀学科，刚及格和不及格的分数给分组统计出来     \n**scala版本**\n```scala\nscala> val scoreDetail = sc.parallelize(List((\"xiaoming\",95),(\"xiaoming\",90),(\"lihua\",95),(\"lihua\",98),(\"xiaofeng\",97)))\nscala> val scoreDetai2 = sc.parallelize(List((\"xiaoming\",65),(\"lihua\",63),(\"lihua\",62),(\"xiaofeng\",67)))\nscala> val scoreDetai3 = sc.parallelize(List((\"xiaoming\",25),(\"xiaoming\",15),(\"lihua\",35),(\"lihua\",28),(\"xiaofeng\",36)))\nscala> scoreDetail.cogroup(scoreDetai2,scoreDetai3)\n\n//输出\nres1: Array[(String, (Iterable[Int], Iterable[Int], Iterable[Int]))] = Array((xiaoming,(CompactBuffer(95, 90),CompactBuffer(65),CompactBuffer(25, 15))), (lihua,(CompactBuffer(95, 98),CompactBuffer(63, 62),CompactBuffer(35, 28))), (xiaofeng,(CompactBuffer(97),CompactBuffer(67),CompactBuffer(36))))\n\n```  \n**java版本**  \n```java\n        JavaRDD<Tuple2<String,Float>> scoreDetails1 = sc.parallelize(Arrays.asList(new Tuple2(\"xiaoming\", 75)\n                , new Tuple2(\"xiaoming\", 90)\n                , new Tuple2(\"lihua\", 95)\n                , new Tuple2(\"lihua\", 96)));\n        JavaRDD<Tuple2<String,Float>> scoreDetails2 = sc.parallelize(Arrays.asList(new Tuple2(\"xiaoming\", 75)\n                , new Tuple2(\"lihua\", 60)\n                , new Tuple2(\"lihua\", 62)));\n        JavaRDD<Tuple2<String,Float>> scoreDetails3 = sc.parallelize(Arrays.asList(new Tuple2(\"xiaoming\", 75)\n                , new Tuple2(\"xiaoming\", 45)\n                , new Tuple2(\"lihua\", 24)\n                , new Tuple2(\"lihua\", 57)));\n        \n        JavaPairRDD<String, Float> scoreMapRDD1 = JavaPairRDD.fromJavaRDD(scoreDetails1);\n        JavaPairRDD<String, Float> scoreMapRDD2 = JavaPairRDD.fromJavaRDD(scoreDetails2);\n        JavaPairRDD<String, Float> scoreMapRDD3 = JavaPairRDD.fromJavaRDD(scoreDetails2);\n        \n        JavaPairRDD<String, Tuple3<Iterable<Float>, Iterable<Float>, Iterable<Float>>> cogroupRDD = (JavaPairRDD<String, Tuple3<Iterable<Float>, Iterable<Float>, Iterable<Float>>>) scoreMapRDD1.cogroup(scoreMapRDD2, scoreMapRDD3);\n        Map<String, Tuple3<Iterable<Float>, Iterable<Float>, Iterable<Float>>> tuple3 = cogroupRDD.collectAsMap();\n        for (String key:tuple3.keySet()) {\n            System.out.println(\"(\"+key+\", \"+tuple3.get(key)+\")\");\n        }\n        \n-----输出----------\n(lihua, ([95, 96],[60, 62],[60, 62]))\n(xiaoming, ([75, 90],[75],[75]))\n\n```","slug":"spark-rdd-7","published":1,"updated":"2018-01-22T15:26:27.931Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcrpnzqp002j2wv3o0j8upj7","content":"<h1 id=\"groupByKey\"><a href=\"#groupByKey\" class=\"headerlink\" title=\"groupByKey\"></a><strong>groupByKey</strong></h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">def groupByKey(): RDD[(K, Iterable[V])]</div><div class=\"line\"></div><div class=\"line\">def groupByKey(numPartitions: Int): RDD[(K, Iterable[V])]</div><div class=\"line\"></div><div class=\"line\">def groupByKey(partitioner: Partitioner): RDD[(K, Iterable[V])]</div></pre></td></tr></table></figure>\n<p>groupByKey会将RDD[key,value] 按照相同的key进行分组，形成RDD[key,Iterable[value]]的形式， 有点类似于sql中的groupby，例如类似于mysql中的group_concat<br>例如这个例子， 我们对学生的成绩进行分组<br><strong>scala版本</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">    <span class=\"keyword\">val</span> scoreDetail = sc.parallelize(<span class=\"type\">List</span>((<span class=\"string\">\"xiaoming\"</span>,<span class=\"number\">75</span>),(<span class=\"string\">\"xiaoming\"</span>,<span class=\"number\">90</span>),(<span class=\"string\">\"lihua\"</span>,<span class=\"number\">95</span>),(<span class=\"string\">\"lihua\"</span>,<span class=\"number\">100</span>),(<span class=\"string\">\"xiaofeng\"</span>,<span class=\"number\">85</span>)))</div><div class=\"line\">    scoreDetail.groupByKey().collect().foreach(println(_));</div><div class=\"line\">    <span class=\"comment\">/*输出</span></div><div class=\"line\"><span class=\"comment\">(lihua,CompactBuffer(95, 100))</span></div><div class=\"line\"><span class=\"comment\">(xiaoming,CompactBuffer(75, 90))</span></div><div class=\"line\"><span class=\"comment\">(xiaofeng,CompactBuffer(85))</span></div><div class=\"line\"><span class=\"comment\">    */</span></div></pre></td></tr></table></figure></p>\n<p><strong>java版本</strong><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">JavaRDD&lt;Tuple2&lt;String,Float&gt;&gt; scoreDetails = sc.parallelize(Arrays.asList(<span class=\"keyword\">new</span> Tuple2(<span class=\"string\">\"xiaoming\"</span>, <span class=\"number\">75</span>)</div><div class=\"line\">        , <span class=\"keyword\">new</span> Tuple2(<span class=\"string\">\"xiaoming\"</span>, <span class=\"number\">90</span>)</div><div class=\"line\">        , <span class=\"keyword\">new</span> Tuple2(<span class=\"string\">\"lihua\"</span>, <span class=\"number\">95</span>)</div><div class=\"line\">        , <span class=\"keyword\">new</span> Tuple2(<span class=\"string\">\"lihua\"</span>, <span class=\"number\">188</span>)));</div><div class=\"line\"><span class=\"comment\">//将JavaRDD&lt;Tuple2&lt;String,Float&gt;&gt; 类型转换为 JavaPairRDD&lt;String, Float&gt;</span></div><div class=\"line\">JavaPairRDD&lt;String, Float&gt; scoreMapRDD = JavaPairRDD.fromJavaRDD(scoreDetails);</div><div class=\"line\">Map&lt;String, Iterable&lt;Float&gt;&gt; resultMap = scoreMapRDD.groupByKey().collectAsMap();</div><div class=\"line\"><span class=\"keyword\">for</span> (String key:resultMap.keySet()) &#123;</div><div class=\"line\">    System.out.println(<span class=\"string\">\"(\"</span>+key+<span class=\"string\">\", \"</span>+resultMap.get(key)+<span class=\"string\">\")\"</span>);</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h1 id=\"cogroup\"><a href=\"#cogroup\" class=\"headerlink\" title=\"cogroup\"></a><strong>cogroup</strong></h1><p>groupByKey是对单个 RDD 的数据进行分组，还可以使用一个叫作 cogroup() 的函数对多个共享同一个键的 RDD 进行分组<br>例如<br>RDD1.cogroup(RDD2) 会将RDD1和RDD2按照相同的key进行分组，得到(key,RDD[key,Iterable[value1],Iterable[value2]])的形式<br>cogroup也可以多个进行分组<br>例如RDD1.cogroup(RDD2,RDD3,…RDDN), 可以得到(key,Iterable[value1],Iterable[value2],Iterable[value3],…,Iterable[valueN])<br>案例,scoreDetail存放的是学生的优秀学科的分数，scoreDetai2存放的是刚刚及格的分数，scoreDetai3存放的是没有及格的科目的分数，我们要对每一个学生的优秀学科，刚及格和不及格的分数给分组统计出来<br><strong>scala版本</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> scoreDetail = sc.parallelize(<span class=\"type\">List</span>((<span class=\"string\">\"xiaoming\"</span>,<span class=\"number\">95</span>),(<span class=\"string\">\"xiaoming\"</span>,<span class=\"number\">90</span>),(<span class=\"string\">\"lihua\"</span>,<span class=\"number\">95</span>),(<span class=\"string\">\"lihua\"</span>,<span class=\"number\">98</span>),(<span class=\"string\">\"xiaofeng\"</span>,<span class=\"number\">97</span>)))</div><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> scoreDetai2 = sc.parallelize(<span class=\"type\">List</span>((<span class=\"string\">\"xiaoming\"</span>,<span class=\"number\">65</span>),(<span class=\"string\">\"lihua\"</span>,<span class=\"number\">63</span>),(<span class=\"string\">\"lihua\"</span>,<span class=\"number\">62</span>),(<span class=\"string\">\"xiaofeng\"</span>,<span class=\"number\">67</span>)))</div><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> scoreDetai3 = sc.parallelize(<span class=\"type\">List</span>((<span class=\"string\">\"xiaoming\"</span>,<span class=\"number\">25</span>),(<span class=\"string\">\"xiaoming\"</span>,<span class=\"number\">15</span>),(<span class=\"string\">\"lihua\"</span>,<span class=\"number\">35</span>),(<span class=\"string\">\"lihua\"</span>,<span class=\"number\">28</span>),(<span class=\"string\">\"xiaofeng\"</span>,<span class=\"number\">36</span>)))</div><div class=\"line\">scala&gt; scoreDetail.cogroup(scoreDetai2,scoreDetai3)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">//输出</span></div><div class=\"line\">res1: <span class=\"type\">Array</span>[(<span class=\"type\">String</span>, (<span class=\"type\">Iterable</span>[<span class=\"type\">Int</span>], <span class=\"type\">Iterable</span>[<span class=\"type\">Int</span>], <span class=\"type\">Iterable</span>[<span class=\"type\">Int</span>]))] = <span class=\"type\">Array</span>((xiaoming,(<span class=\"type\">CompactBuffer</span>(<span class=\"number\">95</span>, <span class=\"number\">90</span>),<span class=\"type\">CompactBuffer</span>(<span class=\"number\">65</span>),<span class=\"type\">CompactBuffer</span>(<span class=\"number\">25</span>, <span class=\"number\">15</span>))), (lihua,(<span class=\"type\">CompactBuffer</span>(<span class=\"number\">95</span>, <span class=\"number\">98</span>),<span class=\"type\">CompactBuffer</span>(<span class=\"number\">63</span>, <span class=\"number\">62</span>),<span class=\"type\">CompactBuffer</span>(<span class=\"number\">35</span>, <span class=\"number\">28</span>))), (xiaofeng,(<span class=\"type\">CompactBuffer</span>(<span class=\"number\">97</span>),<span class=\"type\">CompactBuffer</span>(<span class=\"number\">67</span>),<span class=\"type\">CompactBuffer</span>(<span class=\"number\">36</span>))))</div><div class=\"line\"></div><div class=\"line\">```  </div><div class=\"line\">**java版本**  </div><div class=\"line\">```java</div><div class=\"line\">        <span class=\"type\">JavaRDD</span>&lt;<span class=\"type\">Tuple2</span>&lt;<span class=\"type\">String</span>,<span class=\"type\">Float</span>&gt;&gt; scoreDetails1 = sc.parallelize(<span class=\"type\">Arrays</span>.asList(<span class=\"keyword\">new</span> <span class=\"type\">Tuple2</span>(<span class=\"string\">\"xiaoming\"</span>, <span class=\"number\">75</span>)</div><div class=\"line\">                , <span class=\"keyword\">new</span> <span class=\"type\">Tuple2</span>(<span class=\"string\">\"xiaoming\"</span>, <span class=\"number\">90</span>)</div><div class=\"line\">                , <span class=\"keyword\">new</span> <span class=\"type\">Tuple2</span>(<span class=\"string\">\"lihua\"</span>, <span class=\"number\">95</span>)</div><div class=\"line\">                , <span class=\"keyword\">new</span> <span class=\"type\">Tuple2</span>(<span class=\"string\">\"lihua\"</span>, <span class=\"number\">96</span>)));</div><div class=\"line\">        <span class=\"type\">JavaRDD</span>&lt;<span class=\"type\">Tuple2</span>&lt;<span class=\"type\">String</span>,<span class=\"type\">Float</span>&gt;&gt; scoreDetails2 = sc.parallelize(<span class=\"type\">Arrays</span>.asList(<span class=\"keyword\">new</span> <span class=\"type\">Tuple2</span>(<span class=\"string\">\"xiaoming\"</span>, <span class=\"number\">75</span>)</div><div class=\"line\">                , <span class=\"keyword\">new</span> <span class=\"type\">Tuple2</span>(<span class=\"string\">\"lihua\"</span>, <span class=\"number\">60</span>)</div><div class=\"line\">                , <span class=\"keyword\">new</span> <span class=\"type\">Tuple2</span>(<span class=\"string\">\"lihua\"</span>, <span class=\"number\">62</span>)));</div><div class=\"line\">        <span class=\"type\">JavaRDD</span>&lt;<span class=\"type\">Tuple2</span>&lt;<span class=\"type\">String</span>,<span class=\"type\">Float</span>&gt;&gt; scoreDetails3 = sc.parallelize(<span class=\"type\">Arrays</span>.asList(<span class=\"keyword\">new</span> <span class=\"type\">Tuple2</span>(<span class=\"string\">\"xiaoming\"</span>, <span class=\"number\">75</span>)</div><div class=\"line\">                , <span class=\"keyword\">new</span> <span class=\"type\">Tuple2</span>(<span class=\"string\">\"xiaoming\"</span>, <span class=\"number\">45</span>)</div><div class=\"line\">                , <span class=\"keyword\">new</span> <span class=\"type\">Tuple2</span>(<span class=\"string\">\"lihua\"</span>, <span class=\"number\">24</span>)</div><div class=\"line\">                , <span class=\"keyword\">new</span> <span class=\"type\">Tuple2</span>(<span class=\"string\">\"lihua\"</span>, <span class=\"number\">57</span>)));</div><div class=\"line\">        </div><div class=\"line\">        <span class=\"type\">JavaPairRDD</span>&lt;<span class=\"type\">String</span>, <span class=\"type\">Float</span>&gt; scoreMapRDD1 = <span class=\"type\">JavaPairRDD</span>.fromJavaRDD(scoreDetails1);</div><div class=\"line\">        <span class=\"type\">JavaPairRDD</span>&lt;<span class=\"type\">String</span>, <span class=\"type\">Float</span>&gt; scoreMapRDD2 = <span class=\"type\">JavaPairRDD</span>.fromJavaRDD(scoreDetails2);</div><div class=\"line\">        <span class=\"type\">JavaPairRDD</span>&lt;<span class=\"type\">String</span>, <span class=\"type\">Float</span>&gt; scoreMapRDD3 = <span class=\"type\">JavaPairRDD</span>.fromJavaRDD(scoreDetails2);</div><div class=\"line\">        </div><div class=\"line\">        <span class=\"type\">JavaPairRDD</span>&lt;<span class=\"type\">String</span>, <span class=\"type\">Tuple3</span>&lt;<span class=\"type\">Iterable</span>&lt;<span class=\"type\">Float</span>&gt;, <span class=\"type\">Iterable</span>&lt;<span class=\"type\">Float</span>&gt;, <span class=\"type\">Iterable</span>&lt;<span class=\"type\">Float</span>&gt;&gt;&gt; cogroupRDD = (<span class=\"type\">JavaPairRDD</span>&lt;<span class=\"type\">String</span>, <span class=\"type\">Tuple3</span>&lt;<span class=\"type\">Iterable</span>&lt;<span class=\"type\">Float</span>&gt;, <span class=\"type\">Iterable</span>&lt;<span class=\"type\">Float</span>&gt;, <span class=\"type\">Iterable</span>&lt;<span class=\"type\">Float</span>&gt;&gt;&gt;) scoreMapRDD1.cogroup(scoreMapRDD2, scoreMapRDD3);</div><div class=\"line\">        <span class=\"type\">Map</span>&lt;<span class=\"type\">String</span>, <span class=\"type\">Tuple3</span>&lt;<span class=\"type\">Iterable</span>&lt;<span class=\"type\">Float</span>&gt;, <span class=\"type\">Iterable</span>&lt;<span class=\"type\">Float</span>&gt;, <span class=\"type\">Iterable</span>&lt;<span class=\"type\">Float</span>&gt;&gt;&gt; tuple3 = cogroupRDD.collectAsMap();</div><div class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">String</span> key:tuple3.keySet()) &#123;</div><div class=\"line\">            <span class=\"type\">System</span>.out.println(<span class=\"string\">\"(\"</span>+key+<span class=\"string\">\", \"</span>+tuple3.get(key)+<span class=\"string\">\")\"</span>);</div><div class=\"line\">        &#125;</div><div class=\"line\">        </div><div class=\"line\">-----输出----------</div><div class=\"line\">(lihua, ([<span class=\"number\">95</span>, <span class=\"number\">96</span>],[<span class=\"number\">60</span>, <span class=\"number\">62</span>],[<span class=\"number\">60</span>, <span class=\"number\">62</span>]))</div><div class=\"line\">(xiaoming, ([<span class=\"number\">75</span>, <span class=\"number\">90</span>],[<span class=\"number\">75</span>],[<span class=\"number\">75</span>]))</div></pre></td></tr></table></figure></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"groupByKey\"><a href=\"#groupByKey\" class=\"headerlink\" title=\"groupByKey\"></a><strong>groupByKey</strong></h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">def groupByKey(): RDD[(K, Iterable[V])]</div><div class=\"line\"></div><div class=\"line\">def groupByKey(numPartitions: Int): RDD[(K, Iterable[V])]</div><div class=\"line\"></div><div class=\"line\">def groupByKey(partitioner: Partitioner): RDD[(K, Iterable[V])]</div></pre></td></tr></table></figure>\n<p>groupByKey会将RDD[key,value] 按照相同的key进行分组，形成RDD[key,Iterable[value]]的形式， 有点类似于sql中的groupby，例如类似于mysql中的group_concat<br>例如这个例子， 我们对学生的成绩进行分组<br><strong>scala版本</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">    <span class=\"keyword\">val</span> scoreDetail = sc.parallelize(<span class=\"type\">List</span>((<span class=\"string\">\"xiaoming\"</span>,<span class=\"number\">75</span>),(<span class=\"string\">\"xiaoming\"</span>,<span class=\"number\">90</span>),(<span class=\"string\">\"lihua\"</span>,<span class=\"number\">95</span>),(<span class=\"string\">\"lihua\"</span>,<span class=\"number\">100</span>),(<span class=\"string\">\"xiaofeng\"</span>,<span class=\"number\">85</span>)))</div><div class=\"line\">    scoreDetail.groupByKey().collect().foreach(println(_));</div><div class=\"line\">    <span class=\"comment\">/*输出</span></div><div class=\"line\"><span class=\"comment\">(lihua,CompactBuffer(95, 100))</span></div><div class=\"line\"><span class=\"comment\">(xiaoming,CompactBuffer(75, 90))</span></div><div class=\"line\"><span class=\"comment\">(xiaofeng,CompactBuffer(85))</span></div><div class=\"line\"><span class=\"comment\">    */</span></div></pre></td></tr></table></figure></p>\n<p><strong>java版本</strong><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">JavaRDD&lt;Tuple2&lt;String,Float&gt;&gt; scoreDetails = sc.parallelize(Arrays.asList(<span class=\"keyword\">new</span> Tuple2(<span class=\"string\">\"xiaoming\"</span>, <span class=\"number\">75</span>)</div><div class=\"line\">        , <span class=\"keyword\">new</span> Tuple2(<span class=\"string\">\"xiaoming\"</span>, <span class=\"number\">90</span>)</div><div class=\"line\">        , <span class=\"keyword\">new</span> Tuple2(<span class=\"string\">\"lihua\"</span>, <span class=\"number\">95</span>)</div><div class=\"line\">        , <span class=\"keyword\">new</span> Tuple2(<span class=\"string\">\"lihua\"</span>, <span class=\"number\">188</span>)));</div><div class=\"line\"><span class=\"comment\">//将JavaRDD&lt;Tuple2&lt;String,Float&gt;&gt; 类型转换为 JavaPairRDD&lt;String, Float&gt;</span></div><div class=\"line\">JavaPairRDD&lt;String, Float&gt; scoreMapRDD = JavaPairRDD.fromJavaRDD(scoreDetails);</div><div class=\"line\">Map&lt;String, Iterable&lt;Float&gt;&gt; resultMap = scoreMapRDD.groupByKey().collectAsMap();</div><div class=\"line\"><span class=\"keyword\">for</span> (String key:resultMap.keySet()) &#123;</div><div class=\"line\">    System.out.println(<span class=\"string\">\"(\"</span>+key+<span class=\"string\">\", \"</span>+resultMap.get(key)+<span class=\"string\">\")\"</span>);</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<h1 id=\"cogroup\"><a href=\"#cogroup\" class=\"headerlink\" title=\"cogroup\"></a><strong>cogroup</strong></h1><p>groupByKey是对单个 RDD 的数据进行分组，还可以使用一个叫作 cogroup() 的函数对多个共享同一个键的 RDD 进行分组<br>例如<br>RDD1.cogroup(RDD2) 会将RDD1和RDD2按照相同的key进行分组，得到(key,RDD[key,Iterable[value1],Iterable[value2]])的形式<br>cogroup也可以多个进行分组<br>例如RDD1.cogroup(RDD2,RDD3,…RDDN), 可以得到(key,Iterable[value1],Iterable[value2],Iterable[value3],…,Iterable[valueN])<br>案例,scoreDetail存放的是学生的优秀学科的分数，scoreDetai2存放的是刚刚及格的分数，scoreDetai3存放的是没有及格的科目的分数，我们要对每一个学生的优秀学科，刚及格和不及格的分数给分组统计出来<br><strong>scala版本</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> scoreDetail = sc.parallelize(<span class=\"type\">List</span>((<span class=\"string\">\"xiaoming\"</span>,<span class=\"number\">95</span>),(<span class=\"string\">\"xiaoming\"</span>,<span class=\"number\">90</span>),(<span class=\"string\">\"lihua\"</span>,<span class=\"number\">95</span>),(<span class=\"string\">\"lihua\"</span>,<span class=\"number\">98</span>),(<span class=\"string\">\"xiaofeng\"</span>,<span class=\"number\">97</span>)))</div><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> scoreDetai2 = sc.parallelize(<span class=\"type\">List</span>((<span class=\"string\">\"xiaoming\"</span>,<span class=\"number\">65</span>),(<span class=\"string\">\"lihua\"</span>,<span class=\"number\">63</span>),(<span class=\"string\">\"lihua\"</span>,<span class=\"number\">62</span>),(<span class=\"string\">\"xiaofeng\"</span>,<span class=\"number\">67</span>)))</div><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> scoreDetai3 = sc.parallelize(<span class=\"type\">List</span>((<span class=\"string\">\"xiaoming\"</span>,<span class=\"number\">25</span>),(<span class=\"string\">\"xiaoming\"</span>,<span class=\"number\">15</span>),(<span class=\"string\">\"lihua\"</span>,<span class=\"number\">35</span>),(<span class=\"string\">\"lihua\"</span>,<span class=\"number\">28</span>),(<span class=\"string\">\"xiaofeng\"</span>,<span class=\"number\">36</span>)))</div><div class=\"line\">scala&gt; scoreDetail.cogroup(scoreDetai2,scoreDetai3)</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">//输出</span></div><div class=\"line\">res1: <span class=\"type\">Array</span>[(<span class=\"type\">String</span>, (<span class=\"type\">Iterable</span>[<span class=\"type\">Int</span>], <span class=\"type\">Iterable</span>[<span class=\"type\">Int</span>], <span class=\"type\">Iterable</span>[<span class=\"type\">Int</span>]))] = <span class=\"type\">Array</span>((xiaoming,(<span class=\"type\">CompactBuffer</span>(<span class=\"number\">95</span>, <span class=\"number\">90</span>),<span class=\"type\">CompactBuffer</span>(<span class=\"number\">65</span>),<span class=\"type\">CompactBuffer</span>(<span class=\"number\">25</span>, <span class=\"number\">15</span>))), (lihua,(<span class=\"type\">CompactBuffer</span>(<span class=\"number\">95</span>, <span class=\"number\">98</span>),<span class=\"type\">CompactBuffer</span>(<span class=\"number\">63</span>, <span class=\"number\">62</span>),<span class=\"type\">CompactBuffer</span>(<span class=\"number\">35</span>, <span class=\"number\">28</span>))), (xiaofeng,(<span class=\"type\">CompactBuffer</span>(<span class=\"number\">97</span>),<span class=\"type\">CompactBuffer</span>(<span class=\"number\">67</span>),<span class=\"type\">CompactBuffer</span>(<span class=\"number\">36</span>))))</div><div class=\"line\"></div><div class=\"line\">```  </div><div class=\"line\">**java版本**  </div><div class=\"line\">```java</div><div class=\"line\">        <span class=\"type\">JavaRDD</span>&lt;<span class=\"type\">Tuple2</span>&lt;<span class=\"type\">String</span>,<span class=\"type\">Float</span>&gt;&gt; scoreDetails1 = sc.parallelize(<span class=\"type\">Arrays</span>.asList(<span class=\"keyword\">new</span> <span class=\"type\">Tuple2</span>(<span class=\"string\">\"xiaoming\"</span>, <span class=\"number\">75</span>)</div><div class=\"line\">                , <span class=\"keyword\">new</span> <span class=\"type\">Tuple2</span>(<span class=\"string\">\"xiaoming\"</span>, <span class=\"number\">90</span>)</div><div class=\"line\">                , <span class=\"keyword\">new</span> <span class=\"type\">Tuple2</span>(<span class=\"string\">\"lihua\"</span>, <span class=\"number\">95</span>)</div><div class=\"line\">                , <span class=\"keyword\">new</span> <span class=\"type\">Tuple2</span>(<span class=\"string\">\"lihua\"</span>, <span class=\"number\">96</span>)));</div><div class=\"line\">        <span class=\"type\">JavaRDD</span>&lt;<span class=\"type\">Tuple2</span>&lt;<span class=\"type\">String</span>,<span class=\"type\">Float</span>&gt;&gt; scoreDetails2 = sc.parallelize(<span class=\"type\">Arrays</span>.asList(<span class=\"keyword\">new</span> <span class=\"type\">Tuple2</span>(<span class=\"string\">\"xiaoming\"</span>, <span class=\"number\">75</span>)</div><div class=\"line\">                , <span class=\"keyword\">new</span> <span class=\"type\">Tuple2</span>(<span class=\"string\">\"lihua\"</span>, <span class=\"number\">60</span>)</div><div class=\"line\">                , <span class=\"keyword\">new</span> <span class=\"type\">Tuple2</span>(<span class=\"string\">\"lihua\"</span>, <span class=\"number\">62</span>)));</div><div class=\"line\">        <span class=\"type\">JavaRDD</span>&lt;<span class=\"type\">Tuple2</span>&lt;<span class=\"type\">String</span>,<span class=\"type\">Float</span>&gt;&gt; scoreDetails3 = sc.parallelize(<span class=\"type\">Arrays</span>.asList(<span class=\"keyword\">new</span> <span class=\"type\">Tuple2</span>(<span class=\"string\">\"xiaoming\"</span>, <span class=\"number\">75</span>)</div><div class=\"line\">                , <span class=\"keyword\">new</span> <span class=\"type\">Tuple2</span>(<span class=\"string\">\"xiaoming\"</span>, <span class=\"number\">45</span>)</div><div class=\"line\">                , <span class=\"keyword\">new</span> <span class=\"type\">Tuple2</span>(<span class=\"string\">\"lihua\"</span>, <span class=\"number\">24</span>)</div><div class=\"line\">                , <span class=\"keyword\">new</span> <span class=\"type\">Tuple2</span>(<span class=\"string\">\"lihua\"</span>, <span class=\"number\">57</span>)));</div><div class=\"line\">        </div><div class=\"line\">        <span class=\"type\">JavaPairRDD</span>&lt;<span class=\"type\">String</span>, <span class=\"type\">Float</span>&gt; scoreMapRDD1 = <span class=\"type\">JavaPairRDD</span>.fromJavaRDD(scoreDetails1);</div><div class=\"line\">        <span class=\"type\">JavaPairRDD</span>&lt;<span class=\"type\">String</span>, <span class=\"type\">Float</span>&gt; scoreMapRDD2 = <span class=\"type\">JavaPairRDD</span>.fromJavaRDD(scoreDetails2);</div><div class=\"line\">        <span class=\"type\">JavaPairRDD</span>&lt;<span class=\"type\">String</span>, <span class=\"type\">Float</span>&gt; scoreMapRDD3 = <span class=\"type\">JavaPairRDD</span>.fromJavaRDD(scoreDetails2);</div><div class=\"line\">        </div><div class=\"line\">        <span class=\"type\">JavaPairRDD</span>&lt;<span class=\"type\">String</span>, <span class=\"type\">Tuple3</span>&lt;<span class=\"type\">Iterable</span>&lt;<span class=\"type\">Float</span>&gt;, <span class=\"type\">Iterable</span>&lt;<span class=\"type\">Float</span>&gt;, <span class=\"type\">Iterable</span>&lt;<span class=\"type\">Float</span>&gt;&gt;&gt; cogroupRDD = (<span class=\"type\">JavaPairRDD</span>&lt;<span class=\"type\">String</span>, <span class=\"type\">Tuple3</span>&lt;<span class=\"type\">Iterable</span>&lt;<span class=\"type\">Float</span>&gt;, <span class=\"type\">Iterable</span>&lt;<span class=\"type\">Float</span>&gt;, <span class=\"type\">Iterable</span>&lt;<span class=\"type\">Float</span>&gt;&gt;&gt;) scoreMapRDD1.cogroup(scoreMapRDD2, scoreMapRDD3);</div><div class=\"line\">        <span class=\"type\">Map</span>&lt;<span class=\"type\">String</span>, <span class=\"type\">Tuple3</span>&lt;<span class=\"type\">Iterable</span>&lt;<span class=\"type\">Float</span>&gt;, <span class=\"type\">Iterable</span>&lt;<span class=\"type\">Float</span>&gt;, <span class=\"type\">Iterable</span>&lt;<span class=\"type\">Float</span>&gt;&gt;&gt; tuple3 = cogroupRDD.collectAsMap();</div><div class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">String</span> key:tuple3.keySet()) &#123;</div><div class=\"line\">            <span class=\"type\">System</span>.out.println(<span class=\"string\">\"(\"</span>+key+<span class=\"string\">\", \"</span>+tuple3.get(key)+<span class=\"string\">\")\"</span>);</div><div class=\"line\">        &#125;</div><div class=\"line\">        </div><div class=\"line\">-----输出----------</div><div class=\"line\">(lihua, ([<span class=\"number\">95</span>, <span class=\"number\">96</span>],[<span class=\"number\">60</span>, <span class=\"number\">62</span>],[<span class=\"number\">60</span>, <span class=\"number\">62</span>]))</div><div class=\"line\">(xiaoming, ([<span class=\"number\">75</span>, <span class=\"number\">90</span>],[<span class=\"number\">75</span>],[<span class=\"number\">75</span>]))</div></pre></td></tr></table></figure></p>\n"},{"title":"spark RDD算子（九）之基本的Action操作 first, take, collect, count, countByValue, reduce, aggregate, fold,top","date":"2017-03-28T13:25:21.000Z","author":"kaishun","id":"43","_content":"\n# **first**  \n返回第一个元素  \n**scala**\n```scala\nscala> val rdd = sc.parallelize(List(1,2,3,3))\n\nscala> rdd.first()\nres1: Int = 1\n```\n**java**\n```java\n    JavaRDD<Integer> rdd = sc.parallelize(Arrays.asList(1, 2, 3, 3));\n    Integer first = rdd.first();\n```\n\n# **take**\nrdd.take(n)返回第n个元素  \n**scala**\n```scala\nscala> val rdd = sc.parallelize(List(1,2,3,3))\n\nscala> rdd.take(2)\nres3: Array[Int] = Array(1, 2)\n```\n**java**\n```java\n    JavaRDD<Integer> rdd = sc.parallelize(Arrays.asList(1, 2, 3, 3));\n    List<Integer> take = rdd.take(2);\n```\n\n# **collect**  \nrdd.collect() 返回 RDD 中的所有元素  \n**scala**\n```scala\nscala> val rdd = sc.parallelize(List(1,2,3,3))\n\nscala> rdd.collect()\nres4: Array[Int] = Array(1, 2, 3, 3)\n```\n**java**\n```java\n    JavaRDD<Integer> rdd = sc.parallelize(Arrays.asList(1, 2, 3, 3));\n    List<Integer> collect = rdd.collect();\n```\n\n\n# **count**\nrdd.count() 返回 RDD 中的元素个数  \n**scala**\n```scala\nscala> val rdd = sc.parallelize(List(1,2,3,3))\n\nscala> rdd.count()\nres5: Long = 4\n```\n**java**\n```java\n    JavaRDD<Integer> rdd = sc.parallelize(Arrays.asList(1, 2, 3, 3));\n    long count = rdd.count();\n```\n# **countByValue**\n各元素在 RDD 中出现的次数 返回{(key1,次数),(key2,次数),...(keyn,次数)}  \n**scala**\n```scala\nscala> val rdd = sc.parallelize(List(1,2,3,3))\n\nscala> rdd.countByValue()\nres6: scala.collection.Map[Int,Long] = Map(1 -> 1, 2 -> 1, 3 -> 2)\n```\n**java**\n```java\n    JavaRDD<Integer> rdd = sc.parallelize(Arrays.asList(1, 2, 3, 3));\n    Map<Integer, Long> integerLongMap = rdd.countByValue();\n```\n\n# **reduce**\nrdd.reduce(func)\n并行整合RDD中所有数据， 类似于是scala中集合的reduce  \n**scala**\n```scala\nscala> val rdd = sc.parallelize(List(1,2,3,3))\n\nscala> rdd.reduce((x,y)=>x+y)\nres7: Int = 9\n```\n**java**\n```java\n    Integer reduce = rdd.reduce(new Function2<Integer, Integer, Integer>() {\n        @Override\n        public Integer call(Integer integer, Integer integer2) throws Exception {\n            return integer + integer2;\n        }\n    });\n\n```\n\n# **aggregate**\n和 reduce() 相 似， 但 是 通 常\n返回不同类型的函数  一般不用这个函数\n \n**scala**\n```scala\nscala> val rdd = sc.parallelize(List(1,2,3,3))\nTODO\n```\n**java**\n```java\n\n```\n\n# **fold**\nrdd.fold(num)(func) 一般不用这个函数\n和 reduce() 一 样， 但是提供了初始值num,每个元素计算时，先要合这个初始值进行折叠, 注意，这里会按照每个分区进行fold，然后分区之间还会再次进行fold\n提供初始值  \n**scala**\n```scala\n// 解释 TODO \nscala> val rdd = sc.parallelize(List(1,2,3,3)，2)\n\nscala> rdd.fold(1)((x,y)=>x+y)\nres8: Int = 12\n```\n**java**\n```java\n    JavaRDD<Integer> rdd = sc.parallelize(Arrays.asList(1, 2, 3, 3),2);\n    Integer fold = rdd.fold(1, new Function2<Integer, Integer, Integer>() {\n        @Override\n        public Integer call(Integer integer, Integer integer2) throws Exception {\n            return integer + integer2;\n        }\n    });\n    System.out.println(fold);\n-------输出-----\n12\n```\n\n\n# **top**\nrdd.top(n)  \n按照降序的或者指定的排序规则，返回前n个元素  \n**scala**\n```scala\nscala> val rdd = sc.parallelize(List(1,2,3,3))\n\nscala> rdd.top(2)\nres9: Array[Int] = Array(3, 3)\n```\n**java**\n```java\n    JavaRDD<Integer> rdd = sc.parallelize(Arrays.asList(1, 2, 3, 3),2);\n    List<Integer> top = rdd.top(2);\n```\n\n# **takeOrdered**\nrdd.take(n)  \n对RDD元素进行升序排序,取出前n个元素并返回，也可以自定义比较器（这里不介绍），类似于top的相反的方法  \n**scala**\n```scala\nscala> val rdd = sc.parallelize(List(1,2,3,3))\n\nscala> rdd.takeOrdered(2)\nres10: Array[Int] = Array(1, 2)\n```\n**java**\n```java\n    JavaRDD<Integer> rdd = sc.parallelize(Arrays.asList(1, 2, 3, 3),2);\n    List<Integer> integers = rdd.takeOrdered(2);\n```\n\n# **foreach**  \n对 RDD 中的每个元素使用给\n定的函数  \n**scala**\n```scala\n    val rdd = sc.parallelize(List(1,2,3,3))\n    rdd.foreach(print(_))\n-----输出-----------\n1233\n\n```\n**java**\n```java\n    rdd.foreach(new VoidFunction<Integer>() {\n       @Override\n       public void call(Integer integer) throws Exception {\n           System.out.println(integer);\n       }\n    });\n```\n\n\n","source":"_posts/spark RDD算子（九）之基本的Action操作 first, take, collect, count, countByValue, reduce, aggregate, fold,top.md","raw":"---\ntitle: spark RDD算子（九）之基本的Action操作 first, take, collect, count, countByValue, reduce, aggregate, fold,top\ndate: 2017-03-28 21:25:21\ntags: [spark]\ncategories: [大数据,spark]\nauthor: kaishun\nid: 43\npermalink: spark-rdd-9\n---\n\n# **first**  \n返回第一个元素  \n**scala**\n```scala\nscala> val rdd = sc.parallelize(List(1,2,3,3))\n\nscala> rdd.first()\nres1: Int = 1\n```\n**java**\n```java\n    JavaRDD<Integer> rdd = sc.parallelize(Arrays.asList(1, 2, 3, 3));\n    Integer first = rdd.first();\n```\n\n# **take**\nrdd.take(n)返回第n个元素  \n**scala**\n```scala\nscala> val rdd = sc.parallelize(List(1,2,3,3))\n\nscala> rdd.take(2)\nres3: Array[Int] = Array(1, 2)\n```\n**java**\n```java\n    JavaRDD<Integer> rdd = sc.parallelize(Arrays.asList(1, 2, 3, 3));\n    List<Integer> take = rdd.take(2);\n```\n\n# **collect**  \nrdd.collect() 返回 RDD 中的所有元素  \n**scala**\n```scala\nscala> val rdd = sc.parallelize(List(1,2,3,3))\n\nscala> rdd.collect()\nres4: Array[Int] = Array(1, 2, 3, 3)\n```\n**java**\n```java\n    JavaRDD<Integer> rdd = sc.parallelize(Arrays.asList(1, 2, 3, 3));\n    List<Integer> collect = rdd.collect();\n```\n\n\n# **count**\nrdd.count() 返回 RDD 中的元素个数  \n**scala**\n```scala\nscala> val rdd = sc.parallelize(List(1,2,3,3))\n\nscala> rdd.count()\nres5: Long = 4\n```\n**java**\n```java\n    JavaRDD<Integer> rdd = sc.parallelize(Arrays.asList(1, 2, 3, 3));\n    long count = rdd.count();\n```\n# **countByValue**\n各元素在 RDD 中出现的次数 返回{(key1,次数),(key2,次数),...(keyn,次数)}  \n**scala**\n```scala\nscala> val rdd = sc.parallelize(List(1,2,3,3))\n\nscala> rdd.countByValue()\nres6: scala.collection.Map[Int,Long] = Map(1 -> 1, 2 -> 1, 3 -> 2)\n```\n**java**\n```java\n    JavaRDD<Integer> rdd = sc.parallelize(Arrays.asList(1, 2, 3, 3));\n    Map<Integer, Long> integerLongMap = rdd.countByValue();\n```\n\n# **reduce**\nrdd.reduce(func)\n并行整合RDD中所有数据， 类似于是scala中集合的reduce  \n**scala**\n```scala\nscala> val rdd = sc.parallelize(List(1,2,3,3))\n\nscala> rdd.reduce((x,y)=>x+y)\nres7: Int = 9\n```\n**java**\n```java\n    Integer reduce = rdd.reduce(new Function2<Integer, Integer, Integer>() {\n        @Override\n        public Integer call(Integer integer, Integer integer2) throws Exception {\n            return integer + integer2;\n        }\n    });\n\n```\n\n# **aggregate**\n和 reduce() 相 似， 但 是 通 常\n返回不同类型的函数  一般不用这个函数\n \n**scala**\n```scala\nscala> val rdd = sc.parallelize(List(1,2,3,3))\nTODO\n```\n**java**\n```java\n\n```\n\n# **fold**\nrdd.fold(num)(func) 一般不用这个函数\n和 reduce() 一 样， 但是提供了初始值num,每个元素计算时，先要合这个初始值进行折叠, 注意，这里会按照每个分区进行fold，然后分区之间还会再次进行fold\n提供初始值  \n**scala**\n```scala\n// 解释 TODO \nscala> val rdd = sc.parallelize(List(1,2,3,3)，2)\n\nscala> rdd.fold(1)((x,y)=>x+y)\nres8: Int = 12\n```\n**java**\n```java\n    JavaRDD<Integer> rdd = sc.parallelize(Arrays.asList(1, 2, 3, 3),2);\n    Integer fold = rdd.fold(1, new Function2<Integer, Integer, Integer>() {\n        @Override\n        public Integer call(Integer integer, Integer integer2) throws Exception {\n            return integer + integer2;\n        }\n    });\n    System.out.println(fold);\n-------输出-----\n12\n```\n\n\n# **top**\nrdd.top(n)  \n按照降序的或者指定的排序规则，返回前n个元素  \n**scala**\n```scala\nscala> val rdd = sc.parallelize(List(1,2,3,3))\n\nscala> rdd.top(2)\nres9: Array[Int] = Array(3, 3)\n```\n**java**\n```java\n    JavaRDD<Integer> rdd = sc.parallelize(Arrays.asList(1, 2, 3, 3),2);\n    List<Integer> top = rdd.top(2);\n```\n\n# **takeOrdered**\nrdd.take(n)  \n对RDD元素进行升序排序,取出前n个元素并返回，也可以自定义比较器（这里不介绍），类似于top的相反的方法  \n**scala**\n```scala\nscala> val rdd = sc.parallelize(List(1,2,3,3))\n\nscala> rdd.takeOrdered(2)\nres10: Array[Int] = Array(1, 2)\n```\n**java**\n```java\n    JavaRDD<Integer> rdd = sc.parallelize(Arrays.asList(1, 2, 3, 3),2);\n    List<Integer> integers = rdd.takeOrdered(2);\n```\n\n# **foreach**  \n对 RDD 中的每个元素使用给\n定的函数  \n**scala**\n```scala\n    val rdd = sc.parallelize(List(1,2,3,3))\n    rdd.foreach(print(_))\n-----输出-----------\n1233\n\n```\n**java**\n```java\n    rdd.foreach(new VoidFunction<Integer>() {\n       @Override\n       public void call(Integer integer) throws Exception {\n           System.out.println(integer);\n       }\n    });\n```\n\n\n","slug":"spark-rdd-9","published":1,"updated":"2018-01-22T15:27:40.590Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcrpnzqp002n2wv35gyujqmp","content":"<h1 id=\"first\"><a href=\"#first\" class=\"headerlink\" title=\"first\"></a><strong>first</strong></h1><p>返回第一个元素<br><strong>scala</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> rdd = sc.parallelize(<span class=\"type\">List</span>(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">3</span>))</div><div class=\"line\"></div><div class=\"line\">scala&gt; rdd.first()</div><div class=\"line\">res1: <span class=\"type\">Int</span> = <span class=\"number\">1</span></div></pre></td></tr></table></figure></p>\n<p><strong>java</strong><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">JavaRDD&lt;Integer&gt; rdd = sc.parallelize(Arrays.asList(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">3</span>));</div><div class=\"line\">Integer first = rdd.first();</div></pre></td></tr></table></figure></p>\n<h1 id=\"take\"><a href=\"#take\" class=\"headerlink\" title=\"take\"></a><strong>take</strong></h1><p>rdd.take(n)返回第n个元素<br><strong>scala</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> rdd = sc.parallelize(<span class=\"type\">List</span>(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">3</span>))</div><div class=\"line\"></div><div class=\"line\">scala&gt; rdd.take(<span class=\"number\">2</span>)</div><div class=\"line\">res3: <span class=\"type\">Array</span>[<span class=\"type\">Int</span>] = <span class=\"type\">Array</span>(<span class=\"number\">1</span>, <span class=\"number\">2</span>)</div></pre></td></tr></table></figure></p>\n<p><strong>java</strong><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">JavaRDD&lt;Integer&gt; rdd = sc.parallelize(Arrays.asList(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">3</span>));</div><div class=\"line\">List&lt;Integer&gt; take = rdd.take(<span class=\"number\">2</span>);</div></pre></td></tr></table></figure></p>\n<h1 id=\"collect\"><a href=\"#collect\" class=\"headerlink\" title=\"collect\"></a><strong>collect</strong></h1><p>rdd.collect() 返回 RDD 中的所有元素<br><strong>scala</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> rdd = sc.parallelize(<span class=\"type\">List</span>(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">3</span>))</div><div class=\"line\"></div><div class=\"line\">scala&gt; rdd.collect()</div><div class=\"line\">res4: <span class=\"type\">Array</span>[<span class=\"type\">Int</span>] = <span class=\"type\">Array</span>(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">3</span>)</div></pre></td></tr></table></figure></p>\n<p><strong>java</strong><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">JavaRDD&lt;Integer&gt; rdd = sc.parallelize(Arrays.asList(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">3</span>));</div><div class=\"line\">List&lt;Integer&gt; collect = rdd.collect();</div></pre></td></tr></table></figure></p>\n<h1 id=\"count\"><a href=\"#count\" class=\"headerlink\" title=\"count\"></a><strong>count</strong></h1><p>rdd.count() 返回 RDD 中的元素个数<br><strong>scala</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> rdd = sc.parallelize(<span class=\"type\">List</span>(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">3</span>))</div><div class=\"line\"></div><div class=\"line\">scala&gt; rdd.count()</div><div class=\"line\">res5: <span class=\"type\">Long</span> = <span class=\"number\">4</span></div></pre></td></tr></table></figure></p>\n<p><strong>java</strong><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">JavaRDD&lt;Integer&gt; rdd = sc.parallelize(Arrays.asList(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">3</span>));</div><div class=\"line\"><span class=\"keyword\">long</span> count = rdd.count();</div></pre></td></tr></table></figure></p>\n<h1 id=\"countByValue\"><a href=\"#countByValue\" class=\"headerlink\" title=\"countByValue\"></a><strong>countByValue</strong></h1><p>各元素在 RDD 中出现的次数 返回{(key1,次数),(key2,次数),…(keyn,次数)}<br><strong>scala</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> rdd = sc.parallelize(<span class=\"type\">List</span>(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">3</span>))</div><div class=\"line\"></div><div class=\"line\">scala&gt; rdd.countByValue()</div><div class=\"line\">res6: scala.collection.<span class=\"type\">Map</span>[<span class=\"type\">Int</span>,<span class=\"type\">Long</span>] = <span class=\"type\">Map</span>(<span class=\"number\">1</span> -&gt; <span class=\"number\">1</span>, <span class=\"number\">2</span> -&gt; <span class=\"number\">1</span>, <span class=\"number\">3</span> -&gt; <span class=\"number\">2</span>)</div></pre></td></tr></table></figure></p>\n<p><strong>java</strong><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">JavaRDD&lt;Integer&gt; rdd = sc.parallelize(Arrays.asList(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">3</span>));</div><div class=\"line\">Map&lt;Integer, Long&gt; integerLongMap = rdd.countByValue();</div></pre></td></tr></table></figure></p>\n<h1 id=\"reduce\"><a href=\"#reduce\" class=\"headerlink\" title=\"reduce\"></a><strong>reduce</strong></h1><p>rdd.reduce(func)<br>并行整合RDD中所有数据， 类似于是scala中集合的reduce<br><strong>scala</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> rdd = sc.parallelize(<span class=\"type\">List</span>(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">3</span>))</div><div class=\"line\"></div><div class=\"line\">scala&gt; rdd.reduce((x,y)=&gt;x+y)</div><div class=\"line\">res7: <span class=\"type\">Int</span> = <span class=\"number\">9</span></div></pre></td></tr></table></figure></p>\n<p><strong>java</strong><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">Integer reduce = rdd.reduce(<span class=\"keyword\">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</div><div class=\"line\">    <span class=\"meta\">@Override</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> Integer <span class=\"title\">call</span><span class=\"params\">(Integer integer, Integer integer2)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">return</span> integer + integer2;</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;);</div></pre></td></tr></table></figure></p>\n<h1 id=\"aggregate\"><a href=\"#aggregate\" class=\"headerlink\" title=\"aggregate\"></a><strong>aggregate</strong></h1><p>和 reduce() 相 似， 但 是 通 常<br>返回不同类型的函数  一般不用这个函数</p>\n<p><strong>scala</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> rdd = sc.parallelize(<span class=\"type\">List</span>(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">3</span>))</div><div class=\"line\"><span class=\"type\">TODO</span></div></pre></td></tr></table></figure></p>\n<p><strong>java</strong><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\"></div></pre></td></tr></table></figure></p>\n<h1 id=\"fold\"><a href=\"#fold\" class=\"headerlink\" title=\"fold\"></a><strong>fold</strong></h1><p>rdd.fold(num)(func) 一般不用这个函数<br>和 reduce() 一 样， 但是提供了初始值num,每个元素计算时，先要合这个初始值进行折叠, 注意，这里会按照每个分区进行fold，然后分区之间还会再次进行fold<br>提供初始值<br><strong>scala</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// 解释 TODO </span></div><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> rdd = sc.parallelize(<span class=\"type\">List</span>(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">3</span>)，<span class=\"number\">2</span>)</div><div class=\"line\"></div><div class=\"line\">scala&gt; rdd.fold(<span class=\"number\">1</span>)((x,y)=&gt;x+y)</div><div class=\"line\">res8: <span class=\"type\">Int</span> = <span class=\"number\">12</span></div></pre></td></tr></table></figure></p>\n<p><strong>java</strong><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">    JavaRDD&lt;Integer&gt; rdd = sc.parallelize(Arrays.asList(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">3</span>),<span class=\"number\">2</span>);</div><div class=\"line\">    Integer fold = rdd.fold(<span class=\"number\">1</span>, <span class=\"keyword\">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</div><div class=\"line\">        <span class=\"meta\">@Override</span></div><div class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> Integer <span class=\"title\">call</span><span class=\"params\">(Integer integer, Integer integer2)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">            <span class=\"keyword\">return</span> integer + integer2;</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;);</div><div class=\"line\">    System.out.println(fold);</div><div class=\"line\">-------输出-----</div><div class=\"line\"><span class=\"number\">12</span></div></pre></td></tr></table></figure></p>\n<h1 id=\"top\"><a href=\"#top\" class=\"headerlink\" title=\"top\"></a><strong>top</strong></h1><p>rdd.top(n)<br>按照降序的或者指定的排序规则，返回前n个元素<br><strong>scala</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> rdd = sc.parallelize(<span class=\"type\">List</span>(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">3</span>))</div><div class=\"line\"></div><div class=\"line\">scala&gt; rdd.top(<span class=\"number\">2</span>)</div><div class=\"line\">res9: <span class=\"type\">Array</span>[<span class=\"type\">Int</span>] = <span class=\"type\">Array</span>(<span class=\"number\">3</span>, <span class=\"number\">3</span>)</div></pre></td></tr></table></figure></p>\n<p><strong>java</strong><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">JavaRDD&lt;Integer&gt; rdd = sc.parallelize(Arrays.asList(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">3</span>),<span class=\"number\">2</span>);</div><div class=\"line\">List&lt;Integer&gt; top = rdd.top(<span class=\"number\">2</span>);</div></pre></td></tr></table></figure></p>\n<h1 id=\"takeOrdered\"><a href=\"#takeOrdered\" class=\"headerlink\" title=\"takeOrdered\"></a><strong>takeOrdered</strong></h1><p>rdd.take(n)<br>对RDD元素进行升序排序,取出前n个元素并返回，也可以自定义比较器（这里不介绍），类似于top的相反的方法<br><strong>scala</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> rdd = sc.parallelize(<span class=\"type\">List</span>(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">3</span>))</div><div class=\"line\"></div><div class=\"line\">scala&gt; rdd.takeOrdered(<span class=\"number\">2</span>)</div><div class=\"line\">res10: <span class=\"type\">Array</span>[<span class=\"type\">Int</span>] = <span class=\"type\">Array</span>(<span class=\"number\">1</span>, <span class=\"number\">2</span>)</div></pre></td></tr></table></figure></p>\n<p><strong>java</strong><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">JavaRDD&lt;Integer&gt; rdd = sc.parallelize(Arrays.asList(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">3</span>),<span class=\"number\">2</span>);</div><div class=\"line\">List&lt;Integer&gt; integers = rdd.takeOrdered(<span class=\"number\">2</span>);</div></pre></td></tr></table></figure></p>\n<h1 id=\"foreach\"><a href=\"#foreach\" class=\"headerlink\" title=\"foreach\"></a><strong>foreach</strong></h1><p>对 RDD 中的每个元素使用给<br>定的函数<br><strong>scala</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">    <span class=\"keyword\">val</span> rdd = sc.parallelize(<span class=\"type\">List</span>(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">3</span>))</div><div class=\"line\">    rdd.foreach(print(_))</div><div class=\"line\">-----输出-----------</div><div class=\"line\"><span class=\"number\">1233</span></div></pre></td></tr></table></figure></p>\n<p><strong>java</strong><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">rdd.foreach(<span class=\"keyword\">new</span> VoidFunction&lt;Integer&gt;() &#123;</div><div class=\"line\">   <span class=\"meta\">@Override</span></div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">call</span><span class=\"params\">(Integer integer)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">       System.out.println(integer);</div><div class=\"line\">   &#125;</div><div class=\"line\">&#125;);</div></pre></td></tr></table></figure></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"first\"><a href=\"#first\" class=\"headerlink\" title=\"first\"></a><strong>first</strong></h1><p>返回第一个元素<br><strong>scala</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> rdd = sc.parallelize(<span class=\"type\">List</span>(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">3</span>))</div><div class=\"line\"></div><div class=\"line\">scala&gt; rdd.first()</div><div class=\"line\">res1: <span class=\"type\">Int</span> = <span class=\"number\">1</span></div></pre></td></tr></table></figure></p>\n<p><strong>java</strong><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">JavaRDD&lt;Integer&gt; rdd = sc.parallelize(Arrays.asList(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">3</span>));</div><div class=\"line\">Integer first = rdd.first();</div></pre></td></tr></table></figure></p>\n<h1 id=\"take\"><a href=\"#take\" class=\"headerlink\" title=\"take\"></a><strong>take</strong></h1><p>rdd.take(n)返回第n个元素<br><strong>scala</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> rdd = sc.parallelize(<span class=\"type\">List</span>(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">3</span>))</div><div class=\"line\"></div><div class=\"line\">scala&gt; rdd.take(<span class=\"number\">2</span>)</div><div class=\"line\">res3: <span class=\"type\">Array</span>[<span class=\"type\">Int</span>] = <span class=\"type\">Array</span>(<span class=\"number\">1</span>, <span class=\"number\">2</span>)</div></pre></td></tr></table></figure></p>\n<p><strong>java</strong><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">JavaRDD&lt;Integer&gt; rdd = sc.parallelize(Arrays.asList(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">3</span>));</div><div class=\"line\">List&lt;Integer&gt; take = rdd.take(<span class=\"number\">2</span>);</div></pre></td></tr></table></figure></p>\n<h1 id=\"collect\"><a href=\"#collect\" class=\"headerlink\" title=\"collect\"></a><strong>collect</strong></h1><p>rdd.collect() 返回 RDD 中的所有元素<br><strong>scala</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> rdd = sc.parallelize(<span class=\"type\">List</span>(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">3</span>))</div><div class=\"line\"></div><div class=\"line\">scala&gt; rdd.collect()</div><div class=\"line\">res4: <span class=\"type\">Array</span>[<span class=\"type\">Int</span>] = <span class=\"type\">Array</span>(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">3</span>)</div></pre></td></tr></table></figure></p>\n<p><strong>java</strong><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">JavaRDD&lt;Integer&gt; rdd = sc.parallelize(Arrays.asList(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">3</span>));</div><div class=\"line\">List&lt;Integer&gt; collect = rdd.collect();</div></pre></td></tr></table></figure></p>\n<h1 id=\"count\"><a href=\"#count\" class=\"headerlink\" title=\"count\"></a><strong>count</strong></h1><p>rdd.count() 返回 RDD 中的元素个数<br><strong>scala</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> rdd = sc.parallelize(<span class=\"type\">List</span>(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">3</span>))</div><div class=\"line\"></div><div class=\"line\">scala&gt; rdd.count()</div><div class=\"line\">res5: <span class=\"type\">Long</span> = <span class=\"number\">4</span></div></pre></td></tr></table></figure></p>\n<p><strong>java</strong><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">JavaRDD&lt;Integer&gt; rdd = sc.parallelize(Arrays.asList(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">3</span>));</div><div class=\"line\"><span class=\"keyword\">long</span> count = rdd.count();</div></pre></td></tr></table></figure></p>\n<h1 id=\"countByValue\"><a href=\"#countByValue\" class=\"headerlink\" title=\"countByValue\"></a><strong>countByValue</strong></h1><p>各元素在 RDD 中出现的次数 返回{(key1,次数),(key2,次数),…(keyn,次数)}<br><strong>scala</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> rdd = sc.parallelize(<span class=\"type\">List</span>(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">3</span>))</div><div class=\"line\"></div><div class=\"line\">scala&gt; rdd.countByValue()</div><div class=\"line\">res6: scala.collection.<span class=\"type\">Map</span>[<span class=\"type\">Int</span>,<span class=\"type\">Long</span>] = <span class=\"type\">Map</span>(<span class=\"number\">1</span> -&gt; <span class=\"number\">1</span>, <span class=\"number\">2</span> -&gt; <span class=\"number\">1</span>, <span class=\"number\">3</span> -&gt; <span class=\"number\">2</span>)</div></pre></td></tr></table></figure></p>\n<p><strong>java</strong><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">JavaRDD&lt;Integer&gt; rdd = sc.parallelize(Arrays.asList(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">3</span>));</div><div class=\"line\">Map&lt;Integer, Long&gt; integerLongMap = rdd.countByValue();</div></pre></td></tr></table></figure></p>\n<h1 id=\"reduce\"><a href=\"#reduce\" class=\"headerlink\" title=\"reduce\"></a><strong>reduce</strong></h1><p>rdd.reduce(func)<br>并行整合RDD中所有数据， 类似于是scala中集合的reduce<br><strong>scala</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> rdd = sc.parallelize(<span class=\"type\">List</span>(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">3</span>))</div><div class=\"line\"></div><div class=\"line\">scala&gt; rdd.reduce((x,y)=&gt;x+y)</div><div class=\"line\">res7: <span class=\"type\">Int</span> = <span class=\"number\">9</span></div></pre></td></tr></table></figure></p>\n<p><strong>java</strong><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">Integer reduce = rdd.reduce(<span class=\"keyword\">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</div><div class=\"line\">    <span class=\"meta\">@Override</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> Integer <span class=\"title\">call</span><span class=\"params\">(Integer integer, Integer integer2)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">return</span> integer + integer2;</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;);</div></pre></td></tr></table></figure></p>\n<h1 id=\"aggregate\"><a href=\"#aggregate\" class=\"headerlink\" title=\"aggregate\"></a><strong>aggregate</strong></h1><p>和 reduce() 相 似， 但 是 通 常<br>返回不同类型的函数  一般不用这个函数</p>\n<p><strong>scala</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> rdd = sc.parallelize(<span class=\"type\">List</span>(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">3</span>))</div><div class=\"line\"><span class=\"type\">TODO</span></div></pre></td></tr></table></figure></p>\n<p><strong>java</strong><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\"></div></pre></td></tr></table></figure></p>\n<h1 id=\"fold\"><a href=\"#fold\" class=\"headerlink\" title=\"fold\"></a><strong>fold</strong></h1><p>rdd.fold(num)(func) 一般不用这个函数<br>和 reduce() 一 样， 但是提供了初始值num,每个元素计算时，先要合这个初始值进行折叠, 注意，这里会按照每个分区进行fold，然后分区之间还会再次进行fold<br>提供初始值<br><strong>scala</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"comment\">// 解释 TODO </span></div><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> rdd = sc.parallelize(<span class=\"type\">List</span>(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">3</span>)，<span class=\"number\">2</span>)</div><div class=\"line\"></div><div class=\"line\">scala&gt; rdd.fold(<span class=\"number\">1</span>)((x,y)=&gt;x+y)</div><div class=\"line\">res8: <span class=\"type\">Int</span> = <span class=\"number\">12</span></div></pre></td></tr></table></figure></p>\n<p><strong>java</strong><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">    JavaRDD&lt;Integer&gt; rdd = sc.parallelize(Arrays.asList(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">3</span>),<span class=\"number\">2</span>);</div><div class=\"line\">    Integer fold = rdd.fold(<span class=\"number\">1</span>, <span class=\"keyword\">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</div><div class=\"line\">        <span class=\"meta\">@Override</span></div><div class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> Integer <span class=\"title\">call</span><span class=\"params\">(Integer integer, Integer integer2)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">            <span class=\"keyword\">return</span> integer + integer2;</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;);</div><div class=\"line\">    System.out.println(fold);</div><div class=\"line\">-------输出-----</div><div class=\"line\"><span class=\"number\">12</span></div></pre></td></tr></table></figure></p>\n<h1 id=\"top\"><a href=\"#top\" class=\"headerlink\" title=\"top\"></a><strong>top</strong></h1><p>rdd.top(n)<br>按照降序的或者指定的排序规则，返回前n个元素<br><strong>scala</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> rdd = sc.parallelize(<span class=\"type\">List</span>(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">3</span>))</div><div class=\"line\"></div><div class=\"line\">scala&gt; rdd.top(<span class=\"number\">2</span>)</div><div class=\"line\">res9: <span class=\"type\">Array</span>[<span class=\"type\">Int</span>] = <span class=\"type\">Array</span>(<span class=\"number\">3</span>, <span class=\"number\">3</span>)</div></pre></td></tr></table></figure></p>\n<p><strong>java</strong><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">JavaRDD&lt;Integer&gt; rdd = sc.parallelize(Arrays.asList(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">3</span>),<span class=\"number\">2</span>);</div><div class=\"line\">List&lt;Integer&gt; top = rdd.top(<span class=\"number\">2</span>);</div></pre></td></tr></table></figure></p>\n<h1 id=\"takeOrdered\"><a href=\"#takeOrdered\" class=\"headerlink\" title=\"takeOrdered\"></a><strong>takeOrdered</strong></h1><p>rdd.take(n)<br>对RDD元素进行升序排序,取出前n个元素并返回，也可以自定义比较器（这里不介绍），类似于top的相反的方法<br><strong>scala</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> rdd = sc.parallelize(<span class=\"type\">List</span>(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">3</span>))</div><div class=\"line\"></div><div class=\"line\">scala&gt; rdd.takeOrdered(<span class=\"number\">2</span>)</div><div class=\"line\">res10: <span class=\"type\">Array</span>[<span class=\"type\">Int</span>] = <span class=\"type\">Array</span>(<span class=\"number\">1</span>, <span class=\"number\">2</span>)</div></pre></td></tr></table></figure></p>\n<p><strong>java</strong><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">JavaRDD&lt;Integer&gt; rdd = sc.parallelize(Arrays.asList(<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">3</span>),<span class=\"number\">2</span>);</div><div class=\"line\">List&lt;Integer&gt; integers = rdd.takeOrdered(<span class=\"number\">2</span>);</div></pre></td></tr></table></figure></p>\n<h1 id=\"foreach\"><a href=\"#foreach\" class=\"headerlink\" title=\"foreach\"></a><strong>foreach</strong></h1><p>对 RDD 中的每个元素使用给<br>定的函数<br><strong>scala</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">    <span class=\"keyword\">val</span> rdd = sc.parallelize(<span class=\"type\">List</span>(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">3</span>))</div><div class=\"line\">    rdd.foreach(print(_))</div><div class=\"line\">-----输出-----------</div><div class=\"line\"><span class=\"number\">1233</span></div></pre></td></tr></table></figure></p>\n<p><strong>java</strong><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">rdd.foreach(<span class=\"keyword\">new</span> VoidFunction&lt;Integer&gt;() &#123;</div><div class=\"line\">   <span class=\"meta\">@Override</span></div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">call</span><span class=\"params\">(Integer integer)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">       System.out.println(integer);</div><div class=\"line\">   &#125;</div><div class=\"line\">&#125;);</div></pre></td></tr></table></figure></p>\n"},{"title":"scala高阶函数学习","date":"2017-05-01T13:25:21.000Z","author":"kaishun","id":"53","_content":"\n参考文章   \nhttp://www.cnblogs.com/wzm-xu/p/4063814.html  \nhttp://www.cnblogs.com/wzm-xu/p/4064389.html  \n\n# scala高阶函数上\n高阶函数是函数式编程里面一个非常重要的特色，所谓的高阶函数，就是以其它函数作为参数的函数。\n\n下面以一个小例子演示Scala的高阶函数特性，非常有意思，也非常强大。\n\n首先看这么一个程序：\n\n**code1**：\n\n```scala\nobject higherorderfuntion{\n   def sum1(a:Int,b:Int):Int=\n     if(a>b) 0\n     else a+sum1(a+1,b)\n\n   def sum2(a:Int,b:Int):Int=\n     if(a>b) 0\n     else cube(a)+sum2(a+1,b)\n\n   def sum3(a:Int,b:Int):Int=\n     if(a>b) 0\n     else fac(a)+sum3(a+1,b)\n\n   def cube(a:Int):Int=a*a*a\n\n   def fac(a:Int):Int=\n   if (a==0) 1\n   else a*fac(a-1)\n\n   def main(args:Array[String])={\n     println(sum1(1,3))\n     println(sum2(1,3))\n     println(sum3(1,3))\n    }\n}\n```\n上面这个例子“没有”用到高阶函数，sum1是计算a+(a+1)+(a+2)+...+(b),  sum2是计算a\\^3+(a+1)\\^3+(a+2)\\^3+...+b\\^3,\n\nsum3是计算a!+(a+1)!+(a+2)!+...+b!。分析sum1,sum2,sum3的代码，很容易发现这三个函数有着相似的“Pattern”，\n\n抛开函数名不论，这三个函数唯一的区别在于，在 else语句中对a的处理：a, cube(a) , fac(a).  那么来看，在函数式\n\n编程里面，是如何非常精彩的利用这个“Pattern”来使得代码更加精简的：\n\n**code2**：\n```scala\nobject higherorderfuntion{\n   def sum(f:Int=>Int,a:Int,b:Int):Int=\n     if(a>b) 0\n     else f(a)+sum(f,a+1,b)\n\n   def sumInt(a:Int,b:Int):Int = sum(id,a,b)\n   def sumCube(a:Int,b:Int):Int = sum(cube,a,b)\n   def sumFac(a:Int,b:Int):Int = sum(fac,a,b)\n\n   def id(a:Int) = a\n   def cube(a:Int):Int=a*a*a\n   def fac(a:Int):Int=\n     if (a==0) 1\n     else a*fac(a-1)\n\n   def main(args:Array[String])={\n     println(sumInt(1,3))\n     println(sumCube(1,3))\n     println(sumFac(1,3))\n   }\n\n}\n```  \n重头戏来了，我们来看sum函数的实现：\n```scala\ndef sum(f:Int=>Int,a:Int,b:Int):Int=\n     if(a>b) 0\n     else f(a)+sum(f,a+1,b)\n```  \nsum函数接收三个参数，第二个和第三个参数分别是： a:Int 和 b:Int, 这和第一个例子中是一样的。\n\n比较令人费解的是sum函数的第一个参数：f:Int=>Int  \n\n这个是什么意思呢？意思是sum函数接收一个名字叫做 “f” 的函数作为参数，而 Int=>Int 是对f的说明：\n\n=>左边的Int是说：函数f接收一个Int类型的参数，\n\n=>右边的Int是说：函数f的返回值是Int类型的。\n\n好了，那么既然sum函数接收函数f作为一个参数，那么sum就可以利用f了，事实也是这样的，看sum\n\n函数的else语句就知道了：**f(a)**\n\n然后再看sumInt、sumCube和sumFac三个函数的定义：\n```scala\ndef sumInt(a:Int,b:Int):Int = sum(id,a,b)\ndef sumCube(a:Int,b:Int):Int = sum(cube,a,b)\ndef sumFac(a:Int,b:Int):Int = sum(fac,a,b)\n```\n以sumCube函数为例吧，它在定义的时候，把cube函数作为参数，传递给sum函数，\n\n而我们看cube函数的定义：\n```scala\ndef cube(a:Int):Int=a*a*a\n```\n发现，cube函数接收一个Int作为参数，并且返回一个Int，也就是说cube函数是符合sum函数的第一个\n\n参数:   f:Int=>Int  的\n\n是不是很精彩呢？\n\n事实上上述代码还可以进一步简化，因为我们观察到id函数和cube函数的功能非常简单，不需要单独作为\n\n一个函数出现，进一步简化后的代码如下：    \n**code3**\n```scala\nobject higherorderfuntion{\n   def sum(f:Int=>Int,a:Int,b:Int):Int=\n     if(a>b) 0\n     else f(a)+sum(f,a+1,b)\n\n   def sumInt(a:Int,b:Int):Int = sum(x=>x,a,b)\n   def sumCube(a:Int,b:Int):Int = sum(x=>x*x*x,a,b)\n   def sumFac(a:Int,b:Int):Int = sum(fac,a,b)\n\n   def fac(a:Int):Int=\n     if (a==0) 1\n     else a*fac(a-1)\n\n   def main(args:Array[String])={\n     println(sumInt(1,3))\n     println(sumCube(1,3))\n     println(sumFac(1,3))\n   }\n}\n```\n\ncode3和code2相比，去掉了id函数和cube这两个函数，并且sumInt和sumCube函数的声明也发生了\n\n一点变化：\n```scala\ndef sumInt(a:Int,b:Int):Int = sum(x=>x,a,b)\ndef sumCube(a:Int,b:Int):Int = sum(x=>x*x*x,a,b)\n```\n像： x=>x   和     x=>x*x*x   这样的东西是什么呢？在函数式编程里面，这被叫做“function literal”,又称“匿名函数”，\n\n说白了，这也是函数的一种表达方式，只是这个函数没有名字罢了。\n\ncode3其实也还有点小毛病，那就是sum函数和fac函数都不是“尾递归”，所以呢，把它们改成尾递归如下：\n\n**code4：**\n```\nobject higherorderfuntion{\n   def sum(f:Int=>Int,a:Int,b:Int):Int={\n     def loop(a:Int,acc:Int):Int=\n       if(a>b) acc\n       else loop(a+1,f(a)+acc)\n     loop(a,0)\n   }\n   def sumInt(a:Int,b:Int):Int = sum(x=>x,a,b)\n   def sumCube(a:Int,b:Int):Int = sum(x=>x*x*x,a,b)\n   def sumFac(a:Int,b:Int):Int = sum(fac,a,b)\n\n   def fac(a:Int):Int=\n   {\n     def loop(a:Int,acc:Int):Int=\n       if(a==0) acc\n       else loop(a-1,a*acc)\n     loop(a,1)\n   }\n   def main(args:Array[String])={\n     println(sumInt(1,4))\n     println(sumCube(1,4))\n     println(sumFac(1,4))\n   }\n\n}\n```  \n\n# scala高阶函数下\n在scala高阶函数上我们演示了如何把一个函数作为参数传递给另外一个函数。\n\n在本文里面，我们来演示函数式编程另外一个重要的特性：返回一个函数。首先来看这么一段代码：\n\n**code piece 1：**\n```scala\ndef sum(f:Int=>Int):(Int,Int)=>Int={\n       def sumF(a:Int,b:Int):Int=\n         if(a>b) 0\n         else f(a)+sumF(a+1,b)\n       sumF\n   }\n```\n一点点来看，f:Int=>Int 是sum函数接收的参数，该参数是一个函数。 \n\n\":\" 号后面的 (Int,Int) => Int 是sum函数的返回值，又(Int,Int) => Int是一个函数的类型：接收两个Int型的数，\n\n返回一个Int的数。也就是说调用sum函数，其返回的值是一个函数。  这一点对于已经习惯C、C++、Java等编程语言的\n\n程序员来说有一点难以理解。\n\n继续看例子吧，如果执行下面的一行代码会发生什么呢？\n```scala\nsum(x=>x*x)(1,4)\n```\n结果会返回30。为什么是30呢？ 30= 1^2+ 2^2 + 3^2 + 4^2.\n\n首先，sum(x=>x*x) 是一个函数，并且sum(x=>x*x)的类型是(Int,Int)=>Int，正因为sum(x=>x*x)是一个函数，\n\n所以它才可以继续接收参数(1,4).\n\n好吧，我可能没有把这个事儿说清楚，实在是太抽象了。但是，读者应该有了一个基本的印象，那就是：\n\n在函数式编程里面，函数f可以接收函数g作为参数，也可以返回函数h。\n\n到这里还没完。。。。。。\n\nScala里面可以继续对code piece1 进行简化，如下：\n\n**code piece2**：\n```scala\nobject higherorderfuntion{\n   def sum(f:Int=>Int)(a:Int,b:Int):Int={\n      if(a>b) 0  else f(a)+sum(f)(a+1,b)\n   }\n\n   def fac(a:Int):Int=\n   {\n     def loop(a:Int,acc:Int):Int=\n       if(a==0) acc\n       else loop(a-1,a*acc)\n     loop(a,1)\n   }\n   def main(args:Array[String])={\n     println(sum(x=>x)(1,4))\n     println(sum(x=>x*x*x)(1,4))\n     println(sum(fac)(1,4))\n   }\n\n}\n```  \n这里我们需要特别注意，这里sum函数的参数列表变成了两个：(f:Int=>Int)和(a:Int,b:Int)\n\n在scala里面，函数可以有多个参数列表。比如下面的函数定义：  \n```\ndef f(args_1)(args_2)...(args_n)=E\n```\nE代表一个函数体，且n>1\n\n那么上述定义等价于：  \n```scala\ndef f(args_1)(args_2)...(args_n-1)={def g(args_n)=E;g}\n```\n还是以code piece2中的sum函数为例：\n```\ndef sum(f:Int=>Int)(a:Int,b:Int):Int={\n      if(a>b) 0  else f(a)+sum(f)(a+1,b)\n   }\n```\n等价于：\n```\ndef sum(f:Int=>Int):(Int,Int)=>Int={\n     def g(a:Int,b:Int):Int = \n      if(a>b) 0  else f(a)+sum(f)(a+1,b)\n     g\n   }\n```\n这个定义和code piece1中的sum函数的定义是一致的。\n---\n\n---\n\n---\n\n# 扩展： 什么是尾递归 \n参考https://www.zhihu.com/question/20761771/answer/19996299  \n这个虽然是python的，但是也能让我们理解什么是尾递归\n尾递归和一般的递归不同在对内存的占用，普通递归创建stack累积而后计算收缩，尾递归只会占用恒量的内存（和迭代一样）。SICP中描述了一个内存占用曲线，用以上答案中的Python代码为例（普通递归）：\n```python\ndef recsum(x):\n  if x == 1:\n    return x\n  else:\n    return x + recsum(x - 1)\n```\n当调用recsum(5)，Python调试器中发生如下状况：\n```python\nrecsum(5)\n5 + recsum(4)\n5 + (4 + recsum(3))\n5 + (4 + (3 + recsum(2)))\n5 + (4 + (3 + (2 + recsum(1))))\n5 + (4 + (3 + (2 + 1)))\n5 + (4 + (3 + 3))\n5 + (4 + 6)\n5 + 10\n15\n\n```\n这个曲线就代表内存占用大小的峰值，从左到右，达到顶峰，再从右到左收缩。而我们通常不希望这样的事情发生，所以使用迭代，只占据常量stack space(更新这个栈！而非扩展他)。\n---------------------\n**（一个替代方案：迭代）**  \n```python\nfor i in range(6):\n  sum += i\n```\n因为Python，Java，Pascal等等无法在语言中实现尾递归优化(Tail Call Optimization, TCO)，所以采用了for, while, goto等特殊结构代替recursive的表述。Scheme则不需要这样曲折地表达，一旦写成尾递归形式，就可以进行尾递归优化。---------------------Python中可以写（尾递归）：\n\n```python\ndef tailrecsum(x, running_total=0):\n  if x == 0:\n    return running_total\n  else:\n    return tailrecsum(x - 1, running_total + x)\n\n```\n理论上类似上面：  \n```\ntailrecsum(5, 0)\ntailrecsum(4, 5)\ntailrecsum(3, 9)\ntailrecsum(2, 12)\ntailrecsum(1, 14)\ntailrecsum(0, 15)\n15\n```\n\n观察到，tailrecsum(x, y)中形式变量y的实际变量值是不断更新的，对比普通递归就很清楚，后者每个recsum()调用中y值不变，仅在层级上加深。所以，尾递归是把变化的参数传递给递归函数的变量了。怎么写尾递归？形式上只要最后一个return语句是单纯函数就可以。如：\n```\nreturn tailrec(x+1);\n```  \n而\n```\nreturn tailrec(x+1) + x;\n```\n则不可以，因为无法更新tailrec()函数内的实际变量，只是新建一个栈。但Python不能尾递归优化（Java不行，C可以，我不知道为什么），这里是用它做个例子。====================================如何优化尾递归：在编译器处理过程中生成中间代码（通常是三地址代码），用编译器优化。\n\n\n另外一个的回答\n尾递归就是操作的最后一步是调用自身的递归。\n\n这是尾递归：\n```\nfunction f(x) {\n   if (x === 1) return 1;\n   return f(x-1);\n}\n```  \n（这个程序没什么意义，仅作为理解辅助之用）。\n\n这不是尾递归：\n```\nfunction f(x) {\n   if (x === 1) return 1;\n   return 1 + f(x-1);\n}\n```  \n后者不是尾递归，是因为该函数的最后一步操作是用1加上f(x-1)的返回结果，因此，最后一步操作不是调用自身。注意，后面这段代码的递归也发生在函数末尾，但它不是尾递归。尾递归的判断标准是函数运行最后一步是否调用自身，而不是是否在函数的最后一行调用自身。因此阶乘n!的递归求法，尽管看起来递归发生在函数末尾，其实也不是尾递归：\n```python\nfunction factorial(n) {\n   if (n === 1) return 1;\n   return n * factorial(n-1); // 最后一步不是调用自身，因此不是尾递归\n}\n```\n使用尾递归可以带来一个好处：因为进入最后一步后不再需要参考外层函数（caller）的信息，因此没必要保存外层函数的stack，递归需要用的stack只有目前这层函数的，因此避免了栈溢出风险。一些语言提供了尾递归优化，当识别出使用了尾递归时，会相应地只保留当前层函数的stack，节省内存。所以，在没有尾递归优化的语言，如java, python中，鼓励用迭代iteration来改写尾递归；在有尾递归优化的语言如Erlang中，鼓励用尾递归来改写其他形式的递归。谢谢 @jamesr 的指正~关于如何改写，参考[：尾调用优化 - 阮一峰的网络日志](http://www.ruanyifeng.com/blog/2015/04/tail-call.html)  \n知乎尾递归参考:  https://www.zhihu.com/question/20761771\n\n","source":"_posts/scala高阶函数学习.md","raw":"---\ntitle: scala高阶函数学习\ndate: 2017-05-01 21:25:21\ntags: [scala]\ncategories: [programme]\nauthor: kaishun\nid: 53\npermalink: scala-higher-order-functions\n---\n\n参考文章   \nhttp://www.cnblogs.com/wzm-xu/p/4063814.html  \nhttp://www.cnblogs.com/wzm-xu/p/4064389.html  \n\n# scala高阶函数上\n高阶函数是函数式编程里面一个非常重要的特色，所谓的高阶函数，就是以其它函数作为参数的函数。\n\n下面以一个小例子演示Scala的高阶函数特性，非常有意思，也非常强大。\n\n首先看这么一个程序：\n\n**code1**：\n\n```scala\nobject higherorderfuntion{\n   def sum1(a:Int,b:Int):Int=\n     if(a>b) 0\n     else a+sum1(a+1,b)\n\n   def sum2(a:Int,b:Int):Int=\n     if(a>b) 0\n     else cube(a)+sum2(a+1,b)\n\n   def sum3(a:Int,b:Int):Int=\n     if(a>b) 0\n     else fac(a)+sum3(a+1,b)\n\n   def cube(a:Int):Int=a*a*a\n\n   def fac(a:Int):Int=\n   if (a==0) 1\n   else a*fac(a-1)\n\n   def main(args:Array[String])={\n     println(sum1(1,3))\n     println(sum2(1,3))\n     println(sum3(1,3))\n    }\n}\n```\n上面这个例子“没有”用到高阶函数，sum1是计算a+(a+1)+(a+2)+...+(b),  sum2是计算a\\^3+(a+1)\\^3+(a+2)\\^3+...+b\\^3,\n\nsum3是计算a!+(a+1)!+(a+2)!+...+b!。分析sum1,sum2,sum3的代码，很容易发现这三个函数有着相似的“Pattern”，\n\n抛开函数名不论，这三个函数唯一的区别在于，在 else语句中对a的处理：a, cube(a) , fac(a).  那么来看，在函数式\n\n编程里面，是如何非常精彩的利用这个“Pattern”来使得代码更加精简的：\n\n**code2**：\n```scala\nobject higherorderfuntion{\n   def sum(f:Int=>Int,a:Int,b:Int):Int=\n     if(a>b) 0\n     else f(a)+sum(f,a+1,b)\n\n   def sumInt(a:Int,b:Int):Int = sum(id,a,b)\n   def sumCube(a:Int,b:Int):Int = sum(cube,a,b)\n   def sumFac(a:Int,b:Int):Int = sum(fac,a,b)\n\n   def id(a:Int) = a\n   def cube(a:Int):Int=a*a*a\n   def fac(a:Int):Int=\n     if (a==0) 1\n     else a*fac(a-1)\n\n   def main(args:Array[String])={\n     println(sumInt(1,3))\n     println(sumCube(1,3))\n     println(sumFac(1,3))\n   }\n\n}\n```  \n重头戏来了，我们来看sum函数的实现：\n```scala\ndef sum(f:Int=>Int,a:Int,b:Int):Int=\n     if(a>b) 0\n     else f(a)+sum(f,a+1,b)\n```  \nsum函数接收三个参数，第二个和第三个参数分别是： a:Int 和 b:Int, 这和第一个例子中是一样的。\n\n比较令人费解的是sum函数的第一个参数：f:Int=>Int  \n\n这个是什么意思呢？意思是sum函数接收一个名字叫做 “f” 的函数作为参数，而 Int=>Int 是对f的说明：\n\n=>左边的Int是说：函数f接收一个Int类型的参数，\n\n=>右边的Int是说：函数f的返回值是Int类型的。\n\n好了，那么既然sum函数接收函数f作为一个参数，那么sum就可以利用f了，事实也是这样的，看sum\n\n函数的else语句就知道了：**f(a)**\n\n然后再看sumInt、sumCube和sumFac三个函数的定义：\n```scala\ndef sumInt(a:Int,b:Int):Int = sum(id,a,b)\ndef sumCube(a:Int,b:Int):Int = sum(cube,a,b)\ndef sumFac(a:Int,b:Int):Int = sum(fac,a,b)\n```\n以sumCube函数为例吧，它在定义的时候，把cube函数作为参数，传递给sum函数，\n\n而我们看cube函数的定义：\n```scala\ndef cube(a:Int):Int=a*a*a\n```\n发现，cube函数接收一个Int作为参数，并且返回一个Int，也就是说cube函数是符合sum函数的第一个\n\n参数:   f:Int=>Int  的\n\n是不是很精彩呢？\n\n事实上上述代码还可以进一步简化，因为我们观察到id函数和cube函数的功能非常简单，不需要单独作为\n\n一个函数出现，进一步简化后的代码如下：    \n**code3**\n```scala\nobject higherorderfuntion{\n   def sum(f:Int=>Int,a:Int,b:Int):Int=\n     if(a>b) 0\n     else f(a)+sum(f,a+1,b)\n\n   def sumInt(a:Int,b:Int):Int = sum(x=>x,a,b)\n   def sumCube(a:Int,b:Int):Int = sum(x=>x*x*x,a,b)\n   def sumFac(a:Int,b:Int):Int = sum(fac,a,b)\n\n   def fac(a:Int):Int=\n     if (a==0) 1\n     else a*fac(a-1)\n\n   def main(args:Array[String])={\n     println(sumInt(1,3))\n     println(sumCube(1,3))\n     println(sumFac(1,3))\n   }\n}\n```\n\ncode3和code2相比，去掉了id函数和cube这两个函数，并且sumInt和sumCube函数的声明也发生了\n\n一点变化：\n```scala\ndef sumInt(a:Int,b:Int):Int = sum(x=>x,a,b)\ndef sumCube(a:Int,b:Int):Int = sum(x=>x*x*x,a,b)\n```\n像： x=>x   和     x=>x*x*x   这样的东西是什么呢？在函数式编程里面，这被叫做“function literal”,又称“匿名函数”，\n\n说白了，这也是函数的一种表达方式，只是这个函数没有名字罢了。\n\ncode3其实也还有点小毛病，那就是sum函数和fac函数都不是“尾递归”，所以呢，把它们改成尾递归如下：\n\n**code4：**\n```\nobject higherorderfuntion{\n   def sum(f:Int=>Int,a:Int,b:Int):Int={\n     def loop(a:Int,acc:Int):Int=\n       if(a>b) acc\n       else loop(a+1,f(a)+acc)\n     loop(a,0)\n   }\n   def sumInt(a:Int,b:Int):Int = sum(x=>x,a,b)\n   def sumCube(a:Int,b:Int):Int = sum(x=>x*x*x,a,b)\n   def sumFac(a:Int,b:Int):Int = sum(fac,a,b)\n\n   def fac(a:Int):Int=\n   {\n     def loop(a:Int,acc:Int):Int=\n       if(a==0) acc\n       else loop(a-1,a*acc)\n     loop(a,1)\n   }\n   def main(args:Array[String])={\n     println(sumInt(1,4))\n     println(sumCube(1,4))\n     println(sumFac(1,4))\n   }\n\n}\n```  \n\n# scala高阶函数下\n在scala高阶函数上我们演示了如何把一个函数作为参数传递给另外一个函数。\n\n在本文里面，我们来演示函数式编程另外一个重要的特性：返回一个函数。首先来看这么一段代码：\n\n**code piece 1：**\n```scala\ndef sum(f:Int=>Int):(Int,Int)=>Int={\n       def sumF(a:Int,b:Int):Int=\n         if(a>b) 0\n         else f(a)+sumF(a+1,b)\n       sumF\n   }\n```\n一点点来看，f:Int=>Int 是sum函数接收的参数，该参数是一个函数。 \n\n\":\" 号后面的 (Int,Int) => Int 是sum函数的返回值，又(Int,Int) => Int是一个函数的类型：接收两个Int型的数，\n\n返回一个Int的数。也就是说调用sum函数，其返回的值是一个函数。  这一点对于已经习惯C、C++、Java等编程语言的\n\n程序员来说有一点难以理解。\n\n继续看例子吧，如果执行下面的一行代码会发生什么呢？\n```scala\nsum(x=>x*x)(1,4)\n```\n结果会返回30。为什么是30呢？ 30= 1^2+ 2^2 + 3^2 + 4^2.\n\n首先，sum(x=>x*x) 是一个函数，并且sum(x=>x*x)的类型是(Int,Int)=>Int，正因为sum(x=>x*x)是一个函数，\n\n所以它才可以继续接收参数(1,4).\n\n好吧，我可能没有把这个事儿说清楚，实在是太抽象了。但是，读者应该有了一个基本的印象，那就是：\n\n在函数式编程里面，函数f可以接收函数g作为参数，也可以返回函数h。\n\n到这里还没完。。。。。。\n\nScala里面可以继续对code piece1 进行简化，如下：\n\n**code piece2**：\n```scala\nobject higherorderfuntion{\n   def sum(f:Int=>Int)(a:Int,b:Int):Int={\n      if(a>b) 0  else f(a)+sum(f)(a+1,b)\n   }\n\n   def fac(a:Int):Int=\n   {\n     def loop(a:Int,acc:Int):Int=\n       if(a==0) acc\n       else loop(a-1,a*acc)\n     loop(a,1)\n   }\n   def main(args:Array[String])={\n     println(sum(x=>x)(1,4))\n     println(sum(x=>x*x*x)(1,4))\n     println(sum(fac)(1,4))\n   }\n\n}\n```  \n这里我们需要特别注意，这里sum函数的参数列表变成了两个：(f:Int=>Int)和(a:Int,b:Int)\n\n在scala里面，函数可以有多个参数列表。比如下面的函数定义：  \n```\ndef f(args_1)(args_2)...(args_n)=E\n```\nE代表一个函数体，且n>1\n\n那么上述定义等价于：  \n```scala\ndef f(args_1)(args_2)...(args_n-1)={def g(args_n)=E;g}\n```\n还是以code piece2中的sum函数为例：\n```\ndef sum(f:Int=>Int)(a:Int,b:Int):Int={\n      if(a>b) 0  else f(a)+sum(f)(a+1,b)\n   }\n```\n等价于：\n```\ndef sum(f:Int=>Int):(Int,Int)=>Int={\n     def g(a:Int,b:Int):Int = \n      if(a>b) 0  else f(a)+sum(f)(a+1,b)\n     g\n   }\n```\n这个定义和code piece1中的sum函数的定义是一致的。\n---\n\n---\n\n---\n\n# 扩展： 什么是尾递归 \n参考https://www.zhihu.com/question/20761771/answer/19996299  \n这个虽然是python的，但是也能让我们理解什么是尾递归\n尾递归和一般的递归不同在对内存的占用，普通递归创建stack累积而后计算收缩，尾递归只会占用恒量的内存（和迭代一样）。SICP中描述了一个内存占用曲线，用以上答案中的Python代码为例（普通递归）：\n```python\ndef recsum(x):\n  if x == 1:\n    return x\n  else:\n    return x + recsum(x - 1)\n```\n当调用recsum(5)，Python调试器中发生如下状况：\n```python\nrecsum(5)\n5 + recsum(4)\n5 + (4 + recsum(3))\n5 + (4 + (3 + recsum(2)))\n5 + (4 + (3 + (2 + recsum(1))))\n5 + (4 + (3 + (2 + 1)))\n5 + (4 + (3 + 3))\n5 + (4 + 6)\n5 + 10\n15\n\n```\n这个曲线就代表内存占用大小的峰值，从左到右，达到顶峰，再从右到左收缩。而我们通常不希望这样的事情发生，所以使用迭代，只占据常量stack space(更新这个栈！而非扩展他)。\n---------------------\n**（一个替代方案：迭代）**  \n```python\nfor i in range(6):\n  sum += i\n```\n因为Python，Java，Pascal等等无法在语言中实现尾递归优化(Tail Call Optimization, TCO)，所以采用了for, while, goto等特殊结构代替recursive的表述。Scheme则不需要这样曲折地表达，一旦写成尾递归形式，就可以进行尾递归优化。---------------------Python中可以写（尾递归）：\n\n```python\ndef tailrecsum(x, running_total=0):\n  if x == 0:\n    return running_total\n  else:\n    return tailrecsum(x - 1, running_total + x)\n\n```\n理论上类似上面：  \n```\ntailrecsum(5, 0)\ntailrecsum(4, 5)\ntailrecsum(3, 9)\ntailrecsum(2, 12)\ntailrecsum(1, 14)\ntailrecsum(0, 15)\n15\n```\n\n观察到，tailrecsum(x, y)中形式变量y的实际变量值是不断更新的，对比普通递归就很清楚，后者每个recsum()调用中y值不变，仅在层级上加深。所以，尾递归是把变化的参数传递给递归函数的变量了。怎么写尾递归？形式上只要最后一个return语句是单纯函数就可以。如：\n```\nreturn tailrec(x+1);\n```  \n而\n```\nreturn tailrec(x+1) + x;\n```\n则不可以，因为无法更新tailrec()函数内的实际变量，只是新建一个栈。但Python不能尾递归优化（Java不行，C可以，我不知道为什么），这里是用它做个例子。====================================如何优化尾递归：在编译器处理过程中生成中间代码（通常是三地址代码），用编译器优化。\n\n\n另外一个的回答\n尾递归就是操作的最后一步是调用自身的递归。\n\n这是尾递归：\n```\nfunction f(x) {\n   if (x === 1) return 1;\n   return f(x-1);\n}\n```  \n（这个程序没什么意义，仅作为理解辅助之用）。\n\n这不是尾递归：\n```\nfunction f(x) {\n   if (x === 1) return 1;\n   return 1 + f(x-1);\n}\n```  \n后者不是尾递归，是因为该函数的最后一步操作是用1加上f(x-1)的返回结果，因此，最后一步操作不是调用自身。注意，后面这段代码的递归也发生在函数末尾，但它不是尾递归。尾递归的判断标准是函数运行最后一步是否调用自身，而不是是否在函数的最后一行调用自身。因此阶乘n!的递归求法，尽管看起来递归发生在函数末尾，其实也不是尾递归：\n```python\nfunction factorial(n) {\n   if (n === 1) return 1;\n   return n * factorial(n-1); // 最后一步不是调用自身，因此不是尾递归\n}\n```\n使用尾递归可以带来一个好处：因为进入最后一步后不再需要参考外层函数（caller）的信息，因此没必要保存外层函数的stack，递归需要用的stack只有目前这层函数的，因此避免了栈溢出风险。一些语言提供了尾递归优化，当识别出使用了尾递归时，会相应地只保留当前层函数的stack，节省内存。所以，在没有尾递归优化的语言，如java, python中，鼓励用迭代iteration来改写尾递归；在有尾递归优化的语言如Erlang中，鼓励用尾递归来改写其他形式的递归。谢谢 @jamesr 的指正~关于如何改写，参考[：尾调用优化 - 阮一峰的网络日志](http://www.ruanyifeng.com/blog/2015/04/tail-call.html)  \n知乎尾递归参考:  https://www.zhihu.com/question/20761771\n\n","slug":"scala-higher-order-functions","published":1,"updated":"2018-01-22T15:57:13.935Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcrpnzqp002r2wv3ubopczd5","content":"<p>参考文章<br><a href=\"http://www.cnblogs.com/wzm-xu/p/4063814.html\" target=\"_blank\" rel=\"external\">http://www.cnblogs.com/wzm-xu/p/4063814.html</a><br><a href=\"http://www.cnblogs.com/wzm-xu/p/4064389.html\" target=\"_blank\" rel=\"external\">http://www.cnblogs.com/wzm-xu/p/4064389.html</a>  </p>\n<h1 id=\"scala高阶函数上\"><a href=\"#scala高阶函数上\" class=\"headerlink\" title=\"scala高阶函数上\"></a>scala高阶函数上</h1><p>高阶函数是函数式编程里面一个非常重要的特色，所谓的高阶函数，就是以其它函数作为参数的函数。</p>\n<p>下面以一个小例子演示Scala的高阶函数特性，非常有意思，也非常强大。</p>\n<p>首先看这么一个程序：</p>\n<p><strong>code1</strong>：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">higherorderfuntion</span></span>&#123;</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sum1</span></span>(a:<span class=\"type\">Int</span>,b:<span class=\"type\">Int</span>):<span class=\"type\">Int</span>=</div><div class=\"line\">     <span class=\"keyword\">if</span>(a&gt;b) <span class=\"number\">0</span></div><div class=\"line\">     <span class=\"keyword\">else</span> a+sum1(a+<span class=\"number\">1</span>,b)</div><div class=\"line\"></div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sum2</span></span>(a:<span class=\"type\">Int</span>,b:<span class=\"type\">Int</span>):<span class=\"type\">Int</span>=</div><div class=\"line\">     <span class=\"keyword\">if</span>(a&gt;b) <span class=\"number\">0</span></div><div class=\"line\">     <span class=\"keyword\">else</span> cube(a)+sum2(a+<span class=\"number\">1</span>,b)</div><div class=\"line\"></div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sum3</span></span>(a:<span class=\"type\">Int</span>,b:<span class=\"type\">Int</span>):<span class=\"type\">Int</span>=</div><div class=\"line\">     <span class=\"keyword\">if</span>(a&gt;b) <span class=\"number\">0</span></div><div class=\"line\">     <span class=\"keyword\">else</span> fac(a)+sum3(a+<span class=\"number\">1</span>,b)</div><div class=\"line\"></div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">cube</span></span>(a:<span class=\"type\">Int</span>):<span class=\"type\">Int</span>=a*a*a</div><div class=\"line\"></div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">fac</span></span>(a:<span class=\"type\">Int</span>):<span class=\"type\">Int</span>=</div><div class=\"line\">   <span class=\"keyword\">if</span> (a==<span class=\"number\">0</span>) <span class=\"number\">1</span></div><div class=\"line\">   <span class=\"keyword\">else</span> a*fac(a<span class=\"number\">-1</span>)</div><div class=\"line\"></div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args:<span class=\"type\">Array</span>[<span class=\"type\">String</span>])=&#123;</div><div class=\"line\">     println(sum1(<span class=\"number\">1</span>,<span class=\"number\">3</span>))</div><div class=\"line\">     println(sum2(<span class=\"number\">1</span>,<span class=\"number\">3</span>))</div><div class=\"line\">     println(sum3(<span class=\"number\">1</span>,<span class=\"number\">3</span>))</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>上面这个例子“没有”用到高阶函数，sum1是计算a+(a+1)+(a+2)+…+(b),  sum2是计算a\\^3+(a+1)\\^3+(a+2)\\^3+…+b\\^3,</p>\n<p>sum3是计算a!+(a+1)!+(a+2)!+…+b!。分析sum1,sum2,sum3的代码，很容易发现这三个函数有着相似的“Pattern”，</p>\n<p>抛开函数名不论，这三个函数唯一的区别在于，在 else语句中对a的处理：a, cube(a) , fac(a).  那么来看，在函数式</p>\n<p>编程里面，是如何非常精彩的利用这个“Pattern”来使得代码更加精简的：</p>\n<p><strong>code2</strong>：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">higherorderfuntion</span></span>&#123;</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sum</span></span>(f:<span class=\"type\">Int</span>=&gt;<span class=\"type\">Int</span>,a:<span class=\"type\">Int</span>,b:<span class=\"type\">Int</span>):<span class=\"type\">Int</span>=</div><div class=\"line\">     <span class=\"keyword\">if</span>(a&gt;b) <span class=\"number\">0</span></div><div class=\"line\">     <span class=\"keyword\">else</span> f(a)+sum(f,a+<span class=\"number\">1</span>,b)</div><div class=\"line\"></div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sumInt</span></span>(a:<span class=\"type\">Int</span>,b:<span class=\"type\">Int</span>):<span class=\"type\">Int</span> = sum(id,a,b)</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sumCube</span></span>(a:<span class=\"type\">Int</span>,b:<span class=\"type\">Int</span>):<span class=\"type\">Int</span> = sum(cube,a,b)</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sumFac</span></span>(a:<span class=\"type\">Int</span>,b:<span class=\"type\">Int</span>):<span class=\"type\">Int</span> = sum(fac,a,b)</div><div class=\"line\"></div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">id</span></span>(a:<span class=\"type\">Int</span>) = a</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">cube</span></span>(a:<span class=\"type\">Int</span>):<span class=\"type\">Int</span>=a*a*a</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">fac</span></span>(a:<span class=\"type\">Int</span>):<span class=\"type\">Int</span>=</div><div class=\"line\">     <span class=\"keyword\">if</span> (a==<span class=\"number\">0</span>) <span class=\"number\">1</span></div><div class=\"line\">     <span class=\"keyword\">else</span> a*fac(a<span class=\"number\">-1</span>)</div><div class=\"line\"></div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args:<span class=\"type\">Array</span>[<span class=\"type\">String</span>])=&#123;</div><div class=\"line\">     println(sumInt(<span class=\"number\">1</span>,<span class=\"number\">3</span>))</div><div class=\"line\">     println(sumCube(<span class=\"number\">1</span>,<span class=\"number\">3</span>))</div><div class=\"line\">     println(sumFac(<span class=\"number\">1</span>,<span class=\"number\">3</span>))</div><div class=\"line\">   &#125;</div><div class=\"line\"></div><div class=\"line\">&#125;</div><div class=\"line\">```  </div><div class=\"line\">重头戏来了，我们来看sum函数的实现：</div><div class=\"line\">```scala</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sum</span></span>(f:<span class=\"type\">Int</span>=&gt;<span class=\"type\">Int</span>,a:<span class=\"type\">Int</span>,b:<span class=\"type\">Int</span>):<span class=\"type\">Int</span>=</div><div class=\"line\">     <span class=\"keyword\">if</span>(a&gt;b) <span class=\"number\">0</span></div><div class=\"line\">     <span class=\"keyword\">else</span> f(a)+sum(f,a+<span class=\"number\">1</span>,b)</div><div class=\"line\">```  </div><div class=\"line\">sum函数接收三个参数，第二个和第三个参数分别是： a:<span class=\"type\">Int</span> 和 b:<span class=\"type\">Int</span>, 这和第一个例子中是一样的。</div><div class=\"line\"></div><div class=\"line\">比较令人费解的是sum函数的第一个参数：f:<span class=\"type\">Int</span>=&gt;<span class=\"type\">Int</span>  </div><div class=\"line\"></div><div class=\"line\">这个是什么意思呢？意思是sum函数接收一个名字叫做 “f” 的函数作为参数，而 <span class=\"type\">Int</span>=&gt;<span class=\"type\">Int</span> 是对f的说明：</div><div class=\"line\"></div><div class=\"line\">=&gt;左边的<span class=\"type\">Int</span>是说：函数f接收一个<span class=\"type\">Int</span>类型的参数，</div><div class=\"line\"></div><div class=\"line\">=&gt;右边的<span class=\"type\">Int</span>是说：函数f的返回值是<span class=\"type\">Int</span>类型的。</div><div class=\"line\"></div><div class=\"line\">好了，那么既然sum函数接收函数f作为一个参数，那么sum就可以利用f了，事实也是这样的，看sum</div><div class=\"line\"></div><div class=\"line\">函数的<span class=\"keyword\">else</span>语句就知道了：**f(a)**</div><div class=\"line\"></div><div class=\"line\">然后再看sumInt、sumCube和sumFac三个函数的定义：</div><div class=\"line\">```scala</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sumInt</span></span>(a:<span class=\"type\">Int</span>,b:<span class=\"type\">Int</span>):<span class=\"type\">Int</span> = sum(id,a,b)</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sumCube</span></span>(a:<span class=\"type\">Int</span>,b:<span class=\"type\">Int</span>):<span class=\"type\">Int</span> = sum(cube,a,b)</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sumFac</span></span>(a:<span class=\"type\">Int</span>,b:<span class=\"type\">Int</span>):<span class=\"type\">Int</span> = sum(fac,a,b)</div></pre></td></tr></table></figure></p>\n<p>以sumCube函数为例吧，它在定义的时候，把cube函数作为参数，传递给sum函数，</p>\n<p>而我们看cube函数的定义：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">cube</span></span>(a:<span class=\"type\">Int</span>):<span class=\"type\">Int</span>=a*a*a</div></pre></td></tr></table></figure></p>\n<p>发现，cube函数接收一个Int作为参数，并且返回一个Int，也就是说cube函数是符合sum函数的第一个</p>\n<p>参数:   f:Int=&gt;Int  的</p>\n<p>是不是很精彩呢？</p>\n<p>事实上上述代码还可以进一步简化，因为我们观察到id函数和cube函数的功能非常简单，不需要单独作为</p>\n<p>一个函数出现，进一步简化后的代码如下：<br><strong>code3</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">higherorderfuntion</span></span>&#123;</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sum</span></span>(f:<span class=\"type\">Int</span>=&gt;<span class=\"type\">Int</span>,a:<span class=\"type\">Int</span>,b:<span class=\"type\">Int</span>):<span class=\"type\">Int</span>=</div><div class=\"line\">     <span class=\"keyword\">if</span>(a&gt;b) <span class=\"number\">0</span></div><div class=\"line\">     <span class=\"keyword\">else</span> f(a)+sum(f,a+<span class=\"number\">1</span>,b)</div><div class=\"line\"></div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sumInt</span></span>(a:<span class=\"type\">Int</span>,b:<span class=\"type\">Int</span>):<span class=\"type\">Int</span> = sum(x=&gt;x,a,b)</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sumCube</span></span>(a:<span class=\"type\">Int</span>,b:<span class=\"type\">Int</span>):<span class=\"type\">Int</span> = sum(x=&gt;x*x*x,a,b)</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sumFac</span></span>(a:<span class=\"type\">Int</span>,b:<span class=\"type\">Int</span>):<span class=\"type\">Int</span> = sum(fac,a,b)</div><div class=\"line\"></div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">fac</span></span>(a:<span class=\"type\">Int</span>):<span class=\"type\">Int</span>=</div><div class=\"line\">     <span class=\"keyword\">if</span> (a==<span class=\"number\">0</span>) <span class=\"number\">1</span></div><div class=\"line\">     <span class=\"keyword\">else</span> a*fac(a<span class=\"number\">-1</span>)</div><div class=\"line\"></div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args:<span class=\"type\">Array</span>[<span class=\"type\">String</span>])=&#123;</div><div class=\"line\">     println(sumInt(<span class=\"number\">1</span>,<span class=\"number\">3</span>))</div><div class=\"line\">     println(sumCube(<span class=\"number\">1</span>,<span class=\"number\">3</span>))</div><div class=\"line\">     println(sumFac(<span class=\"number\">1</span>,<span class=\"number\">3</span>))</div><div class=\"line\">   &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>code3和code2相比，去掉了id函数和cube这两个函数，并且sumInt和sumCube函数的声明也发生了</p>\n<p>一点变化：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sumInt</span></span>(a:<span class=\"type\">Int</span>,b:<span class=\"type\">Int</span>):<span class=\"type\">Int</span> = sum(x=&gt;x,a,b)</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sumCube</span></span>(a:<span class=\"type\">Int</span>,b:<span class=\"type\">Int</span>):<span class=\"type\">Int</span> = sum(x=&gt;x*x*x,a,b)</div></pre></td></tr></table></figure></p>\n<p>像： x=&gt;x   和     x=&gt;x<em>x</em>x   这样的东西是什么呢？在函数式编程里面，这被叫做“function literal”,又称“匿名函数”，</p>\n<p>说白了，这也是函数的一种表达方式，只是这个函数没有名字罢了。</p>\n<p>code3其实也还有点小毛病，那就是sum函数和fac函数都不是“尾递归”，所以呢，把它们改成尾递归如下：</p>\n<p><strong>code4：</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div></pre></td><td class=\"code\"><pre><div class=\"line\">object higherorderfuntion&#123;</div><div class=\"line\">   def sum(f:Int=&gt;Int,a:Int,b:Int):Int=&#123;</div><div class=\"line\">     def loop(a:Int,acc:Int):Int=</div><div class=\"line\">       if(a&gt;b) acc</div><div class=\"line\">       else loop(a+1,f(a)+acc)</div><div class=\"line\">     loop(a,0)</div><div class=\"line\">   &#125;</div><div class=\"line\">   def sumInt(a:Int,b:Int):Int = sum(x=&gt;x,a,b)</div><div class=\"line\">   def sumCube(a:Int,b:Int):Int = sum(x=&gt;x*x*x,a,b)</div><div class=\"line\">   def sumFac(a:Int,b:Int):Int = sum(fac,a,b)</div><div class=\"line\"></div><div class=\"line\">   def fac(a:Int):Int=</div><div class=\"line\">   &#123;</div><div class=\"line\">     def loop(a:Int,acc:Int):Int=</div><div class=\"line\">       if(a==0) acc</div><div class=\"line\">       else loop(a-1,a*acc)</div><div class=\"line\">     loop(a,1)</div><div class=\"line\">   &#125;</div><div class=\"line\">   def main(args:Array[String])=&#123;</div><div class=\"line\">     println(sumInt(1,4))</div><div class=\"line\">     println(sumCube(1,4))</div><div class=\"line\">     println(sumFac(1,4))</div><div class=\"line\">   &#125;</div><div class=\"line\"></div><div class=\"line\">&#125;</div><div class=\"line\">```  </div><div class=\"line\"></div><div class=\"line\"># scala高阶函数下</div><div class=\"line\">在scala高阶函数上我们演示了如何把一个函数作为参数传递给另外一个函数。</div><div class=\"line\"></div><div class=\"line\">在本文里面，我们来演示函数式编程另外一个重要的特性：返回一个函数。首先来看这么一段代码：</div><div class=\"line\"></div><div class=\"line\">**code piece 1：**</div><div class=\"line\">```scala</div><div class=\"line\">def sum(f:Int=&gt;Int):(Int,Int)=&gt;Int=&#123;</div><div class=\"line\">       def sumF(a:Int,b:Int):Int=</div><div class=\"line\">         if(a&gt;b) 0</div><div class=\"line\">         else f(a)+sumF(a+1,b)</div><div class=\"line\">       sumF</div><div class=\"line\">   &#125;</div></pre></td></tr></table></figure></p>\n<p>一点点来看，f:Int=&gt;Int 是sum函数接收的参数，该参数是一个函数。 </p>\n<p>“:” 号后面的 (Int,Int) =&gt; Int 是sum函数的返回值，又(Int,Int) =&gt; Int是一个函数的类型：接收两个Int型的数，</p>\n<p>返回一个Int的数。也就是说调用sum函数，其返回的值是一个函数。  这一点对于已经习惯C、C++、Java等编程语言的</p>\n<p>程序员来说有一点难以理解。</p>\n<p>继续看例子吧，如果执行下面的一行代码会发生什么呢？<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">sum(x=&gt;x*x)(<span class=\"number\">1</span>,<span class=\"number\">4</span>)</div></pre></td></tr></table></figure></p>\n<p>结果会返回30。为什么是30呢？ 30= 1^2+ 2^2 + 3^2 + 4^2.</p>\n<p>首先，sum(x=&gt;x<em>x) 是一个函数，并且sum(x=&gt;x</em>x)的类型是(Int,Int)=&gt;Int，正因为sum(x=&gt;x*x)是一个函数，</p>\n<p>所以它才可以继续接收参数(1,4).</p>\n<p>好吧，我可能没有把这个事儿说清楚，实在是太抽象了。但是，读者应该有了一个基本的印象，那就是：</p>\n<p>在函数式编程里面，函数f可以接收函数g作为参数，也可以返回函数h。</p>\n<p>到这里还没完。。。。。。</p>\n<p>Scala里面可以继续对code piece1 进行简化，如下：</p>\n<p><strong>code piece2</strong>：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">higherorderfuntion</span></span>&#123;</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sum</span></span>(f:<span class=\"type\">Int</span>=&gt;<span class=\"type\">Int</span>)(a:<span class=\"type\">Int</span>,b:<span class=\"type\">Int</span>):<span class=\"type\">Int</span>=&#123;</div><div class=\"line\">      <span class=\"keyword\">if</span>(a&gt;b) <span class=\"number\">0</span>  <span class=\"keyword\">else</span> f(a)+sum(f)(a+<span class=\"number\">1</span>,b)</div><div class=\"line\">   &#125;</div><div class=\"line\"></div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">fac</span></span>(a:<span class=\"type\">Int</span>):<span class=\"type\">Int</span>=</div><div class=\"line\">   &#123;</div><div class=\"line\">     <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">loop</span></span>(a:<span class=\"type\">Int</span>,acc:<span class=\"type\">Int</span>):<span class=\"type\">Int</span>=</div><div class=\"line\">       <span class=\"keyword\">if</span>(a==<span class=\"number\">0</span>) acc</div><div class=\"line\">       <span class=\"keyword\">else</span> loop(a<span class=\"number\">-1</span>,a*acc)</div><div class=\"line\">     loop(a,<span class=\"number\">1</span>)</div><div class=\"line\">   &#125;</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args:<span class=\"type\">Array</span>[<span class=\"type\">String</span>])=&#123;</div><div class=\"line\">     println(sum(x=&gt;x)(<span class=\"number\">1</span>,<span class=\"number\">4</span>))</div><div class=\"line\">     println(sum(x=&gt;x*x*x)(<span class=\"number\">1</span>,<span class=\"number\">4</span>))</div><div class=\"line\">     println(sum(fac)(<span class=\"number\">1</span>,<span class=\"number\">4</span>))</div><div class=\"line\">   &#125;</div><div class=\"line\"></div><div class=\"line\">&#125;</div><div class=\"line\">```  </div><div class=\"line\">这里我们需要特别注意，这里sum函数的参数列表变成了两个：(f:<span class=\"type\">Int</span>=&gt;<span class=\"type\">Int</span>)和(a:<span class=\"type\">Int</span>,b:<span class=\"type\">Int</span>)</div><div class=\"line\"></div><div class=\"line\">在scala里面，函数可以有多个参数列表。比如下面的函数定义：</div></pre></td></tr></table></figure></p>\n<p>def f(args_1)(args_2)…(args_n)=E<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">E代表一个函数体，且n&gt;1</div><div class=\"line\"></div><div class=\"line\">那么上述定义等价于：  </div><div class=\"line\">```scala</div><div class=\"line\">def f(args_1)(args_2)...(args_n-1)=&#123;def g(args_n)=E;g&#125;</div></pre></td></tr></table></figure></p>\n<p>还是以code piece2中的sum函数为例：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">def sum(f:Int=&gt;Int)(a:Int,b:Int):Int=&#123;</div><div class=\"line\">      if(a&gt;b) 0  else f(a)+sum(f)(a+1,b)</div><div class=\"line\">   &#125;</div></pre></td></tr></table></figure></p>\n<p>等价于：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">def sum(f:Int=&gt;Int):(Int,Int)=&gt;Int=&#123;</div><div class=\"line\">     def g(a:Int,b:Int):Int = </div><div class=\"line\">      if(a&gt;b) 0  else f(a)+sum(f)(a+1,b)</div><div class=\"line\">     g</div><div class=\"line\">   &#125;</div></pre></td></tr></table></figure></p>\n<h2 id=\"这个定义和code-piece1中的sum函数的定义是一致的。\"><a href=\"#这个定义和code-piece1中的sum函数的定义是一致的。\" class=\"headerlink\" title=\"这个定义和code piece1中的sum函数的定义是一致的。\"></a>这个定义和code piece1中的sum函数的定义是一致的。</h2><hr>\n<hr>\n<h1 id=\"扩展：-什么是尾递归\"><a href=\"#扩展：-什么是尾递归\" class=\"headerlink\" title=\"扩展： 什么是尾递归\"></a>扩展： 什么是尾递归</h1><p>参考<a href=\"https://www.zhihu.com/question/20761771/answer/19996299\" target=\"_blank\" rel=\"external\">https://www.zhihu.com/question/20761771/answer/19996299</a><br>这个虽然是python的，但是也能让我们理解什么是尾递归<br>尾递归和一般的递归不同在对内存的占用，普通递归创建stack累积而后计算收缩，尾递归只会占用恒量的内存（和迭代一样）。SICP中描述了一个内存占用曲线，用以上答案中的Python代码为例（普通递归）：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">recsum</span><span class=\"params\">(x)</span>:</span></div><div class=\"line\">  <span class=\"keyword\">if</span> x == <span class=\"number\">1</span>:</div><div class=\"line\">    <span class=\"keyword\">return</span> x</div><div class=\"line\">  <span class=\"keyword\">else</span>:</div><div class=\"line\">    <span class=\"keyword\">return</span> x + recsum(x - <span class=\"number\">1</span>)</div></pre></td></tr></table></figure></p>\n<p>当调用recsum(5)，Python调试器中发生如下状况：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">recsum(<span class=\"number\">5</span>)</div><div class=\"line\"><span class=\"number\">5</span> + recsum(<span class=\"number\">4</span>)</div><div class=\"line\"><span class=\"number\">5</span> + (<span class=\"number\">4</span> + recsum(<span class=\"number\">3</span>))</div><div class=\"line\"><span class=\"number\">5</span> + (<span class=\"number\">4</span> + (<span class=\"number\">3</span> + recsum(<span class=\"number\">2</span>)))</div><div class=\"line\"><span class=\"number\">5</span> + (<span class=\"number\">4</span> + (<span class=\"number\">3</span> + (<span class=\"number\">2</span> + recsum(<span class=\"number\">1</span>))))</div><div class=\"line\"><span class=\"number\">5</span> + (<span class=\"number\">4</span> + (<span class=\"number\">3</span> + (<span class=\"number\">2</span> + <span class=\"number\">1</span>)))</div><div class=\"line\"><span class=\"number\">5</span> + (<span class=\"number\">4</span> + (<span class=\"number\">3</span> + <span class=\"number\">3</span>))</div><div class=\"line\"><span class=\"number\">5</span> + (<span class=\"number\">4</span> + <span class=\"number\">6</span>)</div><div class=\"line\"><span class=\"number\">5</span> + <span class=\"number\">10</span></div><div class=\"line\"><span class=\"number\">15</span></div></pre></td></tr></table></figure></p>\n<h2 id=\"这个曲线就代表内存占用大小的峰值，从左到右，达到顶峰，再从右到左收缩。而我们通常不希望这样的事情发生，所以使用迭代，只占据常量stack-space-更新这个栈！而非扩展他-。\"><a href=\"#这个曲线就代表内存占用大小的峰值，从左到右，达到顶峰，再从右到左收缩。而我们通常不希望这样的事情发生，所以使用迭代，只占据常量stack-space-更新这个栈！而非扩展他-。\" class=\"headerlink\" title=\"这个曲线就代表内存占用大小的峰值，从左到右，达到顶峰，再从右到左收缩。而我们通常不希望这样的事情发生，所以使用迭代，只占据常量stack space(更新这个栈！而非扩展他)。\"></a>这个曲线就代表内存占用大小的峰值，从左到右，达到顶峰，再从右到左收缩。而我们通常不希望这样的事情发生，所以使用迭代，只占据常量stack space(更新这个栈！而非扩展他)。</h2><p><strong>（一个替代方案：迭代）</strong><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">6</span>):</div><div class=\"line\">  sum += i</div></pre></td></tr></table></figure></p>\n<p>因为Python，Java，Pascal等等无法在语言中实现尾递归优化(Tail Call Optimization, TCO)，所以采用了for, while, goto等特殊结构代替recursive的表述。Scheme则不需要这样曲折地表达，一旦写成尾递归形式，就可以进行尾递归优化。———————Python中可以写（尾递归）：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">tailrecsum</span><span class=\"params\">(x, running_total=<span class=\"number\">0</span>)</span>:</span></div><div class=\"line\">  <span class=\"keyword\">if</span> x == <span class=\"number\">0</span>:</div><div class=\"line\">    <span class=\"keyword\">return</span> running_total</div><div class=\"line\">  <span class=\"keyword\">else</span>:</div><div class=\"line\">    <span class=\"keyword\">return</span> tailrecsum(x - <span class=\"number\">1</span>, running_total + x)</div></pre></td></tr></table></figure>\n<p>理论上类似上面：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">tailrecsum(5, 0)</div><div class=\"line\">tailrecsum(4, 5)</div><div class=\"line\">tailrecsum(3, 9)</div><div class=\"line\">tailrecsum(2, 12)</div><div class=\"line\">tailrecsum(1, 14)</div><div class=\"line\">tailrecsum(0, 15)</div><div class=\"line\">15</div></pre></td></tr></table></figure></p>\n<p>观察到，tailrecsum(x, y)中形式变量y的实际变量值是不断更新的，对比普通递归就很清楚，后者每个recsum()调用中y值不变，仅在层级上加深。所以，尾递归是把变化的参数传递给递归函数的变量了。怎么写尾递归？形式上只要最后一个return语句是单纯函数就可以。如：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">return tailrec(x+1);</div><div class=\"line\">```  </div><div class=\"line\">而</div></pre></td></tr></table></figure></p>\n<p>return tailrec(x+1) + x;<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">则不可以，因为无法更新tailrec()函数内的实际变量，只是新建一个栈。但Python不能尾递归优化（Java不行，C可以，我不知道为什么），这里是用它做个例子。====================================如何优化尾递归：在编译器处理过程中生成中间代码（通常是三地址代码），用编译器优化。</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">另外一个的回答</div><div class=\"line\">尾递归就是操作的最后一步是调用自身的递归。</div><div class=\"line\"></div><div class=\"line\">这是尾递归：</div></pre></td></tr></table></figure></p>\n<p>function f(x) {<br>   if (x === 1) return 1;<br>   return f(x-1);<br>}<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">（这个程序没什么意义，仅作为理解辅助之用）。</div><div class=\"line\"></div><div class=\"line\">这不是尾递归：</div></pre></td></tr></table></figure></p>\n<p>function f(x) {<br>   if (x === 1) return 1;<br>   return 1 + f(x-1);<br>}<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">后者不是尾递归，是因为该函数的最后一步操作是用1加上f(x-1)的返回结果，因此，最后一步操作不是调用自身。注意，后面这段代码的递归也发生在函数末尾，但它不是尾递归。尾递归的判断标准是函数运行最后一步是否调用自身，而不是是否在函数的最后一行调用自身。因此阶乘n!的递归求法，尽管看起来递归发生在函数末尾，其实也不是尾递归：</div><div class=\"line\">```python</div><div class=\"line\">function factorial(n) &#123;</div><div class=\"line\">   if (n === 1) return 1;</div><div class=\"line\">   return n * factorial(n-1); // 最后一步不是调用自身，因此不是尾递归</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>使用尾递归可以带来一个好处：因为进入最后一步后不再需要参考外层函数（caller）的信息，因此没必要保存外层函数的stack，递归需要用的stack只有目前这层函数的，因此避免了栈溢出风险。一些语言提供了尾递归优化，当识别出使用了尾递归时，会相应地只保留当前层函数的stack，节省内存。所以，在没有尾递归优化的语言，如java, python中，鼓励用迭代iteration来改写尾递归；在有尾递归优化的语言如Erlang中，鼓励用尾递归来改写其他形式的递归。谢谢 @jamesr 的指正~关于如何改写，参考<a href=\"http://www.ruanyifeng.com/blog/2015/04/tail-call.html\" target=\"_blank\" rel=\"external\">：尾调用优化 - 阮一峰的网络日志</a><br>知乎尾递归参考:  <a href=\"https://www.zhihu.com/question/20761771\" target=\"_blank\" rel=\"external\">https://www.zhihu.com/question/20761771</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>参考文章<br><a href=\"http://www.cnblogs.com/wzm-xu/p/4063814.html\" target=\"_blank\" rel=\"external\">http://www.cnblogs.com/wzm-xu/p/4063814.html</a><br><a href=\"http://www.cnblogs.com/wzm-xu/p/4064389.html\" target=\"_blank\" rel=\"external\">http://www.cnblogs.com/wzm-xu/p/4064389.html</a>  </p>\n<h1 id=\"scala高阶函数上\"><a href=\"#scala高阶函数上\" class=\"headerlink\" title=\"scala高阶函数上\"></a>scala高阶函数上</h1><p>高阶函数是函数式编程里面一个非常重要的特色，所谓的高阶函数，就是以其它函数作为参数的函数。</p>\n<p>下面以一个小例子演示Scala的高阶函数特性，非常有意思，也非常强大。</p>\n<p>首先看这么一个程序：</p>\n<p><strong>code1</strong>：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">higherorderfuntion</span></span>&#123;</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sum1</span></span>(a:<span class=\"type\">Int</span>,b:<span class=\"type\">Int</span>):<span class=\"type\">Int</span>=</div><div class=\"line\">     <span class=\"keyword\">if</span>(a&gt;b) <span class=\"number\">0</span></div><div class=\"line\">     <span class=\"keyword\">else</span> a+sum1(a+<span class=\"number\">1</span>,b)</div><div class=\"line\"></div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sum2</span></span>(a:<span class=\"type\">Int</span>,b:<span class=\"type\">Int</span>):<span class=\"type\">Int</span>=</div><div class=\"line\">     <span class=\"keyword\">if</span>(a&gt;b) <span class=\"number\">0</span></div><div class=\"line\">     <span class=\"keyword\">else</span> cube(a)+sum2(a+<span class=\"number\">1</span>,b)</div><div class=\"line\"></div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sum3</span></span>(a:<span class=\"type\">Int</span>,b:<span class=\"type\">Int</span>):<span class=\"type\">Int</span>=</div><div class=\"line\">     <span class=\"keyword\">if</span>(a&gt;b) <span class=\"number\">0</span></div><div class=\"line\">     <span class=\"keyword\">else</span> fac(a)+sum3(a+<span class=\"number\">1</span>,b)</div><div class=\"line\"></div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">cube</span></span>(a:<span class=\"type\">Int</span>):<span class=\"type\">Int</span>=a*a*a</div><div class=\"line\"></div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">fac</span></span>(a:<span class=\"type\">Int</span>):<span class=\"type\">Int</span>=</div><div class=\"line\">   <span class=\"keyword\">if</span> (a==<span class=\"number\">0</span>) <span class=\"number\">1</span></div><div class=\"line\">   <span class=\"keyword\">else</span> a*fac(a<span class=\"number\">-1</span>)</div><div class=\"line\"></div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args:<span class=\"type\">Array</span>[<span class=\"type\">String</span>])=&#123;</div><div class=\"line\">     println(sum1(<span class=\"number\">1</span>,<span class=\"number\">3</span>))</div><div class=\"line\">     println(sum2(<span class=\"number\">1</span>,<span class=\"number\">3</span>))</div><div class=\"line\">     println(sum3(<span class=\"number\">1</span>,<span class=\"number\">3</span>))</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>上面这个例子“没有”用到高阶函数，sum1是计算a+(a+1)+(a+2)+…+(b),  sum2是计算a\\^3+(a+1)\\^3+(a+2)\\^3+…+b\\^3,</p>\n<p>sum3是计算a!+(a+1)!+(a+2)!+…+b!。分析sum1,sum2,sum3的代码，很容易发现这三个函数有着相似的“Pattern”，</p>\n<p>抛开函数名不论，这三个函数唯一的区别在于，在 else语句中对a的处理：a, cube(a) , fac(a).  那么来看，在函数式</p>\n<p>编程里面，是如何非常精彩的利用这个“Pattern”来使得代码更加精简的：</p>\n<p><strong>code2</strong>：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">higherorderfuntion</span></span>&#123;</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sum</span></span>(f:<span class=\"type\">Int</span>=&gt;<span class=\"type\">Int</span>,a:<span class=\"type\">Int</span>,b:<span class=\"type\">Int</span>):<span class=\"type\">Int</span>=</div><div class=\"line\">     <span class=\"keyword\">if</span>(a&gt;b) <span class=\"number\">0</span></div><div class=\"line\">     <span class=\"keyword\">else</span> f(a)+sum(f,a+<span class=\"number\">1</span>,b)</div><div class=\"line\"></div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sumInt</span></span>(a:<span class=\"type\">Int</span>,b:<span class=\"type\">Int</span>):<span class=\"type\">Int</span> = sum(id,a,b)</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sumCube</span></span>(a:<span class=\"type\">Int</span>,b:<span class=\"type\">Int</span>):<span class=\"type\">Int</span> = sum(cube,a,b)</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sumFac</span></span>(a:<span class=\"type\">Int</span>,b:<span class=\"type\">Int</span>):<span class=\"type\">Int</span> = sum(fac,a,b)</div><div class=\"line\"></div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">id</span></span>(a:<span class=\"type\">Int</span>) = a</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">cube</span></span>(a:<span class=\"type\">Int</span>):<span class=\"type\">Int</span>=a*a*a</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">fac</span></span>(a:<span class=\"type\">Int</span>):<span class=\"type\">Int</span>=</div><div class=\"line\">     <span class=\"keyword\">if</span> (a==<span class=\"number\">0</span>) <span class=\"number\">1</span></div><div class=\"line\">     <span class=\"keyword\">else</span> a*fac(a<span class=\"number\">-1</span>)</div><div class=\"line\"></div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args:<span class=\"type\">Array</span>[<span class=\"type\">String</span>])=&#123;</div><div class=\"line\">     println(sumInt(<span class=\"number\">1</span>,<span class=\"number\">3</span>))</div><div class=\"line\">     println(sumCube(<span class=\"number\">1</span>,<span class=\"number\">3</span>))</div><div class=\"line\">     println(sumFac(<span class=\"number\">1</span>,<span class=\"number\">3</span>))</div><div class=\"line\">   &#125;</div><div class=\"line\"></div><div class=\"line\">&#125;</div><div class=\"line\">```  </div><div class=\"line\">重头戏来了，我们来看sum函数的实现：</div><div class=\"line\">```scala</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sum</span></span>(f:<span class=\"type\">Int</span>=&gt;<span class=\"type\">Int</span>,a:<span class=\"type\">Int</span>,b:<span class=\"type\">Int</span>):<span class=\"type\">Int</span>=</div><div class=\"line\">     <span class=\"keyword\">if</span>(a&gt;b) <span class=\"number\">0</span></div><div class=\"line\">     <span class=\"keyword\">else</span> f(a)+sum(f,a+<span class=\"number\">1</span>,b)</div><div class=\"line\">```  </div><div class=\"line\">sum函数接收三个参数，第二个和第三个参数分别是： a:<span class=\"type\">Int</span> 和 b:<span class=\"type\">Int</span>, 这和第一个例子中是一样的。</div><div class=\"line\"></div><div class=\"line\">比较令人费解的是sum函数的第一个参数：f:<span class=\"type\">Int</span>=&gt;<span class=\"type\">Int</span>  </div><div class=\"line\"></div><div class=\"line\">这个是什么意思呢？意思是sum函数接收一个名字叫做 “f” 的函数作为参数，而 <span class=\"type\">Int</span>=&gt;<span class=\"type\">Int</span> 是对f的说明：</div><div class=\"line\"></div><div class=\"line\">=&gt;左边的<span class=\"type\">Int</span>是说：函数f接收一个<span class=\"type\">Int</span>类型的参数，</div><div class=\"line\"></div><div class=\"line\">=&gt;右边的<span class=\"type\">Int</span>是说：函数f的返回值是<span class=\"type\">Int</span>类型的。</div><div class=\"line\"></div><div class=\"line\">好了，那么既然sum函数接收函数f作为一个参数，那么sum就可以利用f了，事实也是这样的，看sum</div><div class=\"line\"></div><div class=\"line\">函数的<span class=\"keyword\">else</span>语句就知道了：**f(a)**</div><div class=\"line\"></div><div class=\"line\">然后再看sumInt、sumCube和sumFac三个函数的定义：</div><div class=\"line\">```scala</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sumInt</span></span>(a:<span class=\"type\">Int</span>,b:<span class=\"type\">Int</span>):<span class=\"type\">Int</span> = sum(id,a,b)</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sumCube</span></span>(a:<span class=\"type\">Int</span>,b:<span class=\"type\">Int</span>):<span class=\"type\">Int</span> = sum(cube,a,b)</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sumFac</span></span>(a:<span class=\"type\">Int</span>,b:<span class=\"type\">Int</span>):<span class=\"type\">Int</span> = sum(fac,a,b)</div></pre></td></tr></table></figure></p>\n<p>以sumCube函数为例吧，它在定义的时候，把cube函数作为参数，传递给sum函数，</p>\n<p>而我们看cube函数的定义：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">cube</span></span>(a:<span class=\"type\">Int</span>):<span class=\"type\">Int</span>=a*a*a</div></pre></td></tr></table></figure></p>\n<p>发现，cube函数接收一个Int作为参数，并且返回一个Int，也就是说cube函数是符合sum函数的第一个</p>\n<p>参数:   f:Int=&gt;Int  的</p>\n<p>是不是很精彩呢？</p>\n<p>事实上上述代码还可以进一步简化，因为我们观察到id函数和cube函数的功能非常简单，不需要单独作为</p>\n<p>一个函数出现，进一步简化后的代码如下：<br><strong>code3</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">higherorderfuntion</span></span>&#123;</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sum</span></span>(f:<span class=\"type\">Int</span>=&gt;<span class=\"type\">Int</span>,a:<span class=\"type\">Int</span>,b:<span class=\"type\">Int</span>):<span class=\"type\">Int</span>=</div><div class=\"line\">     <span class=\"keyword\">if</span>(a&gt;b) <span class=\"number\">0</span></div><div class=\"line\">     <span class=\"keyword\">else</span> f(a)+sum(f,a+<span class=\"number\">1</span>,b)</div><div class=\"line\"></div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sumInt</span></span>(a:<span class=\"type\">Int</span>,b:<span class=\"type\">Int</span>):<span class=\"type\">Int</span> = sum(x=&gt;x,a,b)</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sumCube</span></span>(a:<span class=\"type\">Int</span>,b:<span class=\"type\">Int</span>):<span class=\"type\">Int</span> = sum(x=&gt;x*x*x,a,b)</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sumFac</span></span>(a:<span class=\"type\">Int</span>,b:<span class=\"type\">Int</span>):<span class=\"type\">Int</span> = sum(fac,a,b)</div><div class=\"line\"></div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">fac</span></span>(a:<span class=\"type\">Int</span>):<span class=\"type\">Int</span>=</div><div class=\"line\">     <span class=\"keyword\">if</span> (a==<span class=\"number\">0</span>) <span class=\"number\">1</span></div><div class=\"line\">     <span class=\"keyword\">else</span> a*fac(a<span class=\"number\">-1</span>)</div><div class=\"line\"></div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args:<span class=\"type\">Array</span>[<span class=\"type\">String</span>])=&#123;</div><div class=\"line\">     println(sumInt(<span class=\"number\">1</span>,<span class=\"number\">3</span>))</div><div class=\"line\">     println(sumCube(<span class=\"number\">1</span>,<span class=\"number\">3</span>))</div><div class=\"line\">     println(sumFac(<span class=\"number\">1</span>,<span class=\"number\">3</span>))</div><div class=\"line\">   &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>code3和code2相比，去掉了id函数和cube这两个函数，并且sumInt和sumCube函数的声明也发生了</p>\n<p>一点变化：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sumInt</span></span>(a:<span class=\"type\">Int</span>,b:<span class=\"type\">Int</span>):<span class=\"type\">Int</span> = sum(x=&gt;x,a,b)</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sumCube</span></span>(a:<span class=\"type\">Int</span>,b:<span class=\"type\">Int</span>):<span class=\"type\">Int</span> = sum(x=&gt;x*x*x,a,b)</div></pre></td></tr></table></figure></p>\n<p>像： x=&gt;x   和     x=&gt;x<em>x</em>x   这样的东西是什么呢？在函数式编程里面，这被叫做“function literal”,又称“匿名函数”，</p>\n<p>说白了，这也是函数的一种表达方式，只是这个函数没有名字罢了。</p>\n<p>code3其实也还有点小毛病，那就是sum函数和fac函数都不是“尾递归”，所以呢，把它们改成尾递归如下：</p>\n<p><strong>code4：</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div></pre></td><td class=\"code\"><pre><div class=\"line\">object higherorderfuntion&#123;</div><div class=\"line\">   def sum(f:Int=&gt;Int,a:Int,b:Int):Int=&#123;</div><div class=\"line\">     def loop(a:Int,acc:Int):Int=</div><div class=\"line\">       if(a&gt;b) acc</div><div class=\"line\">       else loop(a+1,f(a)+acc)</div><div class=\"line\">     loop(a,0)</div><div class=\"line\">   &#125;</div><div class=\"line\">   def sumInt(a:Int,b:Int):Int = sum(x=&gt;x,a,b)</div><div class=\"line\">   def sumCube(a:Int,b:Int):Int = sum(x=&gt;x*x*x,a,b)</div><div class=\"line\">   def sumFac(a:Int,b:Int):Int = sum(fac,a,b)</div><div class=\"line\"></div><div class=\"line\">   def fac(a:Int):Int=</div><div class=\"line\">   &#123;</div><div class=\"line\">     def loop(a:Int,acc:Int):Int=</div><div class=\"line\">       if(a==0) acc</div><div class=\"line\">       else loop(a-1,a*acc)</div><div class=\"line\">     loop(a,1)</div><div class=\"line\">   &#125;</div><div class=\"line\">   def main(args:Array[String])=&#123;</div><div class=\"line\">     println(sumInt(1,4))</div><div class=\"line\">     println(sumCube(1,4))</div><div class=\"line\">     println(sumFac(1,4))</div><div class=\"line\">   &#125;</div><div class=\"line\"></div><div class=\"line\">&#125;</div><div class=\"line\">```  </div><div class=\"line\"></div><div class=\"line\"># scala高阶函数下</div><div class=\"line\">在scala高阶函数上我们演示了如何把一个函数作为参数传递给另外一个函数。</div><div class=\"line\"></div><div class=\"line\">在本文里面，我们来演示函数式编程另外一个重要的特性：返回一个函数。首先来看这么一段代码：</div><div class=\"line\"></div><div class=\"line\">**code piece 1：**</div><div class=\"line\">```scala</div><div class=\"line\">def sum(f:Int=&gt;Int):(Int,Int)=&gt;Int=&#123;</div><div class=\"line\">       def sumF(a:Int,b:Int):Int=</div><div class=\"line\">         if(a&gt;b) 0</div><div class=\"line\">         else f(a)+sumF(a+1,b)</div><div class=\"line\">       sumF</div><div class=\"line\">   &#125;</div></pre></td></tr></table></figure></p>\n<p>一点点来看，f:Int=&gt;Int 是sum函数接收的参数，该参数是一个函数。 </p>\n<p>“:” 号后面的 (Int,Int) =&gt; Int 是sum函数的返回值，又(Int,Int) =&gt; Int是一个函数的类型：接收两个Int型的数，</p>\n<p>返回一个Int的数。也就是说调用sum函数，其返回的值是一个函数。  这一点对于已经习惯C、C++、Java等编程语言的</p>\n<p>程序员来说有一点难以理解。</p>\n<p>继续看例子吧，如果执行下面的一行代码会发生什么呢？<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">sum(x=&gt;x*x)(<span class=\"number\">1</span>,<span class=\"number\">4</span>)</div></pre></td></tr></table></figure></p>\n<p>结果会返回30。为什么是30呢？ 30= 1^2+ 2^2 + 3^2 + 4^2.</p>\n<p>首先，sum(x=&gt;x<em>x) 是一个函数，并且sum(x=&gt;x</em>x)的类型是(Int,Int)=&gt;Int，正因为sum(x=&gt;x*x)是一个函数，</p>\n<p>所以它才可以继续接收参数(1,4).</p>\n<p>好吧，我可能没有把这个事儿说清楚，实在是太抽象了。但是，读者应该有了一个基本的印象，那就是：</p>\n<p>在函数式编程里面，函数f可以接收函数g作为参数，也可以返回函数h。</p>\n<p>到这里还没完。。。。。。</p>\n<p>Scala里面可以继续对code piece1 进行简化，如下：</p>\n<p><strong>code piece2</strong>：<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">higherorderfuntion</span></span>&#123;</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sum</span></span>(f:<span class=\"type\">Int</span>=&gt;<span class=\"type\">Int</span>)(a:<span class=\"type\">Int</span>,b:<span class=\"type\">Int</span>):<span class=\"type\">Int</span>=&#123;</div><div class=\"line\">      <span class=\"keyword\">if</span>(a&gt;b) <span class=\"number\">0</span>  <span class=\"keyword\">else</span> f(a)+sum(f)(a+<span class=\"number\">1</span>,b)</div><div class=\"line\">   &#125;</div><div class=\"line\"></div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">fac</span></span>(a:<span class=\"type\">Int</span>):<span class=\"type\">Int</span>=</div><div class=\"line\">   &#123;</div><div class=\"line\">     <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">loop</span></span>(a:<span class=\"type\">Int</span>,acc:<span class=\"type\">Int</span>):<span class=\"type\">Int</span>=</div><div class=\"line\">       <span class=\"keyword\">if</span>(a==<span class=\"number\">0</span>) acc</div><div class=\"line\">       <span class=\"keyword\">else</span> loop(a<span class=\"number\">-1</span>,a*acc)</div><div class=\"line\">     loop(a,<span class=\"number\">1</span>)</div><div class=\"line\">   &#125;</div><div class=\"line\">   <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args:<span class=\"type\">Array</span>[<span class=\"type\">String</span>])=&#123;</div><div class=\"line\">     println(sum(x=&gt;x)(<span class=\"number\">1</span>,<span class=\"number\">4</span>))</div><div class=\"line\">     println(sum(x=&gt;x*x*x)(<span class=\"number\">1</span>,<span class=\"number\">4</span>))</div><div class=\"line\">     println(sum(fac)(<span class=\"number\">1</span>,<span class=\"number\">4</span>))</div><div class=\"line\">   &#125;</div><div class=\"line\"></div><div class=\"line\">&#125;</div><div class=\"line\">```  </div><div class=\"line\">这里我们需要特别注意，这里sum函数的参数列表变成了两个：(f:<span class=\"type\">Int</span>=&gt;<span class=\"type\">Int</span>)和(a:<span class=\"type\">Int</span>,b:<span class=\"type\">Int</span>)</div><div class=\"line\"></div><div class=\"line\">在scala里面，函数可以有多个参数列表。比如下面的函数定义：</div></pre></td></tr></table></figure></p>\n<p>def f(args_1)(args_2)…(args_n)=E<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">E代表一个函数体，且n&gt;1</div><div class=\"line\"></div><div class=\"line\">那么上述定义等价于：  </div><div class=\"line\">```scala</div><div class=\"line\">def f(args_1)(args_2)...(args_n-1)=&#123;def g(args_n)=E;g&#125;</div></pre></td></tr></table></figure></p>\n<p>还是以code piece2中的sum函数为例：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">def sum(f:Int=&gt;Int)(a:Int,b:Int):Int=&#123;</div><div class=\"line\">      if(a&gt;b) 0  else f(a)+sum(f)(a+1,b)</div><div class=\"line\">   &#125;</div></pre></td></tr></table></figure></p>\n<p>等价于：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">def sum(f:Int=&gt;Int):(Int,Int)=&gt;Int=&#123;</div><div class=\"line\">     def g(a:Int,b:Int):Int = </div><div class=\"line\">      if(a&gt;b) 0  else f(a)+sum(f)(a+1,b)</div><div class=\"line\">     g</div><div class=\"line\">   &#125;</div></pre></td></tr></table></figure></p>\n<h2 id=\"这个定义和code-piece1中的sum函数的定义是一致的。\"><a href=\"#这个定义和code-piece1中的sum函数的定义是一致的。\" class=\"headerlink\" title=\"这个定义和code piece1中的sum函数的定义是一致的。\"></a>这个定义和code piece1中的sum函数的定义是一致的。</h2><hr>\n<hr>\n<h1 id=\"扩展：-什么是尾递归\"><a href=\"#扩展：-什么是尾递归\" class=\"headerlink\" title=\"扩展： 什么是尾递归\"></a>扩展： 什么是尾递归</h1><p>参考<a href=\"https://www.zhihu.com/question/20761771/answer/19996299\" target=\"_blank\" rel=\"external\">https://www.zhihu.com/question/20761771/answer/19996299</a><br>这个虽然是python的，但是也能让我们理解什么是尾递归<br>尾递归和一般的递归不同在对内存的占用，普通递归创建stack累积而后计算收缩，尾递归只会占用恒量的内存（和迭代一样）。SICP中描述了一个内存占用曲线，用以上答案中的Python代码为例（普通递归）：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">recsum</span><span class=\"params\">(x)</span>:</span></div><div class=\"line\">  <span class=\"keyword\">if</span> x == <span class=\"number\">1</span>:</div><div class=\"line\">    <span class=\"keyword\">return</span> x</div><div class=\"line\">  <span class=\"keyword\">else</span>:</div><div class=\"line\">    <span class=\"keyword\">return</span> x + recsum(x - <span class=\"number\">1</span>)</div></pre></td></tr></table></figure></p>\n<p>当调用recsum(5)，Python调试器中发生如下状况：<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">recsum(<span class=\"number\">5</span>)</div><div class=\"line\"><span class=\"number\">5</span> + recsum(<span class=\"number\">4</span>)</div><div class=\"line\"><span class=\"number\">5</span> + (<span class=\"number\">4</span> + recsum(<span class=\"number\">3</span>))</div><div class=\"line\"><span class=\"number\">5</span> + (<span class=\"number\">4</span> + (<span class=\"number\">3</span> + recsum(<span class=\"number\">2</span>)))</div><div class=\"line\"><span class=\"number\">5</span> + (<span class=\"number\">4</span> + (<span class=\"number\">3</span> + (<span class=\"number\">2</span> + recsum(<span class=\"number\">1</span>))))</div><div class=\"line\"><span class=\"number\">5</span> + (<span class=\"number\">4</span> + (<span class=\"number\">3</span> + (<span class=\"number\">2</span> + <span class=\"number\">1</span>)))</div><div class=\"line\"><span class=\"number\">5</span> + (<span class=\"number\">4</span> + (<span class=\"number\">3</span> + <span class=\"number\">3</span>))</div><div class=\"line\"><span class=\"number\">5</span> + (<span class=\"number\">4</span> + <span class=\"number\">6</span>)</div><div class=\"line\"><span class=\"number\">5</span> + <span class=\"number\">10</span></div><div class=\"line\"><span class=\"number\">15</span></div></pre></td></tr></table></figure></p>\n<h2 id=\"这个曲线就代表内存占用大小的峰值，从左到右，达到顶峰，再从右到左收缩。而我们通常不希望这样的事情发生，所以使用迭代，只占据常量stack-space-更新这个栈！而非扩展他-。\"><a href=\"#这个曲线就代表内存占用大小的峰值，从左到右，达到顶峰，再从右到左收缩。而我们通常不希望这样的事情发生，所以使用迭代，只占据常量stack-space-更新这个栈！而非扩展他-。\" class=\"headerlink\" title=\"这个曲线就代表内存占用大小的峰值，从左到右，达到顶峰，再从右到左收缩。而我们通常不希望这样的事情发生，所以使用迭代，只占据常量stack space(更新这个栈！而非扩展他)。\"></a>这个曲线就代表内存占用大小的峰值，从左到右，达到顶峰，再从右到左收缩。而我们通常不希望这样的事情发生，所以使用迭代，只占据常量stack space(更新这个栈！而非扩展他)。</h2><p><strong>（一个替代方案：迭代）</strong><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">6</span>):</div><div class=\"line\">  sum += i</div></pre></td></tr></table></figure></p>\n<p>因为Python，Java，Pascal等等无法在语言中实现尾递归优化(Tail Call Optimization, TCO)，所以采用了for, while, goto等特殊结构代替recursive的表述。Scheme则不需要这样曲折地表达，一旦写成尾递归形式，就可以进行尾递归优化。———————Python中可以写（尾递归）：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">tailrecsum</span><span class=\"params\">(x, running_total=<span class=\"number\">0</span>)</span>:</span></div><div class=\"line\">  <span class=\"keyword\">if</span> x == <span class=\"number\">0</span>:</div><div class=\"line\">    <span class=\"keyword\">return</span> running_total</div><div class=\"line\">  <span class=\"keyword\">else</span>:</div><div class=\"line\">    <span class=\"keyword\">return</span> tailrecsum(x - <span class=\"number\">1</span>, running_total + x)</div></pre></td></tr></table></figure>\n<p>理论上类似上面：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">tailrecsum(5, 0)</div><div class=\"line\">tailrecsum(4, 5)</div><div class=\"line\">tailrecsum(3, 9)</div><div class=\"line\">tailrecsum(2, 12)</div><div class=\"line\">tailrecsum(1, 14)</div><div class=\"line\">tailrecsum(0, 15)</div><div class=\"line\">15</div></pre></td></tr></table></figure></p>\n<p>观察到，tailrecsum(x, y)中形式变量y的实际变量值是不断更新的，对比普通递归就很清楚，后者每个recsum()调用中y值不变，仅在层级上加深。所以，尾递归是把变化的参数传递给递归函数的变量了。怎么写尾递归？形式上只要最后一个return语句是单纯函数就可以。如：<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">return tailrec(x+1);</div><div class=\"line\">```  </div><div class=\"line\">而</div></pre></td></tr></table></figure></p>\n<p>return tailrec(x+1) + x;<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">则不可以，因为无法更新tailrec()函数内的实际变量，只是新建一个栈。但Python不能尾递归优化（Java不行，C可以，我不知道为什么），这里是用它做个例子。====================================如何优化尾递归：在编译器处理过程中生成中间代码（通常是三地址代码），用编译器优化。</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">另外一个的回答</div><div class=\"line\">尾递归就是操作的最后一步是调用自身的递归。</div><div class=\"line\"></div><div class=\"line\">这是尾递归：</div></pre></td></tr></table></figure></p>\n<p>function f(x) {<br>   if (x === 1) return 1;<br>   return f(x-1);<br>}<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">（这个程序没什么意义，仅作为理解辅助之用）。</div><div class=\"line\"></div><div class=\"line\">这不是尾递归：</div></pre></td></tr></table></figure></p>\n<p>function f(x) {<br>   if (x === 1) return 1;<br>   return 1 + f(x-1);<br>}<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">后者不是尾递归，是因为该函数的最后一步操作是用1加上f(x-1)的返回结果，因此，最后一步操作不是调用自身。注意，后面这段代码的递归也发生在函数末尾，但它不是尾递归。尾递归的判断标准是函数运行最后一步是否调用自身，而不是是否在函数的最后一行调用自身。因此阶乘n!的递归求法，尽管看起来递归发生在函数末尾，其实也不是尾递归：</div><div class=\"line\">```python</div><div class=\"line\">function factorial(n) &#123;</div><div class=\"line\">   if (n === 1) return 1;</div><div class=\"line\">   return n * factorial(n-1); // 最后一步不是调用自身，因此不是尾递归</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>使用尾递归可以带来一个好处：因为进入最后一步后不再需要参考外层函数（caller）的信息，因此没必要保存外层函数的stack，递归需要用的stack只有目前这层函数的，因此避免了栈溢出风险。一些语言提供了尾递归优化，当识别出使用了尾递归时，会相应地只保留当前层函数的stack，节省内存。所以，在没有尾递归优化的语言，如java, python中，鼓励用迭代iteration来改写尾递归；在有尾递归优化的语言如Erlang中，鼓励用尾递归来改写其他形式的递归。谢谢 @jamesr 的指正~关于如何改写，参考<a href=\"http://www.ruanyifeng.com/blog/2015/04/tail-call.html\" target=\"_blank\" rel=\"external\">：尾调用优化 - 阮一峰的网络日志</a><br>知乎尾递归参考:  <a href=\"https://www.zhihu.com/question/20761771\" target=\"_blank\" rel=\"external\">https://www.zhihu.com/question/20761771</a></p>\n"},{"title":"spark RDD算子（二） filter,map ,flatMap","date":"2017-03-02T13:25:21.000Z","author":"kaishun","id":"35","_content":"\n\n先来一张spark快速大数据中的图片进行快速入门，后面有更详细的例子  \n![](http://i4.buimg.com/567571/9bf09aff04368678.png)\n\n\n## **filter**\n举例，在F:\\sparktest\\sample.txt 文件的内容如下  \n```\naa bb cc aa aa aa dd dd ee ee ee ee \nff aa bb zks\nee kks\nee  zz zks\n```  \n我要将包含zks的行的内容给找出来\n**scala版本**\n```scala\n    val lines = sc.textFile(\"F:\\\\sparktest\\\\sample.txt\").filter(line=>line.contains(\"zks\"))\n    //打印内容\n    lines.collect().foreach(println(_));\n-------------输出------------------\nff aa bb zks\nee  zz zks\n\n```  \n**java版本**\n```java\n        JavaRDD<String> lines = sc.textFile(\"F:\\\\sparktest\\\\sample.txt\");\n        JavaRDD<String> zksRDD = lines.filter(new Function<String, Boolean>() {\n            @Override\n            public Boolean call(String s) throws Exception {\n                return s.contains(\"zks\");\n            }\n        });\n        //打印内容\n        List<String> zksCollect = zksRDD.collect();\n        for (String str:zksCollect) {\n            System.out.println(str);\n        }\n----------------输出-------------------\nff aa bb zks\nee  zz zks\n```  \n\n## **map**  \nmap() 接收一个函数，把这个函数用于 RDD 中的每个元素，将函数的返回结果作为结果RDD编程 ｜ 31\nRDD 中对应元素的值 map是一对一的关系\n举例，在F:\\sparktest\\sample.txt 文件的内容如下  \n```\naa bb cc aa aa aa dd dd ee ee ee ee \nff aa bb zks\nee kks\nee  zz zks\n```    \n把每一行变成一个数组\n**scala版本**\n```scala\n//读取数据\nscala> val lines = sc.textFile(\"F:\\\\sparktest\\\\sample.txt\")\n//用map，对于每一行数据，按照空格分割成一个一个数组，然后返回的是一对一的关系\nscala> var mapRDD = lines.map(line => line.split(\"\\\\s+\"))\n---------------输出-----------\nres0: Array[Array[String]] = Array(Array(aa, bb, cc, aa, aa, aa, dd, dd, ee, ee, ee, ee), Array(ff, aa, bb, zks), Array(ee, kks), Array(ee, zz, zks))\n\n//读取第一个元素\nscala> mapRDD.first\n---输出----\nres1: Array[String] = Array(aa, bb, cc, aa, aa, aa, dd, dd, ee, ee, ee, ee)\n```  \n**java版本**\n```java\n        JavaRDD<Iterable<String>> mapRDD = lines.map(new Function<String, Iterable<String>>() {\n            @Override\n            public Iterable<String> call(String s) throws Exception {\n                String[] split = s.split(\"\\\\s+\");\n                return Arrays.asList(split);\n            }\n        });\n        //读取第一个元素\n        System.out.println(mapRDD.first());\n    ---------------输出-------------\n    [aa, bb, cc, aa, aa, aa, dd, dd, ee, ee, ee, ee]\n```\n\n\n## **flatMap**  \n有时候，我们希望对某个元素生成多个元素，实现该功能的操作叫作 flatMap()  \nfaltMap的函数应用于每一个元素，对于每一个元素返回的是多个元素组成的迭代器(想要了解更多，请参考[scala的flatMap和map用法](http://blog.csdn.net/t1dmzks/article/details/69858060#t23))    \n例如我们将数据切分为单词  \n**scala版本**\n```scala\n    scala>  val lines = sc.textFile(\"F:\\\\sparktest\\\\sample.txt\")\n    scala> val flatMapRDD = lines.flatMap(line=>line.split(\"\\\\s\"))\n    scala> flatMapRDD.first() \n---输出----\nres0: String = aa\n```\n**java版本，spark2.0以下**\n```java\n    JavaRDD<String> lines = sc.textFile(\"F:\\\\sparktest\\\\sample.txt\");\n    JavaRDD<String> flatMapRDD = lines.flatMap(new FlatMapFunction<String, String>() {\n        @Override\n        public Iterable<String> call(String s) throws Exception {\n            String[] split = s.split(\"\\\\s+\");\n            return Arrays.asList(split);\n        }\n    });\n    //输出第一个\n    System.out.println(flatMapRDD.first());\n------------输出----------\naa\n```\n\n**java版本，spark2.0以上**\nspark2.0以上，对flatMap的方法有所修改，就是flatMap中的Iterator和Iteratable的小区别  \n```java\n        JavaRDD<String> flatMapRDD = lines.flatMap(new FlatMapFunction<String, String>() {\n            @Override\n            public Iterator<String> call(String s) throws Exception {\n                String[] split = s.split(\"\\\\s+\");\n                return Arrays.asList(split).iterator();\n            }\n        });\n```\n\n","source":"_posts/spark RDD算子（二） filter,map ,flatMap.md","raw":"---\ntitle: spark RDD算子（二） filter,map ,flatMap\ndate: 2017-03-02 21:25:21\ntags: [spark]\ncategories: [大数据,spark]\nauthor: kaishun\nid: 35\npermalink: spark-rdd-2\n---\n\n\n先来一张spark快速大数据中的图片进行快速入门，后面有更详细的例子  \n![](http://i4.buimg.com/567571/9bf09aff04368678.png)\n\n\n## **filter**\n举例，在F:\\sparktest\\sample.txt 文件的内容如下  \n```\naa bb cc aa aa aa dd dd ee ee ee ee \nff aa bb zks\nee kks\nee  zz zks\n```  \n我要将包含zks的行的内容给找出来\n**scala版本**\n```scala\n    val lines = sc.textFile(\"F:\\\\sparktest\\\\sample.txt\").filter(line=>line.contains(\"zks\"))\n    //打印内容\n    lines.collect().foreach(println(_));\n-------------输出------------------\nff aa bb zks\nee  zz zks\n\n```  \n**java版本**\n```java\n        JavaRDD<String> lines = sc.textFile(\"F:\\\\sparktest\\\\sample.txt\");\n        JavaRDD<String> zksRDD = lines.filter(new Function<String, Boolean>() {\n            @Override\n            public Boolean call(String s) throws Exception {\n                return s.contains(\"zks\");\n            }\n        });\n        //打印内容\n        List<String> zksCollect = zksRDD.collect();\n        for (String str:zksCollect) {\n            System.out.println(str);\n        }\n----------------输出-------------------\nff aa bb zks\nee  zz zks\n```  \n\n## **map**  \nmap() 接收一个函数，把这个函数用于 RDD 中的每个元素，将函数的返回结果作为结果RDD编程 ｜ 31\nRDD 中对应元素的值 map是一对一的关系\n举例，在F:\\sparktest\\sample.txt 文件的内容如下  \n```\naa bb cc aa aa aa dd dd ee ee ee ee \nff aa bb zks\nee kks\nee  zz zks\n```    \n把每一行变成一个数组\n**scala版本**\n```scala\n//读取数据\nscala> val lines = sc.textFile(\"F:\\\\sparktest\\\\sample.txt\")\n//用map，对于每一行数据，按照空格分割成一个一个数组，然后返回的是一对一的关系\nscala> var mapRDD = lines.map(line => line.split(\"\\\\s+\"))\n---------------输出-----------\nres0: Array[Array[String]] = Array(Array(aa, bb, cc, aa, aa, aa, dd, dd, ee, ee, ee, ee), Array(ff, aa, bb, zks), Array(ee, kks), Array(ee, zz, zks))\n\n//读取第一个元素\nscala> mapRDD.first\n---输出----\nres1: Array[String] = Array(aa, bb, cc, aa, aa, aa, dd, dd, ee, ee, ee, ee)\n```  \n**java版本**\n```java\n        JavaRDD<Iterable<String>> mapRDD = lines.map(new Function<String, Iterable<String>>() {\n            @Override\n            public Iterable<String> call(String s) throws Exception {\n                String[] split = s.split(\"\\\\s+\");\n                return Arrays.asList(split);\n            }\n        });\n        //读取第一个元素\n        System.out.println(mapRDD.first());\n    ---------------输出-------------\n    [aa, bb, cc, aa, aa, aa, dd, dd, ee, ee, ee, ee]\n```\n\n\n## **flatMap**  \n有时候，我们希望对某个元素生成多个元素，实现该功能的操作叫作 flatMap()  \nfaltMap的函数应用于每一个元素，对于每一个元素返回的是多个元素组成的迭代器(想要了解更多，请参考[scala的flatMap和map用法](http://blog.csdn.net/t1dmzks/article/details/69858060#t23))    \n例如我们将数据切分为单词  \n**scala版本**\n```scala\n    scala>  val lines = sc.textFile(\"F:\\\\sparktest\\\\sample.txt\")\n    scala> val flatMapRDD = lines.flatMap(line=>line.split(\"\\\\s\"))\n    scala> flatMapRDD.first() \n---输出----\nres0: String = aa\n```\n**java版本，spark2.0以下**\n```java\n    JavaRDD<String> lines = sc.textFile(\"F:\\\\sparktest\\\\sample.txt\");\n    JavaRDD<String> flatMapRDD = lines.flatMap(new FlatMapFunction<String, String>() {\n        @Override\n        public Iterable<String> call(String s) throws Exception {\n            String[] split = s.split(\"\\\\s+\");\n            return Arrays.asList(split);\n        }\n    });\n    //输出第一个\n    System.out.println(flatMapRDD.first());\n------------输出----------\naa\n```\n\n**java版本，spark2.0以上**\nspark2.0以上，对flatMap的方法有所修改，就是flatMap中的Iterator和Iteratable的小区别  \n```java\n        JavaRDD<String> flatMapRDD = lines.flatMap(new FlatMapFunction<String, String>() {\n            @Override\n            public Iterator<String> call(String s) throws Exception {\n                String[] split = s.split(\"\\\\s+\");\n                return Arrays.asList(split).iterator();\n            }\n        });\n```\n\n","slug":"spark-rdd-2","published":1,"updated":"2018-01-22T15:23:24.935Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcrpnzr4002v2wv3m8ffyugv","content":"<p>先来一张spark快速大数据中的图片进行快速入门，后面有更详细的例子<br><img src=\"http://i4.buimg.com/567571/9bf09aff04368678.png\" alt=\"\"></p>\n<h2 id=\"filter\"><a href=\"#filter\" class=\"headerlink\" title=\"filter\"></a><strong>filter</strong></h2><p>举例，在F:\\sparktest\\sample.txt 文件的内容如下<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div></pre></td><td class=\"code\"><pre><div class=\"line\">aa bb cc aa aa aa dd dd ee ee ee ee </div><div class=\"line\">ff aa bb zks</div><div class=\"line\">ee kks</div><div class=\"line\">ee  zz zks</div><div class=\"line\">```  </div><div class=\"line\">我要将包含zks的行的内容给找出来</div><div class=\"line\">**scala版本**</div><div class=\"line\">```scala</div><div class=\"line\">    val lines = sc.textFile(&quot;F:\\\\sparktest\\\\sample.txt&quot;).filter(line=&gt;line.contains(&quot;zks&quot;))</div><div class=\"line\">    //打印内容</div><div class=\"line\">    lines.collect().foreach(println(_));</div><div class=\"line\">-------------输出------------------</div><div class=\"line\">ff aa bb zks</div><div class=\"line\">ee  zz zks</div><div class=\"line\"></div><div class=\"line\">```  </div><div class=\"line\">**java版本**</div><div class=\"line\">```java</div><div class=\"line\">        JavaRDD&lt;String&gt; lines = sc.textFile(&quot;F:\\\\sparktest\\\\sample.txt&quot;);</div><div class=\"line\">        JavaRDD&lt;String&gt; zksRDD = lines.filter(new Function&lt;String, Boolean&gt;() &#123;</div><div class=\"line\">            @Override</div><div class=\"line\">            public Boolean call(String s) throws Exception &#123;</div><div class=\"line\">                return s.contains(&quot;zks&quot;);</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;);</div><div class=\"line\">        //打印内容</div><div class=\"line\">        List&lt;String&gt; zksCollect = zksRDD.collect();</div><div class=\"line\">        for (String str:zksCollect) &#123;</div><div class=\"line\">            System.out.println(str);</div><div class=\"line\">        &#125;</div><div class=\"line\">----------------输出-------------------</div><div class=\"line\">ff aa bb zks</div><div class=\"line\">ee  zz zks</div><div class=\"line\">```  </div><div class=\"line\"></div><div class=\"line\">## **map**  </div><div class=\"line\">map() 接收一个函数，把这个函数用于 RDD 中的每个元素，将函数的返回结果作为结果RDD编程 ｜ 31</div><div class=\"line\">RDD 中对应元素的值 map是一对一的关系</div><div class=\"line\">举例，在F:\\sparktest\\sample.txt 文件的内容如下</div></pre></td></tr></table></figure></p>\n<p>aa bb cc aa aa aa dd dd ee ee ee ee<br>ff aa bb zks<br>ee kks<br>ee  zz zks<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div></pre></td><td class=\"code\"><pre><div class=\"line\">把每一行变成一个数组</div><div class=\"line\">**scala版本**</div><div class=\"line\">```scala</div><div class=\"line\">//读取数据</div><div class=\"line\">scala&gt; val lines = sc.textFile(&quot;F:\\\\sparktest\\\\sample.txt&quot;)</div><div class=\"line\">//用map，对于每一行数据，按照空格分割成一个一个数组，然后返回的是一对一的关系</div><div class=\"line\">scala&gt; var mapRDD = lines.map(line =&gt; line.split(&quot;\\\\s+&quot;))</div><div class=\"line\">---------------输出-----------</div><div class=\"line\">res0: Array[Array[String]] = Array(Array(aa, bb, cc, aa, aa, aa, dd, dd, ee, ee, ee, ee), Array(ff, aa, bb, zks), Array(ee, kks), Array(ee, zz, zks))</div><div class=\"line\"></div><div class=\"line\">//读取第一个元素</div><div class=\"line\">scala&gt; mapRDD.first</div><div class=\"line\">---输出----</div><div class=\"line\">res1: Array[String] = Array(aa, bb, cc, aa, aa, aa, dd, dd, ee, ee, ee, ee)</div><div class=\"line\">```  </div><div class=\"line\">**java版本**</div><div class=\"line\">```java</div><div class=\"line\">        JavaRDD&lt;Iterable&lt;String&gt;&gt; mapRDD = lines.map(new Function&lt;String, Iterable&lt;String&gt;&gt;() &#123;</div><div class=\"line\">            @Override</div><div class=\"line\">            public Iterable&lt;String&gt; call(String s) throws Exception &#123;</div><div class=\"line\">                String[] split = s.split(&quot;\\\\s+&quot;);</div><div class=\"line\">                return Arrays.asList(split);</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;);</div><div class=\"line\">        //读取第一个元素</div><div class=\"line\">        System.out.println(mapRDD.first());</div><div class=\"line\">    ---------------输出-------------</div><div class=\"line\">    [aa, bb, cc, aa, aa, aa, dd, dd, ee, ee, ee, ee]</div></pre></td></tr></table></figure></p>\n<h2 id=\"flatMap\"><a href=\"#flatMap\" class=\"headerlink\" title=\"flatMap\"></a><strong>flatMap</strong></h2><p>有时候，我们希望对某个元素生成多个元素，实现该功能的操作叫作 flatMap()<br>faltMap的函数应用于每一个元素，对于每一个元素返回的是多个元素组成的迭代器(想要了解更多，请参考<a href=\"http://blog.csdn.net/t1dmzks/article/details/69858060#t23\" target=\"_blank\" rel=\"external\">scala的flatMap和map用法</a>)<br>例如我们将数据切分为单词<br><strong>scala版本</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">    scala&gt;  <span class=\"keyword\">val</span> lines = sc.textFile(<span class=\"string\">\"F:\\\\sparktest\\\\sample.txt\"</span>)</div><div class=\"line\">    scala&gt; <span class=\"keyword\">val</span> flatMapRDD = lines.flatMap(line=&gt;line.split(<span class=\"string\">\"\\\\s\"</span>))</div><div class=\"line\">    scala&gt; flatMapRDD.first() </div><div class=\"line\">---输出----</div><div class=\"line\">res0: <span class=\"type\">String</span> = aa</div></pre></td></tr></table></figure></p>\n<p><strong>java版本，spark2.0以下</strong><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\">    JavaRDD&lt;String&gt; lines = sc.textFile(<span class=\"string\">\"F:\\\\sparktest\\\\sample.txt\"</span>);</div><div class=\"line\">    JavaRDD&lt;String&gt; flatMapRDD = lines.flatMap(<span class=\"keyword\">new</span> FlatMapFunction&lt;String, String&gt;() &#123;</div><div class=\"line\">        <span class=\"meta\">@Override</span></div><div class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> Iterable&lt;String&gt; <span class=\"title\">call</span><span class=\"params\">(String s)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">            String[] split = s.split(<span class=\"string\">\"\\\\s+\"</span>);</div><div class=\"line\">            <span class=\"keyword\">return</span> Arrays.asList(split);</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;);</div><div class=\"line\">    <span class=\"comment\">//输出第一个</span></div><div class=\"line\">    System.out.println(flatMapRDD.first());</div><div class=\"line\">------------输出----------</div><div class=\"line\">aa</div></pre></td></tr></table></figure></p>\n<p><strong>java版本，spark2.0以上</strong><br>spark2.0以上，对flatMap的方法有所修改，就是flatMap中的Iterator和Iteratable的小区别<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">JavaRDD&lt;String&gt; flatMapRDD = lines.flatMap(<span class=\"keyword\">new</span> FlatMapFunction&lt;String, String&gt;() &#123;</div><div class=\"line\">    <span class=\"meta\">@Override</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> Iterator&lt;String&gt; <span class=\"title\">call</span><span class=\"params\">(String s)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">        String[] split = s.split(<span class=\"string\">\"\\\\s+\"</span>);</div><div class=\"line\">        <span class=\"keyword\">return</span> Arrays.asList(split).iterator();</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;);</div></pre></td></tr></table></figure></p>\n","site":{"data":{}},"excerpt":"","more":"<p>先来一张spark快速大数据中的图片进行快速入门，后面有更详细的例子<br><img src=\"http://i4.buimg.com/567571/9bf09aff04368678.png\" alt=\"\"></p>\n<h2 id=\"filter\"><a href=\"#filter\" class=\"headerlink\" title=\"filter\"></a><strong>filter</strong></h2><p>举例，在F:\\sparktest\\sample.txt 文件的内容如下<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div></pre></td><td class=\"code\"><pre><div class=\"line\">aa bb cc aa aa aa dd dd ee ee ee ee </div><div class=\"line\">ff aa bb zks</div><div class=\"line\">ee kks</div><div class=\"line\">ee  zz zks</div><div class=\"line\">```  </div><div class=\"line\">我要将包含zks的行的内容给找出来</div><div class=\"line\">**scala版本**</div><div class=\"line\">```scala</div><div class=\"line\">    val lines = sc.textFile(&quot;F:\\\\sparktest\\\\sample.txt&quot;).filter(line=&gt;line.contains(&quot;zks&quot;))</div><div class=\"line\">    //打印内容</div><div class=\"line\">    lines.collect().foreach(println(_));</div><div class=\"line\">-------------输出------------------</div><div class=\"line\">ff aa bb zks</div><div class=\"line\">ee  zz zks</div><div class=\"line\"></div><div class=\"line\">```  </div><div class=\"line\">**java版本**</div><div class=\"line\">```java</div><div class=\"line\">        JavaRDD&lt;String&gt; lines = sc.textFile(&quot;F:\\\\sparktest\\\\sample.txt&quot;);</div><div class=\"line\">        JavaRDD&lt;String&gt; zksRDD = lines.filter(new Function&lt;String, Boolean&gt;() &#123;</div><div class=\"line\">            @Override</div><div class=\"line\">            public Boolean call(String s) throws Exception &#123;</div><div class=\"line\">                return s.contains(&quot;zks&quot;);</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;);</div><div class=\"line\">        //打印内容</div><div class=\"line\">        List&lt;String&gt; zksCollect = zksRDD.collect();</div><div class=\"line\">        for (String str:zksCollect) &#123;</div><div class=\"line\">            System.out.println(str);</div><div class=\"line\">        &#125;</div><div class=\"line\">----------------输出-------------------</div><div class=\"line\">ff aa bb zks</div><div class=\"line\">ee  zz zks</div><div class=\"line\">```  </div><div class=\"line\"></div><div class=\"line\">## **map**  </div><div class=\"line\">map() 接收一个函数，把这个函数用于 RDD 中的每个元素，将函数的返回结果作为结果RDD编程 ｜ 31</div><div class=\"line\">RDD 中对应元素的值 map是一对一的关系</div><div class=\"line\">举例，在F:\\sparktest\\sample.txt 文件的内容如下</div></pre></td></tr></table></figure></p>\n<p>aa bb cc aa aa aa dd dd ee ee ee ee<br>ff aa bb zks<br>ee kks<br>ee  zz zks<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div></pre></td><td class=\"code\"><pre><div class=\"line\">把每一行变成一个数组</div><div class=\"line\">**scala版本**</div><div class=\"line\">```scala</div><div class=\"line\">//读取数据</div><div class=\"line\">scala&gt; val lines = sc.textFile(&quot;F:\\\\sparktest\\\\sample.txt&quot;)</div><div class=\"line\">//用map，对于每一行数据，按照空格分割成一个一个数组，然后返回的是一对一的关系</div><div class=\"line\">scala&gt; var mapRDD = lines.map(line =&gt; line.split(&quot;\\\\s+&quot;))</div><div class=\"line\">---------------输出-----------</div><div class=\"line\">res0: Array[Array[String]] = Array(Array(aa, bb, cc, aa, aa, aa, dd, dd, ee, ee, ee, ee), Array(ff, aa, bb, zks), Array(ee, kks), Array(ee, zz, zks))</div><div class=\"line\"></div><div class=\"line\">//读取第一个元素</div><div class=\"line\">scala&gt; mapRDD.first</div><div class=\"line\">---输出----</div><div class=\"line\">res1: Array[String] = Array(aa, bb, cc, aa, aa, aa, dd, dd, ee, ee, ee, ee)</div><div class=\"line\">```  </div><div class=\"line\">**java版本**</div><div class=\"line\">```java</div><div class=\"line\">        JavaRDD&lt;Iterable&lt;String&gt;&gt; mapRDD = lines.map(new Function&lt;String, Iterable&lt;String&gt;&gt;() &#123;</div><div class=\"line\">            @Override</div><div class=\"line\">            public Iterable&lt;String&gt; call(String s) throws Exception &#123;</div><div class=\"line\">                String[] split = s.split(&quot;\\\\s+&quot;);</div><div class=\"line\">                return Arrays.asList(split);</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;);</div><div class=\"line\">        //读取第一个元素</div><div class=\"line\">        System.out.println(mapRDD.first());</div><div class=\"line\">    ---------------输出-------------</div><div class=\"line\">    [aa, bb, cc, aa, aa, aa, dd, dd, ee, ee, ee, ee]</div></pre></td></tr></table></figure></p>\n<h2 id=\"flatMap\"><a href=\"#flatMap\" class=\"headerlink\" title=\"flatMap\"></a><strong>flatMap</strong></h2><p>有时候，我们希望对某个元素生成多个元素，实现该功能的操作叫作 flatMap()<br>faltMap的函数应用于每一个元素，对于每一个元素返回的是多个元素组成的迭代器(想要了解更多，请参考<a href=\"http://blog.csdn.net/t1dmzks/article/details/69858060#t23\" target=\"_blank\" rel=\"external\">scala的flatMap和map用法</a>)<br>例如我们将数据切分为单词<br><strong>scala版本</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">    scala&gt;  <span class=\"keyword\">val</span> lines = sc.textFile(<span class=\"string\">\"F:\\\\sparktest\\\\sample.txt\"</span>)</div><div class=\"line\">    scala&gt; <span class=\"keyword\">val</span> flatMapRDD = lines.flatMap(line=&gt;line.split(<span class=\"string\">\"\\\\s\"</span>))</div><div class=\"line\">    scala&gt; flatMapRDD.first() </div><div class=\"line\">---输出----</div><div class=\"line\">res0: <span class=\"type\">String</span> = aa</div></pre></td></tr></table></figure></p>\n<p><strong>java版本，spark2.0以下</strong><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\">    JavaRDD&lt;String&gt; lines = sc.textFile(<span class=\"string\">\"F:\\\\sparktest\\\\sample.txt\"</span>);</div><div class=\"line\">    JavaRDD&lt;String&gt; flatMapRDD = lines.flatMap(<span class=\"keyword\">new</span> FlatMapFunction&lt;String, String&gt;() &#123;</div><div class=\"line\">        <span class=\"meta\">@Override</span></div><div class=\"line\">        <span class=\"function\"><span class=\"keyword\">public</span> Iterable&lt;String&gt; <span class=\"title\">call</span><span class=\"params\">(String s)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">            String[] split = s.split(<span class=\"string\">\"\\\\s+\"</span>);</div><div class=\"line\">            <span class=\"keyword\">return</span> Arrays.asList(split);</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;);</div><div class=\"line\">    <span class=\"comment\">//输出第一个</span></div><div class=\"line\">    System.out.println(flatMapRDD.first());</div><div class=\"line\">------------输出----------</div><div class=\"line\">aa</div></pre></td></tr></table></figure></p>\n<p><strong>java版本，spark2.0以上</strong><br>spark2.0以上，对flatMap的方法有所修改，就是flatMap中的Iterator和Iteratable的小区别<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">JavaRDD&lt;String&gt; flatMapRDD = lines.flatMap(<span class=\"keyword\">new</span> FlatMapFunction&lt;String, String&gt;() &#123;</div><div class=\"line\">    <span class=\"meta\">@Override</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> Iterator&lt;String&gt; <span class=\"title\">call</span><span class=\"params\">(String s)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">        String[] split = s.split(<span class=\"string\">\"\\\\s+\"</span>);</div><div class=\"line\">        <span class=\"keyword\">return</span> Arrays.asList(split).iterator();</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;);</div></pre></td></tr></table></figure></p>\n"},{"title":"spark RDD算子（五）之键值对聚合操作 combineByKey","date":"2017-03-05T13:25:21.000Z","author":"kaishun","id":"38","_content":"\n# **combineByKey**\n聚合数据一般在集中式数据比较方便，如果涉及到分布式的数据集，该如何去实现呢。这里介绍一下combineByKey, 这个是各种聚集操作的鼻祖，应该要好好了解一下,[参考scala API](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions)  \n## **简要介绍**\n```scala\ndef combineByKey[C](createCombiner: (V) => C,  \n                    mergeValue: (C, V) => C,   \n                    mergeCombiners: (C, C) => C): RD\n```\n- **createCombiner**: combineByKey() 会遍历分区中的所有元素，因此每个元素的键要么还没有遇到过，要么就\n和之前的某个元素的键相同。如果这是一个新的元素， combineByKey() 会使用一个叫作 createCombiner() 的函数来创建\n那个键对应的累加器的初始值\n-  **mergeValue:** 如果这是一个在处理当前分区之前已经遇到的键， 它会使用 mergeValue() 方法将该键的累加器对应的当前值与这个新的值进行合并  \n-  **mergeCombiners:** 由于每个分区都是独立处理的， 因此对于同一个键可以有多个累加器。如果有两个或者更\n多的分区都有对应同一个键的累加器， 就需要使用用户提供的 mergeCombiners() 方法将各\n个分区的结果进行合并。\n\n## **计算学生平均成绩例子**\n\n这里举一个计算学生平均成绩的例子,例子参考至https://www.edureka.co/blog/apache-spark-combinebykey-explained, [github源码](https://github.com/prithvirajbose/spark-dev/blob/master/src/main/scala/examples/TestCombineByKey.scala)  我对此进行了解析\n**创建一个学生成绩说明的类**\n```scala\ncase class ScoreDetail(studentName: String, subject: String, score: Float)\n```\n**下面是一些测试数据**，加载测试数据集合  key = Students name and value = ScoreDetail instance\n```scala\n    val scores = List(\n      ScoreDetail(\"xiaoming\", \"Math\", 98),\n      ScoreDetail(\"xiaoming\", \"English\", 88),\n      ScoreDetail(\"wangwu\", \"Math\", 75),\n      ScoreDetail(\"wangwu\", \"English\", 78),\n      ScoreDetail(\"lihua\", \"Math\", 90),\n      ScoreDetail(\"lihua\", \"English\", 80),\n      ScoreDetail(\"zhangsan\", \"Math\", 91),\n      ScoreDetail(\"zhangsan\", \"English\", 80))\n```\n**将集合转换成二元组**， 也可以理解成转换成一个map, 利用了for 和 yield的组合\n```scala\nval scoresWithKey = for { i <- scores } yield (i.studentName, i)\n```\n**创建RDD, 并且指定三个分区**\n```scala\n    val scoresWithKeyRDD = sc.parallelize(scoresWithKey).partitionBy(new HashPartitioner(3)).cache\n```\n**输出打印一下各个分区的长度和各个分区的一些数据**\n```scala\n    println(\">>>> Elements in each partition\")\n\n    scoresWithKeyRDD.foreachPartition(partition => println(partition.length))\n\n    // explore each partition...\n    println(\">>>> Exploring partitions' data...\")\n\n    scoresWithKeyRDD.foreachPartition(\n      partition => partition.foreach(\n        item => println(item._2)))\n/*\n会输出 \n>>>> Elements in each partition\n6\n2\n0\n>>>> Exploring partitions' data...\nScoreDetail(xiaoming,Math,98.0)\nScoreDetail(xiaoming,English,88.0)\nScoreDetail(lihua,Math,90.0)\nScoreDetail(lihua,English,80.0)\nScoreDetail(zhangsan,Math,91.0)\nScoreDetail(zhangsan,English,80.0)\nScoreDetail(wangwu,Math,75.0)\nScoreDetail(wangwu,English,78.0)\n\n*/\n```\n**聚合求平均值让后打印**\n```scala\n      val avgScoresRDD = scoresWithKeyRDD.combineByKey(\n      (x: ScoreDetail) => (x.score, 1) /*createCombiner*/,\n      (acc: (Float, Int), x: ScoreDetail) => (acc._1 + x.score, acc._2 + 1) /*mergeValue*/,\n      (acc1: (Float, Int), acc2: (Float, Int)) => (acc1._1 + acc2._1, acc1._2 + acc2._2) /*mergeCombiners*/\n      // calculate the average\n    ).map( { case(key, value) => (key, value._1/value._2) })\n\n    avgScoresRDD.collect.foreach(println)\n/*输出:\n(zhangsan,85.5)\n(lihua,85.0)\n(xiaoming,93.0)\n(wangwu,76.5)\n*/\n```   \n**解释一下scoresWithKeyRDD.combineByKey**  \n**createCombiner:**  (x: ScoreDetail) => (x.score, 1)  \n这是第一次遇到zhangsan，创建一个函数，把map中的value转成另外一个类型   ，这里是把(zhangsan,(ScoreDetail类))**转换成**(zhangsan,(91,1))  \n**mergeValue:** (acc: (Float, Int), x: ScoreDetail) => (acc._1 + x.score, acc._2 + 1)  再次碰到张三， 就把这两个合并, 这里是将(zhangsan,(91,1)) 这种类型 和 (zhangsan,(ScoreDetail类))这种类型合并，合并成了(zhangsan,(171,2))  \n**mergeCombiners** (acc1: (Float, Int), acc2: (Float, Int))  这个是将多个分区中的zhangsan的数据进行合并， 我们这里zhansan在同一个分区，这个地方就没有用上  \n\n## java版本的介绍\n**ScoreDetail类**\n```java\n\npublic class ScoreDetail implements Serializable{\n    //case class ScoreDetail(studentName: String, subject: String, score: Float)\n    public String studentName;\n    public String subject;\n    public float score;\n\n    public ScoreDetail(String studentName, String subject, float score) {\n        this.studentName = studentName;\n        this.subject = subject;\n        this.score = score;\n    }\n}\n```\nCombineByKey的测试类\n```java\npublic class CombineTest {\n    public static void main(String[] args) {\n        SparkConf sparkConf = new SparkConf().setAppName(\"JavaWordCount\").setMaster(\"local\");\n        JavaSparkContext sc = new JavaSparkContext(sparkConf);\n        ArrayList<ScoreDetail> scoreDetails = new ArrayList<>();\n        scoreDetails.add(new ScoreDetail(\"xiaoming\", \"Math\", 98));\n        scoreDetails.add(new ScoreDetail(\"xiaoming\", \"English\", 88));\n        scoreDetails.add(new ScoreDetail(\"wangwu\", \"Math\", 75));\n        scoreDetails.add(new ScoreDetail(\"wangwu\", \"Englist\", 78));\n        scoreDetails.add(new ScoreDetail(\"lihua\", \"Math\", 90));\n        scoreDetails.add(new ScoreDetail(\"lihua\", \"English\", 80));\n        scoreDetails.add(new ScoreDetail(\"zhangsan\", \"Math\", 91));\n        scoreDetails.add(new ScoreDetail(\"zhangsan\", \"English\", 80));\n\n        JavaRDD<ScoreDetail> scoreDetailsRDD = sc.parallelize(scoreDetails);\n\n        JavaPairRDD<String, ScoreDetail> pairRDD = scoreDetailsRDD.mapToPair(new PairFunction<ScoreDetail, String, ScoreDetail>() {\n            @Override\n            public Tuple2<String, ScoreDetail> call(ScoreDetail scoreDetail) throws Exception {\n\n                return new Tuple2<>(scoreDetail.studentName, scoreDetail);\n            }\n        });\n//        new Function<ScoreDetail, Float,Integer>();\n\n        Function<ScoreDetail, Tuple2<Float, Integer>> createCombine = new Function<ScoreDetail, Tuple2<Float, Integer>>() {\n            @Override\n            public Tuple2<Float, Integer> call(ScoreDetail scoreDetail) throws Exception {\n                return new Tuple2<>(scoreDetail.score, 1);\n            }\n        };\n\n        // Function2传入两个值，返回一个值\n        Function2<Tuple2<Float, Integer>, ScoreDetail, Tuple2<Float, Integer>> mergeValue = new Function2<Tuple2<Float, Integer>, ScoreDetail, Tuple2<Float, Integer>>() {\n            @Override\n            public Tuple2<Float, Integer> call(Tuple2<Float, Integer> tp, ScoreDetail scoreDetail) throws Exception {\n                return new Tuple2<>(tp._1 + scoreDetail.score, tp._2 + 1);\n            }\n        };\n        Function2<Tuple2<Float, Integer>, Tuple2<Float, Integer>, Tuple2<Float, Integer>> mergeCombiners = new Function2<Tuple2<Float, Integer>, Tuple2<Float, Integer>, Tuple2<Float, Integer>>() {\n            @Override\n            public Tuple2<Float, Integer> call(Tuple2<Float, Integer> tp1, Tuple2<Float, Integer> tp2) throws Exception {\n                return new Tuple2<>(tp1._1 + tp2._1, tp1._2 + tp2._2);\n            }\n        };\n        JavaPairRDD<String, Tuple2<Float,Integer>> combineByRDD  = pairRDD.combineByKey(createCombine,mergeValue,mergeCombiners);\n\n        //打印平均数\n        Map<String, Tuple2<Float, Integer>> stringTuple2Map = combineByRDD.collectAsMap();\n        for ( String et:stringTuple2Map.keySet()) {\n            System.out.println(et+\" \"+stringTuple2Map.get(et)._1/stringTuple2Map.get(et)._2);\n        }\n    }\n}\n```\n**注意有个坑的地方**  createCombine方法必须是这样的\n```java\n        Function<ScoreDetail, Tuple2<Float, Integer>> createCombine = new Function<ScoreDetail, Tuple2<Float, Integer>>() {\n            @Override\n            public Tuple2<Float, Integer> call(ScoreDetail scoreDetail) throws Exception {\n                return new Tuple2<>(scoreDetail.score, 1);\n            }\n        };\n```\n**而不能是这样的**, 即使最后的RDD都类似\n```java\n        PairFunction<ScoreDetail, Float, Integer> createCombine = new PairFunction<ScoreDetail, Float, Integer>() {\n            @Override\n            public Tuple2<Float, Integer> call(ScoreDetail scoreDetail) throws Exception {\n                return new Tuple2<>(scoreDetail.score, 1);\n            }\n        };\n```\n\n再推荐几篇比较好的文章 [LXW的大数据田地: combineByKey](http://lxw1234.com/archives/2015/07/358.htm)  \n 推荐图书快速大数据分析中combineByKey的讲解\n","source":"_posts/spark RDD算子（五）之键值对聚合操作 combineByKey.md","raw":"---\ntitle: spark RDD算子（五）之键值对聚合操作 combineByKey\ndate: 2017-03-05 21:25:21\ntags: [spark]\ncategories: [大数据,spark]\nauthor: kaishun\nid: 38\npermalink: spark-rdd-5\n---\n\n# **combineByKey**\n聚合数据一般在集中式数据比较方便，如果涉及到分布式的数据集，该如何去实现呢。这里介绍一下combineByKey, 这个是各种聚集操作的鼻祖，应该要好好了解一下,[参考scala API](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions)  \n## **简要介绍**\n```scala\ndef combineByKey[C](createCombiner: (V) => C,  \n                    mergeValue: (C, V) => C,   \n                    mergeCombiners: (C, C) => C): RD\n```\n- **createCombiner**: combineByKey() 会遍历分区中的所有元素，因此每个元素的键要么还没有遇到过，要么就\n和之前的某个元素的键相同。如果这是一个新的元素， combineByKey() 会使用一个叫作 createCombiner() 的函数来创建\n那个键对应的累加器的初始值\n-  **mergeValue:** 如果这是一个在处理当前分区之前已经遇到的键， 它会使用 mergeValue() 方法将该键的累加器对应的当前值与这个新的值进行合并  \n-  **mergeCombiners:** 由于每个分区都是独立处理的， 因此对于同一个键可以有多个累加器。如果有两个或者更\n多的分区都有对应同一个键的累加器， 就需要使用用户提供的 mergeCombiners() 方法将各\n个分区的结果进行合并。\n\n## **计算学生平均成绩例子**\n\n这里举一个计算学生平均成绩的例子,例子参考至https://www.edureka.co/blog/apache-spark-combinebykey-explained, [github源码](https://github.com/prithvirajbose/spark-dev/blob/master/src/main/scala/examples/TestCombineByKey.scala)  我对此进行了解析\n**创建一个学生成绩说明的类**\n```scala\ncase class ScoreDetail(studentName: String, subject: String, score: Float)\n```\n**下面是一些测试数据**，加载测试数据集合  key = Students name and value = ScoreDetail instance\n```scala\n    val scores = List(\n      ScoreDetail(\"xiaoming\", \"Math\", 98),\n      ScoreDetail(\"xiaoming\", \"English\", 88),\n      ScoreDetail(\"wangwu\", \"Math\", 75),\n      ScoreDetail(\"wangwu\", \"English\", 78),\n      ScoreDetail(\"lihua\", \"Math\", 90),\n      ScoreDetail(\"lihua\", \"English\", 80),\n      ScoreDetail(\"zhangsan\", \"Math\", 91),\n      ScoreDetail(\"zhangsan\", \"English\", 80))\n```\n**将集合转换成二元组**， 也可以理解成转换成一个map, 利用了for 和 yield的组合\n```scala\nval scoresWithKey = for { i <- scores } yield (i.studentName, i)\n```\n**创建RDD, 并且指定三个分区**\n```scala\n    val scoresWithKeyRDD = sc.parallelize(scoresWithKey).partitionBy(new HashPartitioner(3)).cache\n```\n**输出打印一下各个分区的长度和各个分区的一些数据**\n```scala\n    println(\">>>> Elements in each partition\")\n\n    scoresWithKeyRDD.foreachPartition(partition => println(partition.length))\n\n    // explore each partition...\n    println(\">>>> Exploring partitions' data...\")\n\n    scoresWithKeyRDD.foreachPartition(\n      partition => partition.foreach(\n        item => println(item._2)))\n/*\n会输出 \n>>>> Elements in each partition\n6\n2\n0\n>>>> Exploring partitions' data...\nScoreDetail(xiaoming,Math,98.0)\nScoreDetail(xiaoming,English,88.0)\nScoreDetail(lihua,Math,90.0)\nScoreDetail(lihua,English,80.0)\nScoreDetail(zhangsan,Math,91.0)\nScoreDetail(zhangsan,English,80.0)\nScoreDetail(wangwu,Math,75.0)\nScoreDetail(wangwu,English,78.0)\n\n*/\n```\n**聚合求平均值让后打印**\n```scala\n      val avgScoresRDD = scoresWithKeyRDD.combineByKey(\n      (x: ScoreDetail) => (x.score, 1) /*createCombiner*/,\n      (acc: (Float, Int), x: ScoreDetail) => (acc._1 + x.score, acc._2 + 1) /*mergeValue*/,\n      (acc1: (Float, Int), acc2: (Float, Int)) => (acc1._1 + acc2._1, acc1._2 + acc2._2) /*mergeCombiners*/\n      // calculate the average\n    ).map( { case(key, value) => (key, value._1/value._2) })\n\n    avgScoresRDD.collect.foreach(println)\n/*输出:\n(zhangsan,85.5)\n(lihua,85.0)\n(xiaoming,93.0)\n(wangwu,76.5)\n*/\n```   \n**解释一下scoresWithKeyRDD.combineByKey**  \n**createCombiner:**  (x: ScoreDetail) => (x.score, 1)  \n这是第一次遇到zhangsan，创建一个函数，把map中的value转成另外一个类型   ，这里是把(zhangsan,(ScoreDetail类))**转换成**(zhangsan,(91,1))  \n**mergeValue:** (acc: (Float, Int), x: ScoreDetail) => (acc._1 + x.score, acc._2 + 1)  再次碰到张三， 就把这两个合并, 这里是将(zhangsan,(91,1)) 这种类型 和 (zhangsan,(ScoreDetail类))这种类型合并，合并成了(zhangsan,(171,2))  \n**mergeCombiners** (acc1: (Float, Int), acc2: (Float, Int))  这个是将多个分区中的zhangsan的数据进行合并， 我们这里zhansan在同一个分区，这个地方就没有用上  \n\n## java版本的介绍\n**ScoreDetail类**\n```java\n\npublic class ScoreDetail implements Serializable{\n    //case class ScoreDetail(studentName: String, subject: String, score: Float)\n    public String studentName;\n    public String subject;\n    public float score;\n\n    public ScoreDetail(String studentName, String subject, float score) {\n        this.studentName = studentName;\n        this.subject = subject;\n        this.score = score;\n    }\n}\n```\nCombineByKey的测试类\n```java\npublic class CombineTest {\n    public static void main(String[] args) {\n        SparkConf sparkConf = new SparkConf().setAppName(\"JavaWordCount\").setMaster(\"local\");\n        JavaSparkContext sc = new JavaSparkContext(sparkConf);\n        ArrayList<ScoreDetail> scoreDetails = new ArrayList<>();\n        scoreDetails.add(new ScoreDetail(\"xiaoming\", \"Math\", 98));\n        scoreDetails.add(new ScoreDetail(\"xiaoming\", \"English\", 88));\n        scoreDetails.add(new ScoreDetail(\"wangwu\", \"Math\", 75));\n        scoreDetails.add(new ScoreDetail(\"wangwu\", \"Englist\", 78));\n        scoreDetails.add(new ScoreDetail(\"lihua\", \"Math\", 90));\n        scoreDetails.add(new ScoreDetail(\"lihua\", \"English\", 80));\n        scoreDetails.add(new ScoreDetail(\"zhangsan\", \"Math\", 91));\n        scoreDetails.add(new ScoreDetail(\"zhangsan\", \"English\", 80));\n\n        JavaRDD<ScoreDetail> scoreDetailsRDD = sc.parallelize(scoreDetails);\n\n        JavaPairRDD<String, ScoreDetail> pairRDD = scoreDetailsRDD.mapToPair(new PairFunction<ScoreDetail, String, ScoreDetail>() {\n            @Override\n            public Tuple2<String, ScoreDetail> call(ScoreDetail scoreDetail) throws Exception {\n\n                return new Tuple2<>(scoreDetail.studentName, scoreDetail);\n            }\n        });\n//        new Function<ScoreDetail, Float,Integer>();\n\n        Function<ScoreDetail, Tuple2<Float, Integer>> createCombine = new Function<ScoreDetail, Tuple2<Float, Integer>>() {\n            @Override\n            public Tuple2<Float, Integer> call(ScoreDetail scoreDetail) throws Exception {\n                return new Tuple2<>(scoreDetail.score, 1);\n            }\n        };\n\n        // Function2传入两个值，返回一个值\n        Function2<Tuple2<Float, Integer>, ScoreDetail, Tuple2<Float, Integer>> mergeValue = new Function2<Tuple2<Float, Integer>, ScoreDetail, Tuple2<Float, Integer>>() {\n            @Override\n            public Tuple2<Float, Integer> call(Tuple2<Float, Integer> tp, ScoreDetail scoreDetail) throws Exception {\n                return new Tuple2<>(tp._1 + scoreDetail.score, tp._2 + 1);\n            }\n        };\n        Function2<Tuple2<Float, Integer>, Tuple2<Float, Integer>, Tuple2<Float, Integer>> mergeCombiners = new Function2<Tuple2<Float, Integer>, Tuple2<Float, Integer>, Tuple2<Float, Integer>>() {\n            @Override\n            public Tuple2<Float, Integer> call(Tuple2<Float, Integer> tp1, Tuple2<Float, Integer> tp2) throws Exception {\n                return new Tuple2<>(tp1._1 + tp2._1, tp1._2 + tp2._2);\n            }\n        };\n        JavaPairRDD<String, Tuple2<Float,Integer>> combineByRDD  = pairRDD.combineByKey(createCombine,mergeValue,mergeCombiners);\n\n        //打印平均数\n        Map<String, Tuple2<Float, Integer>> stringTuple2Map = combineByRDD.collectAsMap();\n        for ( String et:stringTuple2Map.keySet()) {\n            System.out.println(et+\" \"+stringTuple2Map.get(et)._1/stringTuple2Map.get(et)._2);\n        }\n    }\n}\n```\n**注意有个坑的地方**  createCombine方法必须是这样的\n```java\n        Function<ScoreDetail, Tuple2<Float, Integer>> createCombine = new Function<ScoreDetail, Tuple2<Float, Integer>>() {\n            @Override\n            public Tuple2<Float, Integer> call(ScoreDetail scoreDetail) throws Exception {\n                return new Tuple2<>(scoreDetail.score, 1);\n            }\n        };\n```\n**而不能是这样的**, 即使最后的RDD都类似\n```java\n        PairFunction<ScoreDetail, Float, Integer> createCombine = new PairFunction<ScoreDetail, Float, Integer>() {\n            @Override\n            public Tuple2<Float, Integer> call(ScoreDetail scoreDetail) throws Exception {\n                return new Tuple2<>(scoreDetail.score, 1);\n            }\n        };\n```\n\n再推荐几篇比较好的文章 [LXW的大数据田地: combineByKey](http://lxw1234.com/archives/2015/07/358.htm)  \n 推荐图书快速大数据分析中combineByKey的讲解\n","slug":"spark-rdd-5","published":1,"updated":"2018-01-22T15:25:27.593Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcrpnzr4002z2wv3ier8eqtg","content":"<h1 id=\"combineByKey\"><a href=\"#combineByKey\" class=\"headerlink\" title=\"combineByKey\"></a><strong>combineByKey</strong></h1><p>聚合数据一般在集中式数据比较方便，如果涉及到分布式的数据集，该如何去实现呢。这里介绍一下combineByKey, 这个是各种聚集操作的鼻祖，应该要好好了解一下,<a href=\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions\" target=\"_blank\" rel=\"external\">参考scala API</a>  </p>\n<h2 id=\"简要介绍\"><a href=\"#简要介绍\" class=\"headerlink\" title=\"简要介绍\"></a><strong>简要介绍</strong></h2><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">combineByKey</span></span>[<span class=\"type\">C</span>](createCombiner: (<span class=\"type\">V</span>) =&gt; <span class=\"type\">C</span>,  </div><div class=\"line\">                    mergeValue: (<span class=\"type\">C</span>, <span class=\"type\">V</span>) =&gt; <span class=\"type\">C</span>,   </div><div class=\"line\">                    mergeCombiners: (<span class=\"type\">C</span>, <span class=\"type\">C</span>) =&gt; <span class=\"type\">C</span>): <span class=\"type\">RD</span></div></pre></td></tr></table></figure>\n<ul>\n<li><strong>createCombiner</strong>: combineByKey() 会遍历分区中的所有元素，因此每个元素的键要么还没有遇到过，要么就<br>和之前的某个元素的键相同。如果这是一个新的元素， combineByKey() 会使用一个叫作 createCombiner() 的函数来创建<br>那个键对应的累加器的初始值</li>\n<li><strong>mergeValue:</strong> 如果这是一个在处理当前分区之前已经遇到的键， 它会使用 mergeValue() 方法将该键的累加器对应的当前值与这个新的值进行合并  </li>\n<li><strong>mergeCombiners:</strong> 由于每个分区都是独立处理的， 因此对于同一个键可以有多个累加器。如果有两个或者更<br>多的分区都有对应同一个键的累加器， 就需要使用用户提供的 mergeCombiners() 方法将各<br>个分区的结果进行合并。</li>\n</ul>\n<h2 id=\"计算学生平均成绩例子\"><a href=\"#计算学生平均成绩例子\" class=\"headerlink\" title=\"计算学生平均成绩例子\"></a><strong>计算学生平均成绩例子</strong></h2><p>这里举一个计算学生平均成绩的例子,例子参考至<a href=\"https://www.edureka.co/blog/apache-spark-combinebykey-explained\" target=\"_blank\" rel=\"external\">https://www.edureka.co/blog/apache-spark-combinebykey-explained</a>, <a href=\"https://github.com/prithvirajbose/spark-dev/blob/master/src/main/scala/examples/TestCombineByKey.scala\" target=\"_blank\" rel=\"external\">github源码</a>  我对此进行了解析<br><strong>创建一个学生成绩说明的类</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">case</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ScoreDetail</span>(<span class=\"params\">studentName: <span class=\"type\">String</span>, subject: <span class=\"type\">String</span>, score: <span class=\"type\">Float</span></span>)</span></div></pre></td></tr></table></figure></p>\n<p><strong>下面是一些测试数据</strong>，加载测试数据集合  key = Students name and value = ScoreDetail instance<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">val</span> scores = <span class=\"type\">List</span>(</div><div class=\"line\">  <span class=\"type\">ScoreDetail</span>(<span class=\"string\">\"xiaoming\"</span>, <span class=\"string\">\"Math\"</span>, <span class=\"number\">98</span>),</div><div class=\"line\">  <span class=\"type\">ScoreDetail</span>(<span class=\"string\">\"xiaoming\"</span>, <span class=\"string\">\"English\"</span>, <span class=\"number\">88</span>),</div><div class=\"line\">  <span class=\"type\">ScoreDetail</span>(<span class=\"string\">\"wangwu\"</span>, <span class=\"string\">\"Math\"</span>, <span class=\"number\">75</span>),</div><div class=\"line\">  <span class=\"type\">ScoreDetail</span>(<span class=\"string\">\"wangwu\"</span>, <span class=\"string\">\"English\"</span>, <span class=\"number\">78</span>),</div><div class=\"line\">  <span class=\"type\">ScoreDetail</span>(<span class=\"string\">\"lihua\"</span>, <span class=\"string\">\"Math\"</span>, <span class=\"number\">90</span>),</div><div class=\"line\">  <span class=\"type\">ScoreDetail</span>(<span class=\"string\">\"lihua\"</span>, <span class=\"string\">\"English\"</span>, <span class=\"number\">80</span>),</div><div class=\"line\">  <span class=\"type\">ScoreDetail</span>(<span class=\"string\">\"zhangsan\"</span>, <span class=\"string\">\"Math\"</span>, <span class=\"number\">91</span>),</div><div class=\"line\">  <span class=\"type\">ScoreDetail</span>(<span class=\"string\">\"zhangsan\"</span>, <span class=\"string\">\"English\"</span>, <span class=\"number\">80</span>))</div></pre></td></tr></table></figure></p>\n<p><strong>将集合转换成二元组</strong>， 也可以理解成转换成一个map, 利用了for 和 yield的组合<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">val</span> scoresWithKey = <span class=\"keyword\">for</span> &#123; i &lt;- scores &#125; <span class=\"keyword\">yield</span> (i.studentName, i)</div></pre></td></tr></table></figure></p>\n<p><strong>创建RDD, 并且指定三个分区</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">val</span> scoresWithKeyRDD = sc.parallelize(scoresWithKey).partitionBy(<span class=\"keyword\">new</span> <span class=\"type\">HashPartitioner</span>(<span class=\"number\">3</span>)).cache</div></pre></td></tr></table></figure></p>\n<p><strong>输出打印一下各个分区的长度和各个分区的一些数据</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div></pre></td><td class=\"code\"><pre><div class=\"line\">    println(<span class=\"string\">\"&gt;&gt;&gt;&gt; Elements in each partition\"</span>)</div><div class=\"line\"></div><div class=\"line\">    scoresWithKeyRDD.foreachPartition(partition =&gt; println(partition.length))</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">// explore each partition...</span></div><div class=\"line\">    println(<span class=\"string\">\"&gt;&gt;&gt;&gt; Exploring partitions' data...\"</span>)</div><div class=\"line\"></div><div class=\"line\">    scoresWithKeyRDD.foreachPartition(</div><div class=\"line\">      partition =&gt; partition.foreach(</div><div class=\"line\">        item =&gt; println(item._2)))</div><div class=\"line\"><span class=\"comment\">/*</span></div><div class=\"line\"><span class=\"comment\">会输出 </span></div><div class=\"line\"><span class=\"comment\">&gt;&gt;&gt;&gt; Elements in each partition</span></div><div class=\"line\"><span class=\"comment\">6</span></div><div class=\"line\"><span class=\"comment\">2</span></div><div class=\"line\"><span class=\"comment\">0</span></div><div class=\"line\"><span class=\"comment\">&gt;&gt;&gt;&gt; Exploring partitions' data...</span></div><div class=\"line\"><span class=\"comment\">ScoreDetail(xiaoming,Math,98.0)</span></div><div class=\"line\"><span class=\"comment\">ScoreDetail(xiaoming,English,88.0)</span></div><div class=\"line\"><span class=\"comment\">ScoreDetail(lihua,Math,90.0)</span></div><div class=\"line\"><span class=\"comment\">ScoreDetail(lihua,English,80.0)</span></div><div class=\"line\"><span class=\"comment\">ScoreDetail(zhangsan,Math,91.0)</span></div><div class=\"line\"><span class=\"comment\">ScoreDetail(zhangsan,English,80.0)</span></div><div class=\"line\"><span class=\"comment\">ScoreDetail(wangwu,Math,75.0)</span></div><div class=\"line\"><span class=\"comment\">ScoreDetail(wangwu,English,78.0)</span></div><div class=\"line\"><span class=\"comment\"></span></div><div class=\"line\"><span class=\"comment\">*/</span></div></pre></td></tr></table></figure></p>\n<p><strong>聚合求平均值让后打印</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div></pre></td><td class=\"code\"><pre><div class=\"line\">      <span class=\"keyword\">val</span> avgScoresRDD = scoresWithKeyRDD.combineByKey(</div><div class=\"line\">      (x: <span class=\"type\">ScoreDetail</span>) =&gt; (x.score, <span class=\"number\">1</span>) <span class=\"comment\">/*createCombiner*/</span>,</div><div class=\"line\">      (acc: (<span class=\"type\">Float</span>, <span class=\"type\">Int</span>), x: <span class=\"type\">ScoreDetail</span>) =&gt; (acc._1 + x.score, acc._2 + <span class=\"number\">1</span>) <span class=\"comment\">/*mergeValue*/</span>,</div><div class=\"line\">      (acc1: (<span class=\"type\">Float</span>, <span class=\"type\">Int</span>), acc2: (<span class=\"type\">Float</span>, <span class=\"type\">Int</span>)) =&gt; (acc1._1 + acc2._1, acc1._2 + acc2._2) <span class=\"comment\">/*mergeCombiners*/</span></div><div class=\"line\">      <span class=\"comment\">// calculate the average</span></div><div class=\"line\">    ).map( &#123; <span class=\"keyword\">case</span>(key, value) =&gt; (key, value._1/value._2) &#125;)</div><div class=\"line\"></div><div class=\"line\">    avgScoresRDD.collect.foreach(println)</div><div class=\"line\"><span class=\"comment\">/*输出:</span></div><div class=\"line\"><span class=\"comment\">(zhangsan,85.5)</span></div><div class=\"line\"><span class=\"comment\">(lihua,85.0)</span></div><div class=\"line\"><span class=\"comment\">(xiaoming,93.0)</span></div><div class=\"line\"><span class=\"comment\">(wangwu,76.5)</span></div><div class=\"line\"><span class=\"comment\">*/</span></div><div class=\"line\">```   </div><div class=\"line\">**解释一下scoresWithKeyRDD.combineByKey**  </div><div class=\"line\">**createCombiner:**  (x: <span class=\"type\">ScoreDetail</span>) =&gt; (x.score, <span class=\"number\">1</span>)  </div><div class=\"line\">这是第一次遇到zhangsan，创建一个函数，把map中的value转成另外一个类型   ，这里是把(zhangsan,(<span class=\"type\">ScoreDetail</span>类))**转换成**(zhangsan,(<span class=\"number\">91</span>,<span class=\"number\">1</span>))  </div><div class=\"line\">**mergeValue:** (acc: (<span class=\"type\">Float</span>, <span class=\"type\">Int</span>), x: <span class=\"type\">ScoreDetail</span>) =&gt; (acc._1 + x.score, acc._2 + <span class=\"number\">1</span>)  再次碰到张三， 就把这两个合并, 这里是将(zhangsan,(<span class=\"number\">91</span>,<span class=\"number\">1</span>)) 这种类型 和 (zhangsan,(<span class=\"type\">ScoreDetail</span>类))这种类型合并，合并成了(zhangsan,(<span class=\"number\">171</span>,<span class=\"number\">2</span>))  </div><div class=\"line\">**mergeCombiners** (acc1: (<span class=\"type\">Float</span>, <span class=\"type\">Int</span>), acc2: (<span class=\"type\">Float</span>, <span class=\"type\">Int</span>))  这个是将多个分区中的zhangsan的数据进行合并， 我们这里zhansan在同一个分区，这个地方就没有用上  </div><div class=\"line\"></div><div class=\"line\">## java版本的介绍</div><div class=\"line\">**<span class=\"type\">ScoreDetail</span>类**</div><div class=\"line\">```java</div><div class=\"line\"></div><div class=\"line\">public <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ScoreDetail</span> <span class=\"title\">implements</span> <span class=\"title\">Serializable</span></span>&#123;</div><div class=\"line\">    <span class=\"comment\">//case class ScoreDetail(studentName: String, subject: String, score: Float)</span></div><div class=\"line\">    public <span class=\"type\">String</span> studentName;</div><div class=\"line\">    public <span class=\"type\">String</span> subject;</div><div class=\"line\">    public float score;</div><div class=\"line\"></div><div class=\"line\">    public <span class=\"type\">ScoreDetail</span>(<span class=\"type\">String</span> studentName, <span class=\"type\">String</span> subject, float score) &#123;</div><div class=\"line\">        <span class=\"keyword\">this</span>.studentName = studentName;</div><div class=\"line\">        <span class=\"keyword\">this</span>.subject = subject;</div><div class=\"line\">        <span class=\"keyword\">this</span>.score = score;</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>CombineByKey的测试类<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">CombineTest</span> </span>&#123;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> </span>&#123;</div><div class=\"line\">        SparkConf sparkConf = <span class=\"keyword\">new</span> SparkConf().setAppName(<span class=\"string\">\"JavaWordCount\"</span>).setMaster(<span class=\"string\">\"local\"</span>);</div><div class=\"line\">        JavaSparkContext sc = <span class=\"keyword\">new</span> JavaSparkContext(sparkConf);</div><div class=\"line\">        ArrayList&lt;ScoreDetail&gt; scoreDetails = <span class=\"keyword\">new</span> ArrayList&lt;&gt;();</div><div class=\"line\">        scoreDetails.add(<span class=\"keyword\">new</span> ScoreDetail(<span class=\"string\">\"xiaoming\"</span>, <span class=\"string\">\"Math\"</span>, <span class=\"number\">98</span>));</div><div class=\"line\">        scoreDetails.add(<span class=\"keyword\">new</span> ScoreDetail(<span class=\"string\">\"xiaoming\"</span>, <span class=\"string\">\"English\"</span>, <span class=\"number\">88</span>));</div><div class=\"line\">        scoreDetails.add(<span class=\"keyword\">new</span> ScoreDetail(<span class=\"string\">\"wangwu\"</span>, <span class=\"string\">\"Math\"</span>, <span class=\"number\">75</span>));</div><div class=\"line\">        scoreDetails.add(<span class=\"keyword\">new</span> ScoreDetail(<span class=\"string\">\"wangwu\"</span>, <span class=\"string\">\"Englist\"</span>, <span class=\"number\">78</span>));</div><div class=\"line\">        scoreDetails.add(<span class=\"keyword\">new</span> ScoreDetail(<span class=\"string\">\"lihua\"</span>, <span class=\"string\">\"Math\"</span>, <span class=\"number\">90</span>));</div><div class=\"line\">        scoreDetails.add(<span class=\"keyword\">new</span> ScoreDetail(<span class=\"string\">\"lihua\"</span>, <span class=\"string\">\"English\"</span>, <span class=\"number\">80</span>));</div><div class=\"line\">        scoreDetails.add(<span class=\"keyword\">new</span> ScoreDetail(<span class=\"string\">\"zhangsan\"</span>, <span class=\"string\">\"Math\"</span>, <span class=\"number\">91</span>));</div><div class=\"line\">        scoreDetails.add(<span class=\"keyword\">new</span> ScoreDetail(<span class=\"string\">\"zhangsan\"</span>, <span class=\"string\">\"English\"</span>, <span class=\"number\">80</span>));</div><div class=\"line\"></div><div class=\"line\">        JavaRDD&lt;ScoreDetail&gt; scoreDetailsRDD = sc.parallelize(scoreDetails);</div><div class=\"line\"></div><div class=\"line\">        JavaPairRDD&lt;String, ScoreDetail&gt; pairRDD = scoreDetailsRDD.mapToPair(<span class=\"keyword\">new</span> PairFunction&lt;ScoreDetail, String, ScoreDetail&gt;() &#123;</div><div class=\"line\">            <span class=\"meta\">@Override</span></div><div class=\"line\">            <span class=\"function\"><span class=\"keyword\">public</span> Tuple2&lt;String, ScoreDetail&gt; <span class=\"title\">call</span><span class=\"params\">(ScoreDetail scoreDetail)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\"></div><div class=\"line\">                <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> Tuple2&lt;&gt;(scoreDetail.studentName, scoreDetail);</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;);</div><div class=\"line\"><span class=\"comment\">//        new Function&lt;ScoreDetail, Float,Integer&gt;();</span></div><div class=\"line\"></div><div class=\"line\">        Function&lt;ScoreDetail, Tuple2&lt;Float, Integer&gt;&gt; createCombine = <span class=\"keyword\">new</span> Function&lt;ScoreDetail, Tuple2&lt;Float, Integer&gt;&gt;() &#123;</div><div class=\"line\">            <span class=\"meta\">@Override</span></div><div class=\"line\">            <span class=\"function\"><span class=\"keyword\">public</span> Tuple2&lt;Float, Integer&gt; <span class=\"title\">call</span><span class=\"params\">(ScoreDetail scoreDetail)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">                <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> Tuple2&lt;&gt;(scoreDetail.score, <span class=\"number\">1</span>);</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;;</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\">// Function2传入两个值，返回一个值</span></div><div class=\"line\">        Function2&lt;Tuple2&lt;Float, Integer&gt;, ScoreDetail, Tuple2&lt;Float, Integer&gt;&gt; mergeValue = <span class=\"keyword\">new</span> Function2&lt;Tuple2&lt;Float, Integer&gt;, ScoreDetail, Tuple2&lt;Float, Integer&gt;&gt;() &#123;</div><div class=\"line\">            <span class=\"meta\">@Override</span></div><div class=\"line\">            <span class=\"function\"><span class=\"keyword\">public</span> Tuple2&lt;Float, Integer&gt; <span class=\"title\">call</span><span class=\"params\">(Tuple2&lt;Float, Integer&gt; tp, ScoreDetail scoreDetail)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">                <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> Tuple2&lt;&gt;(tp._1 + scoreDetail.score, tp._2 + <span class=\"number\">1</span>);</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;;</div><div class=\"line\">        Function2&lt;Tuple2&lt;Float, Integer&gt;, Tuple2&lt;Float, Integer&gt;, Tuple2&lt;Float, Integer&gt;&gt; mergeCombiners = <span class=\"keyword\">new</span> Function2&lt;Tuple2&lt;Float, Integer&gt;, Tuple2&lt;Float, Integer&gt;, Tuple2&lt;Float, Integer&gt;&gt;() &#123;</div><div class=\"line\">            <span class=\"meta\">@Override</span></div><div class=\"line\">            <span class=\"function\"><span class=\"keyword\">public</span> Tuple2&lt;Float, Integer&gt; <span class=\"title\">call</span><span class=\"params\">(Tuple2&lt;Float, Integer&gt; tp1, Tuple2&lt;Float, Integer&gt; tp2)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">                <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> Tuple2&lt;&gt;(tp1._1 + tp2._1, tp1._2 + tp2._2);</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;;</div><div class=\"line\">        JavaPairRDD&lt;String, Tuple2&lt;Float,Integer&gt;&gt; combineByRDD  = pairRDD.combineByKey(createCombine,mergeValue,mergeCombiners);</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\">//打印平均数</span></div><div class=\"line\">        Map&lt;String, Tuple2&lt;Float, Integer&gt;&gt; stringTuple2Map = combineByRDD.collectAsMap();</div><div class=\"line\">        <span class=\"keyword\">for</span> ( String et:stringTuple2Map.keySet()) &#123;</div><div class=\"line\">            System.out.println(et+<span class=\"string\">\" \"</span>+stringTuple2Map.get(et)._1/stringTuple2Map.get(et)._2);</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p><strong>注意有个坑的地方</strong>  createCombine方法必须是这样的<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">Function&lt;ScoreDetail, Tuple2&lt;Float, Integer&gt;&gt; createCombine = <span class=\"keyword\">new</span> Function&lt;ScoreDetail, Tuple2&lt;Float, Integer&gt;&gt;() &#123;</div><div class=\"line\">    <span class=\"meta\">@Override</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> Tuple2&lt;Float, Integer&gt; <span class=\"title\">call</span><span class=\"params\">(ScoreDetail scoreDetail)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> Tuple2&lt;&gt;(scoreDetail.score, <span class=\"number\">1</span>);</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure></p>\n<p><strong>而不能是这样的</strong>, 即使最后的RDD都类似<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">PairFunction&lt;ScoreDetail, Float, Integer&gt; createCombine = <span class=\"keyword\">new</span> PairFunction&lt;ScoreDetail, Float, Integer&gt;() &#123;</div><div class=\"line\">    <span class=\"meta\">@Override</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> Tuple2&lt;Float, Integer&gt; <span class=\"title\">call</span><span class=\"params\">(ScoreDetail scoreDetail)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> Tuple2&lt;&gt;(scoreDetail.score, <span class=\"number\">1</span>);</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure></p>\n<p>再推荐几篇比较好的文章 <a href=\"http://lxw1234.com/archives/2015/07/358.htm\" target=\"_blank\" rel=\"external\">LXW的大数据田地: combineByKey</a><br> 推荐图书快速大数据分析中combineByKey的讲解</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"combineByKey\"><a href=\"#combineByKey\" class=\"headerlink\" title=\"combineByKey\"></a><strong>combineByKey</strong></h1><p>聚合数据一般在集中式数据比较方便，如果涉及到分布式的数据集，该如何去实现呢。这里介绍一下combineByKey, 这个是各种聚集操作的鼻祖，应该要好好了解一下,<a href=\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions\" target=\"_blank\" rel=\"external\">参考scala API</a>  </p>\n<h2 id=\"简要介绍\"><a href=\"#简要介绍\" class=\"headerlink\" title=\"简要介绍\"></a><strong>简要介绍</strong></h2><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">combineByKey</span></span>[<span class=\"type\">C</span>](createCombiner: (<span class=\"type\">V</span>) =&gt; <span class=\"type\">C</span>,  </div><div class=\"line\">                    mergeValue: (<span class=\"type\">C</span>, <span class=\"type\">V</span>) =&gt; <span class=\"type\">C</span>,   </div><div class=\"line\">                    mergeCombiners: (<span class=\"type\">C</span>, <span class=\"type\">C</span>) =&gt; <span class=\"type\">C</span>): <span class=\"type\">RD</span></div></pre></td></tr></table></figure>\n<ul>\n<li><strong>createCombiner</strong>: combineByKey() 会遍历分区中的所有元素，因此每个元素的键要么还没有遇到过，要么就<br>和之前的某个元素的键相同。如果这是一个新的元素， combineByKey() 会使用一个叫作 createCombiner() 的函数来创建<br>那个键对应的累加器的初始值</li>\n<li><strong>mergeValue:</strong> 如果这是一个在处理当前分区之前已经遇到的键， 它会使用 mergeValue() 方法将该键的累加器对应的当前值与这个新的值进行合并  </li>\n<li><strong>mergeCombiners:</strong> 由于每个分区都是独立处理的， 因此对于同一个键可以有多个累加器。如果有两个或者更<br>多的分区都有对应同一个键的累加器， 就需要使用用户提供的 mergeCombiners() 方法将各<br>个分区的结果进行合并。</li>\n</ul>\n<h2 id=\"计算学生平均成绩例子\"><a href=\"#计算学生平均成绩例子\" class=\"headerlink\" title=\"计算学生平均成绩例子\"></a><strong>计算学生平均成绩例子</strong></h2><p>这里举一个计算学生平均成绩的例子,例子参考至<a href=\"https://www.edureka.co/blog/apache-spark-combinebykey-explained\" target=\"_blank\" rel=\"external\">https://www.edureka.co/blog/apache-spark-combinebykey-explained</a>, <a href=\"https://github.com/prithvirajbose/spark-dev/blob/master/src/main/scala/examples/TestCombineByKey.scala\" target=\"_blank\" rel=\"external\">github源码</a>  我对此进行了解析<br><strong>创建一个学生成绩说明的类</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">case</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ScoreDetail</span>(<span class=\"params\">studentName: <span class=\"type\">String</span>, subject: <span class=\"type\">String</span>, score: <span class=\"type\">Float</span></span>)</span></div></pre></td></tr></table></figure></p>\n<p><strong>下面是一些测试数据</strong>，加载测试数据集合  key = Students name and value = ScoreDetail instance<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">val</span> scores = <span class=\"type\">List</span>(</div><div class=\"line\">  <span class=\"type\">ScoreDetail</span>(<span class=\"string\">\"xiaoming\"</span>, <span class=\"string\">\"Math\"</span>, <span class=\"number\">98</span>),</div><div class=\"line\">  <span class=\"type\">ScoreDetail</span>(<span class=\"string\">\"xiaoming\"</span>, <span class=\"string\">\"English\"</span>, <span class=\"number\">88</span>),</div><div class=\"line\">  <span class=\"type\">ScoreDetail</span>(<span class=\"string\">\"wangwu\"</span>, <span class=\"string\">\"Math\"</span>, <span class=\"number\">75</span>),</div><div class=\"line\">  <span class=\"type\">ScoreDetail</span>(<span class=\"string\">\"wangwu\"</span>, <span class=\"string\">\"English\"</span>, <span class=\"number\">78</span>),</div><div class=\"line\">  <span class=\"type\">ScoreDetail</span>(<span class=\"string\">\"lihua\"</span>, <span class=\"string\">\"Math\"</span>, <span class=\"number\">90</span>),</div><div class=\"line\">  <span class=\"type\">ScoreDetail</span>(<span class=\"string\">\"lihua\"</span>, <span class=\"string\">\"English\"</span>, <span class=\"number\">80</span>),</div><div class=\"line\">  <span class=\"type\">ScoreDetail</span>(<span class=\"string\">\"zhangsan\"</span>, <span class=\"string\">\"Math\"</span>, <span class=\"number\">91</span>),</div><div class=\"line\">  <span class=\"type\">ScoreDetail</span>(<span class=\"string\">\"zhangsan\"</span>, <span class=\"string\">\"English\"</span>, <span class=\"number\">80</span>))</div></pre></td></tr></table></figure></p>\n<p><strong>将集合转换成二元组</strong>， 也可以理解成转换成一个map, 利用了for 和 yield的组合<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">val</span> scoresWithKey = <span class=\"keyword\">for</span> &#123; i &lt;- scores &#125; <span class=\"keyword\">yield</span> (i.studentName, i)</div></pre></td></tr></table></figure></p>\n<p><strong>创建RDD, 并且指定三个分区</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">val</span> scoresWithKeyRDD = sc.parallelize(scoresWithKey).partitionBy(<span class=\"keyword\">new</span> <span class=\"type\">HashPartitioner</span>(<span class=\"number\">3</span>)).cache</div></pre></td></tr></table></figure></p>\n<p><strong>输出打印一下各个分区的长度和各个分区的一些数据</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div></pre></td><td class=\"code\"><pre><div class=\"line\">    println(<span class=\"string\">\"&gt;&gt;&gt;&gt; Elements in each partition\"</span>)</div><div class=\"line\"></div><div class=\"line\">    scoresWithKeyRDD.foreachPartition(partition =&gt; println(partition.length))</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\">// explore each partition...</span></div><div class=\"line\">    println(<span class=\"string\">\"&gt;&gt;&gt;&gt; Exploring partitions' data...\"</span>)</div><div class=\"line\"></div><div class=\"line\">    scoresWithKeyRDD.foreachPartition(</div><div class=\"line\">      partition =&gt; partition.foreach(</div><div class=\"line\">        item =&gt; println(item._2)))</div><div class=\"line\"><span class=\"comment\">/*</span></div><div class=\"line\"><span class=\"comment\">会输出 </span></div><div class=\"line\"><span class=\"comment\">&gt;&gt;&gt;&gt; Elements in each partition</span></div><div class=\"line\"><span class=\"comment\">6</span></div><div class=\"line\"><span class=\"comment\">2</span></div><div class=\"line\"><span class=\"comment\">0</span></div><div class=\"line\"><span class=\"comment\">&gt;&gt;&gt;&gt; Exploring partitions' data...</span></div><div class=\"line\"><span class=\"comment\">ScoreDetail(xiaoming,Math,98.0)</span></div><div class=\"line\"><span class=\"comment\">ScoreDetail(xiaoming,English,88.0)</span></div><div class=\"line\"><span class=\"comment\">ScoreDetail(lihua,Math,90.0)</span></div><div class=\"line\"><span class=\"comment\">ScoreDetail(lihua,English,80.0)</span></div><div class=\"line\"><span class=\"comment\">ScoreDetail(zhangsan,Math,91.0)</span></div><div class=\"line\"><span class=\"comment\">ScoreDetail(zhangsan,English,80.0)</span></div><div class=\"line\"><span class=\"comment\">ScoreDetail(wangwu,Math,75.0)</span></div><div class=\"line\"><span class=\"comment\">ScoreDetail(wangwu,English,78.0)</span></div><div class=\"line\"><span class=\"comment\"></span></div><div class=\"line\"><span class=\"comment\">*/</span></div></pre></td></tr></table></figure></p>\n<p><strong>聚合求平均值让后打印</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div></pre></td><td class=\"code\"><pre><div class=\"line\">      <span class=\"keyword\">val</span> avgScoresRDD = scoresWithKeyRDD.combineByKey(</div><div class=\"line\">      (x: <span class=\"type\">ScoreDetail</span>) =&gt; (x.score, <span class=\"number\">1</span>) <span class=\"comment\">/*createCombiner*/</span>,</div><div class=\"line\">      (acc: (<span class=\"type\">Float</span>, <span class=\"type\">Int</span>), x: <span class=\"type\">ScoreDetail</span>) =&gt; (acc._1 + x.score, acc._2 + <span class=\"number\">1</span>) <span class=\"comment\">/*mergeValue*/</span>,</div><div class=\"line\">      (acc1: (<span class=\"type\">Float</span>, <span class=\"type\">Int</span>), acc2: (<span class=\"type\">Float</span>, <span class=\"type\">Int</span>)) =&gt; (acc1._1 + acc2._1, acc1._2 + acc2._2) <span class=\"comment\">/*mergeCombiners*/</span></div><div class=\"line\">      <span class=\"comment\">// calculate the average</span></div><div class=\"line\">    ).map( &#123; <span class=\"keyword\">case</span>(key, value) =&gt; (key, value._1/value._2) &#125;)</div><div class=\"line\"></div><div class=\"line\">    avgScoresRDD.collect.foreach(println)</div><div class=\"line\"><span class=\"comment\">/*输出:</span></div><div class=\"line\"><span class=\"comment\">(zhangsan,85.5)</span></div><div class=\"line\"><span class=\"comment\">(lihua,85.0)</span></div><div class=\"line\"><span class=\"comment\">(xiaoming,93.0)</span></div><div class=\"line\"><span class=\"comment\">(wangwu,76.5)</span></div><div class=\"line\"><span class=\"comment\">*/</span></div><div class=\"line\">```   </div><div class=\"line\">**解释一下scoresWithKeyRDD.combineByKey**  </div><div class=\"line\">**createCombiner:**  (x: <span class=\"type\">ScoreDetail</span>) =&gt; (x.score, <span class=\"number\">1</span>)  </div><div class=\"line\">这是第一次遇到zhangsan，创建一个函数，把map中的value转成另外一个类型   ，这里是把(zhangsan,(<span class=\"type\">ScoreDetail</span>类))**转换成**(zhangsan,(<span class=\"number\">91</span>,<span class=\"number\">1</span>))  </div><div class=\"line\">**mergeValue:** (acc: (<span class=\"type\">Float</span>, <span class=\"type\">Int</span>), x: <span class=\"type\">ScoreDetail</span>) =&gt; (acc._1 + x.score, acc._2 + <span class=\"number\">1</span>)  再次碰到张三， 就把这两个合并, 这里是将(zhangsan,(<span class=\"number\">91</span>,<span class=\"number\">1</span>)) 这种类型 和 (zhangsan,(<span class=\"type\">ScoreDetail</span>类))这种类型合并，合并成了(zhangsan,(<span class=\"number\">171</span>,<span class=\"number\">2</span>))  </div><div class=\"line\">**mergeCombiners** (acc1: (<span class=\"type\">Float</span>, <span class=\"type\">Int</span>), acc2: (<span class=\"type\">Float</span>, <span class=\"type\">Int</span>))  这个是将多个分区中的zhangsan的数据进行合并， 我们这里zhansan在同一个分区，这个地方就没有用上  </div><div class=\"line\"></div><div class=\"line\">## java版本的介绍</div><div class=\"line\">**<span class=\"type\">ScoreDetail</span>类**</div><div class=\"line\">```java</div><div class=\"line\"></div><div class=\"line\">public <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">ScoreDetail</span> <span class=\"title\">implements</span> <span class=\"title\">Serializable</span></span>&#123;</div><div class=\"line\">    <span class=\"comment\">//case class ScoreDetail(studentName: String, subject: String, score: Float)</span></div><div class=\"line\">    public <span class=\"type\">String</span> studentName;</div><div class=\"line\">    public <span class=\"type\">String</span> subject;</div><div class=\"line\">    public float score;</div><div class=\"line\"></div><div class=\"line\">    public <span class=\"type\">ScoreDetail</span>(<span class=\"type\">String</span> studentName, <span class=\"type\">String</span> subject, float score) &#123;</div><div class=\"line\">        <span class=\"keyword\">this</span>.studentName = studentName;</div><div class=\"line\">        <span class=\"keyword\">this</span>.subject = subject;</div><div class=\"line\">        <span class=\"keyword\">this</span>.score = score;</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>CombineByKey的测试类<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">CombineTest</span> </span>&#123;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> </span>&#123;</div><div class=\"line\">        SparkConf sparkConf = <span class=\"keyword\">new</span> SparkConf().setAppName(<span class=\"string\">\"JavaWordCount\"</span>).setMaster(<span class=\"string\">\"local\"</span>);</div><div class=\"line\">        JavaSparkContext sc = <span class=\"keyword\">new</span> JavaSparkContext(sparkConf);</div><div class=\"line\">        ArrayList&lt;ScoreDetail&gt; scoreDetails = <span class=\"keyword\">new</span> ArrayList&lt;&gt;();</div><div class=\"line\">        scoreDetails.add(<span class=\"keyword\">new</span> ScoreDetail(<span class=\"string\">\"xiaoming\"</span>, <span class=\"string\">\"Math\"</span>, <span class=\"number\">98</span>));</div><div class=\"line\">        scoreDetails.add(<span class=\"keyword\">new</span> ScoreDetail(<span class=\"string\">\"xiaoming\"</span>, <span class=\"string\">\"English\"</span>, <span class=\"number\">88</span>));</div><div class=\"line\">        scoreDetails.add(<span class=\"keyword\">new</span> ScoreDetail(<span class=\"string\">\"wangwu\"</span>, <span class=\"string\">\"Math\"</span>, <span class=\"number\">75</span>));</div><div class=\"line\">        scoreDetails.add(<span class=\"keyword\">new</span> ScoreDetail(<span class=\"string\">\"wangwu\"</span>, <span class=\"string\">\"Englist\"</span>, <span class=\"number\">78</span>));</div><div class=\"line\">        scoreDetails.add(<span class=\"keyword\">new</span> ScoreDetail(<span class=\"string\">\"lihua\"</span>, <span class=\"string\">\"Math\"</span>, <span class=\"number\">90</span>));</div><div class=\"line\">        scoreDetails.add(<span class=\"keyword\">new</span> ScoreDetail(<span class=\"string\">\"lihua\"</span>, <span class=\"string\">\"English\"</span>, <span class=\"number\">80</span>));</div><div class=\"line\">        scoreDetails.add(<span class=\"keyword\">new</span> ScoreDetail(<span class=\"string\">\"zhangsan\"</span>, <span class=\"string\">\"Math\"</span>, <span class=\"number\">91</span>));</div><div class=\"line\">        scoreDetails.add(<span class=\"keyword\">new</span> ScoreDetail(<span class=\"string\">\"zhangsan\"</span>, <span class=\"string\">\"English\"</span>, <span class=\"number\">80</span>));</div><div class=\"line\"></div><div class=\"line\">        JavaRDD&lt;ScoreDetail&gt; scoreDetailsRDD = sc.parallelize(scoreDetails);</div><div class=\"line\"></div><div class=\"line\">        JavaPairRDD&lt;String, ScoreDetail&gt; pairRDD = scoreDetailsRDD.mapToPair(<span class=\"keyword\">new</span> PairFunction&lt;ScoreDetail, String, ScoreDetail&gt;() &#123;</div><div class=\"line\">            <span class=\"meta\">@Override</span></div><div class=\"line\">            <span class=\"function\"><span class=\"keyword\">public</span> Tuple2&lt;String, ScoreDetail&gt; <span class=\"title\">call</span><span class=\"params\">(ScoreDetail scoreDetail)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\"></div><div class=\"line\">                <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> Tuple2&lt;&gt;(scoreDetail.studentName, scoreDetail);</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;);</div><div class=\"line\"><span class=\"comment\">//        new Function&lt;ScoreDetail, Float,Integer&gt;();</span></div><div class=\"line\"></div><div class=\"line\">        Function&lt;ScoreDetail, Tuple2&lt;Float, Integer&gt;&gt; createCombine = <span class=\"keyword\">new</span> Function&lt;ScoreDetail, Tuple2&lt;Float, Integer&gt;&gt;() &#123;</div><div class=\"line\">            <span class=\"meta\">@Override</span></div><div class=\"line\">            <span class=\"function\"><span class=\"keyword\">public</span> Tuple2&lt;Float, Integer&gt; <span class=\"title\">call</span><span class=\"params\">(ScoreDetail scoreDetail)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">                <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> Tuple2&lt;&gt;(scoreDetail.score, <span class=\"number\">1</span>);</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;;</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\">// Function2传入两个值，返回一个值</span></div><div class=\"line\">        Function2&lt;Tuple2&lt;Float, Integer&gt;, ScoreDetail, Tuple2&lt;Float, Integer&gt;&gt; mergeValue = <span class=\"keyword\">new</span> Function2&lt;Tuple2&lt;Float, Integer&gt;, ScoreDetail, Tuple2&lt;Float, Integer&gt;&gt;() &#123;</div><div class=\"line\">            <span class=\"meta\">@Override</span></div><div class=\"line\">            <span class=\"function\"><span class=\"keyword\">public</span> Tuple2&lt;Float, Integer&gt; <span class=\"title\">call</span><span class=\"params\">(Tuple2&lt;Float, Integer&gt; tp, ScoreDetail scoreDetail)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">                <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> Tuple2&lt;&gt;(tp._1 + scoreDetail.score, tp._2 + <span class=\"number\">1</span>);</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;;</div><div class=\"line\">        Function2&lt;Tuple2&lt;Float, Integer&gt;, Tuple2&lt;Float, Integer&gt;, Tuple2&lt;Float, Integer&gt;&gt; mergeCombiners = <span class=\"keyword\">new</span> Function2&lt;Tuple2&lt;Float, Integer&gt;, Tuple2&lt;Float, Integer&gt;, Tuple2&lt;Float, Integer&gt;&gt;() &#123;</div><div class=\"line\">            <span class=\"meta\">@Override</span></div><div class=\"line\">            <span class=\"function\"><span class=\"keyword\">public</span> Tuple2&lt;Float, Integer&gt; <span class=\"title\">call</span><span class=\"params\">(Tuple2&lt;Float, Integer&gt; tp1, Tuple2&lt;Float, Integer&gt; tp2)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">                <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> Tuple2&lt;&gt;(tp1._1 + tp2._1, tp1._2 + tp2._2);</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;;</div><div class=\"line\">        JavaPairRDD&lt;String, Tuple2&lt;Float,Integer&gt;&gt; combineByRDD  = pairRDD.combineByKey(createCombine,mergeValue,mergeCombiners);</div><div class=\"line\"></div><div class=\"line\">        <span class=\"comment\">//打印平均数</span></div><div class=\"line\">        Map&lt;String, Tuple2&lt;Float, Integer&gt;&gt; stringTuple2Map = combineByRDD.collectAsMap();</div><div class=\"line\">        <span class=\"keyword\">for</span> ( String et:stringTuple2Map.keySet()) &#123;</div><div class=\"line\">            System.out.println(et+<span class=\"string\">\" \"</span>+stringTuple2Map.get(et)._1/stringTuple2Map.get(et)._2);</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p><strong>注意有个坑的地方</strong>  createCombine方法必须是这样的<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">Function&lt;ScoreDetail, Tuple2&lt;Float, Integer&gt;&gt; createCombine = <span class=\"keyword\">new</span> Function&lt;ScoreDetail, Tuple2&lt;Float, Integer&gt;&gt;() &#123;</div><div class=\"line\">    <span class=\"meta\">@Override</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> Tuple2&lt;Float, Integer&gt; <span class=\"title\">call</span><span class=\"params\">(ScoreDetail scoreDetail)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> Tuple2&lt;&gt;(scoreDetail.score, <span class=\"number\">1</span>);</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure></p>\n<p><strong>而不能是这样的</strong>, 即使最后的RDD都类似<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">PairFunction&lt;ScoreDetail, Float, Integer&gt; createCombine = <span class=\"keyword\">new</span> PairFunction&lt;ScoreDetail, Float, Integer&gt;() &#123;</div><div class=\"line\">    <span class=\"meta\">@Override</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> Tuple2&lt;Float, Integer&gt; <span class=\"title\">call</span><span class=\"params\">(ScoreDetail scoreDetail)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> Tuple2&lt;&gt;(scoreDetail.score, <span class=\"number\">1</span>);</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;;</div></pre></td></tr></table></figure></p>\n<p>再推荐几篇比较好的文章 <a href=\"http://lxw1234.com/archives/2015/07/358.htm\" target=\"_blank\" rel=\"external\">LXW的大数据田地: combineByKey</a><br> 推荐图书快速大数据分析中combineByKey的讲解</p>\n"},{"title":"spark RDD算子（八）之键值对关联操作 subtractByKey, join, rightOuterJoin, leftOuterJoin","date":"2017-03-18T13:25:21.000Z","author":"kaishun","id":"42","_content":"\n先从spark-learning中的一张图大致了解其功能  \n![](http://i1.piimg.com/567571/1ccee438414f9c6f.png)\n\n# **subtractByKey**\n**函数定义**\n```\ndef subtractByKey[W](other: RDD[(K, W)])(implicit arg0: ClassTag[W]): RDD[(K, V)]\n\ndef subtractByKey[W](other: RDD[(K, W)], numPartitions: Int)(implicit arg0: ClassTag[W]): RDD[(K, V)]\n\ndef subtractByKey[W](other: RDD[(K, W)], p: Partitioner)(implicit arg0: ClassTag[W]): RDD[(K, V)]\n```\n类似于subtrac，删掉 RDD 中键与 other RDD 中的键相同的元素\n# **join**  \n**函数定义**\n```\ndef join[W](other: RDD[(K, W)]): RDD[(K, (V, W))]\n\ndef join[W](other: RDD[(K, W)], numPartitions: Int): RDD[(K, (V, W))]\n\ndef join[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (V, W))]\n```\nRDD1.join(RDD2)  \n可以把RDD1,RDD2中的相同的key给连接起来，类似于sql中的join操作  \n\n# **leftOuterJoin**\n```java\ndef leftOuterJoin[W](other: RDD[(K, W)]): RDD[(K, (V, Option[W]))]\n\ndef leftOuterJoin[W](other: RDD[(K, W)], numPartitions: Int): RDD[(K, (V, Option[W]))]\n\ndef leftOuterJoin[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (V, Option[W]))]\n```  \n直接看图即可 \n对两个 RDD 进行连接操作，类似于sql中的左外连接\n# **rightOuterJoin**\n对两个 RDD 进行连接操作，类似于sql中的右外连接，存在的话，value用的Some, 不存在用的None,具体的看上面的图和下面的代码即可  \n\n# **代码示例**\n\n**scala语言**\n```scala  \n    scala> val rdd = sc.makeRDD(Array((1,2),(3,4),(3,6)))\n    scala> val other = sc.makeRDD(Array((3,9)))\n    \n    scala>  rdd.subtractByKey(other).collect()\n    res0: Array[(Int, Int)] = Array((1,2))\n    \n    scala> rdd.join(other).collect()\n    res1: Array[(Int, (Int, Int))] = Array((3,(4,9)), (3,(6,9)))\n    \n    scala> rdd.leftOuterJoin(other).collect()\n    res2: Array[(Int, (Int, Option[Int]))] = Array((1,(2,None)), (3,(4,Some(9))), (3,(6,Some(9))))\n    \n    scala> rdd.rightOuterJoin(other).collect()\n    res3: Array[(Int, (Option[Int], Int))] = Array((3,(Some(4),9)), (3,(Some(6),9)))\n```\n\n\n**java语言**\n```java\n    JavaRDD<Tuple2<Integer,Integer>> rddPre = sc.parallelize(Arrays.asList(new Tuple2(1,2)\n            , new Tuple2(3,4)\n            , new Tuple2(3,6)));\n    JavaRDD<Tuple2<Integer,Integer>> otherPre = sc.parallelize(Arrays.asList(new Tuple2(3,10)));\n\n\t//JavaRDD转换成JavaPairRDD\n    JavaPairRDD<Integer, Integer> rdd = JavaPairRDD.fromJavaRDD(rddPre);\n    JavaPairRDD<Integer, Integer> other = JavaPairRDD.fromJavaRDD(otherPre);\n    //subtractByKey\n    JavaPairRDD<Integer, Integer> subRDD = rdd.subtractByKey(other);\n    \n    //join\n    JavaPairRDD<Integer, Tuple2<Float, Integer>> joinRDD =  rdd.join(other);\n    \n    //leftOuterJoin\n    JavaPairRDD<Integer, Tuple2<Integer, Optional<Integer>>> integerTuple2JavaPairRDD = rdd.leftOuterJoin(other);\n    \n    //rightOutJoin\n    JavaPairRDD<Integer, Tuple2<Optional<Integer>, Integer>> rightOutJoin = rdd.rightOuterJoin(other);\n```\n\n","source":"_posts/spark RDD算子（八）之键值对关联操作 subtractByKey, join, rightOuterJoin, leftOuterJoin.md","raw":"---\ntitle: spark RDD算子（八）之键值对关联操作 subtractByKey, join, rightOuterJoin, leftOuterJoin\ndate: 2017-03-18 21:25:21\ntags: [spark]\ncategories: [大数据,spark]\nauthor: kaishun\nid: 42\npermalink: spark-rdd-8\n---\n\n先从spark-learning中的一张图大致了解其功能  \n![](http://i1.piimg.com/567571/1ccee438414f9c6f.png)\n\n# **subtractByKey**\n**函数定义**\n```\ndef subtractByKey[W](other: RDD[(K, W)])(implicit arg0: ClassTag[W]): RDD[(K, V)]\n\ndef subtractByKey[W](other: RDD[(K, W)], numPartitions: Int)(implicit arg0: ClassTag[W]): RDD[(K, V)]\n\ndef subtractByKey[W](other: RDD[(K, W)], p: Partitioner)(implicit arg0: ClassTag[W]): RDD[(K, V)]\n```\n类似于subtrac，删掉 RDD 中键与 other RDD 中的键相同的元素\n# **join**  \n**函数定义**\n```\ndef join[W](other: RDD[(K, W)]): RDD[(K, (V, W))]\n\ndef join[W](other: RDD[(K, W)], numPartitions: Int): RDD[(K, (V, W))]\n\ndef join[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (V, W))]\n```\nRDD1.join(RDD2)  \n可以把RDD1,RDD2中的相同的key给连接起来，类似于sql中的join操作  \n\n# **leftOuterJoin**\n```java\ndef leftOuterJoin[W](other: RDD[(K, W)]): RDD[(K, (V, Option[W]))]\n\ndef leftOuterJoin[W](other: RDD[(K, W)], numPartitions: Int): RDD[(K, (V, Option[W]))]\n\ndef leftOuterJoin[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (V, Option[W]))]\n```  \n直接看图即可 \n对两个 RDD 进行连接操作，类似于sql中的左外连接\n# **rightOuterJoin**\n对两个 RDD 进行连接操作，类似于sql中的右外连接，存在的话，value用的Some, 不存在用的None,具体的看上面的图和下面的代码即可  \n\n# **代码示例**\n\n**scala语言**\n```scala  \n    scala> val rdd = sc.makeRDD(Array((1,2),(3,4),(3,6)))\n    scala> val other = sc.makeRDD(Array((3,9)))\n    \n    scala>  rdd.subtractByKey(other).collect()\n    res0: Array[(Int, Int)] = Array((1,2))\n    \n    scala> rdd.join(other).collect()\n    res1: Array[(Int, (Int, Int))] = Array((3,(4,9)), (3,(6,9)))\n    \n    scala> rdd.leftOuterJoin(other).collect()\n    res2: Array[(Int, (Int, Option[Int]))] = Array((1,(2,None)), (3,(4,Some(9))), (3,(6,Some(9))))\n    \n    scala> rdd.rightOuterJoin(other).collect()\n    res3: Array[(Int, (Option[Int], Int))] = Array((3,(Some(4),9)), (3,(Some(6),9)))\n```\n\n\n**java语言**\n```java\n    JavaRDD<Tuple2<Integer,Integer>> rddPre = sc.parallelize(Arrays.asList(new Tuple2(1,2)\n            , new Tuple2(3,4)\n            , new Tuple2(3,6)));\n    JavaRDD<Tuple2<Integer,Integer>> otherPre = sc.parallelize(Arrays.asList(new Tuple2(3,10)));\n\n\t//JavaRDD转换成JavaPairRDD\n    JavaPairRDD<Integer, Integer> rdd = JavaPairRDD.fromJavaRDD(rddPre);\n    JavaPairRDD<Integer, Integer> other = JavaPairRDD.fromJavaRDD(otherPre);\n    //subtractByKey\n    JavaPairRDD<Integer, Integer> subRDD = rdd.subtractByKey(other);\n    \n    //join\n    JavaPairRDD<Integer, Tuple2<Float, Integer>> joinRDD =  rdd.join(other);\n    \n    //leftOuterJoin\n    JavaPairRDD<Integer, Tuple2<Integer, Optional<Integer>>> integerTuple2JavaPairRDD = rdd.leftOuterJoin(other);\n    \n    //rightOutJoin\n    JavaPairRDD<Integer, Tuple2<Optional<Integer>, Integer>> rightOutJoin = rdd.rightOuterJoin(other);\n```\n\n","slug":"spark-rdd-8","published":1,"updated":"2018-01-22T15:27:02.310Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcrpnzr400332wv3r2tzmuji","content":"<p>先从spark-learning中的一张图大致了解其功能<br><img src=\"http://i1.piimg.com/567571/1ccee438414f9c6f.png\" alt=\"\"></p>\n<h1 id=\"subtractByKey\"><a href=\"#subtractByKey\" class=\"headerlink\" title=\"subtractByKey\"></a><strong>subtractByKey</strong></h1><p><strong>函数定义</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">def subtractByKey[W](other: RDD[(K, W)])(implicit arg0: ClassTag[W]): RDD[(K, V)]</div><div class=\"line\"></div><div class=\"line\">def subtractByKey[W](other: RDD[(K, W)], numPartitions: Int)(implicit arg0: ClassTag[W]): RDD[(K, V)]</div><div class=\"line\"></div><div class=\"line\">def subtractByKey[W](other: RDD[(K, W)], p: Partitioner)(implicit arg0: ClassTag[W]): RDD[(K, V)]</div></pre></td></tr></table></figure></p>\n<p>类似于subtrac，删掉 RDD 中键与 other RDD 中的键相同的元素</p>\n<h1 id=\"join\"><a href=\"#join\" class=\"headerlink\" title=\"join\"></a><strong>join</strong></h1><p><strong>函数定义</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">def join[W](other: RDD[(K, W)]): RDD[(K, (V, W))]</div><div class=\"line\"></div><div class=\"line\">def join[W](other: RDD[(K, W)], numPartitions: Int): RDD[(K, (V, W))]</div><div class=\"line\"></div><div class=\"line\">def join[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (V, W))]</div></pre></td></tr></table></figure></p>\n<p>RDD1.join(RDD2)<br>可以把RDD1,RDD2中的相同的key给连接起来，类似于sql中的join操作  </p>\n<h1 id=\"leftOuterJoin\"><a href=\"#leftOuterJoin\" class=\"headerlink\" title=\"leftOuterJoin\"></a><strong>leftOuterJoin</strong></h1><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div></pre></td><td class=\"code\"><pre><div class=\"line\">def leftOuterJoin[W](other: RDD[(K, W)]): RDD[(K, (V, Option[W]))]</div><div class=\"line\"></div><div class=\"line\">def leftOuterJoin[W](other: RDD[(K, W)], numPartitions: Int): RDD[(K, (V, Option[W]))]</div><div class=\"line\"></div><div class=\"line\">def leftOuterJoin[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (V, Option[W]))]</div><div class=\"line\">```  </div><div class=\"line\">直接看图即可 </div><div class=\"line\">对两个 RDD 进行连接操作，类似于sql中的左外连接</div><div class=\"line\"># **rightOuterJoin**</div><div class=\"line\">对两个 RDD 进行连接操作，类似于sql中的右外连接，存在的话，value用的Some, 不存在用的None,具体的看上面的图和下面的代码即可  </div><div class=\"line\"></div><div class=\"line\"># **代码示例**</div><div class=\"line\"></div><div class=\"line\">**scala语言**</div><div class=\"line\">```scala  </div><div class=\"line\">    scala&gt; val rdd = sc.makeRDD(Array((<span class=\"number\">1</span>,<span class=\"number\">2</span>),(<span class=\"number\">3</span>,<span class=\"number\">4</span>),(<span class=\"number\">3</span>,<span class=\"number\">6</span>)))</div><div class=\"line\">    scala&gt; val other = sc.makeRDD(Array((<span class=\"number\">3</span>,<span class=\"number\">9</span>)))</div><div class=\"line\">    </div><div class=\"line\">    scala&gt;  rdd.subtractByKey(other).collect()</div><div class=\"line\">    res0: Array[(Int, Int)] = Array((<span class=\"number\">1</span>,<span class=\"number\">2</span>))</div><div class=\"line\">    </div><div class=\"line\">    scala&gt; rdd.join(other).collect()</div><div class=\"line\">    res1: Array[(Int, (Int, Int))] = Array((<span class=\"number\">3</span>,(<span class=\"number\">4</span>,<span class=\"number\">9</span>)), (<span class=\"number\">3</span>,(<span class=\"number\">6</span>,<span class=\"number\">9</span>)))</div><div class=\"line\">    </div><div class=\"line\">    scala&gt; rdd.leftOuterJoin(other).collect()</div><div class=\"line\">    res2: Array[(Int, (Int, Option[Int]))] = Array((<span class=\"number\">1</span>,(<span class=\"number\">2</span>,None)), (<span class=\"number\">3</span>,(<span class=\"number\">4</span>,Some(<span class=\"number\">9</span>))), (<span class=\"number\">3</span>,(<span class=\"number\">6</span>,Some(<span class=\"number\">9</span>))))</div><div class=\"line\">    </div><div class=\"line\">    scala&gt; rdd.rightOuterJoin(other).collect()</div><div class=\"line\">    res3: Array[(Int, (Option[Int], Int))] = Array((<span class=\"number\">3</span>,(Some(<span class=\"number\">4</span>),<span class=\"number\">9</span>)), (<span class=\"number\">3</span>,(Some(<span class=\"number\">6</span>),<span class=\"number\">9</span>)))</div></pre></td></tr></table></figure>\n<p><strong>java语言</strong><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div></pre></td><td class=\"code\"><pre><div class=\"line\">   JavaRDD&lt;Tuple2&lt;Integer,Integer&gt;&gt; rddPre = sc.parallelize(Arrays.asList(<span class=\"keyword\">new</span> Tuple2(<span class=\"number\">1</span>,<span class=\"number\">2</span>)</div><div class=\"line\">           , <span class=\"keyword\">new</span> Tuple2(<span class=\"number\">3</span>,<span class=\"number\">4</span>)</div><div class=\"line\">           , <span class=\"keyword\">new</span> Tuple2(<span class=\"number\">3</span>,<span class=\"number\">6</span>)));</div><div class=\"line\">   JavaRDD&lt;Tuple2&lt;Integer,Integer&gt;&gt; otherPre = sc.parallelize(Arrays.asList(<span class=\"keyword\">new</span> Tuple2(<span class=\"number\">3</span>,<span class=\"number\">10</span>)));</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">//JavaRDD转换成JavaPairRDD</span></div><div class=\"line\">   JavaPairRDD&lt;Integer, Integer&gt; rdd = JavaPairRDD.fromJavaRDD(rddPre);</div><div class=\"line\">   JavaPairRDD&lt;Integer, Integer&gt; other = JavaPairRDD.fromJavaRDD(otherPre);</div><div class=\"line\">   <span class=\"comment\">//subtractByKey</span></div><div class=\"line\">   JavaPairRDD&lt;Integer, Integer&gt; subRDD = rdd.subtractByKey(other);</div><div class=\"line\">   </div><div class=\"line\">   <span class=\"comment\">//join</span></div><div class=\"line\">   JavaPairRDD&lt;Integer, Tuple2&lt;Float, Integer&gt;&gt; joinRDD =  rdd.join(other);</div><div class=\"line\">   </div><div class=\"line\">   <span class=\"comment\">//leftOuterJoin</span></div><div class=\"line\">   JavaPairRDD&lt;Integer, Tuple2&lt;Integer, Optional&lt;Integer&gt;&gt;&gt; integerTuple2JavaPairRDD = rdd.leftOuterJoin(other);</div><div class=\"line\">   </div><div class=\"line\">   <span class=\"comment\">//rightOutJoin</span></div><div class=\"line\">   JavaPairRDD&lt;Integer, Tuple2&lt;Optional&lt;Integer&gt;, Integer&gt;&gt; rightOutJoin = rdd.rightOuterJoin(other);</div></pre></td></tr></table></figure></p>\n","site":{"data":{}},"excerpt":"","more":"<p>先从spark-learning中的一张图大致了解其功能<br><img src=\"http://i1.piimg.com/567571/1ccee438414f9c6f.png\" alt=\"\"></p>\n<h1 id=\"subtractByKey\"><a href=\"#subtractByKey\" class=\"headerlink\" title=\"subtractByKey\"></a><strong>subtractByKey</strong></h1><p><strong>函数定义</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">def subtractByKey[W](other: RDD[(K, W)])(implicit arg0: ClassTag[W]): RDD[(K, V)]</div><div class=\"line\"></div><div class=\"line\">def subtractByKey[W](other: RDD[(K, W)], numPartitions: Int)(implicit arg0: ClassTag[W]): RDD[(K, V)]</div><div class=\"line\"></div><div class=\"line\">def subtractByKey[W](other: RDD[(K, W)], p: Partitioner)(implicit arg0: ClassTag[W]): RDD[(K, V)]</div></pre></td></tr></table></figure></p>\n<p>类似于subtrac，删掉 RDD 中键与 other RDD 中的键相同的元素</p>\n<h1 id=\"join\"><a href=\"#join\" class=\"headerlink\" title=\"join\"></a><strong>join</strong></h1><p><strong>函数定义</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">def join[W](other: RDD[(K, W)]): RDD[(K, (V, W))]</div><div class=\"line\"></div><div class=\"line\">def join[W](other: RDD[(K, W)], numPartitions: Int): RDD[(K, (V, W))]</div><div class=\"line\"></div><div class=\"line\">def join[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (V, W))]</div></pre></td></tr></table></figure></p>\n<p>RDD1.join(RDD2)<br>可以把RDD1,RDD2中的相同的key给连接起来，类似于sql中的join操作  </p>\n<h1 id=\"leftOuterJoin\"><a href=\"#leftOuterJoin\" class=\"headerlink\" title=\"leftOuterJoin\"></a><strong>leftOuterJoin</strong></h1><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div></pre></td><td class=\"code\"><pre><div class=\"line\">def leftOuterJoin[W](other: RDD[(K, W)]): RDD[(K, (V, Option[W]))]</div><div class=\"line\"></div><div class=\"line\">def leftOuterJoin[W](other: RDD[(K, W)], numPartitions: Int): RDD[(K, (V, Option[W]))]</div><div class=\"line\"></div><div class=\"line\">def leftOuterJoin[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (V, Option[W]))]</div><div class=\"line\">```  </div><div class=\"line\">直接看图即可 </div><div class=\"line\">对两个 RDD 进行连接操作，类似于sql中的左外连接</div><div class=\"line\"># **rightOuterJoin**</div><div class=\"line\">对两个 RDD 进行连接操作，类似于sql中的右外连接，存在的话，value用的Some, 不存在用的None,具体的看上面的图和下面的代码即可  </div><div class=\"line\"></div><div class=\"line\"># **代码示例**</div><div class=\"line\"></div><div class=\"line\">**scala语言**</div><div class=\"line\">```scala  </div><div class=\"line\">    scala&gt; val rdd = sc.makeRDD(Array((<span class=\"number\">1</span>,<span class=\"number\">2</span>),(<span class=\"number\">3</span>,<span class=\"number\">4</span>),(<span class=\"number\">3</span>,<span class=\"number\">6</span>)))</div><div class=\"line\">    scala&gt; val other = sc.makeRDD(Array((<span class=\"number\">3</span>,<span class=\"number\">9</span>)))</div><div class=\"line\">    </div><div class=\"line\">    scala&gt;  rdd.subtractByKey(other).collect()</div><div class=\"line\">    res0: Array[(Int, Int)] = Array((<span class=\"number\">1</span>,<span class=\"number\">2</span>))</div><div class=\"line\">    </div><div class=\"line\">    scala&gt; rdd.join(other).collect()</div><div class=\"line\">    res1: Array[(Int, (Int, Int))] = Array((<span class=\"number\">3</span>,(<span class=\"number\">4</span>,<span class=\"number\">9</span>)), (<span class=\"number\">3</span>,(<span class=\"number\">6</span>,<span class=\"number\">9</span>)))</div><div class=\"line\">    </div><div class=\"line\">    scala&gt; rdd.leftOuterJoin(other).collect()</div><div class=\"line\">    res2: Array[(Int, (Int, Option[Int]))] = Array((<span class=\"number\">1</span>,(<span class=\"number\">2</span>,None)), (<span class=\"number\">3</span>,(<span class=\"number\">4</span>,Some(<span class=\"number\">9</span>))), (<span class=\"number\">3</span>,(<span class=\"number\">6</span>,Some(<span class=\"number\">9</span>))))</div><div class=\"line\">    </div><div class=\"line\">    scala&gt; rdd.rightOuterJoin(other).collect()</div><div class=\"line\">    res3: Array[(Int, (Option[Int], Int))] = Array((<span class=\"number\">3</span>,(Some(<span class=\"number\">4</span>),<span class=\"number\">9</span>)), (<span class=\"number\">3</span>,(Some(<span class=\"number\">6</span>),<span class=\"number\">9</span>)))</div></pre></td></tr></table></figure>\n<p><strong>java语言</strong><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div></pre></td><td class=\"code\"><pre><div class=\"line\">   JavaRDD&lt;Tuple2&lt;Integer,Integer&gt;&gt; rddPre = sc.parallelize(Arrays.asList(<span class=\"keyword\">new</span> Tuple2(<span class=\"number\">1</span>,<span class=\"number\">2</span>)</div><div class=\"line\">           , <span class=\"keyword\">new</span> Tuple2(<span class=\"number\">3</span>,<span class=\"number\">4</span>)</div><div class=\"line\">           , <span class=\"keyword\">new</span> Tuple2(<span class=\"number\">3</span>,<span class=\"number\">6</span>)));</div><div class=\"line\">   JavaRDD&lt;Tuple2&lt;Integer,Integer&gt;&gt; otherPre = sc.parallelize(Arrays.asList(<span class=\"keyword\">new</span> Tuple2(<span class=\"number\">3</span>,<span class=\"number\">10</span>)));</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">//JavaRDD转换成JavaPairRDD</span></div><div class=\"line\">   JavaPairRDD&lt;Integer, Integer&gt; rdd = JavaPairRDD.fromJavaRDD(rddPre);</div><div class=\"line\">   JavaPairRDD&lt;Integer, Integer&gt; other = JavaPairRDD.fromJavaRDD(otherPre);</div><div class=\"line\">   <span class=\"comment\">//subtractByKey</span></div><div class=\"line\">   JavaPairRDD&lt;Integer, Integer&gt; subRDD = rdd.subtractByKey(other);</div><div class=\"line\">   </div><div class=\"line\">   <span class=\"comment\">//join</span></div><div class=\"line\">   JavaPairRDD&lt;Integer, Tuple2&lt;Float, Integer&gt;&gt; joinRDD =  rdd.join(other);</div><div class=\"line\">   </div><div class=\"line\">   <span class=\"comment\">//leftOuterJoin</span></div><div class=\"line\">   JavaPairRDD&lt;Integer, Tuple2&lt;Integer, Optional&lt;Integer&gt;&gt;&gt; integerTuple2JavaPairRDD = rdd.leftOuterJoin(other);</div><div class=\"line\">   </div><div class=\"line\">   <span class=\"comment\">//rightOutJoin</span></div><div class=\"line\">   JavaPairRDD&lt;Integer, Tuple2&lt;Optional&lt;Integer&gt;, Integer&gt;&gt; rightOutJoin = rdd.rightOuterJoin(other);</div></pre></td></tr></table></figure></p>\n"},{"title":"spark RDD算子（六）之键值对聚合操作reduceByKey，foldByKey，排序操作sortByKey","date":"2017-03-06T13:25:21.000Z","author":"kaishun","id":"39","_content":"\n# **reduceByKey**\n```\ndef reduceByKey(func: (V, V) => V): RDD[(K, V)]\n\ndef reduceByKey(func: (V, V) => V, numPartitions: Int): RDD[(K, V)]\n\ndef reduceByKey(partitioner: Partitioner, func: (V, V) => V): RDD[(K, V)]\n```\n接收一个函数，按照相同的key进行reduce操作，类似于scala的reduce的操作  \n例如RDD {(1, 2), (3, 4), (3, 6)}进行reduce  \n**scala版本**\n```scala\n    var mapRDD = sc.parallelize(List((1,2),(3,4),(3,6)))\n    var reduceRDD = mapRDD.reduceByKey((x,y)=>x+y)\n    reduceRDD.foreach(x=>println(x))\n------输出---------\n(1,2)\n(3,10)\n```\n再举例  \n**单词计数**  \nF:\\sparktest\\sample.txt中的内容如下\n```\naa bb cc aa aa aa dd dd ee ee ee ee \nff aa bb zks\nee kks\nee  zz zks\n\n```\n**scala版本**\n\n```scala\n    val lines = sc.textFile(\"F:\\\\sparktest\\\\sample.txt\")\n    val wordsRDD = lines.flatMap(x=>x.split(\"\\\\s+\")).map(x=>(x,1))\n    val wordCountRDD = wordsRDD.reduceByKey((x,y)=>x+y)\n    wordCountRDD.foreach(x=>println(x))\n---------输出-----------\n(ee,6)\n(aa,5)\n(dd,2)\n(zz,1)\n(zks,2)\n(kks,1)\n(ff,1)\n(bb,2)\n(cc,1)\n```\n**java版本**\n```java\n JavaRDD<String> lines = sc.textFile(\"F:\\\\sparktest\\\\sample.txt\");\n\n        JavaPairRDD<String, Integer> wordPairRDD = lines.flatMapToPair(new PairFlatMapFunction<String, String, Integer>() {\n            @Override\n            public Iterable<Tuple2<String, Integer>> call(String s) throws Exception {\n                ArrayList<Tuple2<String, Integer>> tpLists = new ArrayList<Tuple2<String, Integer>>();\n                String[] split = s.split(\"\\\\s+\");\n                for (int i = 0; i <split.length ; i++) {\n                    Tuple2 tp = new Tuple2<String,Integer>(split[i], 1);\n                    tpLists.add(tp);\n                }\n            return tpLists;\n            }\n        });\n\n        JavaPairRDD<String, Integer> wordCountRDD = wordPairRDD.reduceByKey(new Function2<Integer, Integer, Integer>() {\n            @Override\n            public Integer call(Integer i1, Integer i2) throws Exception {\n                return i1 + i2;\n            }\n        });\n        Map<String, Integer> collectAsMap = wordCountRDD.collectAsMap();\n        for (String key:collectAsMap.keySet()) {\n            System.out.println(\"(\"+key+\",\"+collectAsMap.get(key)+\")\");\n        }\n----------输出-------------------------------\n(kks,1)\n(ee,6)\n(bb,2)\n(zz,1)\n(ff,1)\n(cc,1)\n(zks,2)\n(dd,2)\n(aa,5)\n```\n# **foldByKey**  \n```scala\ndef foldByKey(zeroValue: V)(func: (V, V) => V): RDD[(K, V)]\n\ndef foldByKey(zeroValue: V, numPartitions: Int)(func: (V, V) => V): RDD[(K, V)]\n\ndef foldByKey(zeroValue: V, partitioner: Partitioner)(func: (V, V) => V): RDD[(K, V)]\n\n```\n该函数用于RDD[K,V]根据K将V做折叠、合并处理，其中的参数zeroValue表示先根据映射函数将zeroValue应用于V,进行初始化V,再将映射函数应用于初始化后的V.  \nfoldByKey可以参考我之前的[scala的fold的介绍](http://blog.csdn.net/t1dmzks/article/details/69858060#t27)  \n与reduce不同的是 foldByKey开始折叠的第一个元素不是集合中的第一个元素，而是传入的一个元素  \n[参考LXW的博客  scala的例子](http://lxw1234.com/archives/2015/07/358.htm)  \n**直接看例子**\n```scala\nscala> var rdd1 = sc.makeRDD(Array((\"A\",0),(\"A\",2),(\"B\",1),(\"B\",2),(\"C\",1)))\nscala> rdd1.foldByKey(0)(_+_).collect\nres75: Array[(String, Int)] = Array((A,2), (B,3), (C,1)) \n//将rdd1中每个key对应的V进行累加，注意zeroValue=0,需要先初始化V,映射函数为+操\n//作，比如(\"A\",0), (\"A\",2)，先将zeroValue应用于每个V,得到：(\"A\",0+0), (\"A\",2+0)，即：\n//(\"A\",0), (\"A\",2)，再将映射函数应用于初始化后的V，最后得到(A,0+2),即(A,2)\n```\n**再看：**\n```scala\nscala> rdd1.foldByKey(2)(_+_).collect\nres76: Array[(String, Int)] = Array((A,6), (B,7), (C,3))\n//先将zeroValue=2应用于每个V,得到：(\"A\",0+2), (\"A\",2+2)，即：(\"A\",2), (\"A\",4)，再将映射函\n//数应用于初始化后的V，最后得到：(A,2+4)，即：(A,6)\n```\n**再看乘法操作：**  \n```scala\nscala> rdd1.foldByKey(0)(_*_).collect\nres77: Array[(String, Int)] = Array((A,0), (B,0), (C,0))\n//先将zeroValue=0应用于每个V,注意，这次映射函数为乘法，得到：(\"A\",0*0), (\"A\",2*0)，\n//即：(\"A\",0), (\"A\",0)，再将映射函//数应用于初始化后的V，最后得到：(A,0*0)，即：(A,0)\n//其他K也一样，最终都得到了V=0\n \nscala> rdd1.foldByKey(1)(_*_).collect\nres78: Array[(String, Int)] = Array((A,0), (B,2), (C,1))\n//映射函数为乘法时，需要将zeroValue设为1，才能得到我们想要的结果。\n```\n\n# **SortByKey**\n```\n  def sortByKey(ascending : scala.Boolean = { /* compiled code */ }, numPartitions : scala.Int = { /* compiled code */ }) : org.apache.spark.rdd.RDD[scala.Tuple2[K, V]] = { /* compiled code */ }\n\n```\nSortByKey用于对pairRDD按照key进行排序，第一个参数可以设置true或者false，默认是true\n**scala例子**\n```scala\nscala> val rdd = sc.parallelize(Array((3, 4),(1, 2),(4,4),(2,5), (6,5), (5, 6)))  \n\n// sortByKey不是Action操作，只能算是转换操作\nscala> rdd.sortByKey()\nres9: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[28] at sortByKey at <console>:24 \n\n//看看sortByKey后是什么类型\nscala> rdd.sortByKey().collect() \nres10: Array[(Int, Int)] = Array((1,2), (2,5), (3,4), (4,4), (5,6), (6,5)) \n\n//降序排序\nscala> rdd.sortByKey(false).collect() \nres12: Array[(Int, Int)] = Array((6,5), (5,6), (4,4), (3,4), (2,5), (1,2)) \n```  \njava例子也是一样的，这里就不写了\n\n\n\n参考文章: Spark快速大数据分析,[LXW的大数据田地](http://lxw1234.com/archives/2015/07/358.htm),spark官网  \n","source":"_posts/spark RDD算子（六）之键值对聚合操作reduceByKey，foldByKey，排序操作sortByKey.md","raw":"---\ntitle: spark RDD算子（六）之键值对聚合操作reduceByKey，foldByKey，排序操作sortByKey\ndate: 2017-03-06 21:25:21\ntags: [spark]\ncategories: [大数据,spark]\nauthor: kaishun\nid: 39\npermalink: spark-rdd-6\n---\n\n# **reduceByKey**\n```\ndef reduceByKey(func: (V, V) => V): RDD[(K, V)]\n\ndef reduceByKey(func: (V, V) => V, numPartitions: Int): RDD[(K, V)]\n\ndef reduceByKey(partitioner: Partitioner, func: (V, V) => V): RDD[(K, V)]\n```\n接收一个函数，按照相同的key进行reduce操作，类似于scala的reduce的操作  \n例如RDD {(1, 2), (3, 4), (3, 6)}进行reduce  \n**scala版本**\n```scala\n    var mapRDD = sc.parallelize(List((1,2),(3,4),(3,6)))\n    var reduceRDD = mapRDD.reduceByKey((x,y)=>x+y)\n    reduceRDD.foreach(x=>println(x))\n------输出---------\n(1,2)\n(3,10)\n```\n再举例  \n**单词计数**  \nF:\\sparktest\\sample.txt中的内容如下\n```\naa bb cc aa aa aa dd dd ee ee ee ee \nff aa bb zks\nee kks\nee  zz zks\n\n```\n**scala版本**\n\n```scala\n    val lines = sc.textFile(\"F:\\\\sparktest\\\\sample.txt\")\n    val wordsRDD = lines.flatMap(x=>x.split(\"\\\\s+\")).map(x=>(x,1))\n    val wordCountRDD = wordsRDD.reduceByKey((x,y)=>x+y)\n    wordCountRDD.foreach(x=>println(x))\n---------输出-----------\n(ee,6)\n(aa,5)\n(dd,2)\n(zz,1)\n(zks,2)\n(kks,1)\n(ff,1)\n(bb,2)\n(cc,1)\n```\n**java版本**\n```java\n JavaRDD<String> lines = sc.textFile(\"F:\\\\sparktest\\\\sample.txt\");\n\n        JavaPairRDD<String, Integer> wordPairRDD = lines.flatMapToPair(new PairFlatMapFunction<String, String, Integer>() {\n            @Override\n            public Iterable<Tuple2<String, Integer>> call(String s) throws Exception {\n                ArrayList<Tuple2<String, Integer>> tpLists = new ArrayList<Tuple2<String, Integer>>();\n                String[] split = s.split(\"\\\\s+\");\n                for (int i = 0; i <split.length ; i++) {\n                    Tuple2 tp = new Tuple2<String,Integer>(split[i], 1);\n                    tpLists.add(tp);\n                }\n            return tpLists;\n            }\n        });\n\n        JavaPairRDD<String, Integer> wordCountRDD = wordPairRDD.reduceByKey(new Function2<Integer, Integer, Integer>() {\n            @Override\n            public Integer call(Integer i1, Integer i2) throws Exception {\n                return i1 + i2;\n            }\n        });\n        Map<String, Integer> collectAsMap = wordCountRDD.collectAsMap();\n        for (String key:collectAsMap.keySet()) {\n            System.out.println(\"(\"+key+\",\"+collectAsMap.get(key)+\")\");\n        }\n----------输出-------------------------------\n(kks,1)\n(ee,6)\n(bb,2)\n(zz,1)\n(ff,1)\n(cc,1)\n(zks,2)\n(dd,2)\n(aa,5)\n```\n# **foldByKey**  \n```scala\ndef foldByKey(zeroValue: V)(func: (V, V) => V): RDD[(K, V)]\n\ndef foldByKey(zeroValue: V, numPartitions: Int)(func: (V, V) => V): RDD[(K, V)]\n\ndef foldByKey(zeroValue: V, partitioner: Partitioner)(func: (V, V) => V): RDD[(K, V)]\n\n```\n该函数用于RDD[K,V]根据K将V做折叠、合并处理，其中的参数zeroValue表示先根据映射函数将zeroValue应用于V,进行初始化V,再将映射函数应用于初始化后的V.  \nfoldByKey可以参考我之前的[scala的fold的介绍](http://blog.csdn.net/t1dmzks/article/details/69858060#t27)  \n与reduce不同的是 foldByKey开始折叠的第一个元素不是集合中的第一个元素，而是传入的一个元素  \n[参考LXW的博客  scala的例子](http://lxw1234.com/archives/2015/07/358.htm)  \n**直接看例子**\n```scala\nscala> var rdd1 = sc.makeRDD(Array((\"A\",0),(\"A\",2),(\"B\",1),(\"B\",2),(\"C\",1)))\nscala> rdd1.foldByKey(0)(_+_).collect\nres75: Array[(String, Int)] = Array((A,2), (B,3), (C,1)) \n//将rdd1中每个key对应的V进行累加，注意zeroValue=0,需要先初始化V,映射函数为+操\n//作，比如(\"A\",0), (\"A\",2)，先将zeroValue应用于每个V,得到：(\"A\",0+0), (\"A\",2+0)，即：\n//(\"A\",0), (\"A\",2)，再将映射函数应用于初始化后的V，最后得到(A,0+2),即(A,2)\n```\n**再看：**\n```scala\nscala> rdd1.foldByKey(2)(_+_).collect\nres76: Array[(String, Int)] = Array((A,6), (B,7), (C,3))\n//先将zeroValue=2应用于每个V,得到：(\"A\",0+2), (\"A\",2+2)，即：(\"A\",2), (\"A\",4)，再将映射函\n//数应用于初始化后的V，最后得到：(A,2+4)，即：(A,6)\n```\n**再看乘法操作：**  \n```scala\nscala> rdd1.foldByKey(0)(_*_).collect\nres77: Array[(String, Int)] = Array((A,0), (B,0), (C,0))\n//先将zeroValue=0应用于每个V,注意，这次映射函数为乘法，得到：(\"A\",0*0), (\"A\",2*0)，\n//即：(\"A\",0), (\"A\",0)，再将映射函//数应用于初始化后的V，最后得到：(A,0*0)，即：(A,0)\n//其他K也一样，最终都得到了V=0\n \nscala> rdd1.foldByKey(1)(_*_).collect\nres78: Array[(String, Int)] = Array((A,0), (B,2), (C,1))\n//映射函数为乘法时，需要将zeroValue设为1，才能得到我们想要的结果。\n```\n\n# **SortByKey**\n```\n  def sortByKey(ascending : scala.Boolean = { /* compiled code */ }, numPartitions : scala.Int = { /* compiled code */ }) : org.apache.spark.rdd.RDD[scala.Tuple2[K, V]] = { /* compiled code */ }\n\n```\nSortByKey用于对pairRDD按照key进行排序，第一个参数可以设置true或者false，默认是true\n**scala例子**\n```scala\nscala> val rdd = sc.parallelize(Array((3, 4),(1, 2),(4,4),(2,5), (6,5), (5, 6)))  \n\n// sortByKey不是Action操作，只能算是转换操作\nscala> rdd.sortByKey()\nres9: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[28] at sortByKey at <console>:24 \n\n//看看sortByKey后是什么类型\nscala> rdd.sortByKey().collect() \nres10: Array[(Int, Int)] = Array((1,2), (2,5), (3,4), (4,4), (5,6), (6,5)) \n\n//降序排序\nscala> rdd.sortByKey(false).collect() \nres12: Array[(Int, Int)] = Array((6,5), (5,6), (4,4), (3,4), (2,5), (1,2)) \n```  \njava例子也是一样的，这里就不写了\n\n\n\n参考文章: Spark快速大数据分析,[LXW的大数据田地](http://lxw1234.com/archives/2015/07/358.htm),spark官网  \n","slug":"spark-rdd-6","published":1,"updated":"2018-01-22T15:25:53.238Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcrpnzr400372wv3t8mb5g9y","content":"<h1 id=\"reduceByKey\"><a href=\"#reduceByKey\" class=\"headerlink\" title=\"reduceByKey\"></a><strong>reduceByKey</strong></h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">def reduceByKey(func: (V, V) =&gt; V): RDD[(K, V)]</div><div class=\"line\"></div><div class=\"line\">def reduceByKey(func: (V, V) =&gt; V, numPartitions: Int): RDD[(K, V)]</div><div class=\"line\"></div><div class=\"line\">def reduceByKey(partitioner: Partitioner, func: (V, V) =&gt; V): RDD[(K, V)]</div></pre></td></tr></table></figure>\n<p>接收一个函数，按照相同的key进行reduce操作，类似于scala的reduce的操作<br>例如RDD {(1, 2), (3, 4), (3, 6)}进行reduce<br><strong>scala版本</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">    <span class=\"keyword\">var</span> mapRDD = sc.parallelize(<span class=\"type\">List</span>((<span class=\"number\">1</span>,<span class=\"number\">2</span>),(<span class=\"number\">3</span>,<span class=\"number\">4</span>),(<span class=\"number\">3</span>,<span class=\"number\">6</span>)))</div><div class=\"line\">    <span class=\"keyword\">var</span> reduceRDD = mapRDD.reduceByKey((x,y)=&gt;x+y)</div><div class=\"line\">    reduceRDD.foreach(x=&gt;println(x))</div><div class=\"line\">------输出---------</div><div class=\"line\">(<span class=\"number\">1</span>,<span class=\"number\">2</span>)</div><div class=\"line\">(<span class=\"number\">3</span>,<span class=\"number\">10</span>)</div></pre></td></tr></table></figure></p>\n<p>再举例<br><strong>单词计数</strong><br>F:\\sparktest\\sample.txt中的内容如下<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">aa bb cc aa aa aa dd dd ee ee ee ee </div><div class=\"line\">ff aa bb zks</div><div class=\"line\">ee kks</div><div class=\"line\">ee  zz zks</div></pre></td></tr></table></figure></p>\n<p><strong>scala版本</strong></p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\">    <span class=\"keyword\">val</span> lines = sc.textFile(<span class=\"string\">\"F:\\\\sparktest\\\\sample.txt\"</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> wordsRDD = lines.flatMap(x=&gt;x.split(<span class=\"string\">\"\\\\s+\"</span>)).map(x=&gt;(x,<span class=\"number\">1</span>))</div><div class=\"line\">    <span class=\"keyword\">val</span> wordCountRDD = wordsRDD.reduceByKey((x,y)=&gt;x+y)</div><div class=\"line\">    wordCountRDD.foreach(x=&gt;println(x))</div><div class=\"line\">---------输出-----------</div><div class=\"line\">(ee,<span class=\"number\">6</span>)</div><div class=\"line\">(aa,<span class=\"number\">5</span>)</div><div class=\"line\">(dd,<span class=\"number\">2</span>)</div><div class=\"line\">(zz,<span class=\"number\">1</span>)</div><div class=\"line\">(zks,<span class=\"number\">2</span>)</div><div class=\"line\">(kks,<span class=\"number\">1</span>)</div><div class=\"line\">(ff,<span class=\"number\">1</span>)</div><div class=\"line\">(bb,<span class=\"number\">2</span>)</div><div class=\"line\">(cc,<span class=\"number\">1</span>)</div></pre></td></tr></table></figure>\n<p><strong>java版本</strong><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div></pre></td><td class=\"code\"><pre><div class=\"line\"> JavaRDD&lt;String&gt; lines = sc.textFile(<span class=\"string\">\"F:\\\\sparktest\\\\sample.txt\"</span>);</div><div class=\"line\"></div><div class=\"line\">        JavaPairRDD&lt;String, Integer&gt; wordPairRDD = lines.flatMapToPair(<span class=\"keyword\">new</span> PairFlatMapFunction&lt;String, String, Integer&gt;() &#123;</div><div class=\"line\">            <span class=\"meta\">@Override</span></div><div class=\"line\">            <span class=\"keyword\">public</span> Iterable&lt;Tuple2&lt;String, Integer&gt;&gt; call(String s) <span class=\"keyword\">throws</span> Exception &#123;</div><div class=\"line\">                ArrayList&lt;Tuple2&lt;String, Integer&gt;&gt; tpLists = <span class=\"keyword\">new</span> ArrayList&lt;Tuple2&lt;String, Integer&gt;&gt;();</div><div class=\"line\">                String[] split = s.split(<span class=\"string\">\"\\\\s+\"</span>);</div><div class=\"line\">                <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt;split.length ; i++) &#123;</div><div class=\"line\">                    Tuple2 tp = <span class=\"keyword\">new</span> Tuple2&lt;String,Integer&gt;(split[i], <span class=\"number\">1</span>);</div><div class=\"line\">                    tpLists.add(tp);</div><div class=\"line\">                &#125;</div><div class=\"line\">            <span class=\"keyword\">return</span> tpLists;</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;);</div><div class=\"line\"></div><div class=\"line\">        JavaPairRDD&lt;String, Integer&gt; wordCountRDD = wordPairRDD.reduceByKey(<span class=\"keyword\">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</div><div class=\"line\">            <span class=\"meta\">@Override</span></div><div class=\"line\">            <span class=\"function\"><span class=\"keyword\">public</span> Integer <span class=\"title\">call</span><span class=\"params\">(Integer i1, Integer i2)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">                <span class=\"keyword\">return</span> i1 + i2;</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;);</div><div class=\"line\">        Map&lt;String, Integer&gt; collectAsMap = wordCountRDD.collectAsMap();</div><div class=\"line\">        <span class=\"keyword\">for</span> (String key:collectAsMap.keySet()) &#123;</div><div class=\"line\">            System.out.println(<span class=\"string\">\"(\"</span>+key+<span class=\"string\">\",\"</span>+collectAsMap.get(key)+<span class=\"string\">\")\"</span>);</div><div class=\"line\">        &#125;</div><div class=\"line\">----------输出-------------------------------</div><div class=\"line\">(kks,<span class=\"number\">1</span>)</div><div class=\"line\">(ee,<span class=\"number\">6</span>)</div><div class=\"line\">(bb,<span class=\"number\">2</span>)</div><div class=\"line\">(zz,<span class=\"number\">1</span>)</div><div class=\"line\">(ff,<span class=\"number\">1</span>)</div><div class=\"line\">(cc,<span class=\"number\">1</span>)</div><div class=\"line\">(zks,<span class=\"number\">2</span>)</div><div class=\"line\">(dd,<span class=\"number\">2</span>)</div><div class=\"line\">(aa,<span class=\"number\">5</span>)</div></pre></td></tr></table></figure></p>\n<h1 id=\"foldByKey\"><a href=\"#foldByKey\" class=\"headerlink\" title=\"foldByKey\"></a><strong>foldByKey</strong></h1><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">foldByKey</span></span>(zeroValue: <span class=\"type\">V</span>)(func: (<span class=\"type\">V</span>, <span class=\"type\">V</span>) =&gt; <span class=\"type\">V</span>): <span class=\"type\">RDD</span>[(<span class=\"type\">K</span>, <span class=\"type\">V</span>)]</div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">foldByKey</span></span>(zeroValue: <span class=\"type\">V</span>, numPartitions: <span class=\"type\">Int</span>)(func: (<span class=\"type\">V</span>, <span class=\"type\">V</span>) =&gt; <span class=\"type\">V</span>): <span class=\"type\">RDD</span>[(<span class=\"type\">K</span>, <span class=\"type\">V</span>)]</div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">foldByKey</span></span>(zeroValue: <span class=\"type\">V</span>, partitioner: <span class=\"type\">Partitioner</span>)(func: (<span class=\"type\">V</span>, <span class=\"type\">V</span>) =&gt; <span class=\"type\">V</span>): <span class=\"type\">RDD</span>[(<span class=\"type\">K</span>, <span class=\"type\">V</span>)]</div></pre></td></tr></table></figure>\n<p>该函数用于RDD[K,V]根据K将V做折叠、合并处理，其中的参数zeroValue表示先根据映射函数将zeroValue应用于V,进行初始化V,再将映射函数应用于初始化后的V.<br>foldByKey可以参考我之前的<a href=\"http://blog.csdn.net/t1dmzks/article/details/69858060#t27\" target=\"_blank\" rel=\"external\">scala的fold的介绍</a><br>与reduce不同的是 foldByKey开始折叠的第一个元素不是集合中的第一个元素，而是传入的一个元素<br><a href=\"http://lxw1234.com/archives/2015/07/358.htm\" target=\"_blank\" rel=\"external\">参考LXW的博客  scala的例子</a><br><strong>直接看例子</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">var</span> rdd1 = sc.makeRDD(<span class=\"type\">Array</span>((<span class=\"string\">\"A\"</span>,<span class=\"number\">0</span>),(<span class=\"string\">\"A\"</span>,<span class=\"number\">2</span>),(<span class=\"string\">\"B\"</span>,<span class=\"number\">1</span>),(<span class=\"string\">\"B\"</span>,<span class=\"number\">2</span>),(<span class=\"string\">\"C\"</span>,<span class=\"number\">1</span>)))</div><div class=\"line\">scala&gt; rdd1.foldByKey(<span class=\"number\">0</span>)(_+_).collect</div><div class=\"line\">res75: <span class=\"type\">Array</span>[(<span class=\"type\">String</span>, <span class=\"type\">Int</span>)] = <span class=\"type\">Array</span>((<span class=\"type\">A</span>,<span class=\"number\">2</span>), (<span class=\"type\">B</span>,<span class=\"number\">3</span>), (<span class=\"type\">C</span>,<span class=\"number\">1</span>)) </div><div class=\"line\"><span class=\"comment\">//将rdd1中每个key对应的V进行累加，注意zeroValue=0,需要先初始化V,映射函数为+操</span></div><div class=\"line\"><span class=\"comment\">//作，比如(\"A\",0), (\"A\",2)，先将zeroValue应用于每个V,得到：(\"A\",0+0), (\"A\",2+0)，即：</span></div><div class=\"line\"><span class=\"comment\">//(\"A\",0), (\"A\",2)，再将映射函数应用于初始化后的V，最后得到(A,0+2),即(A,2)</span></div></pre></td></tr></table></figure></p>\n<p><strong>再看：</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; rdd1.foldByKey(<span class=\"number\">2</span>)(_+_).collect</div><div class=\"line\">res76: <span class=\"type\">Array</span>[(<span class=\"type\">String</span>, <span class=\"type\">Int</span>)] = <span class=\"type\">Array</span>((<span class=\"type\">A</span>,<span class=\"number\">6</span>), (<span class=\"type\">B</span>,<span class=\"number\">7</span>), (<span class=\"type\">C</span>,<span class=\"number\">3</span>))</div><div class=\"line\"><span class=\"comment\">//先将zeroValue=2应用于每个V,得到：(\"A\",0+2), (\"A\",2+2)，即：(\"A\",2), (\"A\",4)，再将映射函</span></div><div class=\"line\"><span class=\"comment\">//数应用于初始化后的V，最后得到：(A,2+4)，即：(A,6)</span></div></pre></td></tr></table></figure></p>\n<p><strong>再看乘法操作：</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; rdd1.foldByKey(<span class=\"number\">0</span>)(_*_).collect</div><div class=\"line\">res77: <span class=\"type\">Array</span>[(<span class=\"type\">String</span>, <span class=\"type\">Int</span>)] = <span class=\"type\">Array</span>((<span class=\"type\">A</span>,<span class=\"number\">0</span>), (<span class=\"type\">B</span>,<span class=\"number\">0</span>), (<span class=\"type\">C</span>,<span class=\"number\">0</span>))</div><div class=\"line\"><span class=\"comment\">//先将zeroValue=0应用于每个V,注意，这次映射函数为乘法，得到：(\"A\",0*0), (\"A\",2*0)，</span></div><div class=\"line\"><span class=\"comment\">//即：(\"A\",0), (\"A\",0)，再将映射函//数应用于初始化后的V，最后得到：(A,0*0)，即：(A,0)</span></div><div class=\"line\"><span class=\"comment\">//其他K也一样，最终都得到了V=0</span></div><div class=\"line\"> </div><div class=\"line\">scala&gt; rdd1.foldByKey(<span class=\"number\">1</span>)(_*_).collect</div><div class=\"line\">res78: <span class=\"type\">Array</span>[(<span class=\"type\">String</span>, <span class=\"type\">Int</span>)] = <span class=\"type\">Array</span>((<span class=\"type\">A</span>,<span class=\"number\">0</span>), (<span class=\"type\">B</span>,<span class=\"number\">2</span>), (<span class=\"type\">C</span>,<span class=\"number\">1</span>))</div><div class=\"line\"><span class=\"comment\">//映射函数为乘法时，需要将zeroValue设为1，才能得到我们想要的结果。</span></div></pre></td></tr></table></figure></p>\n<h1 id=\"SortByKey\"><a href=\"#SortByKey\" class=\"headerlink\" title=\"SortByKey\"></a><strong>SortByKey</strong></h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">def sortByKey(ascending : scala.Boolean = &#123; /* compiled code */ &#125;, numPartitions : scala.Int = &#123; /* compiled code */ &#125;) : org.apache.spark.rdd.RDD[scala.Tuple2[K, V]] = &#123; /* compiled code */ &#125;</div></pre></td></tr></table></figure>\n<p>SortByKey用于对pairRDD按照key进行排序，第一个参数可以设置true或者false，默认是true<br><strong>scala例子</strong></p>\n<pre><code class=\"scala\">scala&gt; <span class=\"keyword\">val</span> rdd = sc.parallelize(<span class=\"type\">Array</span>((<span class=\"number\">3</span>, <span class=\"number\">4</span>),(<span class=\"number\">1</span>, <span class=\"number\">2</span>),(<span class=\"number\">4</span>,<span class=\"number\">4</span>),(<span class=\"number\">2</span>,<span class=\"number\">5</span>), (<span class=\"number\">6</span>,<span class=\"number\">5</span>), (<span class=\"number\">5</span>, <span class=\"number\">6</span>)))  \n\n<span class=\"comment\">// sortByKey不是Action操作，只能算是转换操作</span>\nscala&gt; rdd.sortByKey()\nres9: org.apache.spark.rdd.<span class=\"type\">RDD</span>[(<span class=\"type\">Int</span>, <span class=\"type\">Int</span>)] = <span class=\"type\">ShuffledRDD</span>[<span class=\"number\">28</span>] at sortByKey at &lt;console&gt;:<span class=\"number\">24</span> \n\n<span class=\"comment\">//看看sortByKey后是什么类型</span>\nscala&gt; rdd.sortByKey().collect() \nres10: <span class=\"type\">Array</span>[(<span class=\"type\">Int</span>, <span class=\"type\">Int</span>)] = <span class=\"type\">Array</span>((<span class=\"number\">1</span>,<span class=\"number\">2</span>), (<span class=\"number\">2</span>,<span class=\"number\">5</span>), (<span class=\"number\">3</span>,<span class=\"number\">4</span>), (<span class=\"number\">4</span>,<span class=\"number\">4</span>), (<span class=\"number\">5</span>,<span class=\"number\">6</span>), (<span class=\"number\">6</span>,<span class=\"number\">5</span>)) \n\n<span class=\"comment\">//降序排序</span>\nscala&gt; rdd.sortByKey(<span class=\"literal\">false</span>).collect() \nres12: <span class=\"type\">Array</span>[(<span class=\"type\">Int</span>, <span class=\"type\">Int</span>)] = <span class=\"type\">Array</span>((<span class=\"number\">6</span>,<span class=\"number\">5</span>), (<span class=\"number\">5</span>,<span class=\"number\">6</span>), (<span class=\"number\">4</span>,<span class=\"number\">4</span>), (<span class=\"number\">3</span>,<span class=\"number\">4</span>), (<span class=\"number\">2</span>,<span class=\"number\">5</span>), (<span class=\"number\">1</span>,<span class=\"number\">2</span>))\n</code></pre>\n<p>java例子也是一样的，这里就不写了</p>\n<p>参考文章: Spark快速大数据分析,<a href=\"http://lxw1234.com/archives/2015/07/358.htm\" target=\"_blank\" rel=\"external\">LXW的大数据田地</a>,spark官网  </p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"reduceByKey\"><a href=\"#reduceByKey\" class=\"headerlink\" title=\"reduceByKey\"></a><strong>reduceByKey</strong></h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">def reduceByKey(func: (V, V) =&gt; V): RDD[(K, V)]</div><div class=\"line\"></div><div class=\"line\">def reduceByKey(func: (V, V) =&gt; V, numPartitions: Int): RDD[(K, V)]</div><div class=\"line\"></div><div class=\"line\">def reduceByKey(partitioner: Partitioner, func: (V, V) =&gt; V): RDD[(K, V)]</div></pre></td></tr></table></figure>\n<p>接收一个函数，按照相同的key进行reduce操作，类似于scala的reduce的操作<br>例如RDD {(1, 2), (3, 4), (3, 6)}进行reduce<br><strong>scala版本</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">    <span class=\"keyword\">var</span> mapRDD = sc.parallelize(<span class=\"type\">List</span>((<span class=\"number\">1</span>,<span class=\"number\">2</span>),(<span class=\"number\">3</span>,<span class=\"number\">4</span>),(<span class=\"number\">3</span>,<span class=\"number\">6</span>)))</div><div class=\"line\">    <span class=\"keyword\">var</span> reduceRDD = mapRDD.reduceByKey((x,y)=&gt;x+y)</div><div class=\"line\">    reduceRDD.foreach(x=&gt;println(x))</div><div class=\"line\">------输出---------</div><div class=\"line\">(<span class=\"number\">1</span>,<span class=\"number\">2</span>)</div><div class=\"line\">(<span class=\"number\">3</span>,<span class=\"number\">10</span>)</div></pre></td></tr></table></figure></p>\n<p>再举例<br><strong>单词计数</strong><br>F:\\sparktest\\sample.txt中的内容如下<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">aa bb cc aa aa aa dd dd ee ee ee ee </div><div class=\"line\">ff aa bb zks</div><div class=\"line\">ee kks</div><div class=\"line\">ee  zz zks</div></pre></td></tr></table></figure></p>\n<p><strong>scala版本</strong></p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\">    <span class=\"keyword\">val</span> lines = sc.textFile(<span class=\"string\">\"F:\\\\sparktest\\\\sample.txt\"</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> wordsRDD = lines.flatMap(x=&gt;x.split(<span class=\"string\">\"\\\\s+\"</span>)).map(x=&gt;(x,<span class=\"number\">1</span>))</div><div class=\"line\">    <span class=\"keyword\">val</span> wordCountRDD = wordsRDD.reduceByKey((x,y)=&gt;x+y)</div><div class=\"line\">    wordCountRDD.foreach(x=&gt;println(x))</div><div class=\"line\">---------输出-----------</div><div class=\"line\">(ee,<span class=\"number\">6</span>)</div><div class=\"line\">(aa,<span class=\"number\">5</span>)</div><div class=\"line\">(dd,<span class=\"number\">2</span>)</div><div class=\"line\">(zz,<span class=\"number\">1</span>)</div><div class=\"line\">(zks,<span class=\"number\">2</span>)</div><div class=\"line\">(kks,<span class=\"number\">1</span>)</div><div class=\"line\">(ff,<span class=\"number\">1</span>)</div><div class=\"line\">(bb,<span class=\"number\">2</span>)</div><div class=\"line\">(cc,<span class=\"number\">1</span>)</div></pre></td></tr></table></figure>\n<p><strong>java版本</strong><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div></pre></td><td class=\"code\"><pre><div class=\"line\"> JavaRDD&lt;String&gt; lines = sc.textFile(<span class=\"string\">\"F:\\\\sparktest\\\\sample.txt\"</span>);</div><div class=\"line\"></div><div class=\"line\">        JavaPairRDD&lt;String, Integer&gt; wordPairRDD = lines.flatMapToPair(<span class=\"keyword\">new</span> PairFlatMapFunction&lt;String, String, Integer&gt;() &#123;</div><div class=\"line\">            <span class=\"meta\">@Override</span></div><div class=\"line\">            <span class=\"keyword\">public</span> Iterable&lt;Tuple2&lt;String, Integer&gt;&gt; call(String s) <span class=\"keyword\">throws</span> Exception &#123;</div><div class=\"line\">                ArrayList&lt;Tuple2&lt;String, Integer&gt;&gt; tpLists = <span class=\"keyword\">new</span> ArrayList&lt;Tuple2&lt;String, Integer&gt;&gt;();</div><div class=\"line\">                String[] split = s.split(<span class=\"string\">\"\\\\s+\"</span>);</div><div class=\"line\">                <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt;split.length ; i++) &#123;</div><div class=\"line\">                    Tuple2 tp = <span class=\"keyword\">new</span> Tuple2&lt;String,Integer&gt;(split[i], <span class=\"number\">1</span>);</div><div class=\"line\">                    tpLists.add(tp);</div><div class=\"line\">                &#125;</div><div class=\"line\">            <span class=\"keyword\">return</span> tpLists;</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;);</div><div class=\"line\"></div><div class=\"line\">        JavaPairRDD&lt;String, Integer&gt; wordCountRDD = wordPairRDD.reduceByKey(<span class=\"keyword\">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</div><div class=\"line\">            <span class=\"meta\">@Override</span></div><div class=\"line\">            <span class=\"function\"><span class=\"keyword\">public</span> Integer <span class=\"title\">call</span><span class=\"params\">(Integer i1, Integer i2)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">                <span class=\"keyword\">return</span> i1 + i2;</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;);</div><div class=\"line\">        Map&lt;String, Integer&gt; collectAsMap = wordCountRDD.collectAsMap();</div><div class=\"line\">        <span class=\"keyword\">for</span> (String key:collectAsMap.keySet()) &#123;</div><div class=\"line\">            System.out.println(<span class=\"string\">\"(\"</span>+key+<span class=\"string\">\",\"</span>+collectAsMap.get(key)+<span class=\"string\">\")\"</span>);</div><div class=\"line\">        &#125;</div><div class=\"line\">----------输出-------------------------------</div><div class=\"line\">(kks,<span class=\"number\">1</span>)</div><div class=\"line\">(ee,<span class=\"number\">6</span>)</div><div class=\"line\">(bb,<span class=\"number\">2</span>)</div><div class=\"line\">(zz,<span class=\"number\">1</span>)</div><div class=\"line\">(ff,<span class=\"number\">1</span>)</div><div class=\"line\">(cc,<span class=\"number\">1</span>)</div><div class=\"line\">(zks,<span class=\"number\">2</span>)</div><div class=\"line\">(dd,<span class=\"number\">2</span>)</div><div class=\"line\">(aa,<span class=\"number\">5</span>)</div></pre></td></tr></table></figure></p>\n<h1 id=\"foldByKey\"><a href=\"#foldByKey\" class=\"headerlink\" title=\"foldByKey\"></a><strong>foldByKey</strong></h1><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">foldByKey</span></span>(zeroValue: <span class=\"type\">V</span>)(func: (<span class=\"type\">V</span>, <span class=\"type\">V</span>) =&gt; <span class=\"type\">V</span>): <span class=\"type\">RDD</span>[(<span class=\"type\">K</span>, <span class=\"type\">V</span>)]</div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">foldByKey</span></span>(zeroValue: <span class=\"type\">V</span>, numPartitions: <span class=\"type\">Int</span>)(func: (<span class=\"type\">V</span>, <span class=\"type\">V</span>) =&gt; <span class=\"type\">V</span>): <span class=\"type\">RDD</span>[(<span class=\"type\">K</span>, <span class=\"type\">V</span>)]</div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">foldByKey</span></span>(zeroValue: <span class=\"type\">V</span>, partitioner: <span class=\"type\">Partitioner</span>)(func: (<span class=\"type\">V</span>, <span class=\"type\">V</span>) =&gt; <span class=\"type\">V</span>): <span class=\"type\">RDD</span>[(<span class=\"type\">K</span>, <span class=\"type\">V</span>)]</div></pre></td></tr></table></figure>\n<p>该函数用于RDD[K,V]根据K将V做折叠、合并处理，其中的参数zeroValue表示先根据映射函数将zeroValue应用于V,进行初始化V,再将映射函数应用于初始化后的V.<br>foldByKey可以参考我之前的<a href=\"http://blog.csdn.net/t1dmzks/article/details/69858060#t27\" target=\"_blank\" rel=\"external\">scala的fold的介绍</a><br>与reduce不同的是 foldByKey开始折叠的第一个元素不是集合中的第一个元素，而是传入的一个元素<br><a href=\"http://lxw1234.com/archives/2015/07/358.htm\" target=\"_blank\" rel=\"external\">参考LXW的博客  scala的例子</a><br><strong>直接看例子</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">var</span> rdd1 = sc.makeRDD(<span class=\"type\">Array</span>((<span class=\"string\">\"A\"</span>,<span class=\"number\">0</span>),(<span class=\"string\">\"A\"</span>,<span class=\"number\">2</span>),(<span class=\"string\">\"B\"</span>,<span class=\"number\">1</span>),(<span class=\"string\">\"B\"</span>,<span class=\"number\">2</span>),(<span class=\"string\">\"C\"</span>,<span class=\"number\">1</span>)))</div><div class=\"line\">scala&gt; rdd1.foldByKey(<span class=\"number\">0</span>)(_+_).collect</div><div class=\"line\">res75: <span class=\"type\">Array</span>[(<span class=\"type\">String</span>, <span class=\"type\">Int</span>)] = <span class=\"type\">Array</span>((<span class=\"type\">A</span>,<span class=\"number\">2</span>), (<span class=\"type\">B</span>,<span class=\"number\">3</span>), (<span class=\"type\">C</span>,<span class=\"number\">1</span>)) </div><div class=\"line\"><span class=\"comment\">//将rdd1中每个key对应的V进行累加，注意zeroValue=0,需要先初始化V,映射函数为+操</span></div><div class=\"line\"><span class=\"comment\">//作，比如(\"A\",0), (\"A\",2)，先将zeroValue应用于每个V,得到：(\"A\",0+0), (\"A\",2+0)，即：</span></div><div class=\"line\"><span class=\"comment\">//(\"A\",0), (\"A\",2)，再将映射函数应用于初始化后的V，最后得到(A,0+2),即(A,2)</span></div></pre></td></tr></table></figure></p>\n<p><strong>再看：</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; rdd1.foldByKey(<span class=\"number\">2</span>)(_+_).collect</div><div class=\"line\">res76: <span class=\"type\">Array</span>[(<span class=\"type\">String</span>, <span class=\"type\">Int</span>)] = <span class=\"type\">Array</span>((<span class=\"type\">A</span>,<span class=\"number\">6</span>), (<span class=\"type\">B</span>,<span class=\"number\">7</span>), (<span class=\"type\">C</span>,<span class=\"number\">3</span>))</div><div class=\"line\"><span class=\"comment\">//先将zeroValue=2应用于每个V,得到：(\"A\",0+2), (\"A\",2+2)，即：(\"A\",2), (\"A\",4)，再将映射函</span></div><div class=\"line\"><span class=\"comment\">//数应用于初始化后的V，最后得到：(A,2+4)，即：(A,6)</span></div></pre></td></tr></table></figure></p>\n<p><strong>再看乘法操作：</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; rdd1.foldByKey(<span class=\"number\">0</span>)(_*_).collect</div><div class=\"line\">res77: <span class=\"type\">Array</span>[(<span class=\"type\">String</span>, <span class=\"type\">Int</span>)] = <span class=\"type\">Array</span>((<span class=\"type\">A</span>,<span class=\"number\">0</span>), (<span class=\"type\">B</span>,<span class=\"number\">0</span>), (<span class=\"type\">C</span>,<span class=\"number\">0</span>))</div><div class=\"line\"><span class=\"comment\">//先将zeroValue=0应用于每个V,注意，这次映射函数为乘法，得到：(\"A\",0*0), (\"A\",2*0)，</span></div><div class=\"line\"><span class=\"comment\">//即：(\"A\",0), (\"A\",0)，再将映射函//数应用于初始化后的V，最后得到：(A,0*0)，即：(A,0)</span></div><div class=\"line\"><span class=\"comment\">//其他K也一样，最终都得到了V=0</span></div><div class=\"line\"> </div><div class=\"line\">scala&gt; rdd1.foldByKey(<span class=\"number\">1</span>)(_*_).collect</div><div class=\"line\">res78: <span class=\"type\">Array</span>[(<span class=\"type\">String</span>, <span class=\"type\">Int</span>)] = <span class=\"type\">Array</span>((<span class=\"type\">A</span>,<span class=\"number\">0</span>), (<span class=\"type\">B</span>,<span class=\"number\">2</span>), (<span class=\"type\">C</span>,<span class=\"number\">1</span>))</div><div class=\"line\"><span class=\"comment\">//映射函数为乘法时，需要将zeroValue设为1，才能得到我们想要的结果。</span></div></pre></td></tr></table></figure></p>\n<h1 id=\"SortByKey\"><a href=\"#SortByKey\" class=\"headerlink\" title=\"SortByKey\"></a><strong>SortByKey</strong></h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">def sortByKey(ascending : scala.Boolean = &#123; /* compiled code */ &#125;, numPartitions : scala.Int = &#123; /* compiled code */ &#125;) : org.apache.spark.rdd.RDD[scala.Tuple2[K, V]] = &#123; /* compiled code */ &#125;</div></pre></td></tr></table></figure>\n<p>SortByKey用于对pairRDD按照key进行排序，第一个参数可以设置true或者false，默认是true<br><strong>scala例子</strong></p>\n<pre><code class=\"scala\">scala&gt; <span class=\"keyword\">val</span> rdd = sc.parallelize(<span class=\"type\">Array</span>((<span class=\"number\">3</span>, <span class=\"number\">4</span>),(<span class=\"number\">1</span>, <span class=\"number\">2</span>),(<span class=\"number\">4</span>,<span class=\"number\">4</span>),(<span class=\"number\">2</span>,<span class=\"number\">5</span>), (<span class=\"number\">6</span>,<span class=\"number\">5</span>), (<span class=\"number\">5</span>, <span class=\"number\">6</span>)))  \n\n<span class=\"comment\">// sortByKey不是Action操作，只能算是转换操作</span>\nscala&gt; rdd.sortByKey()\nres9: org.apache.spark.rdd.<span class=\"type\">RDD</span>[(<span class=\"type\">Int</span>, <span class=\"type\">Int</span>)] = <span class=\"type\">ShuffledRDD</span>[<span class=\"number\">28</span>] at sortByKey at &lt;console&gt;:<span class=\"number\">24</span> \n\n<span class=\"comment\">//看看sortByKey后是什么类型</span>\nscala&gt; rdd.sortByKey().collect() \nres10: <span class=\"type\">Array</span>[(<span class=\"type\">Int</span>, <span class=\"type\">Int</span>)] = <span class=\"type\">Array</span>((<span class=\"number\">1</span>,<span class=\"number\">2</span>), (<span class=\"number\">2</span>,<span class=\"number\">5</span>), (<span class=\"number\">3</span>,<span class=\"number\">4</span>), (<span class=\"number\">4</span>,<span class=\"number\">4</span>), (<span class=\"number\">5</span>,<span class=\"number\">6</span>), (<span class=\"number\">6</span>,<span class=\"number\">5</span>)) \n\n<span class=\"comment\">//降序排序</span>\nscala&gt; rdd.sortByKey(<span class=\"literal\">false</span>).collect() \nres12: <span class=\"type\">Array</span>[(<span class=\"type\">Int</span>, <span class=\"type\">Int</span>)] = <span class=\"type\">Array</span>((<span class=\"number\">6</span>,<span class=\"number\">5</span>), (<span class=\"number\">5</span>,<span class=\"number\">6</span>), (<span class=\"number\">4</span>,<span class=\"number\">4</span>), (<span class=\"number\">3</span>,<span class=\"number\">4</span>), (<span class=\"number\">2</span>,<span class=\"number\">5</span>), (<span class=\"number\">1</span>,<span class=\"number\">2</span>))\n</code></pre>\n<p>java例子也是一样的，这里就不写了</p>\n<p>参考文章: Spark快速大数据分析,<a href=\"http://lxw1234.com/archives/2015/07/358.htm\" target=\"_blank\" rel=\"external\">LXW的大数据田地</a>,spark官网  </p>\n"},{"title":"Hadoop入门案例（一） wordcount","date":"2016-07-16T13:25:21.000Z","author":"kaishun","id":"13","_content":"\n个人感想： \n虽然现在一直用的是Spark，但是由于目前公司是将Hadoop的程序转移到Spark，这个过程需要用到hadoop，加上现在大多数的大数据平台依旧还是使用的MapReduce的编程模型，所以MapReduce也应该是比较熟悉才行。并且，一般的大数据学习，都是从一些分布式计算框架开始看起，MapReduce一般都是必学的内容，接下来几篇文章，不讲述原理（因为原理网上太多了），只记录一些非常简单，但却非常实用应用案例，以便以后自己查看阅读\n<!-- more -->\n\n# **1. 需求说明**\n大数据中，经常可能会碰到一些需要单词的出现个数，例如top n 等等。下面介绍一个hadoop的入门案例，对一个或多个文本中的单词进行统计\n## 1.1 需求输入\n输入一个或者多个文本  测试的文本内容如下\n```\naa bb cc aa aa aa dd dd ee ee ee ee \nff aa bb zks\nee kks\nee  zz zks\n```\n## **1.2 需求输出**\n将文本中的内容按照单词进行计数，并且将各个单词的统计记录到制定的路径下\n# **2. 代码如下**  \n```java\npackage com.myhadoop.mapreduce.test;\n\nimport java.io.IOException;\nimport java.util.*;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.conf.*;\nimport org.apache.hadoop.io.*;\nimport org.apache.hadoop.mapreduce.lib.input.*;\nimport org.apache.hadoop.mapreduce.lib.output.*;\nimport org.apache.hadoop.mapreduce.*;\n\n\npublic class WordCount{\n\n\tpublic static class Map extends Mapper<LongWritable, Text, Text, IntWritable>\n\t{\n\t\tprivate final static IntWritable one = new IntWritable(1);\n\t\tprivate Text word = new Text();\n\t\tpublic void map(LongWritable key,Text value,Context context) throws IOException,InterruptedException\n\t\t{\n\t\t\tString lines = value.toString();\n\t\t\tStringTokenizer tokenizer = new StringTokenizer(lines,\" \");\n\t\t\twhile(tokenizer.hasMoreElements())\n\t\t\t{\n\t\t\t\tword.set(tokenizer.nextToken());\n\t\t\t\tcontext.write(word, one);\n\t\t\t}\n\t\t}\n\t}\n\n\tpublic static class Reduce extends Reducer<Text, IntWritable, Text, IntWritable>\n\t{\n\t\tpublic void reduce(Text key,Iterable<IntWritable> values,Context context) throws IOException,InterruptedException\n\t\t{\n\t\t\tint sum = 0;\n\t\t\tfor (IntWritable val : values) {\n\t\t\t\tsum += val.get();\n\t\t\t}\n\t\t\tcontext.write(key, new IntWritable(sum));\n\t\t}\n\t}\n\n\tpublic static void main(String[] args) throws Exception {\n\t\tConfiguration conf = new Configuration();\n\t\tJob job = Job.getInstance(conf);\n\t\tjob.setJarByClass(WordCount.class);\n\t\tjob.setJobName(\"WordCount\");\n\n\t\tjob.setOutputKeyClass(Text.class);\n\t\tjob.setOutputValueClass(IntWritable.class);\n\n\t\tjob.setMapperClass(Map.class);\n\t\tjob.setReducerClass(Reduce.class);\n\n\t\tjob.setInputFormatClass(TextInputFormat.class);\n\t\tjob.setOutputFormatClass(TextOutputFormat.class);\n\t\tFileInputFormat.setInputPaths(job, new Path(args[0]));\n\t\tFileOutputFormat.setOutputPath(job, new Path(args[1]));\n\t\tSystem.exit(job.waitForCompletion(true) ? 0 : 1);\n\n\t}\n}\n\n\n\n```  \n# **3. 代码输出**\n```\naa\t5\nbb\t2\ncc\t1\ndd\t2\nee\t6\nff\t1\nkks\t1\nzks\t2\nzz\t1\n```\n# **4. 代码解析**  \n原理：先将文本每行内容进行分割，每行都得到一些单词，将单词都转变成map，并且key设置成单词，value设置成1，经过shuffle，把相同key的value放在一个list中，reduce过程把  \n相同key中的value进行相加，最后输出key为单词，value为总数\n**Map类：**用于将每一行的单词变成map，如 (aa,1),(bb,1)...等。    \n输入是： LongWritable, Text  \n输出是： Text, IntWritable\n**Reduce类：**用于将map后，并且经过shuffle的数据，进行整合处理  \n输入是：Text，Iterable<IntWritable>  \n输出是：Text, IntWritable  \n","source":"_posts/Hadoop入门案例（一） wordcount.md","raw":"---\ntitle: Hadoop入门案例（一） wordcount\ndate: 2016-07-16 21:25:21\ntags: [hadoop]\ncategories: [大数据,hadoop]\nauthor: kaishun\nid: 13\npermalink: hadoop-example-1\n---\n\n个人感想： \n虽然现在一直用的是Spark，但是由于目前公司是将Hadoop的程序转移到Spark，这个过程需要用到hadoop，加上现在大多数的大数据平台依旧还是使用的MapReduce的编程模型，所以MapReduce也应该是比较熟悉才行。并且，一般的大数据学习，都是从一些分布式计算框架开始看起，MapReduce一般都是必学的内容，接下来几篇文章，不讲述原理（因为原理网上太多了），只记录一些非常简单，但却非常实用应用案例，以便以后自己查看阅读\n<!-- more -->\n\n# **1. 需求说明**\n大数据中，经常可能会碰到一些需要单词的出现个数，例如top n 等等。下面介绍一个hadoop的入门案例，对一个或多个文本中的单词进行统计\n## 1.1 需求输入\n输入一个或者多个文本  测试的文本内容如下\n```\naa bb cc aa aa aa dd dd ee ee ee ee \nff aa bb zks\nee kks\nee  zz zks\n```\n## **1.2 需求输出**\n将文本中的内容按照单词进行计数，并且将各个单词的统计记录到制定的路径下\n# **2. 代码如下**  \n```java\npackage com.myhadoop.mapreduce.test;\n\nimport java.io.IOException;\nimport java.util.*;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.conf.*;\nimport org.apache.hadoop.io.*;\nimport org.apache.hadoop.mapreduce.lib.input.*;\nimport org.apache.hadoop.mapreduce.lib.output.*;\nimport org.apache.hadoop.mapreduce.*;\n\n\npublic class WordCount{\n\n\tpublic static class Map extends Mapper<LongWritable, Text, Text, IntWritable>\n\t{\n\t\tprivate final static IntWritable one = new IntWritable(1);\n\t\tprivate Text word = new Text();\n\t\tpublic void map(LongWritable key,Text value,Context context) throws IOException,InterruptedException\n\t\t{\n\t\t\tString lines = value.toString();\n\t\t\tStringTokenizer tokenizer = new StringTokenizer(lines,\" \");\n\t\t\twhile(tokenizer.hasMoreElements())\n\t\t\t{\n\t\t\t\tword.set(tokenizer.nextToken());\n\t\t\t\tcontext.write(word, one);\n\t\t\t}\n\t\t}\n\t}\n\n\tpublic static class Reduce extends Reducer<Text, IntWritable, Text, IntWritable>\n\t{\n\t\tpublic void reduce(Text key,Iterable<IntWritable> values,Context context) throws IOException,InterruptedException\n\t\t{\n\t\t\tint sum = 0;\n\t\t\tfor (IntWritable val : values) {\n\t\t\t\tsum += val.get();\n\t\t\t}\n\t\t\tcontext.write(key, new IntWritable(sum));\n\t\t}\n\t}\n\n\tpublic static void main(String[] args) throws Exception {\n\t\tConfiguration conf = new Configuration();\n\t\tJob job = Job.getInstance(conf);\n\t\tjob.setJarByClass(WordCount.class);\n\t\tjob.setJobName(\"WordCount\");\n\n\t\tjob.setOutputKeyClass(Text.class);\n\t\tjob.setOutputValueClass(IntWritable.class);\n\n\t\tjob.setMapperClass(Map.class);\n\t\tjob.setReducerClass(Reduce.class);\n\n\t\tjob.setInputFormatClass(TextInputFormat.class);\n\t\tjob.setOutputFormatClass(TextOutputFormat.class);\n\t\tFileInputFormat.setInputPaths(job, new Path(args[0]));\n\t\tFileOutputFormat.setOutputPath(job, new Path(args[1]));\n\t\tSystem.exit(job.waitForCompletion(true) ? 0 : 1);\n\n\t}\n}\n\n\n\n```  \n# **3. 代码输出**\n```\naa\t5\nbb\t2\ncc\t1\ndd\t2\nee\t6\nff\t1\nkks\t1\nzks\t2\nzz\t1\n```\n# **4. 代码解析**  \n原理：先将文本每行内容进行分割，每行都得到一些单词，将单词都转变成map，并且key设置成单词，value设置成1，经过shuffle，把相同key的value放在一个list中，reduce过程把  \n相同key中的value进行相加，最后输出key为单词，value为总数\n**Map类：**用于将每一行的单词变成map，如 (aa,1),(bb,1)...等。    \n输入是： LongWritable, Text  \n输出是： Text, IntWritable\n**Reduce类：**用于将map后，并且经过shuffle的数据，进行整合处理  \n输入是：Text，Iterable<IntWritable>  \n输出是：Text, IntWritable  \n","slug":"hadoop-example-1","published":1,"updated":"2018-01-23T14:12:45.072Z","_id":"cjcrpnzrk003a2wv3qdyuepl2","comments":1,"layout":"post","photos":[],"link":"","content":"<p>个人感想：<br>虽然现在一直用的是Spark，但是由于目前公司是将Hadoop的程序转移到Spark，这个过程需要用到hadoop，加上现在大多数的大数据平台依旧还是使用的MapReduce的编程模型，所以MapReduce也应该是比较熟悉才行。并且，一般的大数据学习，都是从一些分布式计算框架开始看起，MapReduce一般都是必学的内容，接下来几篇文章，不讲述原理（因为原理网上太多了），只记录一些非常简单，但却非常实用应用案例，以便以后自己查看阅读<br><a id=\"more\"></a></p>\n<h1 id=\"1-需求说明\"><a href=\"#1-需求说明\" class=\"headerlink\" title=\"1. 需求说明\"></a><strong>1. 需求说明</strong></h1><p>大数据中，经常可能会碰到一些需要单词的出现个数，例如top n 等等。下面介绍一个hadoop的入门案例，对一个或多个文本中的单词进行统计</p>\n<h2 id=\"1-1-需求输入\"><a href=\"#1-1-需求输入\" class=\"headerlink\" title=\"1.1 需求输入\"></a>1.1 需求输入</h2><p>输入一个或者多个文本  测试的文本内容如下<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">aa bb cc aa aa aa dd dd ee ee ee ee </div><div class=\"line\">ff aa bb zks</div><div class=\"line\">ee kks</div><div class=\"line\">ee  zz zks</div></pre></td></tr></table></figure></p>\n<h2 id=\"1-2-需求输出\"><a href=\"#1-2-需求输出\" class=\"headerlink\" title=\"1.2 需求输出\"></a><strong>1.2 需求输出</strong></h2><p>将文本中的内容按照单词进行计数，并且将各个单词的统计记录到制定的路径下</p>\n<h1 id=\"2-代码如下\"><a href=\"#2-代码如下\" class=\"headerlink\" title=\"2. 代码如下\"></a><strong>2. 代码如下</strong></h1><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">package</span> com.myhadoop.mapreduce.test;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">import</span> java.io.IOException;</div><div class=\"line\"><span class=\"keyword\">import</span> java.util.*;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.fs.Path;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.conf.*;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.*;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.lib.input.*;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.lib.output.*;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.*;</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">WordCount</span></span>&#123;</div><div class=\"line\"></div><div class=\"line\">\t<span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Map</span> <span class=\"keyword\">extends</span> <span class=\"title\">Mapper</span>&lt;<span class=\"title\">LongWritable</span>, <span class=\"title\">Text</span>, <span class=\"title\">Text</span>, <span class=\"title\">IntWritable</span>&gt;</span></div><div class=\"line\"><span class=\"class\">\t</span>&#123;</div><div class=\"line\">\t\t<span class=\"keyword\">private</span> <span class=\"keyword\">final</span> <span class=\"keyword\">static</span> IntWritable one = <span class=\"keyword\">new</span> IntWritable(<span class=\"number\">1</span>);</div><div class=\"line\">\t\t<span class=\"keyword\">private</span> Text word = <span class=\"keyword\">new</span> Text();</div><div class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">map</span><span class=\"params\">(LongWritable key,Text value,Context context)</span> <span class=\"keyword\">throws</span> IOException,InterruptedException</span></div><div class=\"line\"><span class=\"function\">\t\t</span>&#123;</div><div class=\"line\">\t\t\tString lines = value.toString();</div><div class=\"line\">\t\t\tStringTokenizer tokenizer = <span class=\"keyword\">new</span> StringTokenizer(lines,<span class=\"string\">\" \"</span>);</div><div class=\"line\">\t\t\t<span class=\"keyword\">while</span>(tokenizer.hasMoreElements())</div><div class=\"line\">\t\t\t&#123;</div><div class=\"line\">\t\t\t\tword.set(tokenizer.nextToken());</div><div class=\"line\">\t\t\t\tcontext.write(word, one);</div><div class=\"line\">\t\t\t&#125;</div><div class=\"line\">\t\t&#125;</div><div class=\"line\">\t&#125;</div><div class=\"line\"></div><div class=\"line\">\t<span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Reduce</span> <span class=\"keyword\">extends</span> <span class=\"title\">Reducer</span>&lt;<span class=\"title\">Text</span>, <span class=\"title\">IntWritable</span>, <span class=\"title\">Text</span>, <span class=\"title\">IntWritable</span>&gt;</span></div><div class=\"line\"><span class=\"class\">\t</span>&#123;</div><div class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">reduce</span><span class=\"params\">(Text key,Iterable&lt;IntWritable&gt; values,Context context)</span> <span class=\"keyword\">throws</span> IOException,InterruptedException</span></div><div class=\"line\"><span class=\"function\">\t\t</span>&#123;</div><div class=\"line\">\t\t\t<span class=\"keyword\">int</span> sum = <span class=\"number\">0</span>;</div><div class=\"line\">\t\t\t<span class=\"keyword\">for</span> (IntWritable val : values) &#123;</div><div class=\"line\">\t\t\t\tsum += val.get();</div><div class=\"line\">\t\t\t&#125;</div><div class=\"line\">\t\t\tcontext.write(key, <span class=\"keyword\">new</span> IntWritable(sum));</div><div class=\"line\">\t\t&#125;</div><div class=\"line\">\t&#125;</div><div class=\"line\"></div><div class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">\t\tConfiguration conf = <span class=\"keyword\">new</span> Configuration();</div><div class=\"line\">\t\tJob job = Job.getInstance(conf);</div><div class=\"line\">\t\tjob.setJarByClass(WordCount.class);</div><div class=\"line\">\t\tjob.setJobName(<span class=\"string\">\"WordCount\"</span>);</div><div class=\"line\"></div><div class=\"line\">\t\tjob.setOutputKeyClass(Text.class);</div><div class=\"line\">\t\tjob.setOutputValueClass(IntWritable.class);</div><div class=\"line\"></div><div class=\"line\">\t\tjob.setMapperClass(Map.class);</div><div class=\"line\">\t\tjob.setReducerClass(Reduce.class);</div><div class=\"line\"></div><div class=\"line\">\t\tjob.setInputFormatClass(TextInputFormat.class);</div><div class=\"line\">\t\tjob.setOutputFormatClass(TextOutputFormat.class);</div><div class=\"line\">\t\tFileInputFormat.setInputPaths(job, <span class=\"keyword\">new</span> Path(args[<span class=\"number\">0</span>]));</div><div class=\"line\">\t\tFileOutputFormat.setOutputPath(job, <span class=\"keyword\">new</span> Path(args[<span class=\"number\">1</span>]));</div><div class=\"line\">\t\tSystem.exit(job.waitForCompletion(<span class=\"keyword\">true</span>) ? <span class=\"number\">0</span> : <span class=\"number\">1</span>);</div><div class=\"line\"></div><div class=\"line\">\t&#125;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">```  </div><div class=\"line\"># **3. 代码输出**</div></pre></td></tr></table></figure>\n<p>aa    5<br>bb    2<br>cc    1<br>dd    2<br>ee    6<br>ff    1<br>kks    1<br>zks    2<br>zz    1<br>```</p>\n<h1 id=\"4-代码解析\"><a href=\"#4-代码解析\" class=\"headerlink\" title=\"4. 代码解析\"></a><strong>4. 代码解析</strong></h1><p>原理：先将文本每行内容进行分割，每行都得到一些单词，将单词都转变成map，并且key设置成单词，value设置成1，经过shuffle，把相同key的value放在一个list中，reduce过程把<br>相同key中的value进行相加，最后输出key为单词，value为总数<br><strong>Map类：</strong>用于将每一行的单词变成map，如 (aa,1),(bb,1)…等。<br>输入是： LongWritable, Text<br>输出是： Text, IntWritable<br><strong>Reduce类：</strong>用于将map后，并且经过shuffle的数据，进行整合处理<br>输入是：Text，Iterable<intwritable><br>输出是：Text, IntWritable  </intwritable></p>\n","site":{"data":{}},"excerpt":"<p>个人感想：<br>虽然现在一直用的是Spark，但是由于目前公司是将Hadoop的程序转移到Spark，这个过程需要用到hadoop，加上现在大多数的大数据平台依旧还是使用的MapReduce的编程模型，所以MapReduce也应该是比较熟悉才行。并且，一般的大数据学习，都是从一些分布式计算框架开始看起，MapReduce一般都是必学的内容，接下来几篇文章，不讲述原理（因为原理网上太多了），只记录一些非常简单，但却非常实用应用案例，以便以后自己查看阅读<br>","more":"</p>\n<h1 id=\"1-需求说明\"><a href=\"#1-需求说明\" class=\"headerlink\" title=\"1. 需求说明\"></a><strong>1. 需求说明</strong></h1><p>大数据中，经常可能会碰到一些需要单词的出现个数，例如top n 等等。下面介绍一个hadoop的入门案例，对一个或多个文本中的单词进行统计</p>\n<h2 id=\"1-1-需求输入\"><a href=\"#1-1-需求输入\" class=\"headerlink\" title=\"1.1 需求输入\"></a>1.1 需求输入</h2><p>输入一个或者多个文本  测试的文本内容如下<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">aa bb cc aa aa aa dd dd ee ee ee ee </div><div class=\"line\">ff aa bb zks</div><div class=\"line\">ee kks</div><div class=\"line\">ee  zz zks</div></pre></td></tr></table></figure></p>\n<h2 id=\"1-2-需求输出\"><a href=\"#1-2-需求输出\" class=\"headerlink\" title=\"1.2 需求输出\"></a><strong>1.2 需求输出</strong></h2><p>将文本中的内容按照单词进行计数，并且将各个单词的统计记录到制定的路径下</p>\n<h1 id=\"2-代码如下\"><a href=\"#2-代码如下\" class=\"headerlink\" title=\"2. 代码如下\"></a><strong>2. 代码如下</strong></h1><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">package</span> com.myhadoop.mapreduce.test;</div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">import</span> java.io.IOException;</div><div class=\"line\"><span class=\"keyword\">import</span> java.util.*;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.fs.Path;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.conf.*;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.*;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.lib.input.*;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.lib.output.*;</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.*;</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"><span class=\"keyword\">public</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">WordCount</span></span>&#123;</div><div class=\"line\"></div><div class=\"line\">\t<span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Map</span> <span class=\"keyword\">extends</span> <span class=\"title\">Mapper</span>&lt;<span class=\"title\">LongWritable</span>, <span class=\"title\">Text</span>, <span class=\"title\">Text</span>, <span class=\"title\">IntWritable</span>&gt;</span></div><div class=\"line\"><span class=\"class\">\t</span>&#123;</div><div class=\"line\">\t\t<span class=\"keyword\">private</span> <span class=\"keyword\">final</span> <span class=\"keyword\">static</span> IntWritable one = <span class=\"keyword\">new</span> IntWritable(<span class=\"number\">1</span>);</div><div class=\"line\">\t\t<span class=\"keyword\">private</span> Text word = <span class=\"keyword\">new</span> Text();</div><div class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">map</span><span class=\"params\">(LongWritable key,Text value,Context context)</span> <span class=\"keyword\">throws</span> IOException,InterruptedException</span></div><div class=\"line\"><span class=\"function\">\t\t</span>&#123;</div><div class=\"line\">\t\t\tString lines = value.toString();</div><div class=\"line\">\t\t\tStringTokenizer tokenizer = <span class=\"keyword\">new</span> StringTokenizer(lines,<span class=\"string\">\" \"</span>);</div><div class=\"line\">\t\t\t<span class=\"keyword\">while</span>(tokenizer.hasMoreElements())</div><div class=\"line\">\t\t\t&#123;</div><div class=\"line\">\t\t\t\tword.set(tokenizer.nextToken());</div><div class=\"line\">\t\t\t\tcontext.write(word, one);</div><div class=\"line\">\t\t\t&#125;</div><div class=\"line\">\t\t&#125;</div><div class=\"line\">\t&#125;</div><div class=\"line\"></div><div class=\"line\">\t<span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Reduce</span> <span class=\"keyword\">extends</span> <span class=\"title\">Reducer</span>&lt;<span class=\"title\">Text</span>, <span class=\"title\">IntWritable</span>, <span class=\"title\">Text</span>, <span class=\"title\">IntWritable</span>&gt;</span></div><div class=\"line\"><span class=\"class\">\t</span>&#123;</div><div class=\"line\">\t\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">reduce</span><span class=\"params\">(Text key,Iterable&lt;IntWritable&gt; values,Context context)</span> <span class=\"keyword\">throws</span> IOException,InterruptedException</span></div><div class=\"line\"><span class=\"function\">\t\t</span>&#123;</div><div class=\"line\">\t\t\t<span class=\"keyword\">int</span> sum = <span class=\"number\">0</span>;</div><div class=\"line\">\t\t\t<span class=\"keyword\">for</span> (IntWritable val : values) &#123;</div><div class=\"line\">\t\t\t\tsum += val.get();</div><div class=\"line\">\t\t\t&#125;</div><div class=\"line\">\t\t\tcontext.write(key, <span class=\"keyword\">new</span> IntWritable(sum));</div><div class=\"line\">\t\t&#125;</div><div class=\"line\">\t&#125;</div><div class=\"line\"></div><div class=\"line\">\t<span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">\t\tConfiguration conf = <span class=\"keyword\">new</span> Configuration();</div><div class=\"line\">\t\tJob job = Job.getInstance(conf);</div><div class=\"line\">\t\tjob.setJarByClass(WordCount.class);</div><div class=\"line\">\t\tjob.setJobName(<span class=\"string\">\"WordCount\"</span>);</div><div class=\"line\"></div><div class=\"line\">\t\tjob.setOutputKeyClass(Text.class);</div><div class=\"line\">\t\tjob.setOutputValueClass(IntWritable.class);</div><div class=\"line\"></div><div class=\"line\">\t\tjob.setMapperClass(Map.class);</div><div class=\"line\">\t\tjob.setReducerClass(Reduce.class);</div><div class=\"line\"></div><div class=\"line\">\t\tjob.setInputFormatClass(TextInputFormat.class);</div><div class=\"line\">\t\tjob.setOutputFormatClass(TextOutputFormat.class);</div><div class=\"line\">\t\tFileInputFormat.setInputPaths(job, <span class=\"keyword\">new</span> Path(args[<span class=\"number\">0</span>]));</div><div class=\"line\">\t\tFileOutputFormat.setOutputPath(job, <span class=\"keyword\">new</span> Path(args[<span class=\"number\">1</span>]));</div><div class=\"line\">\t\tSystem.exit(job.waitForCompletion(<span class=\"keyword\">true</span>) ? <span class=\"number\">0</span> : <span class=\"number\">1</span>);</div><div class=\"line\"></div><div class=\"line\">\t&#125;</div><div class=\"line\">&#125;</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">```  </div><div class=\"line\"># **3. 代码输出**</div></pre></td></tr></table></figure>\n<p>aa    5<br>bb    2<br>cc    1<br>dd    2<br>ee    6<br>ff    1<br>kks    1<br>zks    2<br>zz    1<br>```</p>\n<h1 id=\"4-代码解析\"><a href=\"#4-代码解析\" class=\"headerlink\" title=\"4. 代码解析\"></a><strong>4. 代码解析</strong></h1><p>原理：先将文本每行内容进行分割，每行都得到一些单词，将单词都转变成map，并且key设置成单词，value设置成1，经过shuffle，把相同key的value放在一个list中，reduce过程把<br>相同key中的value进行相加，最后输出key为单词，value为总数<br><strong>Map类：</strong>用于将每一行的单词变成map，如 (aa,1),(bb,1)…等。<br>输入是： LongWritable, Text<br>输出是： Text, IntWritable<br><strong>Reduce类：</strong>用于将map后，并且经过shuffle的数据，进行整合处理<br>输入是：Text，Iterable<intwritable><br>输出是：Text, IntWritable  </intwritable></p>"},{"title":"spark RDD算子（十一）之RDD Action 保存操作saveAsTextFile,saveAsSequenceFile,saveAsObjectFile,saveAsHadoopFile 等","date":"2017-04-11T13:25:21.000Z","author":"kaishun","id":"45","_content":"---\n\n关键字:Spark算子、Spark函数、Spark RDD行动Action、Spark RDD存储操作、saveAsTextFile、saveAsSequenceFile、saveAsObjectFile,saveAsHadoopFile、saveAsHadoopDataset,saveAsNewAPIHadoopFile、saveAsNewAPIHadoopDataset \n\n---\n\n# **saveAsTextFile**\ndef saveAsTextFile(path: String): Unit\n\ndef saveAsTextFile(path: String, codec: Class[_ <: CompressionCodec]): Unit\n\nsaveAsTextFile用于将RDD以文本文件的格式存储到文件系统中。\n\ncodec参数可以指定压缩的类名。\n```scala\nvar rdd1 = sc.makeRDD(1 to 10,2)\nscala> rdd1.saveAsTextFile(\"hdfs://cdh5/tmp/lxw1234.com/\") //保存到HDFS\nhadoop fs -ls /tmp/lxw1234.com\nFound 2 items\n-rw-r--r--   2 lxw1234 supergroup        0 2015-07-10 09:15 /tmp/lxw1234.com/_SUCCESS\n-rw-r--r--   2 lxw1234 supergroup        21 2015-07-10 09:15 /tmp/lxw1234.com/part-00000\n \nhadoop fs -cat /tmp/lxw1234.com/part-00000\n```\n注意：如果使用rdd1.saveAsTextFile(“file:///tmp/lxw1234.com”)将文件保存到本地文件系统，那么只会保存在Executor所在机器的本地目录。  \n**指定压缩格式保存**  \n```scala\nrdd1.saveAsTextFile(\"hdfs://cdh5/tmp/lxw1234.com/\",classOf[com.hadoop.compression.lzo.LzopCodec])\n \nhadoop fs -ls /tmp/lxw1234.com\n-rw-r--r--   2 lxw1234 supergroup    0 2015-07-10 09:20 /tmp/lxw1234.com/_SUCCESS\n-rw-r--r--   2 lxw1234 supergroup    71 2015-07-10 09:20 /tmp/lxw1234.com/part-00000.lzo\n \nhadoop fs -text /tmp/lxw1234.com/part-00000.lzo\n```\n# **saveAsSequenceFile**  \nsaveAsSequenceFile用于将RDD以SequenceFile的文件格式保存到HDFS上。\n\n用法同saveAsTextFile。\n\n\n# **saveAsObjectFile**\ndef saveAsObjectFile(path: String): Unit\n\nsaveAsObjectFile用于将RDD中的元素序列化成对象，存储到文件中。\n\n对于HDFS，默认采用SequenceFile保存。  \n```scala\nvar rdd1 = sc.makeRDD(1 to 10,2)\nrdd1.saveAsObjectFile(\"hdfs://cdh5/tmp/lxw1234.com/\")\n \nhadoop fs -cat /tmp/lxw1234.com/part-00000\nSEQ !org.apache.hadoop.io.NullWritable\"org.apache.hadoop.io.BytesWritableT\n```\n# **saveAsHadoopFile**  \ndef saveAsHadoopFile(path: String, keyClass: Class[_], valueClass: Class[_], outputFormatClass: Class[_ <: OutputFormat[_, _]], codec: Class[_ <: CompressionCodec]): Unit\n\ndef saveAsHadoopFile(path: String, keyClass: Class[_], valueClass: Class[_], outputFormatClass: Class[_ <: OutputFormat[_, _]], conf: JobConf = …, codec: Option[Class[_ <: CompressionCodec]] = None): Unit\n\nsaveAsHadoopFile是将RDD存储在HDFS上的文件中，支持老版本Hadoop API。\n\n可以指定outputKeyClass、outputValueClass以及压缩格式。\n\n每个分区输出一个文件。\n```scala\nvar rdd1 = sc.makeRDD(Array((\"A\",2),(\"A\",1),(\"B\",6),(\"B\",3),(\"B\",7)))\n \nimport org.apache.hadoop.mapred.TextOutputFormat\nimport org.apache.hadoop.io.Text\nimport org.apache.hadoop.io.IntWritable\n \nrdd1.saveAsHadoopFile(\"/tmp/lxw1234.com/\",classOf[Text],classOf[IntWritable],classOf[TextOutputFormat[Text,IntWritable]])\n \nrdd1.saveAsHadoopFile(\"/tmp/lxw1234.com/\",classOf[Text],classOf[IntWritable],classOf[TextOutputFormat[Text,IntWritable]],\n                      classOf[com.hadoop.compression.lzo.LzopCodec])\n```  \n# **saveAsHadoopDataset**  \ndef saveAsHadoopDataset(conf: JobConf): Unit\n\nsaveAsHadoopDataset用于将RDD保存到除了HDFS的其他存储中，比如HBase。\n\n在JobConf中，通常需要关注或者设置五个参数：\n\n文件的保存路径、key值的class类型、value值的class类型、RDD的输出格式(OutputFormat)、以及压缩相关的参数。  \n**##使用saveAsHadoopDataset将RDD保存到HDFS中**  \n```scala\nimport org.apache.spark.SparkConf\nimport org.apache.spark.SparkContext\nimport SparkContext._\nimport org.apache.hadoop.mapred.TextOutputFormat\nimport org.apache.hadoop.io.Text\nimport org.apache.hadoop.io.IntWritable\nimport org.apache.hadoop.mapred.JobConf\n \n \n \nvar rdd1 = sc.makeRDD(Array((\"A\",2),(\"A\",1),(\"B\",6),(\"B\",3),(\"B\",7)))\nvar jobConf = new JobConf()\njobConf.setOutputFormat(classOf[TextOutputFormat[Text,IntWritable]])\njobConf.setOutputKeyClass(classOf[Text])\njobConf.setOutputValueClass(classOf[IntWritable])\njobConf.set(\"mapred.output.dir\",\"/tmp/lxw1234/\")\nrdd1.saveAsHadoopDataset(jobConf)\n \n结果：\nhadoop fs -cat /tmp/lxw1234/part-00000\nA       2\nA       1\nhadoop fs -cat /tmp/lxw1234/part-00001\nB       6\nB       3\nB       7\n```  \n**##保存数据到HBASE**  \nHBase建表：\n\ncreate ‘lxw1234′,{NAME => ‘f1′,VERSIONS => 1},{NAME => ‘f2′,VERSIONS => 1},{NAME => ‘f3′,VERSIONS => 1}  \n```scala\nimport org.apache.spark.SparkConf\nimport org.apache.spark.SparkContext\nimport SparkContext._\nimport org.apache.hadoop.mapred.TextOutputFormat\nimport org.apache.hadoop.io.Text\nimport org.apache.hadoop.io.IntWritable\nimport org.apache.hadoop.mapred.JobConf\nimport org.apache.hadoop.hbase.HBaseConfiguration\nimport org.apache.hadoop.hbase.mapred.TableOutputFormat\nimport org.apache.hadoop.hbase.client.Put\nimport org.apache.hadoop.hbase.util.Bytes\nimport org.apache.hadoop.hbase.io.ImmutableBytesWritable\n \nvar conf = HBaseConfiguration.create()\n    var jobConf = new JobConf(conf)\n    jobConf.set(\"hbase.zookeeper.quorum\",\"zkNode1,zkNode2,zkNode3\")\n    jobConf.set(\"zookeeper.znode.parent\",\"/hbase\")\n    jobConf.set(TableOutputFormat.OUTPUT_TABLE,\"lxw1234\")\n    jobConf.setOutputFormat(classOf[TableOutputFormat])\n    \n    var rdd1 = sc.makeRDD(Array((\"A\",2),(\"B\",6),(\"C\",7)))\n    rdd1.map(x => \n      {\n        var put = new Put(Bytes.toBytes(x._1))\n        put.add(Bytes.toBytes(\"f1\"), Bytes.toBytes(\"c1\"), Bytes.toBytes(x._2))\n        (new ImmutableBytesWritable,put)\n      }\n    ).saveAsHadoopDataset(jobConf)\n \n##结果：\nhbase(main):005:0> scan 'lxw1234'\nROW     COLUMN+CELL                                                                                                \n A       column=f1:c1, timestamp=1436504941187, value=\\x00\\x00\\x00\\x02                                              \n B       column=f1:c1, timestamp=1436504941187, value=\\x00\\x00\\x00\\x06                                              \n C       column=f1:c1, timestamp=1436504941187, value=\\x00\\x00\\x00\\x07                                              \n3 row(s) in 0.0550 seconds\n```  \n注意：保存到HBase，运行时候需要在SPARK_CLASSPATH中加入HBase相关的jar包。\n\n可参考：http://lxw1234.com/archives/2015/07/332.htm  \n# **saveAsNewAPIHadoopFile**  \ndef saveAsNewAPIHadoopFile[F <: OutputFormat[K, V]](path: String)(implicit fm: ClassTag[F]): Unit\n\ndef saveAsNewAPIHadoopFile(path: String, keyClass: Class[_], valueClass: Class[_], outputFormatClass: Class[_ <: OutputFormat[_, _]], conf: Configuration = self.context.hadoopConfiguration): Unit\n\n \n\nsaveAsNewAPIHadoopFile用于将RDD数据保存到HDFS上，使用新版本Hadoop API。\n\n用法基本同saveAsHadoopFile。\n```scala\nimport org.apache.spark.SparkConf\nimport org.apache.spark.SparkContext\nimport SparkContext._\nimport org.apache.hadoop.mapreduce.lib.output.TextOutputFormat\nimport org.apache.hadoop.io.Text\nimport org.apache.hadoop.io.IntWritable\n \nvar rdd1 = sc.makeRDD(Array((\"A\",2),(\"A\",1),(\"B\",6),(\"B\",3),(\"B\",7)))\nrdd1.saveAsNewAPIHadoopFile(\"/tmp/lxw1234/\",classOf[Text],classOf[IntWritable],classOf[TextOutputFormat[Text,IntWritable]])\n```  \n# **saveAsNewAPIHadoopDataset**\ndef saveAsNewAPIHadoopDataset(conf: Configuration): Unit\n\n作用同saveAsHadoopDataset,只不过采用新版本Hadoop API。\n\n以写入HBase为例：\n\n \n\nHBase建表：\n\ncreate ‘lxw1234′,{NAME => ‘f1′,VERSIONS => 1},{NAME => ‘f2′,VERSIONS => 1},{NAME => ‘f3′,VERSIONS => 1}\n\n \n\n完整的Spark应用程序：  \n```scala\npackage com.lxw1234.test\n \nimport org.apache.spark.SparkConf\nimport org.apache.spark.SparkContext\nimport SparkContext._\nimport org.apache.hadoop.hbase.HBaseConfiguration\nimport org.apache.hadoop.mapreduce.Job\nimport org.apache.hadoop.hbase.mapreduce.TableOutputFormat\nimport org.apache.hadoop.hbase.io.ImmutableBytesWritable\nimport org.apache.hadoop.hbase.client.Result\nimport org.apache.hadoop.hbase.util.Bytes\nimport org.apache.hadoop.hbase.client.Put\n \nobject Test {\n  def main(args : Array[String]) {\n   val sparkConf = new SparkConf().setMaster(\"spark://lxw1234.com:7077\").setAppName(\"lxw1234.com\")\n   val sc = new SparkContext(sparkConf);\n   var rdd1 = sc.makeRDD(Array((\"A\",2),(\"B\",6),(\"C\",7)))\n   \n    sc.hadoopConfiguration.set(\"hbase.zookeeper.quorum \",\"zkNode1,zkNode2,zkNode3\")\n    sc.hadoopConfiguration.set(\"zookeeper.znode.parent\",\"/hbase\")\n    sc.hadoopConfiguration.set(TableOutputFormat.OUTPUT_TABLE,\"lxw1234\")\n    var job = new Job(sc.hadoopConfiguration)\n    job.setOutputKeyClass(classOf[ImmutableBytesWritable])\n    job.setOutputValueClass(classOf[Result])\n    job.setOutputFormatClass(classOf[TableOutputFormat[ImmutableBytesWritable]])\n    \n    rdd1.map(\n      x => {\n        var put = new Put(Bytes.toBytes(x._1))\n        put.add(Bytes.toBytes(\"f1\"), Bytes.toBytes(\"c1\"), Bytes.toBytes(x._2))\n        (new ImmutableBytesWritable,put)\n      }    \n    ).saveAsNewAPIHadoopDataset(job.getConfiguration)\n    \n    sc.stop()   \n  }\n}\n \n```\n注意：保存到HBase，运行时候需要在SPARK_CLASSPATH中加入HBase相关的jar包。\n\n可参考：http://lxw1234.com/archives/2015/07/332.htm  \n\n感谢原作者的总结  \n本文转自: lxw的大数据田地  \nhttp://lxw1234.com/archives/2015/07/402.htm  \nhttp://lxw1234.com/archives/2015/07/404.htm  \nhttp://lxw1234.com/archives/2015/07/406.htm","source":"_posts/spark RDD算子（十一）之RDD Action 保存操作saveAsTextFile,saveAsSequenceFile,saveAsObjectFile,saveAsHadoopFile 等.md","raw":"---\ntitle: spark RDD算子（十一）之RDD Action 保存操作saveAsTextFile,saveAsSequenceFile,saveAsObjectFile,saveAsHadoopFile 等\ndate: 2017-04-11 21:25:21\ntags: [spark]\ncategories: [大数据,spark]\nauthor: kaishun\nid: 45\npermalink: spark-rdd-11\n---\n---\n\n关键字:Spark算子、Spark函数、Spark RDD行动Action、Spark RDD存储操作、saveAsTextFile、saveAsSequenceFile、saveAsObjectFile,saveAsHadoopFile、saveAsHadoopDataset,saveAsNewAPIHadoopFile、saveAsNewAPIHadoopDataset \n\n---\n\n# **saveAsTextFile**\ndef saveAsTextFile(path: String): Unit\n\ndef saveAsTextFile(path: String, codec: Class[_ <: CompressionCodec]): Unit\n\nsaveAsTextFile用于将RDD以文本文件的格式存储到文件系统中。\n\ncodec参数可以指定压缩的类名。\n```scala\nvar rdd1 = sc.makeRDD(1 to 10,2)\nscala> rdd1.saveAsTextFile(\"hdfs://cdh5/tmp/lxw1234.com/\") //保存到HDFS\nhadoop fs -ls /tmp/lxw1234.com\nFound 2 items\n-rw-r--r--   2 lxw1234 supergroup        0 2015-07-10 09:15 /tmp/lxw1234.com/_SUCCESS\n-rw-r--r--   2 lxw1234 supergroup        21 2015-07-10 09:15 /tmp/lxw1234.com/part-00000\n \nhadoop fs -cat /tmp/lxw1234.com/part-00000\n```\n注意：如果使用rdd1.saveAsTextFile(“file:///tmp/lxw1234.com”)将文件保存到本地文件系统，那么只会保存在Executor所在机器的本地目录。  \n**指定压缩格式保存**  \n```scala\nrdd1.saveAsTextFile(\"hdfs://cdh5/tmp/lxw1234.com/\",classOf[com.hadoop.compression.lzo.LzopCodec])\n \nhadoop fs -ls /tmp/lxw1234.com\n-rw-r--r--   2 lxw1234 supergroup    0 2015-07-10 09:20 /tmp/lxw1234.com/_SUCCESS\n-rw-r--r--   2 lxw1234 supergroup    71 2015-07-10 09:20 /tmp/lxw1234.com/part-00000.lzo\n \nhadoop fs -text /tmp/lxw1234.com/part-00000.lzo\n```\n# **saveAsSequenceFile**  \nsaveAsSequenceFile用于将RDD以SequenceFile的文件格式保存到HDFS上。\n\n用法同saveAsTextFile。\n\n\n# **saveAsObjectFile**\ndef saveAsObjectFile(path: String): Unit\n\nsaveAsObjectFile用于将RDD中的元素序列化成对象，存储到文件中。\n\n对于HDFS，默认采用SequenceFile保存。  \n```scala\nvar rdd1 = sc.makeRDD(1 to 10,2)\nrdd1.saveAsObjectFile(\"hdfs://cdh5/tmp/lxw1234.com/\")\n \nhadoop fs -cat /tmp/lxw1234.com/part-00000\nSEQ !org.apache.hadoop.io.NullWritable\"org.apache.hadoop.io.BytesWritableT\n```\n# **saveAsHadoopFile**  \ndef saveAsHadoopFile(path: String, keyClass: Class[_], valueClass: Class[_], outputFormatClass: Class[_ <: OutputFormat[_, _]], codec: Class[_ <: CompressionCodec]): Unit\n\ndef saveAsHadoopFile(path: String, keyClass: Class[_], valueClass: Class[_], outputFormatClass: Class[_ <: OutputFormat[_, _]], conf: JobConf = …, codec: Option[Class[_ <: CompressionCodec]] = None): Unit\n\nsaveAsHadoopFile是将RDD存储在HDFS上的文件中，支持老版本Hadoop API。\n\n可以指定outputKeyClass、outputValueClass以及压缩格式。\n\n每个分区输出一个文件。\n```scala\nvar rdd1 = sc.makeRDD(Array((\"A\",2),(\"A\",1),(\"B\",6),(\"B\",3),(\"B\",7)))\n \nimport org.apache.hadoop.mapred.TextOutputFormat\nimport org.apache.hadoop.io.Text\nimport org.apache.hadoop.io.IntWritable\n \nrdd1.saveAsHadoopFile(\"/tmp/lxw1234.com/\",classOf[Text],classOf[IntWritable],classOf[TextOutputFormat[Text,IntWritable]])\n \nrdd1.saveAsHadoopFile(\"/tmp/lxw1234.com/\",classOf[Text],classOf[IntWritable],classOf[TextOutputFormat[Text,IntWritable]],\n                      classOf[com.hadoop.compression.lzo.LzopCodec])\n```  \n# **saveAsHadoopDataset**  \ndef saveAsHadoopDataset(conf: JobConf): Unit\n\nsaveAsHadoopDataset用于将RDD保存到除了HDFS的其他存储中，比如HBase。\n\n在JobConf中，通常需要关注或者设置五个参数：\n\n文件的保存路径、key值的class类型、value值的class类型、RDD的输出格式(OutputFormat)、以及压缩相关的参数。  \n**##使用saveAsHadoopDataset将RDD保存到HDFS中**  \n```scala\nimport org.apache.spark.SparkConf\nimport org.apache.spark.SparkContext\nimport SparkContext._\nimport org.apache.hadoop.mapred.TextOutputFormat\nimport org.apache.hadoop.io.Text\nimport org.apache.hadoop.io.IntWritable\nimport org.apache.hadoop.mapred.JobConf\n \n \n \nvar rdd1 = sc.makeRDD(Array((\"A\",2),(\"A\",1),(\"B\",6),(\"B\",3),(\"B\",7)))\nvar jobConf = new JobConf()\njobConf.setOutputFormat(classOf[TextOutputFormat[Text,IntWritable]])\njobConf.setOutputKeyClass(classOf[Text])\njobConf.setOutputValueClass(classOf[IntWritable])\njobConf.set(\"mapred.output.dir\",\"/tmp/lxw1234/\")\nrdd1.saveAsHadoopDataset(jobConf)\n \n结果：\nhadoop fs -cat /tmp/lxw1234/part-00000\nA       2\nA       1\nhadoop fs -cat /tmp/lxw1234/part-00001\nB       6\nB       3\nB       7\n```  \n**##保存数据到HBASE**  \nHBase建表：\n\ncreate ‘lxw1234′,{NAME => ‘f1′,VERSIONS => 1},{NAME => ‘f2′,VERSIONS => 1},{NAME => ‘f3′,VERSIONS => 1}  \n```scala\nimport org.apache.spark.SparkConf\nimport org.apache.spark.SparkContext\nimport SparkContext._\nimport org.apache.hadoop.mapred.TextOutputFormat\nimport org.apache.hadoop.io.Text\nimport org.apache.hadoop.io.IntWritable\nimport org.apache.hadoop.mapred.JobConf\nimport org.apache.hadoop.hbase.HBaseConfiguration\nimport org.apache.hadoop.hbase.mapred.TableOutputFormat\nimport org.apache.hadoop.hbase.client.Put\nimport org.apache.hadoop.hbase.util.Bytes\nimport org.apache.hadoop.hbase.io.ImmutableBytesWritable\n \nvar conf = HBaseConfiguration.create()\n    var jobConf = new JobConf(conf)\n    jobConf.set(\"hbase.zookeeper.quorum\",\"zkNode1,zkNode2,zkNode3\")\n    jobConf.set(\"zookeeper.znode.parent\",\"/hbase\")\n    jobConf.set(TableOutputFormat.OUTPUT_TABLE,\"lxw1234\")\n    jobConf.setOutputFormat(classOf[TableOutputFormat])\n    \n    var rdd1 = sc.makeRDD(Array((\"A\",2),(\"B\",6),(\"C\",7)))\n    rdd1.map(x => \n      {\n        var put = new Put(Bytes.toBytes(x._1))\n        put.add(Bytes.toBytes(\"f1\"), Bytes.toBytes(\"c1\"), Bytes.toBytes(x._2))\n        (new ImmutableBytesWritable,put)\n      }\n    ).saveAsHadoopDataset(jobConf)\n \n##结果：\nhbase(main):005:0> scan 'lxw1234'\nROW     COLUMN+CELL                                                                                                \n A       column=f1:c1, timestamp=1436504941187, value=\\x00\\x00\\x00\\x02                                              \n B       column=f1:c1, timestamp=1436504941187, value=\\x00\\x00\\x00\\x06                                              \n C       column=f1:c1, timestamp=1436504941187, value=\\x00\\x00\\x00\\x07                                              \n3 row(s) in 0.0550 seconds\n```  \n注意：保存到HBase，运行时候需要在SPARK_CLASSPATH中加入HBase相关的jar包。\n\n可参考：http://lxw1234.com/archives/2015/07/332.htm  \n# **saveAsNewAPIHadoopFile**  \ndef saveAsNewAPIHadoopFile[F <: OutputFormat[K, V]](path: String)(implicit fm: ClassTag[F]): Unit\n\ndef saveAsNewAPIHadoopFile(path: String, keyClass: Class[_], valueClass: Class[_], outputFormatClass: Class[_ <: OutputFormat[_, _]], conf: Configuration = self.context.hadoopConfiguration): Unit\n\n \n\nsaveAsNewAPIHadoopFile用于将RDD数据保存到HDFS上，使用新版本Hadoop API。\n\n用法基本同saveAsHadoopFile。\n```scala\nimport org.apache.spark.SparkConf\nimport org.apache.spark.SparkContext\nimport SparkContext._\nimport org.apache.hadoop.mapreduce.lib.output.TextOutputFormat\nimport org.apache.hadoop.io.Text\nimport org.apache.hadoop.io.IntWritable\n \nvar rdd1 = sc.makeRDD(Array((\"A\",2),(\"A\",1),(\"B\",6),(\"B\",3),(\"B\",7)))\nrdd1.saveAsNewAPIHadoopFile(\"/tmp/lxw1234/\",classOf[Text],classOf[IntWritable],classOf[TextOutputFormat[Text,IntWritable]])\n```  \n# **saveAsNewAPIHadoopDataset**\ndef saveAsNewAPIHadoopDataset(conf: Configuration): Unit\n\n作用同saveAsHadoopDataset,只不过采用新版本Hadoop API。\n\n以写入HBase为例：\n\n \n\nHBase建表：\n\ncreate ‘lxw1234′,{NAME => ‘f1′,VERSIONS => 1},{NAME => ‘f2′,VERSIONS => 1},{NAME => ‘f3′,VERSIONS => 1}\n\n \n\n完整的Spark应用程序：  \n```scala\npackage com.lxw1234.test\n \nimport org.apache.spark.SparkConf\nimport org.apache.spark.SparkContext\nimport SparkContext._\nimport org.apache.hadoop.hbase.HBaseConfiguration\nimport org.apache.hadoop.mapreduce.Job\nimport org.apache.hadoop.hbase.mapreduce.TableOutputFormat\nimport org.apache.hadoop.hbase.io.ImmutableBytesWritable\nimport org.apache.hadoop.hbase.client.Result\nimport org.apache.hadoop.hbase.util.Bytes\nimport org.apache.hadoop.hbase.client.Put\n \nobject Test {\n  def main(args : Array[String]) {\n   val sparkConf = new SparkConf().setMaster(\"spark://lxw1234.com:7077\").setAppName(\"lxw1234.com\")\n   val sc = new SparkContext(sparkConf);\n   var rdd1 = sc.makeRDD(Array((\"A\",2),(\"B\",6),(\"C\",7)))\n   \n    sc.hadoopConfiguration.set(\"hbase.zookeeper.quorum \",\"zkNode1,zkNode2,zkNode3\")\n    sc.hadoopConfiguration.set(\"zookeeper.znode.parent\",\"/hbase\")\n    sc.hadoopConfiguration.set(TableOutputFormat.OUTPUT_TABLE,\"lxw1234\")\n    var job = new Job(sc.hadoopConfiguration)\n    job.setOutputKeyClass(classOf[ImmutableBytesWritable])\n    job.setOutputValueClass(classOf[Result])\n    job.setOutputFormatClass(classOf[TableOutputFormat[ImmutableBytesWritable]])\n    \n    rdd1.map(\n      x => {\n        var put = new Put(Bytes.toBytes(x._1))\n        put.add(Bytes.toBytes(\"f1\"), Bytes.toBytes(\"c1\"), Bytes.toBytes(x._2))\n        (new ImmutableBytesWritable,put)\n      }    \n    ).saveAsNewAPIHadoopDataset(job.getConfiguration)\n    \n    sc.stop()   \n  }\n}\n \n```\n注意：保存到HBase，运行时候需要在SPARK_CLASSPATH中加入HBase相关的jar包。\n\n可参考：http://lxw1234.com/archives/2015/07/332.htm  \n\n感谢原作者的总结  \n本文转自: lxw的大数据田地  \nhttp://lxw1234.com/archives/2015/07/402.htm  \nhttp://lxw1234.com/archives/2015/07/404.htm  \nhttp://lxw1234.com/archives/2015/07/406.htm","slug":"spark-rdd-11","published":1,"updated":"2018-01-22T15:29:19.847Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcrpnzrk003f2wv37l6ul63e","content":"<hr>\n<p>关键字:Spark算子、Spark函数、Spark RDD行动Action、Spark RDD存储操作、saveAsTextFile、saveAsSequenceFile、saveAsObjectFile,saveAsHadoopFile、saveAsHadoopDataset,saveAsNewAPIHadoopFile、saveAsNewAPIHadoopDataset </p>\n<hr>\n<h1 id=\"saveAsTextFile\"><a href=\"#saveAsTextFile\" class=\"headerlink\" title=\"saveAsTextFile\"></a><strong>saveAsTextFile</strong></h1><p>def saveAsTextFile(path: String): Unit</p>\n<p>def saveAsTextFile(path: String, codec: Class[_ &lt;: CompressionCodec]): Unit</p>\n<p>saveAsTextFile用于将RDD以文本文件的格式存储到文件系统中。</p>\n<p>codec参数可以指定压缩的类名。<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">var</span> rdd1 = sc.makeRDD(<span class=\"number\">1</span> to <span class=\"number\">10</span>,<span class=\"number\">2</span>)</div><div class=\"line\">scala&gt; rdd1.saveAsTextFile(<span class=\"string\">\"hdfs://cdh5/tmp/lxw1234.com/\"</span>) <span class=\"comment\">//保存到HDFS</span></div><div class=\"line\">hadoop fs -ls /tmp/lxw1234.com</div><div class=\"line\"><span class=\"type\">Found</span> <span class=\"number\">2</span> items</div><div class=\"line\">-rw-r--r--   <span class=\"number\">2</span> lxw1234 supergroup        <span class=\"number\">0</span> <span class=\"number\">2015</span><span class=\"number\">-07</span><span class=\"number\">-10</span> <span class=\"number\">09</span>:<span class=\"number\">15</span> /tmp/lxw1234.com/_SUCCESS</div><div class=\"line\">-rw-r--r--   <span class=\"number\">2</span> lxw1234 supergroup        <span class=\"number\">21</span> <span class=\"number\">2015</span><span class=\"number\">-07</span><span class=\"number\">-10</span> <span class=\"number\">09</span>:<span class=\"number\">15</span> /tmp/lxw1234.com/part<span class=\"number\">-00000</span></div><div class=\"line\"> </div><div class=\"line\">hadoop fs -cat /tmp/lxw1234.com/part<span class=\"number\">-00000</span></div></pre></td></tr></table></figure></p>\n<p>注意：如果使用rdd1.saveAsTextFile(“file:///tmp/lxw1234.com”)将文件保存到本地文件系统，那么只会保存在Executor所在机器的本地目录。<br><strong>指定压缩格式保存</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">rdd1.saveAsTextFile(<span class=\"string\">\"hdfs://cdh5/tmp/lxw1234.com/\"</span>,classOf[com.hadoop.compression.lzo.<span class=\"type\">LzopCodec</span>])</div><div class=\"line\"> </div><div class=\"line\">hadoop fs -ls /tmp/lxw1234.com</div><div class=\"line\">-rw-r--r--   <span class=\"number\">2</span> lxw1234 supergroup    <span class=\"number\">0</span> <span class=\"number\">2015</span><span class=\"number\">-07</span><span class=\"number\">-10</span> <span class=\"number\">09</span>:<span class=\"number\">20</span> /tmp/lxw1234.com/_SUCCESS</div><div class=\"line\">-rw-r--r--   <span class=\"number\">2</span> lxw1234 supergroup    <span class=\"number\">71</span> <span class=\"number\">2015</span><span class=\"number\">-07</span><span class=\"number\">-10</span> <span class=\"number\">09</span>:<span class=\"number\">20</span> /tmp/lxw1234.com/part<span class=\"number\">-00000.</span>lzo</div><div class=\"line\"> </div><div class=\"line\">hadoop fs -text /tmp/lxw1234.com/part<span class=\"number\">-00000.</span>lzo</div></pre></td></tr></table></figure></p>\n<h1 id=\"saveAsSequenceFile\"><a href=\"#saveAsSequenceFile\" class=\"headerlink\" title=\"saveAsSequenceFile\"></a><strong>saveAsSequenceFile</strong></h1><p>saveAsSequenceFile用于将RDD以SequenceFile的文件格式保存到HDFS上。</p>\n<p>用法同saveAsTextFile。</p>\n<h1 id=\"saveAsObjectFile\"><a href=\"#saveAsObjectFile\" class=\"headerlink\" title=\"saveAsObjectFile\"></a><strong>saveAsObjectFile</strong></h1><p>def saveAsObjectFile(path: String): Unit</p>\n<p>saveAsObjectFile用于将RDD中的元素序列化成对象，存储到文件中。</p>\n<p>对于HDFS，默认采用SequenceFile保存。<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">var</span> rdd1 = sc.makeRDD(<span class=\"number\">1</span> to <span class=\"number\">10</span>,<span class=\"number\">2</span>)</div><div class=\"line\">rdd1.saveAsObjectFile(<span class=\"string\">\"hdfs://cdh5/tmp/lxw1234.com/\"</span>)</div><div class=\"line\"> </div><div class=\"line\">hadoop fs -cat /tmp/lxw1234.com/part<span class=\"number\">-00000</span></div><div class=\"line\"><span class=\"type\">SEQ</span> !org.apache.hadoop.io.<span class=\"type\">NullWritable</span><span class=\"string\">\"org.apache.hadoop.io.BytesWritableT</span></div></pre></td></tr></table></figure></p>\n<h1 id=\"saveAsHadoopFile\"><a href=\"#saveAsHadoopFile\" class=\"headerlink\" title=\"saveAsHadoopFile\"></a><strong>saveAsHadoopFile</strong></h1><p>def saveAsHadoopFile(path: String, keyClass: Class[<em>], valueClass: Class[</em>], outputFormatClass: Class[<em> &lt;: OutputFormat[</em>, <em>]], codec: Class[</em> &lt;: CompressionCodec]): Unit</p>\n<p>def saveAsHadoopFile(path: String, keyClass: Class[<em>], valueClass: Class[</em>], outputFormatClass: Class[<em> &lt;: OutputFormat[</em>, <em>]], conf: JobConf = …, codec: Option[Class[</em> &lt;: CompressionCodec]] = None): Unit</p>\n<p>saveAsHadoopFile是将RDD存储在HDFS上的文件中，支持老版本Hadoop API。</p>\n<p>可以指定outputKeyClass、outputValueClass以及压缩格式。</p>\n<p>每个分区输出一个文件。<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div><div class=\"line\">106</div><div class=\"line\">107</div><div class=\"line\">108</div><div class=\"line\">109</div><div class=\"line\">110</div><div class=\"line\">111</div><div class=\"line\">112</div><div class=\"line\">113</div><div class=\"line\">114</div><div class=\"line\">115</div><div class=\"line\">116</div><div class=\"line\">117</div><div class=\"line\">118</div><div class=\"line\">119</div><div class=\"line\">120</div><div class=\"line\">121</div><div class=\"line\">122</div><div class=\"line\">123</div><div class=\"line\">124</div><div class=\"line\">125</div><div class=\"line\">126</div><div class=\"line\">127</div><div class=\"line\">128</div><div class=\"line\">129</div><div class=\"line\">130</div><div class=\"line\">131</div><div class=\"line\">132</div><div class=\"line\">133</div><div class=\"line\">134</div><div class=\"line\">135</div><div class=\"line\">136</div><div class=\"line\">137</div><div class=\"line\">138</div><div class=\"line\">139</div><div class=\"line\">140</div><div class=\"line\">141</div><div class=\"line\">142</div><div class=\"line\">143</div><div class=\"line\">144</div><div class=\"line\">145</div><div class=\"line\">146</div><div class=\"line\">147</div><div class=\"line\">148</div><div class=\"line\">149</div><div class=\"line\">150</div><div class=\"line\">151</div><div class=\"line\">152</div><div class=\"line\">153</div><div class=\"line\">154</div><div class=\"line\">155</div><div class=\"line\">156</div><div class=\"line\">157</div><div class=\"line\">158</div><div class=\"line\">159</div><div class=\"line\">160</div><div class=\"line\">161</div><div class=\"line\">162</div><div class=\"line\">163</div><div class=\"line\">164</div><div class=\"line\">165</div><div class=\"line\">166</div><div class=\"line\">167</div><div class=\"line\">168</div><div class=\"line\">169</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">var</span> rdd1 = sc.makeRDD(<span class=\"type\">Array</span>((<span class=\"string\">\"A\"</span>,<span class=\"number\">2</span>),(<span class=\"string\">\"A\"</span>,<span class=\"number\">1</span>),(<span class=\"string\">\"B\"</span>,<span class=\"number\">6</span>),(<span class=\"string\">\"B\"</span>,<span class=\"number\">3</span>),(<span class=\"string\">\"B\"</span>,<span class=\"number\">7</span>)))</div><div class=\"line\"> </div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapred.<span class=\"type\">TextOutputFormat</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.<span class=\"type\">Text</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.<span class=\"type\">IntWritable</span></div><div class=\"line\"> </div><div class=\"line\">rdd1.saveAsHadoopFile(<span class=\"string\">\"/tmp/lxw1234.com/\"</span>,classOf[<span class=\"type\">Text</span>],classOf[<span class=\"type\">IntWritable</span>],classOf[<span class=\"type\">TextOutputFormat</span>[<span class=\"type\">Text</span>,<span class=\"type\">IntWritable</span>]])</div><div class=\"line\"> </div><div class=\"line\">rdd1.saveAsHadoopFile(<span class=\"string\">\"/tmp/lxw1234.com/\"</span>,classOf[<span class=\"type\">Text</span>],classOf[<span class=\"type\">IntWritable</span>],classOf[<span class=\"type\">TextOutputFormat</span>[<span class=\"type\">Text</span>,<span class=\"type\">IntWritable</span>]],</div><div class=\"line\">                      classOf[com.hadoop.compression.lzo.<span class=\"type\">LzopCodec</span>])</div><div class=\"line\">```  </div><div class=\"line\"># **saveAsHadoopDataset**  </div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">saveAsHadoopDataset</span></span>(conf: <span class=\"type\">JobConf</span>): <span class=\"type\">Unit</span></div><div class=\"line\"></div><div class=\"line\">saveAsHadoopDataset用于将<span class=\"type\">RDD</span>保存到除了<span class=\"type\">HDFS</span>的其他存储中，比如<span class=\"type\">HBase</span>。</div><div class=\"line\"></div><div class=\"line\">在<span class=\"type\">JobConf</span>中，通常需要关注或者设置五个参数：</div><div class=\"line\"></div><div class=\"line\">文件的保存路径、key值的<span class=\"class\"><span class=\"keyword\">class</span><span class=\"title\">类型、value值的class类型、RDD的输出格式</span>(<span class=\"params\"><span class=\"type\">OutputFormat</span></span>)<span class=\"title\">、以及压缩相关的参数。</span>  </span></div><div class=\"line\"><span class=\"class\"><span class=\"title\">**##使用saveAsHadoopDataset将RDD保存到HDFS中**</span>  </span></div><div class=\"line\"><span class=\"class\">```<span class=\"title\">scala</span></span></div><div class=\"line\"><span class=\"class\"><span class=\"title\">import</span> <span class=\"title\">org</span>.<span class=\"title\">apache</span>.<span class=\"title\">spark</span>.<span class=\"title\">SparkConf</span></span></div><div class=\"line\"><span class=\"class\"><span class=\"title\">import</span> <span class=\"title\">org</span>.<span class=\"title\">apache</span>.<span class=\"title\">spark</span>.<span class=\"title\">SparkContext</span></span></div><div class=\"line\"><span class=\"class\"><span class=\"title\">import</span> <span class=\"title\">SparkContext</span>.<span class=\"title\">_</span></span></div><div class=\"line\"><span class=\"class\"><span class=\"title\">import</span> <span class=\"title\">org</span>.<span class=\"title\">apache</span>.<span class=\"title\">hadoop</span>.<span class=\"title\">mapred</span>.<span class=\"title\">TextOutputFormat</span></span></div><div class=\"line\"><span class=\"class\"><span class=\"title\">import</span> <span class=\"title\">org</span>.<span class=\"title\">apache</span>.<span class=\"title\">hadoop</span>.<span class=\"title\">io</span>.<span class=\"title\">Text</span></span></div><div class=\"line\"><span class=\"class\"><span class=\"title\">import</span> <span class=\"title\">org</span>.<span class=\"title\">apache</span>.<span class=\"title\">hadoop</span>.<span class=\"title\">io</span>.<span class=\"title\">IntWritable</span></span></div><div class=\"line\"><span class=\"class\"><span class=\"title\">import</span> <span class=\"title\">org</span>.<span class=\"title\">apache</span>.<span class=\"title\">hadoop</span>.<span class=\"title\">mapred</span>.<span class=\"title\">JobConf</span></span></div><div class=\"line\"><span class=\"class\"> </span></div><div class=\"line\"><span class=\"class\"> </span></div><div class=\"line\"><span class=\"class\"> </span></div><div class=\"line\"><span class=\"class\"><span class=\"title\">var</span> <span class=\"title\">rdd1</span> </span>= sc.makeRDD(<span class=\"type\">Array</span>((<span class=\"string\">\"A\"</span>,<span class=\"number\">2</span>),(<span class=\"string\">\"A\"</span>,<span class=\"number\">1</span>),(<span class=\"string\">\"B\"</span>,<span class=\"number\">6</span>),(<span class=\"string\">\"B\"</span>,<span class=\"number\">3</span>),(<span class=\"string\">\"B\"</span>,<span class=\"number\">7</span>)))</div><div class=\"line\"><span class=\"keyword\">var</span> jobConf = <span class=\"keyword\">new</span> <span class=\"type\">JobConf</span>()</div><div class=\"line\">jobConf.setOutputFormat(classOf[<span class=\"type\">TextOutputFormat</span>[<span class=\"type\">Text</span>,<span class=\"type\">IntWritable</span>]])</div><div class=\"line\">jobConf.setOutputKeyClass(classOf[<span class=\"type\">Text</span>])</div><div class=\"line\">jobConf.setOutputValueClass(classOf[<span class=\"type\">IntWritable</span>])</div><div class=\"line\">jobConf.set(<span class=\"string\">\"mapred.output.dir\"</span>,<span class=\"string\">\"/tmp/lxw1234/\"</span>)</div><div class=\"line\">rdd1.saveAsHadoopDataset(jobConf)</div><div class=\"line\"> </div><div class=\"line\">结果：</div><div class=\"line\">hadoop fs -cat /tmp/lxw1234/part<span class=\"number\">-00000</span></div><div class=\"line\"><span class=\"type\">A</span>       <span class=\"number\">2</span></div><div class=\"line\"><span class=\"type\">A</span>       <span class=\"number\">1</span></div><div class=\"line\">hadoop fs -cat /tmp/lxw1234/part<span class=\"number\">-00001</span></div><div class=\"line\"><span class=\"type\">B</span>       <span class=\"number\">6</span></div><div class=\"line\"><span class=\"type\">B</span>       <span class=\"number\">3</span></div><div class=\"line\"><span class=\"type\">B</span>       <span class=\"number\">7</span></div><div class=\"line\">```  </div><div class=\"line\">**##保存数据到<span class=\"type\">HBASE</span>**  </div><div class=\"line\"><span class=\"type\">HBase</span>建表：</div><div class=\"line\"></div><div class=\"line\">create ‘lxw1234′,&#123;<span class=\"type\">NAME</span> =&gt; ‘f1′,<span class=\"type\">VERSIONS</span> =&gt; <span class=\"number\">1</span>&#125;,&#123;<span class=\"type\">NAME</span> =&gt; ‘f2′,<span class=\"type\">VERSIONS</span> =&gt; <span class=\"number\">1</span>&#125;,&#123;<span class=\"type\">NAME</span> =&gt; ‘f3′,<span class=\"type\">VERSIONS</span> =&gt; <span class=\"number\">1</span>&#125;  </div><div class=\"line\">```scala</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.<span class=\"type\">SparkConf</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.<span class=\"type\">SparkContext</span></div><div class=\"line\"><span class=\"keyword\">import</span> <span class=\"type\">SparkContext</span>._</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapred.<span class=\"type\">TextOutputFormat</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.<span class=\"type\">Text</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.<span class=\"type\">IntWritable</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapred.<span class=\"type\">JobConf</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.hbase.<span class=\"type\">HBaseConfiguration</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.hbase.mapred.<span class=\"type\">TableOutputFormat</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.hbase.client.<span class=\"type\">Put</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.hbase.util.<span class=\"type\">Bytes</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.hbase.io.<span class=\"type\">ImmutableBytesWritable</span></div><div class=\"line\"> </div><div class=\"line\"><span class=\"keyword\">var</span> conf = <span class=\"type\">HBaseConfiguration</span>.create()</div><div class=\"line\">    <span class=\"keyword\">var</span> jobConf = <span class=\"keyword\">new</span> <span class=\"type\">JobConf</span>(conf)</div><div class=\"line\">    jobConf.set(<span class=\"string\">\"hbase.zookeeper.quorum\"</span>,<span class=\"string\">\"zkNode1,zkNode2,zkNode3\"</span>)</div><div class=\"line\">    jobConf.set(<span class=\"string\">\"zookeeper.znode.parent\"</span>,<span class=\"string\">\"/hbase\"</span>)</div><div class=\"line\">    jobConf.set(<span class=\"type\">TableOutputFormat</span>.<span class=\"type\">OUTPUT_TABLE</span>,<span class=\"string\">\"lxw1234\"</span>)</div><div class=\"line\">    jobConf.setOutputFormat(classOf[<span class=\"type\">TableOutputFormat</span>])</div><div class=\"line\">    </div><div class=\"line\">    <span class=\"keyword\">var</span> rdd1 = sc.makeRDD(<span class=\"type\">Array</span>((<span class=\"string\">\"A\"</span>,<span class=\"number\">2</span>),(<span class=\"string\">\"B\"</span>,<span class=\"number\">6</span>),(<span class=\"string\">\"C\"</span>,<span class=\"number\">7</span>)))</div><div class=\"line\">    rdd1.map(x =&gt; </div><div class=\"line\">      &#123;</div><div class=\"line\">        <span class=\"keyword\">var</span> put = <span class=\"keyword\">new</span> <span class=\"type\">Put</span>(<span class=\"type\">Bytes</span>.toBytes(x._1))</div><div class=\"line\">        put.add(<span class=\"type\">Bytes</span>.toBytes(<span class=\"string\">\"f1\"</span>), <span class=\"type\">Bytes</span>.toBytes(<span class=\"string\">\"c1\"</span>), <span class=\"type\">Bytes</span>.toBytes(x._2))</div><div class=\"line\">        (<span class=\"keyword\">new</span> <span class=\"type\">ImmutableBytesWritable</span>,put)</div><div class=\"line\">      &#125;</div><div class=\"line\">    ).saveAsHadoopDataset(jobConf)</div><div class=\"line\"> </div><div class=\"line\">##结果：</div><div class=\"line\">hbase(main):<span class=\"number\">005</span>:<span class=\"number\">0</span>&gt; scan <span class=\"symbol\">'lxw123</span>4'</div><div class=\"line\"><span class=\"type\">ROW</span>     <span class=\"type\">COLUMN</span>+<span class=\"type\">CELL</span>                                                                                                </div><div class=\"line\"> <span class=\"type\">A</span>       column=f1:c1, timestamp=<span class=\"number\">1436504941187</span>, value=\\x00\\x00\\x00\\x02                                              </div><div class=\"line\"> <span class=\"type\">B</span>       column=f1:c1, timestamp=<span class=\"number\">1436504941187</span>, value=\\x00\\x00\\x00\\x06                                              </div><div class=\"line\"> <span class=\"type\">C</span>       column=f1:c1, timestamp=<span class=\"number\">1436504941187</span>, value=\\x00\\x00\\x00\\x07                                              </div><div class=\"line\"><span class=\"number\">3</span> row(s) in <span class=\"number\">0.0550</span> seconds</div><div class=\"line\">```  </div><div class=\"line\">注意：保存到<span class=\"type\">HBase</span>，运行时候需要在<span class=\"type\">SPARK_CLASSPATH</span>中加入<span class=\"type\">HBase</span>相关的jar包。</div><div class=\"line\"></div><div class=\"line\">可参考：http:<span class=\"comment\">//lxw1234.com/archives/2015/07/332.htm  </span></div><div class=\"line\"># **saveAsNewAPIHadoopFile**  </div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">saveAsNewAPIHadoopFile</span></span>[<span class=\"type\">F</span> &lt;: <span class=\"type\">OutputFormat</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>]](path: <span class=\"type\">String</span>)(<span class=\"keyword\">implicit</span> fm: <span class=\"type\">ClassTag</span>[<span class=\"type\">F</span>]): <span class=\"type\">Unit</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">saveAsNewAPIHadoopFile</span></span>(path: <span class=\"type\">String</span>, keyClass: <span class=\"type\">Class</span>[_], valueClass: <span class=\"type\">Class</span>[_], outputFormatClass: <span class=\"type\">Class</span>[_ &lt;: <span class=\"type\">OutputFormat</span>[_, _]], conf: <span class=\"type\">Configuration</span> = self.context.hadoopConfiguration): <span class=\"type\">Unit</span></div><div class=\"line\"></div><div class=\"line\"> </div><div class=\"line\"></div><div class=\"line\">saveAsNewAPIHadoopFile用于将<span class=\"type\">RDD</span>数据保存到<span class=\"type\">HDFS</span>上，使用新版本<span class=\"type\">Hadoop</span> <span class=\"type\">API</span>。</div><div class=\"line\"></div><div class=\"line\">用法基本同saveAsHadoopFile。</div><div class=\"line\">```scala</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.<span class=\"type\">SparkConf</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.<span class=\"type\">SparkContext</span></div><div class=\"line\"><span class=\"keyword\">import</span> <span class=\"type\">SparkContext</span>._</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.lib.output.<span class=\"type\">TextOutputFormat</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.<span class=\"type\">Text</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.<span class=\"type\">IntWritable</span></div><div class=\"line\"> </div><div class=\"line\"><span class=\"keyword\">var</span> rdd1 = sc.makeRDD(<span class=\"type\">Array</span>((<span class=\"string\">\"A\"</span>,<span class=\"number\">2</span>),(<span class=\"string\">\"A\"</span>,<span class=\"number\">1</span>),(<span class=\"string\">\"B\"</span>,<span class=\"number\">6</span>),(<span class=\"string\">\"B\"</span>,<span class=\"number\">3</span>),(<span class=\"string\">\"B\"</span>,<span class=\"number\">7</span>)))</div><div class=\"line\">rdd1.saveAsNewAPIHadoopFile(<span class=\"string\">\"/tmp/lxw1234/\"</span>,classOf[<span class=\"type\">Text</span>],classOf[<span class=\"type\">IntWritable</span>],classOf[<span class=\"type\">TextOutputFormat</span>[<span class=\"type\">Text</span>,<span class=\"type\">IntWritable</span>]])</div><div class=\"line\">```  </div><div class=\"line\"># **saveAsNewAPIHadoopDataset**</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">saveAsNewAPIHadoopDataset</span></span>(conf: <span class=\"type\">Configuration</span>): <span class=\"type\">Unit</span></div><div class=\"line\"></div><div class=\"line\">作用同saveAsHadoopDataset,只不过采用新版本<span class=\"type\">Hadoop</span> <span class=\"type\">API</span>。</div><div class=\"line\"></div><div class=\"line\">以写入<span class=\"type\">HBase</span>为例：</div><div class=\"line\"></div><div class=\"line\"> </div><div class=\"line\"></div><div class=\"line\"><span class=\"type\">HBase</span>建表：</div><div class=\"line\"></div><div class=\"line\">create ‘lxw1234′,&#123;<span class=\"type\">NAME</span> =&gt; ‘f1′,<span class=\"type\">VERSIONS</span> =&gt; <span class=\"number\">1</span>&#125;,&#123;<span class=\"type\">NAME</span> =&gt; ‘f2′,<span class=\"type\">VERSIONS</span> =&gt; <span class=\"number\">1</span>&#125;,&#123;<span class=\"type\">NAME</span> =&gt; ‘f3′,<span class=\"type\">VERSIONS</span> =&gt; <span class=\"number\">1</span>&#125;</div><div class=\"line\"></div><div class=\"line\"> </div><div class=\"line\"></div><div class=\"line\">完整的<span class=\"type\">Spark</span>应用程序：  </div><div class=\"line\">```scala</div><div class=\"line\"><span class=\"keyword\">package</span> com.lxw1234.test</div><div class=\"line\"> </div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.<span class=\"type\">SparkConf</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.<span class=\"type\">SparkContext</span></div><div class=\"line\"><span class=\"keyword\">import</span> <span class=\"type\">SparkContext</span>._</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.hbase.<span class=\"type\">HBaseConfiguration</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.<span class=\"type\">Job</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.hbase.mapreduce.<span class=\"type\">TableOutputFormat</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.hbase.io.<span class=\"type\">ImmutableBytesWritable</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.hbase.client.<span class=\"type\">Result</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.hbase.util.<span class=\"type\">Bytes</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.hbase.client.<span class=\"type\">Put</span></div><div class=\"line\"> </div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Test</span> </span>&#123;</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args : <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">   <span class=\"keyword\">val</span> sparkConf = <span class=\"keyword\">new</span> <span class=\"type\">SparkConf</span>().setMaster(<span class=\"string\">\"spark://lxw1234.com:7077\"</span>).setAppName(<span class=\"string\">\"lxw1234.com\"</span>)</div><div class=\"line\">   <span class=\"keyword\">val</span> sc = <span class=\"keyword\">new</span> <span class=\"type\">SparkContext</span>(sparkConf);</div><div class=\"line\">   <span class=\"keyword\">var</span> rdd1 = sc.makeRDD(<span class=\"type\">Array</span>((<span class=\"string\">\"A\"</span>,<span class=\"number\">2</span>),(<span class=\"string\">\"B\"</span>,<span class=\"number\">6</span>),(<span class=\"string\">\"C\"</span>,<span class=\"number\">7</span>)))</div><div class=\"line\">   </div><div class=\"line\">    sc.hadoopConfiguration.set(<span class=\"string\">\"hbase.zookeeper.quorum \"</span>,<span class=\"string\">\"zkNode1,zkNode2,zkNode3\"</span>)</div><div class=\"line\">    sc.hadoopConfiguration.set(<span class=\"string\">\"zookeeper.znode.parent\"</span>,<span class=\"string\">\"/hbase\"</span>)</div><div class=\"line\">    sc.hadoopConfiguration.set(<span class=\"type\">TableOutputFormat</span>.<span class=\"type\">OUTPUT_TABLE</span>,<span class=\"string\">\"lxw1234\"</span>)</div><div class=\"line\">    <span class=\"keyword\">var</span> job = <span class=\"keyword\">new</span> <span class=\"type\">Job</span>(sc.hadoopConfiguration)</div><div class=\"line\">    job.setOutputKeyClass(classOf[<span class=\"type\">ImmutableBytesWritable</span>])</div><div class=\"line\">    job.setOutputValueClass(classOf[<span class=\"type\">Result</span>])</div><div class=\"line\">    job.setOutputFormatClass(classOf[<span class=\"type\">TableOutputFormat</span>[<span class=\"type\">ImmutableBytesWritable</span>]])</div><div class=\"line\">    </div><div class=\"line\">    rdd1.map(</div><div class=\"line\">      x =&gt; &#123;</div><div class=\"line\">        <span class=\"keyword\">var</span> put = <span class=\"keyword\">new</span> <span class=\"type\">Put</span>(<span class=\"type\">Bytes</span>.toBytes(x._1))</div><div class=\"line\">        put.add(<span class=\"type\">Bytes</span>.toBytes(<span class=\"string\">\"f1\"</span>), <span class=\"type\">Bytes</span>.toBytes(<span class=\"string\">\"c1\"</span>), <span class=\"type\">Bytes</span>.toBytes(x._2))</div><div class=\"line\">        (<span class=\"keyword\">new</span> <span class=\"type\">ImmutableBytesWritable</span>,put)</div><div class=\"line\">      &#125;    </div><div class=\"line\">    ).saveAsNewAPIHadoopDataset(job.getConfiguration)</div><div class=\"line\">    </div><div class=\"line\">    sc.stop()   </div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>注意：保存到HBase，运行时候需要在SPARK_CLASSPATH中加入HBase相关的jar包。</p>\n<p>可参考：<a href=\"http://lxw1234.com/archives/2015/07/332.htm\" target=\"_blank\" rel=\"external\">http://lxw1234.com/archives/2015/07/332.htm</a>  </p>\n<p>感谢原作者的总结<br>本文转自: lxw的大数据田地<br><a href=\"http://lxw1234.com/archives/2015/07/402.htm\" target=\"_blank\" rel=\"external\">http://lxw1234.com/archives/2015/07/402.htm</a><br><a href=\"http://lxw1234.com/archives/2015/07/404.htm\" target=\"_blank\" rel=\"external\">http://lxw1234.com/archives/2015/07/404.htm</a><br><a href=\"http://lxw1234.com/archives/2015/07/406.htm\" target=\"_blank\" rel=\"external\">http://lxw1234.com/archives/2015/07/406.htm</a></p>\n","site":{"data":{}},"excerpt":"","more":"<hr>\n<p>关键字:Spark算子、Spark函数、Spark RDD行动Action、Spark RDD存储操作、saveAsTextFile、saveAsSequenceFile、saveAsObjectFile,saveAsHadoopFile、saveAsHadoopDataset,saveAsNewAPIHadoopFile、saveAsNewAPIHadoopDataset </p>\n<hr>\n<h1 id=\"saveAsTextFile\"><a href=\"#saveAsTextFile\" class=\"headerlink\" title=\"saveAsTextFile\"></a><strong>saveAsTextFile</strong></h1><p>def saveAsTextFile(path: String): Unit</p>\n<p>def saveAsTextFile(path: String, codec: Class[_ &lt;: CompressionCodec]): Unit</p>\n<p>saveAsTextFile用于将RDD以文本文件的格式存储到文件系统中。</p>\n<p>codec参数可以指定压缩的类名。<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">var</span> rdd1 = sc.makeRDD(<span class=\"number\">1</span> to <span class=\"number\">10</span>,<span class=\"number\">2</span>)</div><div class=\"line\">scala&gt; rdd1.saveAsTextFile(<span class=\"string\">\"hdfs://cdh5/tmp/lxw1234.com/\"</span>) <span class=\"comment\">//保存到HDFS</span></div><div class=\"line\">hadoop fs -ls /tmp/lxw1234.com</div><div class=\"line\"><span class=\"type\">Found</span> <span class=\"number\">2</span> items</div><div class=\"line\">-rw-r--r--   <span class=\"number\">2</span> lxw1234 supergroup        <span class=\"number\">0</span> <span class=\"number\">2015</span><span class=\"number\">-07</span><span class=\"number\">-10</span> <span class=\"number\">09</span>:<span class=\"number\">15</span> /tmp/lxw1234.com/_SUCCESS</div><div class=\"line\">-rw-r--r--   <span class=\"number\">2</span> lxw1234 supergroup        <span class=\"number\">21</span> <span class=\"number\">2015</span><span class=\"number\">-07</span><span class=\"number\">-10</span> <span class=\"number\">09</span>:<span class=\"number\">15</span> /tmp/lxw1234.com/part<span class=\"number\">-00000</span></div><div class=\"line\"> </div><div class=\"line\">hadoop fs -cat /tmp/lxw1234.com/part<span class=\"number\">-00000</span></div></pre></td></tr></table></figure></p>\n<p>注意：如果使用rdd1.saveAsTextFile(“file:///tmp/lxw1234.com”)将文件保存到本地文件系统，那么只会保存在Executor所在机器的本地目录。<br><strong>指定压缩格式保存</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">rdd1.saveAsTextFile(<span class=\"string\">\"hdfs://cdh5/tmp/lxw1234.com/\"</span>,classOf[com.hadoop.compression.lzo.<span class=\"type\">LzopCodec</span>])</div><div class=\"line\"> </div><div class=\"line\">hadoop fs -ls /tmp/lxw1234.com</div><div class=\"line\">-rw-r--r--   <span class=\"number\">2</span> lxw1234 supergroup    <span class=\"number\">0</span> <span class=\"number\">2015</span><span class=\"number\">-07</span><span class=\"number\">-10</span> <span class=\"number\">09</span>:<span class=\"number\">20</span> /tmp/lxw1234.com/_SUCCESS</div><div class=\"line\">-rw-r--r--   <span class=\"number\">2</span> lxw1234 supergroup    <span class=\"number\">71</span> <span class=\"number\">2015</span><span class=\"number\">-07</span><span class=\"number\">-10</span> <span class=\"number\">09</span>:<span class=\"number\">20</span> /tmp/lxw1234.com/part<span class=\"number\">-00000.</span>lzo</div><div class=\"line\"> </div><div class=\"line\">hadoop fs -text /tmp/lxw1234.com/part<span class=\"number\">-00000.</span>lzo</div></pre></td></tr></table></figure></p>\n<h1 id=\"saveAsSequenceFile\"><a href=\"#saveAsSequenceFile\" class=\"headerlink\" title=\"saveAsSequenceFile\"></a><strong>saveAsSequenceFile</strong></h1><p>saveAsSequenceFile用于将RDD以SequenceFile的文件格式保存到HDFS上。</p>\n<p>用法同saveAsTextFile。</p>\n<h1 id=\"saveAsObjectFile\"><a href=\"#saveAsObjectFile\" class=\"headerlink\" title=\"saveAsObjectFile\"></a><strong>saveAsObjectFile</strong></h1><p>def saveAsObjectFile(path: String): Unit</p>\n<p>saveAsObjectFile用于将RDD中的元素序列化成对象，存储到文件中。</p>\n<p>对于HDFS，默认采用SequenceFile保存。<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">var</span> rdd1 = sc.makeRDD(<span class=\"number\">1</span> to <span class=\"number\">10</span>,<span class=\"number\">2</span>)</div><div class=\"line\">rdd1.saveAsObjectFile(<span class=\"string\">\"hdfs://cdh5/tmp/lxw1234.com/\"</span>)</div><div class=\"line\"> </div><div class=\"line\">hadoop fs -cat /tmp/lxw1234.com/part<span class=\"number\">-00000</span></div><div class=\"line\"><span class=\"type\">SEQ</span> !org.apache.hadoop.io.<span class=\"type\">NullWritable</span><span class=\"string\">\"org.apache.hadoop.io.BytesWritableT</span></div></pre></td></tr></table></figure></p>\n<h1 id=\"saveAsHadoopFile\"><a href=\"#saveAsHadoopFile\" class=\"headerlink\" title=\"saveAsHadoopFile\"></a><strong>saveAsHadoopFile</strong></h1><p>def saveAsHadoopFile(path: String, keyClass: Class[<em>], valueClass: Class[</em>], outputFormatClass: Class[<em> &lt;: OutputFormat[</em>, <em>]], codec: Class[</em> &lt;: CompressionCodec]): Unit</p>\n<p>def saveAsHadoopFile(path: String, keyClass: Class[<em>], valueClass: Class[</em>], outputFormatClass: Class[<em> &lt;: OutputFormat[</em>, <em>]], conf: JobConf = …, codec: Option[Class[</em> &lt;: CompressionCodec]] = None): Unit</p>\n<p>saveAsHadoopFile是将RDD存储在HDFS上的文件中，支持老版本Hadoop API。</p>\n<p>可以指定outputKeyClass、outputValueClass以及压缩格式。</p>\n<p>每个分区输出一个文件。<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div><div class=\"line\">106</div><div class=\"line\">107</div><div class=\"line\">108</div><div class=\"line\">109</div><div class=\"line\">110</div><div class=\"line\">111</div><div class=\"line\">112</div><div class=\"line\">113</div><div class=\"line\">114</div><div class=\"line\">115</div><div class=\"line\">116</div><div class=\"line\">117</div><div class=\"line\">118</div><div class=\"line\">119</div><div class=\"line\">120</div><div class=\"line\">121</div><div class=\"line\">122</div><div class=\"line\">123</div><div class=\"line\">124</div><div class=\"line\">125</div><div class=\"line\">126</div><div class=\"line\">127</div><div class=\"line\">128</div><div class=\"line\">129</div><div class=\"line\">130</div><div class=\"line\">131</div><div class=\"line\">132</div><div class=\"line\">133</div><div class=\"line\">134</div><div class=\"line\">135</div><div class=\"line\">136</div><div class=\"line\">137</div><div class=\"line\">138</div><div class=\"line\">139</div><div class=\"line\">140</div><div class=\"line\">141</div><div class=\"line\">142</div><div class=\"line\">143</div><div class=\"line\">144</div><div class=\"line\">145</div><div class=\"line\">146</div><div class=\"line\">147</div><div class=\"line\">148</div><div class=\"line\">149</div><div class=\"line\">150</div><div class=\"line\">151</div><div class=\"line\">152</div><div class=\"line\">153</div><div class=\"line\">154</div><div class=\"line\">155</div><div class=\"line\">156</div><div class=\"line\">157</div><div class=\"line\">158</div><div class=\"line\">159</div><div class=\"line\">160</div><div class=\"line\">161</div><div class=\"line\">162</div><div class=\"line\">163</div><div class=\"line\">164</div><div class=\"line\">165</div><div class=\"line\">166</div><div class=\"line\">167</div><div class=\"line\">168</div><div class=\"line\">169</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">var</span> rdd1 = sc.makeRDD(<span class=\"type\">Array</span>((<span class=\"string\">\"A\"</span>,<span class=\"number\">2</span>),(<span class=\"string\">\"A\"</span>,<span class=\"number\">1</span>),(<span class=\"string\">\"B\"</span>,<span class=\"number\">6</span>),(<span class=\"string\">\"B\"</span>,<span class=\"number\">3</span>),(<span class=\"string\">\"B\"</span>,<span class=\"number\">7</span>)))</div><div class=\"line\"> </div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapred.<span class=\"type\">TextOutputFormat</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.<span class=\"type\">Text</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.<span class=\"type\">IntWritable</span></div><div class=\"line\"> </div><div class=\"line\">rdd1.saveAsHadoopFile(<span class=\"string\">\"/tmp/lxw1234.com/\"</span>,classOf[<span class=\"type\">Text</span>],classOf[<span class=\"type\">IntWritable</span>],classOf[<span class=\"type\">TextOutputFormat</span>[<span class=\"type\">Text</span>,<span class=\"type\">IntWritable</span>]])</div><div class=\"line\"> </div><div class=\"line\">rdd1.saveAsHadoopFile(<span class=\"string\">\"/tmp/lxw1234.com/\"</span>,classOf[<span class=\"type\">Text</span>],classOf[<span class=\"type\">IntWritable</span>],classOf[<span class=\"type\">TextOutputFormat</span>[<span class=\"type\">Text</span>,<span class=\"type\">IntWritable</span>]],</div><div class=\"line\">                      classOf[com.hadoop.compression.lzo.<span class=\"type\">LzopCodec</span>])</div><div class=\"line\">```  </div><div class=\"line\"># **saveAsHadoopDataset**  </div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">saveAsHadoopDataset</span></span>(conf: <span class=\"type\">JobConf</span>): <span class=\"type\">Unit</span></div><div class=\"line\"></div><div class=\"line\">saveAsHadoopDataset用于将<span class=\"type\">RDD</span>保存到除了<span class=\"type\">HDFS</span>的其他存储中，比如<span class=\"type\">HBase</span>。</div><div class=\"line\"></div><div class=\"line\">在<span class=\"type\">JobConf</span>中，通常需要关注或者设置五个参数：</div><div class=\"line\"></div><div class=\"line\">文件的保存路径、key值的<span class=\"class\"><span class=\"keyword\">class</span><span class=\"title\">类型、value值的class类型、RDD的输出格式</span>(<span class=\"params\"><span class=\"type\">OutputFormat</span></span>)<span class=\"title\">、以及压缩相关的参数。</span>  </span></div><div class=\"line\"><span class=\"class\"><span class=\"title\">**##使用saveAsHadoopDataset将RDD保存到HDFS中**</span>  </span></div><div class=\"line\"><span class=\"class\">```<span class=\"title\">scala</span></span></div><div class=\"line\"><span class=\"class\"><span class=\"title\">import</span> <span class=\"title\">org</span>.<span class=\"title\">apache</span>.<span class=\"title\">spark</span>.<span class=\"title\">SparkConf</span></span></div><div class=\"line\"><span class=\"class\"><span class=\"title\">import</span> <span class=\"title\">org</span>.<span class=\"title\">apache</span>.<span class=\"title\">spark</span>.<span class=\"title\">SparkContext</span></span></div><div class=\"line\"><span class=\"class\"><span class=\"title\">import</span> <span class=\"title\">SparkContext</span>.<span class=\"title\">_</span></span></div><div class=\"line\"><span class=\"class\"><span class=\"title\">import</span> <span class=\"title\">org</span>.<span class=\"title\">apache</span>.<span class=\"title\">hadoop</span>.<span class=\"title\">mapred</span>.<span class=\"title\">TextOutputFormat</span></span></div><div class=\"line\"><span class=\"class\"><span class=\"title\">import</span> <span class=\"title\">org</span>.<span class=\"title\">apache</span>.<span class=\"title\">hadoop</span>.<span class=\"title\">io</span>.<span class=\"title\">Text</span></span></div><div class=\"line\"><span class=\"class\"><span class=\"title\">import</span> <span class=\"title\">org</span>.<span class=\"title\">apache</span>.<span class=\"title\">hadoop</span>.<span class=\"title\">io</span>.<span class=\"title\">IntWritable</span></span></div><div class=\"line\"><span class=\"class\"><span class=\"title\">import</span> <span class=\"title\">org</span>.<span class=\"title\">apache</span>.<span class=\"title\">hadoop</span>.<span class=\"title\">mapred</span>.<span class=\"title\">JobConf</span></span></div><div class=\"line\"><span class=\"class\"> </span></div><div class=\"line\"><span class=\"class\"> </span></div><div class=\"line\"><span class=\"class\"> </span></div><div class=\"line\"><span class=\"class\"><span class=\"title\">var</span> <span class=\"title\">rdd1</span> </span>= sc.makeRDD(<span class=\"type\">Array</span>((<span class=\"string\">\"A\"</span>,<span class=\"number\">2</span>),(<span class=\"string\">\"A\"</span>,<span class=\"number\">1</span>),(<span class=\"string\">\"B\"</span>,<span class=\"number\">6</span>),(<span class=\"string\">\"B\"</span>,<span class=\"number\">3</span>),(<span class=\"string\">\"B\"</span>,<span class=\"number\">7</span>)))</div><div class=\"line\"><span class=\"keyword\">var</span> jobConf = <span class=\"keyword\">new</span> <span class=\"type\">JobConf</span>()</div><div class=\"line\">jobConf.setOutputFormat(classOf[<span class=\"type\">TextOutputFormat</span>[<span class=\"type\">Text</span>,<span class=\"type\">IntWritable</span>]])</div><div class=\"line\">jobConf.setOutputKeyClass(classOf[<span class=\"type\">Text</span>])</div><div class=\"line\">jobConf.setOutputValueClass(classOf[<span class=\"type\">IntWritable</span>])</div><div class=\"line\">jobConf.set(<span class=\"string\">\"mapred.output.dir\"</span>,<span class=\"string\">\"/tmp/lxw1234/\"</span>)</div><div class=\"line\">rdd1.saveAsHadoopDataset(jobConf)</div><div class=\"line\"> </div><div class=\"line\">结果：</div><div class=\"line\">hadoop fs -cat /tmp/lxw1234/part<span class=\"number\">-00000</span></div><div class=\"line\"><span class=\"type\">A</span>       <span class=\"number\">2</span></div><div class=\"line\"><span class=\"type\">A</span>       <span class=\"number\">1</span></div><div class=\"line\">hadoop fs -cat /tmp/lxw1234/part<span class=\"number\">-00001</span></div><div class=\"line\"><span class=\"type\">B</span>       <span class=\"number\">6</span></div><div class=\"line\"><span class=\"type\">B</span>       <span class=\"number\">3</span></div><div class=\"line\"><span class=\"type\">B</span>       <span class=\"number\">7</span></div><div class=\"line\">```  </div><div class=\"line\">**##保存数据到<span class=\"type\">HBASE</span>**  </div><div class=\"line\"><span class=\"type\">HBase</span>建表：</div><div class=\"line\"></div><div class=\"line\">create ‘lxw1234′,&#123;<span class=\"type\">NAME</span> =&gt; ‘f1′,<span class=\"type\">VERSIONS</span> =&gt; <span class=\"number\">1</span>&#125;,&#123;<span class=\"type\">NAME</span> =&gt; ‘f2′,<span class=\"type\">VERSIONS</span> =&gt; <span class=\"number\">1</span>&#125;,&#123;<span class=\"type\">NAME</span> =&gt; ‘f3′,<span class=\"type\">VERSIONS</span> =&gt; <span class=\"number\">1</span>&#125;  </div><div class=\"line\">```scala</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.<span class=\"type\">SparkConf</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.<span class=\"type\">SparkContext</span></div><div class=\"line\"><span class=\"keyword\">import</span> <span class=\"type\">SparkContext</span>._</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapred.<span class=\"type\">TextOutputFormat</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.<span class=\"type\">Text</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.<span class=\"type\">IntWritable</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapred.<span class=\"type\">JobConf</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.hbase.<span class=\"type\">HBaseConfiguration</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.hbase.mapred.<span class=\"type\">TableOutputFormat</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.hbase.client.<span class=\"type\">Put</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.hbase.util.<span class=\"type\">Bytes</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.hbase.io.<span class=\"type\">ImmutableBytesWritable</span></div><div class=\"line\"> </div><div class=\"line\"><span class=\"keyword\">var</span> conf = <span class=\"type\">HBaseConfiguration</span>.create()</div><div class=\"line\">    <span class=\"keyword\">var</span> jobConf = <span class=\"keyword\">new</span> <span class=\"type\">JobConf</span>(conf)</div><div class=\"line\">    jobConf.set(<span class=\"string\">\"hbase.zookeeper.quorum\"</span>,<span class=\"string\">\"zkNode1,zkNode2,zkNode3\"</span>)</div><div class=\"line\">    jobConf.set(<span class=\"string\">\"zookeeper.znode.parent\"</span>,<span class=\"string\">\"/hbase\"</span>)</div><div class=\"line\">    jobConf.set(<span class=\"type\">TableOutputFormat</span>.<span class=\"type\">OUTPUT_TABLE</span>,<span class=\"string\">\"lxw1234\"</span>)</div><div class=\"line\">    jobConf.setOutputFormat(classOf[<span class=\"type\">TableOutputFormat</span>])</div><div class=\"line\">    </div><div class=\"line\">    <span class=\"keyword\">var</span> rdd1 = sc.makeRDD(<span class=\"type\">Array</span>((<span class=\"string\">\"A\"</span>,<span class=\"number\">2</span>),(<span class=\"string\">\"B\"</span>,<span class=\"number\">6</span>),(<span class=\"string\">\"C\"</span>,<span class=\"number\">7</span>)))</div><div class=\"line\">    rdd1.map(x =&gt; </div><div class=\"line\">      &#123;</div><div class=\"line\">        <span class=\"keyword\">var</span> put = <span class=\"keyword\">new</span> <span class=\"type\">Put</span>(<span class=\"type\">Bytes</span>.toBytes(x._1))</div><div class=\"line\">        put.add(<span class=\"type\">Bytes</span>.toBytes(<span class=\"string\">\"f1\"</span>), <span class=\"type\">Bytes</span>.toBytes(<span class=\"string\">\"c1\"</span>), <span class=\"type\">Bytes</span>.toBytes(x._2))</div><div class=\"line\">        (<span class=\"keyword\">new</span> <span class=\"type\">ImmutableBytesWritable</span>,put)</div><div class=\"line\">      &#125;</div><div class=\"line\">    ).saveAsHadoopDataset(jobConf)</div><div class=\"line\"> </div><div class=\"line\">##结果：</div><div class=\"line\">hbase(main):<span class=\"number\">005</span>:<span class=\"number\">0</span>&gt; scan <span class=\"symbol\">'lxw123</span>4'</div><div class=\"line\"><span class=\"type\">ROW</span>     <span class=\"type\">COLUMN</span>+<span class=\"type\">CELL</span>                                                                                                </div><div class=\"line\"> <span class=\"type\">A</span>       column=f1:c1, timestamp=<span class=\"number\">1436504941187</span>, value=\\x00\\x00\\x00\\x02                                              </div><div class=\"line\"> <span class=\"type\">B</span>       column=f1:c1, timestamp=<span class=\"number\">1436504941187</span>, value=\\x00\\x00\\x00\\x06                                              </div><div class=\"line\"> <span class=\"type\">C</span>       column=f1:c1, timestamp=<span class=\"number\">1436504941187</span>, value=\\x00\\x00\\x00\\x07                                              </div><div class=\"line\"><span class=\"number\">3</span> row(s) in <span class=\"number\">0.0550</span> seconds</div><div class=\"line\">```  </div><div class=\"line\">注意：保存到<span class=\"type\">HBase</span>，运行时候需要在<span class=\"type\">SPARK_CLASSPATH</span>中加入<span class=\"type\">HBase</span>相关的jar包。</div><div class=\"line\"></div><div class=\"line\">可参考：http:<span class=\"comment\">//lxw1234.com/archives/2015/07/332.htm  </span></div><div class=\"line\"># **saveAsNewAPIHadoopFile**  </div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">saveAsNewAPIHadoopFile</span></span>[<span class=\"type\">F</span> &lt;: <span class=\"type\">OutputFormat</span>[<span class=\"type\">K</span>, <span class=\"type\">V</span>]](path: <span class=\"type\">String</span>)(<span class=\"keyword\">implicit</span> fm: <span class=\"type\">ClassTag</span>[<span class=\"type\">F</span>]): <span class=\"type\">Unit</span></div><div class=\"line\"></div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">saveAsNewAPIHadoopFile</span></span>(path: <span class=\"type\">String</span>, keyClass: <span class=\"type\">Class</span>[_], valueClass: <span class=\"type\">Class</span>[_], outputFormatClass: <span class=\"type\">Class</span>[_ &lt;: <span class=\"type\">OutputFormat</span>[_, _]], conf: <span class=\"type\">Configuration</span> = self.context.hadoopConfiguration): <span class=\"type\">Unit</span></div><div class=\"line\"></div><div class=\"line\"> </div><div class=\"line\"></div><div class=\"line\">saveAsNewAPIHadoopFile用于将<span class=\"type\">RDD</span>数据保存到<span class=\"type\">HDFS</span>上，使用新版本<span class=\"type\">Hadoop</span> <span class=\"type\">API</span>。</div><div class=\"line\"></div><div class=\"line\">用法基本同saveAsHadoopFile。</div><div class=\"line\">```scala</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.<span class=\"type\">SparkConf</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.<span class=\"type\">SparkContext</span></div><div class=\"line\"><span class=\"keyword\">import</span> <span class=\"type\">SparkContext</span>._</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.lib.output.<span class=\"type\">TextOutputFormat</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.<span class=\"type\">Text</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.io.<span class=\"type\">IntWritable</span></div><div class=\"line\"> </div><div class=\"line\"><span class=\"keyword\">var</span> rdd1 = sc.makeRDD(<span class=\"type\">Array</span>((<span class=\"string\">\"A\"</span>,<span class=\"number\">2</span>),(<span class=\"string\">\"A\"</span>,<span class=\"number\">1</span>),(<span class=\"string\">\"B\"</span>,<span class=\"number\">6</span>),(<span class=\"string\">\"B\"</span>,<span class=\"number\">3</span>),(<span class=\"string\">\"B\"</span>,<span class=\"number\">7</span>)))</div><div class=\"line\">rdd1.saveAsNewAPIHadoopFile(<span class=\"string\">\"/tmp/lxw1234/\"</span>,classOf[<span class=\"type\">Text</span>],classOf[<span class=\"type\">IntWritable</span>],classOf[<span class=\"type\">TextOutputFormat</span>[<span class=\"type\">Text</span>,<span class=\"type\">IntWritable</span>]])</div><div class=\"line\">```  </div><div class=\"line\"># **saveAsNewAPIHadoopDataset**</div><div class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">saveAsNewAPIHadoopDataset</span></span>(conf: <span class=\"type\">Configuration</span>): <span class=\"type\">Unit</span></div><div class=\"line\"></div><div class=\"line\">作用同saveAsHadoopDataset,只不过采用新版本<span class=\"type\">Hadoop</span> <span class=\"type\">API</span>。</div><div class=\"line\"></div><div class=\"line\">以写入<span class=\"type\">HBase</span>为例：</div><div class=\"line\"></div><div class=\"line\"> </div><div class=\"line\"></div><div class=\"line\"><span class=\"type\">HBase</span>建表：</div><div class=\"line\"></div><div class=\"line\">create ‘lxw1234′,&#123;<span class=\"type\">NAME</span> =&gt; ‘f1′,<span class=\"type\">VERSIONS</span> =&gt; <span class=\"number\">1</span>&#125;,&#123;<span class=\"type\">NAME</span> =&gt; ‘f2′,<span class=\"type\">VERSIONS</span> =&gt; <span class=\"number\">1</span>&#125;,&#123;<span class=\"type\">NAME</span> =&gt; ‘f3′,<span class=\"type\">VERSIONS</span> =&gt; <span class=\"number\">1</span>&#125;</div><div class=\"line\"></div><div class=\"line\"> </div><div class=\"line\"></div><div class=\"line\">完整的<span class=\"type\">Spark</span>应用程序：  </div><div class=\"line\">```scala</div><div class=\"line\"><span class=\"keyword\">package</span> com.lxw1234.test</div><div class=\"line\"> </div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.<span class=\"type\">SparkConf</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.spark.<span class=\"type\">SparkContext</span></div><div class=\"line\"><span class=\"keyword\">import</span> <span class=\"type\">SparkContext</span>._</div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.hbase.<span class=\"type\">HBaseConfiguration</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.mapreduce.<span class=\"type\">Job</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.hbase.mapreduce.<span class=\"type\">TableOutputFormat</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.hbase.io.<span class=\"type\">ImmutableBytesWritable</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.hbase.client.<span class=\"type\">Result</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.hbase.util.<span class=\"type\">Bytes</span></div><div class=\"line\"><span class=\"keyword\">import</span> org.apache.hadoop.hbase.client.<span class=\"type\">Put</span></div><div class=\"line\"> </div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">object</span> <span class=\"title\">Test</span> </span>&#123;</div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">main</span></span>(args : <span class=\"type\">Array</span>[<span class=\"type\">String</span>]) &#123;</div><div class=\"line\">   <span class=\"keyword\">val</span> sparkConf = <span class=\"keyword\">new</span> <span class=\"type\">SparkConf</span>().setMaster(<span class=\"string\">\"spark://lxw1234.com:7077\"</span>).setAppName(<span class=\"string\">\"lxw1234.com\"</span>)</div><div class=\"line\">   <span class=\"keyword\">val</span> sc = <span class=\"keyword\">new</span> <span class=\"type\">SparkContext</span>(sparkConf);</div><div class=\"line\">   <span class=\"keyword\">var</span> rdd1 = sc.makeRDD(<span class=\"type\">Array</span>((<span class=\"string\">\"A\"</span>,<span class=\"number\">2</span>),(<span class=\"string\">\"B\"</span>,<span class=\"number\">6</span>),(<span class=\"string\">\"C\"</span>,<span class=\"number\">7</span>)))</div><div class=\"line\">   </div><div class=\"line\">    sc.hadoopConfiguration.set(<span class=\"string\">\"hbase.zookeeper.quorum \"</span>,<span class=\"string\">\"zkNode1,zkNode2,zkNode3\"</span>)</div><div class=\"line\">    sc.hadoopConfiguration.set(<span class=\"string\">\"zookeeper.znode.parent\"</span>,<span class=\"string\">\"/hbase\"</span>)</div><div class=\"line\">    sc.hadoopConfiguration.set(<span class=\"type\">TableOutputFormat</span>.<span class=\"type\">OUTPUT_TABLE</span>,<span class=\"string\">\"lxw1234\"</span>)</div><div class=\"line\">    <span class=\"keyword\">var</span> job = <span class=\"keyword\">new</span> <span class=\"type\">Job</span>(sc.hadoopConfiguration)</div><div class=\"line\">    job.setOutputKeyClass(classOf[<span class=\"type\">ImmutableBytesWritable</span>])</div><div class=\"line\">    job.setOutputValueClass(classOf[<span class=\"type\">Result</span>])</div><div class=\"line\">    job.setOutputFormatClass(classOf[<span class=\"type\">TableOutputFormat</span>[<span class=\"type\">ImmutableBytesWritable</span>]])</div><div class=\"line\">    </div><div class=\"line\">    rdd1.map(</div><div class=\"line\">      x =&gt; &#123;</div><div class=\"line\">        <span class=\"keyword\">var</span> put = <span class=\"keyword\">new</span> <span class=\"type\">Put</span>(<span class=\"type\">Bytes</span>.toBytes(x._1))</div><div class=\"line\">        put.add(<span class=\"type\">Bytes</span>.toBytes(<span class=\"string\">\"f1\"</span>), <span class=\"type\">Bytes</span>.toBytes(<span class=\"string\">\"c1\"</span>), <span class=\"type\">Bytes</span>.toBytes(x._2))</div><div class=\"line\">        (<span class=\"keyword\">new</span> <span class=\"type\">ImmutableBytesWritable</span>,put)</div><div class=\"line\">      &#125;    </div><div class=\"line\">    ).saveAsNewAPIHadoopDataset(job.getConfiguration)</div><div class=\"line\">    </div><div class=\"line\">    sc.stop()   </div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>注意：保存到HBase，运行时候需要在SPARK_CLASSPATH中加入HBase相关的jar包。</p>\n<p>可参考：<a href=\"http://lxw1234.com/archives/2015/07/332.htm\" target=\"_blank\" rel=\"external\">http://lxw1234.com/archives/2015/07/332.htm</a>  </p>\n<p>感谢原作者的总结<br>本文转自: lxw的大数据田地<br><a href=\"http://lxw1234.com/archives/2015/07/402.htm\" target=\"_blank\" rel=\"external\">http://lxw1234.com/archives/2015/07/402.htm</a><br><a href=\"http://lxw1234.com/archives/2015/07/404.htm\" target=\"_blank\" rel=\"external\">http://lxw1234.com/archives/2015/07/404.htm</a><br><a href=\"http://lxw1234.com/archives/2015/07/406.htm\" target=\"_blank\" rel=\"external\">http://lxw1234.com/archives/2015/07/406.htm</a></p>\n"},{"title":"spark RDD算子（四）之创建键值对RDD mapToPair flatMapToPair","date":"2017-03-04T13:25:21.000Z","author":"kaishun","id":"37","_content":"\n# **mapToPair**\n举例，在F:\\sparktest\\sample.txt 文件的内容如下  \n```\naa bb cc aa aa aa dd dd ee ee ee ee \nff aa bb zks\nee kks\nee  zz zks\n``` \n将每一行的第一个单词作为键，1 作为value创建pairRDD\n**scala版本**  \nscala是没有mapToPair函数的，scala版本只需要map就可以了\n```scala\n    scala> val lines = sc.textFile(\"F:\\\\sparktest\\\\sample.txt\")\n    \n    scala> val pairs = lines.map(x => (x.split(\"\\\\s+\")(0), 1))\n    \n    scala> pairs.collect\n    res0: Array[(String, Int)] = Array((aa,1), (ff,1), (ee,1), (ee,1))\n```\n**java版本**\n```java\n    JavaRDD<String> lines = sc.textFile(\"F:\\\\sparktest\\\\sample.txt\");\n    //输入的是一个string的字符串，输出的是一个(String, Integer) 的map\n    JavaPairRDD<String, Integer> pairRDD = lines.mapToPair(new PairFunction<String, String, Integer>() {\n        @Override\n        public Tuple2<String, Integer> call(String s) throws Exception {\n            return new Tuple2<String, Integer>(s.split(\"\\\\s+\")[0], 1);\n        }\n    });\n```\n# **flatMapToPair**\n类似于xxx连接  mapToPair是一对一，一个元素返回一个元素，而flatMapToPair可以一个元素返回多个，相当于先flatMap,在mapToPair\n例子: 将每一个单词都分成键为\n**scala版本**\n```scala\n    val lines = sc.textFile(\"F:\\\\sparktest\\\\sample.txt\")\n    val flatRDD = lines.flatMap(x => (x.split(\"\\\\s+\")))\n    val pairs = flatRDD.map(x=>(x,1))\n    \n    scala> pairs.collect\n    res1: Array[(String, Int)] = Array((aa,1), (bb,1), (cc,1), (aa,1), (aa,1), (aa,1), (dd,1), (dd,1), (ee,1), (ee,1), (ee,1), (ee,1), (ff,1), (aa,1), (bb,1), (zks,1), (ee,1), (kks,1), (ee,1), (zz,1), (zks,1))\n```\n**java版本 spark2.0以下**\n```java\nJavaPairRDD<String, Integer> wordPairRDD = lines.flatMapToPair(new PairFlatMapFunction<String, String, Integer>() {\n            @Override\n            public Iterable<Tuple2<String, Integer>> call(String s) throws Exception {\n                ArrayList<Tuple2<String, Integer>> tpLists = new ArrayList<Tuple2<String, Integer>>();\n                String[] split = s.split(\"\\\\s+\");\n                for (int i = 0; i <split.length ; i++) {\n                    Tuple2 tp = new Tuple2<String,Integer>(split[i], 1);\n                    tpLists.add(tp);\n                }\n            return tpLists;\n            }\n        });\n```  \n**java版本 spark2.0以上**\n主要是iterator和iteratable的一些区别\n```java\n        JavaPairRDD<String, Integer> wordPairRDD = lines.flatMapToPair(new PairFlatMapFunction<String, String, Integer>() {\n            @Override\n            public Iterator<Tuple2<String, Integer>> call(String s) throws Exception {\n                ArrayList<Tuple2<String, Integer>> tpLists = new ArrayList<Tuple2<String, Integer>>();\n                String[] split = s.split(\"\\\\s+\");\n                for (int i = 0; i <split.length ; i++) {\n                    Tuple2 tp = new Tuple2<String,Integer>(split[i], 1);\n                    tpLists.add(tp);\n                }\n                return tpLists.iterator();\n            }\n        });\n```","source":"_posts/spark RDD算子（四）之创建键值对RDD mapToPair flatMapToPair.md","raw":"---\ntitle: spark RDD算子（四）之创建键值对RDD mapToPair flatMapToPair\ndate: 2017-03-04 21:25:21\ntags: [spark]\ncategories: [大数据,spark]\nauthor: kaishun\nid: 37\npermalink: spark-rdd-4\n---\n\n# **mapToPair**\n举例，在F:\\sparktest\\sample.txt 文件的内容如下  \n```\naa bb cc aa aa aa dd dd ee ee ee ee \nff aa bb zks\nee kks\nee  zz zks\n``` \n将每一行的第一个单词作为键，1 作为value创建pairRDD\n**scala版本**  \nscala是没有mapToPair函数的，scala版本只需要map就可以了\n```scala\n    scala> val lines = sc.textFile(\"F:\\\\sparktest\\\\sample.txt\")\n    \n    scala> val pairs = lines.map(x => (x.split(\"\\\\s+\")(0), 1))\n    \n    scala> pairs.collect\n    res0: Array[(String, Int)] = Array((aa,1), (ff,1), (ee,1), (ee,1))\n```\n**java版本**\n```java\n    JavaRDD<String> lines = sc.textFile(\"F:\\\\sparktest\\\\sample.txt\");\n    //输入的是一个string的字符串，输出的是一个(String, Integer) 的map\n    JavaPairRDD<String, Integer> pairRDD = lines.mapToPair(new PairFunction<String, String, Integer>() {\n        @Override\n        public Tuple2<String, Integer> call(String s) throws Exception {\n            return new Tuple2<String, Integer>(s.split(\"\\\\s+\")[0], 1);\n        }\n    });\n```\n# **flatMapToPair**\n类似于xxx连接  mapToPair是一对一，一个元素返回一个元素，而flatMapToPair可以一个元素返回多个，相当于先flatMap,在mapToPair\n例子: 将每一个单词都分成键为\n**scala版本**\n```scala\n    val lines = sc.textFile(\"F:\\\\sparktest\\\\sample.txt\")\n    val flatRDD = lines.flatMap(x => (x.split(\"\\\\s+\")))\n    val pairs = flatRDD.map(x=>(x,1))\n    \n    scala> pairs.collect\n    res1: Array[(String, Int)] = Array((aa,1), (bb,1), (cc,1), (aa,1), (aa,1), (aa,1), (dd,1), (dd,1), (ee,1), (ee,1), (ee,1), (ee,1), (ff,1), (aa,1), (bb,1), (zks,1), (ee,1), (kks,1), (ee,1), (zz,1), (zks,1))\n```\n**java版本 spark2.0以下**\n```java\nJavaPairRDD<String, Integer> wordPairRDD = lines.flatMapToPair(new PairFlatMapFunction<String, String, Integer>() {\n            @Override\n            public Iterable<Tuple2<String, Integer>> call(String s) throws Exception {\n                ArrayList<Tuple2<String, Integer>> tpLists = new ArrayList<Tuple2<String, Integer>>();\n                String[] split = s.split(\"\\\\s+\");\n                for (int i = 0; i <split.length ; i++) {\n                    Tuple2 tp = new Tuple2<String,Integer>(split[i], 1);\n                    tpLists.add(tp);\n                }\n            return tpLists;\n            }\n        });\n```  \n**java版本 spark2.0以上**\n主要是iterator和iteratable的一些区别\n```java\n        JavaPairRDD<String, Integer> wordPairRDD = lines.flatMapToPair(new PairFlatMapFunction<String, String, Integer>() {\n            @Override\n            public Iterator<Tuple2<String, Integer>> call(String s) throws Exception {\n                ArrayList<Tuple2<String, Integer>> tpLists = new ArrayList<Tuple2<String, Integer>>();\n                String[] split = s.split(\"\\\\s+\");\n                for (int i = 0; i <split.length ; i++) {\n                    Tuple2 tp = new Tuple2<String,Integer>(split[i], 1);\n                    tpLists.add(tp);\n                }\n                return tpLists.iterator();\n            }\n        });\n```","slug":"spark-rdd-4","published":1,"updated":"2018-01-22T15:24:40.859Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcrpnzrk003i2wv36xp0leea","content":"<h1 id=\"mapToPair\"><a href=\"#mapToPair\" class=\"headerlink\" title=\"mapToPair\"></a><strong>mapToPair</strong></h1><p>举例，在F:\\sparktest\\sample.txt 文件的内容如下<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\">aa bb cc aa aa aa dd dd ee ee ee ee </div><div class=\"line\">ff aa bb zks</div><div class=\"line\">ee kks</div><div class=\"line\">ee  zz zks</div><div class=\"line\">``` </div><div class=\"line\">将每一行的第一个单词作为键，1 作为value创建pairRDD</div><div class=\"line\">**scala版本**  </div><div class=\"line\">scala是没有mapToPair函数的，scala版本只需要map就可以了</div><div class=\"line\">```scala</div><div class=\"line\">    scala&gt; val lines = sc.textFile(&quot;F:\\\\sparktest\\\\sample.txt&quot;)</div><div class=\"line\">    </div><div class=\"line\">    scala&gt; val pairs = lines.map(x =&gt; (x.split(&quot;\\\\s+&quot;)(0), 1))</div><div class=\"line\">    </div><div class=\"line\">    scala&gt; pairs.collect</div><div class=\"line\">    res0: Array[(String, Int)] = Array((aa,1), (ff,1), (ee,1), (ee,1))</div></pre></td></tr></table></figure></p>\n<p><strong>java版本</strong><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">JavaRDD&lt;String&gt; lines = sc.textFile(<span class=\"string\">\"F:\\\\sparktest\\\\sample.txt\"</span>);</div><div class=\"line\"><span class=\"comment\">//输入的是一个string的字符串，输出的是一个(String, Integer) 的map</span></div><div class=\"line\">JavaPairRDD&lt;String, Integer&gt; pairRDD = lines.mapToPair(<span class=\"keyword\">new</span> PairFunction&lt;String, String, Integer&gt;() &#123;</div><div class=\"line\">    <span class=\"meta\">@Override</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> Tuple2&lt;String, Integer&gt; <span class=\"title\">call</span><span class=\"params\">(String s)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> Tuple2&lt;String, Integer&gt;(s.split(<span class=\"string\">\"\\\\s+\"</span>)[<span class=\"number\">0</span>], <span class=\"number\">1</span>);</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;);</div></pre></td></tr></table></figure></p>\n<h1 id=\"flatMapToPair\"><a href=\"#flatMapToPair\" class=\"headerlink\" title=\"flatMapToPair\"></a><strong>flatMapToPair</strong></h1><p>类似于xxx连接  mapToPair是一对一，一个元素返回一个元素，而flatMapToPair可以一个元素返回多个，相当于先flatMap,在mapToPair<br>例子: 将每一个单词都分成键为<br><strong>scala版本</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">val</span> lines = sc.textFile(<span class=\"string\">\"F:\\\\sparktest\\\\sample.txt\"</span>)</div><div class=\"line\"><span class=\"keyword\">val</span> flatRDD = lines.flatMap(x =&gt; (x.split(<span class=\"string\">\"\\\\s+\"</span>)))</div><div class=\"line\"><span class=\"keyword\">val</span> pairs = flatRDD.map(x=&gt;(x,<span class=\"number\">1</span>))</div><div class=\"line\"></div><div class=\"line\">scala&gt; pairs.collect</div><div class=\"line\">res1: <span class=\"type\">Array</span>[(<span class=\"type\">String</span>, <span class=\"type\">Int</span>)] = <span class=\"type\">Array</span>((aa,<span class=\"number\">1</span>), (bb,<span class=\"number\">1</span>), (cc,<span class=\"number\">1</span>), (aa,<span class=\"number\">1</span>), (aa,<span class=\"number\">1</span>), (aa,<span class=\"number\">1</span>), (dd,<span class=\"number\">1</span>), (dd,<span class=\"number\">1</span>), (ee,<span class=\"number\">1</span>), (ee,<span class=\"number\">1</span>), (ee,<span class=\"number\">1</span>), (ee,<span class=\"number\">1</span>), (ff,<span class=\"number\">1</span>), (aa,<span class=\"number\">1</span>), (bb,<span class=\"number\">1</span>), (zks,<span class=\"number\">1</span>), (ee,<span class=\"number\">1</span>), (kks,<span class=\"number\">1</span>), (ee,<span class=\"number\">1</span>), (zz,<span class=\"number\">1</span>), (zks,<span class=\"number\">1</span>))</div></pre></td></tr></table></figure></p>\n<p><strong>java版本 spark2.0以下</strong><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div></pre></td><td class=\"code\"><pre><div class=\"line\">JavaPairRDD&lt;String, Integer&gt; wordPairRDD = lines.flatMapToPair(<span class=\"keyword\">new</span> PairFlatMapFunction&lt;String, String, Integer&gt;() &#123;</div><div class=\"line\">            <span class=\"meta\">@Override</span></div><div class=\"line\">            <span class=\"keyword\">public</span> Iterable&lt;Tuple2&lt;String, Integer&gt;&gt; call(String s) <span class=\"keyword\">throws</span> Exception &#123;</div><div class=\"line\">                ArrayList&lt;Tuple2&lt;String, Integer&gt;&gt; tpLists = <span class=\"keyword\">new</span> ArrayList&lt;Tuple2&lt;String, Integer&gt;&gt;();</div><div class=\"line\">                String[] split = s.split(<span class=\"string\">\"\\\\s+\"</span>);</div><div class=\"line\">                <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt;split.length ; i++) &#123;</div><div class=\"line\">                    Tuple2 tp = <span class=\"keyword\">new</span> Tuple2&lt;String,Integer&gt;(split[i], <span class=\"number\">1</span>);</div><div class=\"line\">                    tpLists.add(tp);</div><div class=\"line\">                &#125;</div><div class=\"line\">            <span class=\"keyword\">return</span> tpLists;</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;);</div><div class=\"line\">```  </div><div class=\"line\">**java版本 spark2.0以上**</div><div class=\"line\">主要是iterator和iteratable的一些区别</div><div class=\"line\">```java</div><div class=\"line\">        JavaPairRDD&lt;String, Integer&gt; wordPairRDD = lines.flatMapToPair(<span class=\"keyword\">new</span> PairFlatMapFunction&lt;String, String, Integer&gt;() &#123;</div><div class=\"line\">            <span class=\"meta\">@Override</span></div><div class=\"line\">            <span class=\"keyword\">public</span> Iterator&lt;Tuple2&lt;String, Integer&gt;&gt; call(String s) <span class=\"keyword\">throws</span> Exception &#123;</div><div class=\"line\">                ArrayList&lt;Tuple2&lt;String, Integer&gt;&gt; tpLists = <span class=\"keyword\">new</span> ArrayList&lt;Tuple2&lt;String, Integer&gt;&gt;();</div><div class=\"line\">                String[] split = s.split(<span class=\"string\">\"\\\\s+\"</span>);</div><div class=\"line\">                <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt;split.length ; i++) &#123;</div><div class=\"line\">                    Tuple2 tp = <span class=\"keyword\">new</span> Tuple2&lt;String,Integer&gt;(split[i], <span class=\"number\">1</span>);</div><div class=\"line\">                    tpLists.add(tp);</div><div class=\"line\">                &#125;</div><div class=\"line\">                <span class=\"keyword\">return</span> tpLists.iterator();</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;);</div></pre></td></tr></table></figure></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"mapToPair\"><a href=\"#mapToPair\" class=\"headerlink\" title=\"mapToPair\"></a><strong>mapToPair</strong></h1><p>举例，在F:\\sparktest\\sample.txt 文件的内容如下<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div></pre></td><td class=\"code\"><pre><div class=\"line\">aa bb cc aa aa aa dd dd ee ee ee ee </div><div class=\"line\">ff aa bb zks</div><div class=\"line\">ee kks</div><div class=\"line\">ee  zz zks</div><div class=\"line\">``` </div><div class=\"line\">将每一行的第一个单词作为键，1 作为value创建pairRDD</div><div class=\"line\">**scala版本**  </div><div class=\"line\">scala是没有mapToPair函数的，scala版本只需要map就可以了</div><div class=\"line\">```scala</div><div class=\"line\">    scala&gt; val lines = sc.textFile(&quot;F:\\\\sparktest\\\\sample.txt&quot;)</div><div class=\"line\">    </div><div class=\"line\">    scala&gt; val pairs = lines.map(x =&gt; (x.split(&quot;\\\\s+&quot;)(0), 1))</div><div class=\"line\">    </div><div class=\"line\">    scala&gt; pairs.collect</div><div class=\"line\">    res0: Array[(String, Int)] = Array((aa,1), (ff,1), (ee,1), (ee,1))</div></pre></td></tr></table></figure></p>\n<p><strong>java版本</strong><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">JavaRDD&lt;String&gt; lines = sc.textFile(<span class=\"string\">\"F:\\\\sparktest\\\\sample.txt\"</span>);</div><div class=\"line\"><span class=\"comment\">//输入的是一个string的字符串，输出的是一个(String, Integer) 的map</span></div><div class=\"line\">JavaPairRDD&lt;String, Integer&gt; pairRDD = lines.mapToPair(<span class=\"keyword\">new</span> PairFunction&lt;String, String, Integer&gt;() &#123;</div><div class=\"line\">    <span class=\"meta\">@Override</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> Tuple2&lt;String, Integer&gt; <span class=\"title\">call</span><span class=\"params\">(String s)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> Tuple2&lt;String, Integer&gt;(s.split(<span class=\"string\">\"\\\\s+\"</span>)[<span class=\"number\">0</span>], <span class=\"number\">1</span>);</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;);</div></pre></td></tr></table></figure></p>\n<h1 id=\"flatMapToPair\"><a href=\"#flatMapToPair\" class=\"headerlink\" title=\"flatMapToPair\"></a><strong>flatMapToPair</strong></h1><p>类似于xxx连接  mapToPair是一对一，一个元素返回一个元素，而flatMapToPair可以一个元素返回多个，相当于先flatMap,在mapToPair<br>例子: 将每一个单词都分成键为<br><strong>scala版本</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">val</span> lines = sc.textFile(<span class=\"string\">\"F:\\\\sparktest\\\\sample.txt\"</span>)</div><div class=\"line\"><span class=\"keyword\">val</span> flatRDD = lines.flatMap(x =&gt; (x.split(<span class=\"string\">\"\\\\s+\"</span>)))</div><div class=\"line\"><span class=\"keyword\">val</span> pairs = flatRDD.map(x=&gt;(x,<span class=\"number\">1</span>))</div><div class=\"line\"></div><div class=\"line\">scala&gt; pairs.collect</div><div class=\"line\">res1: <span class=\"type\">Array</span>[(<span class=\"type\">String</span>, <span class=\"type\">Int</span>)] = <span class=\"type\">Array</span>((aa,<span class=\"number\">1</span>), (bb,<span class=\"number\">1</span>), (cc,<span class=\"number\">1</span>), (aa,<span class=\"number\">1</span>), (aa,<span class=\"number\">1</span>), (aa,<span class=\"number\">1</span>), (dd,<span class=\"number\">1</span>), (dd,<span class=\"number\">1</span>), (ee,<span class=\"number\">1</span>), (ee,<span class=\"number\">1</span>), (ee,<span class=\"number\">1</span>), (ee,<span class=\"number\">1</span>), (ff,<span class=\"number\">1</span>), (aa,<span class=\"number\">1</span>), (bb,<span class=\"number\">1</span>), (zks,<span class=\"number\">1</span>), (ee,<span class=\"number\">1</span>), (kks,<span class=\"number\">1</span>), (ee,<span class=\"number\">1</span>), (zz,<span class=\"number\">1</span>), (zks,<span class=\"number\">1</span>))</div></pre></td></tr></table></figure></p>\n<p><strong>java版本 spark2.0以下</strong><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div></pre></td><td class=\"code\"><pre><div class=\"line\">JavaPairRDD&lt;String, Integer&gt; wordPairRDD = lines.flatMapToPair(<span class=\"keyword\">new</span> PairFlatMapFunction&lt;String, String, Integer&gt;() &#123;</div><div class=\"line\">            <span class=\"meta\">@Override</span></div><div class=\"line\">            <span class=\"keyword\">public</span> Iterable&lt;Tuple2&lt;String, Integer&gt;&gt; call(String s) <span class=\"keyword\">throws</span> Exception &#123;</div><div class=\"line\">                ArrayList&lt;Tuple2&lt;String, Integer&gt;&gt; tpLists = <span class=\"keyword\">new</span> ArrayList&lt;Tuple2&lt;String, Integer&gt;&gt;();</div><div class=\"line\">                String[] split = s.split(<span class=\"string\">\"\\\\s+\"</span>);</div><div class=\"line\">                <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt;split.length ; i++) &#123;</div><div class=\"line\">                    Tuple2 tp = <span class=\"keyword\">new</span> Tuple2&lt;String,Integer&gt;(split[i], <span class=\"number\">1</span>);</div><div class=\"line\">                    tpLists.add(tp);</div><div class=\"line\">                &#125;</div><div class=\"line\">            <span class=\"keyword\">return</span> tpLists;</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;);</div><div class=\"line\">```  </div><div class=\"line\">**java版本 spark2.0以上**</div><div class=\"line\">主要是iterator和iteratable的一些区别</div><div class=\"line\">```java</div><div class=\"line\">        JavaPairRDD&lt;String, Integer&gt; wordPairRDD = lines.flatMapToPair(<span class=\"keyword\">new</span> PairFlatMapFunction&lt;String, String, Integer&gt;() &#123;</div><div class=\"line\">            <span class=\"meta\">@Override</span></div><div class=\"line\">            <span class=\"keyword\">public</span> Iterator&lt;Tuple2&lt;String, Integer&gt;&gt; call(String s) <span class=\"keyword\">throws</span> Exception &#123;</div><div class=\"line\">                ArrayList&lt;Tuple2&lt;String, Integer&gt;&gt; tpLists = <span class=\"keyword\">new</span> ArrayList&lt;Tuple2&lt;String, Integer&gt;&gt;();</div><div class=\"line\">                String[] split = s.split(<span class=\"string\">\"\\\\s+\"</span>);</div><div class=\"line\">                <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt;split.length ; i++) &#123;</div><div class=\"line\">                    Tuple2 tp = <span class=\"keyword\">new</span> Tuple2&lt;String,Integer&gt;(split[i], <span class=\"number\">1</span>);</div><div class=\"line\">                    tpLists.add(tp);</div><div class=\"line\">                &#125;</div><div class=\"line\">                <span class=\"keyword\">return</span> tpLists.iterator();</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;);</div></pre></td></tr></table></figure></p>\n"},{"title":"spark RDD算子（十三）之RDD 分区 HashPartitioner，RangePartitioner，自定义分区","date":"2017-04-22T13:25:21.000Z","author":"kaishun","id":"48","_content":"\n**关键字**: spark分区方式，java HashPartitioner分区，scala HashPartitioner分区， java RangePartitioner 分区，scala RangePartitioner分区， java 自定义分区，scala自定义分区 \n#  **默认分区和HashPartitioner分区**   \n默认的分区就是HashPartition分区,默认分区不再介绍，下面介绍HashPartition的使用\n\n通过上一章 [mapPartitionsWithIndex的例子](http://blog.csdn.net/t1dmzks/article/details/71336119#t1)，我们可以构建一个方法，用来查看RDD的分区  \n```scala\n    def mapPartIndexFunc(i1:Int,iter: Iterator[(Int,Int)]):Iterator[(Int,(Int,Int))]={\n      var res = List[(Int,(Int,Int))]()\n      while(iter.hasNext){\n        var next = iter.next()\n        res=res.::(i1,next)\n      }\n      res.iterator\n    }\n    def printRDDPart(rdd:RDD[(Int,Int)]): Unit ={\n      var mapPartIndexRDDs = rdd.mapPartitionsWithIndex(mapPartIndexFunc)\n      mapPartIndexRDDs.foreach(println( _))\n    }\n```  \n**HashPartitioner分区  scala**  \n使用pairRdd.partitionBy(new spark.HashPartitioner(n)), 可以分为n个区\n```scala\n    var pairRdd = sc.parallelize(List((1,1), (1,2), (2,3), (2,4), (3,5), (3,6),(4,7), (4,8),(5,9), (5,10)))\n    //未分区的输出\n    printRDDPart(pairRdd)\n    println(\"=========================\")\n    val partitioned = pairRdd.partitionBy(new spark.HashPartitioner(3))\n    //分区后的输出\n    printRDDPart(partitioned)\n-----------输出------------\n(0,(5,10))\n(0,(5,9))\n(0,(4,8))\n(0,(4,7))\n(0,(3,6))\n(0,(3,5))\n(0,(2,4))\n(0,(2,3))\n(0,(1,2))\n(0,(1,1))\n=========================\n(0,(3,6))\n(0,(3,5))\n(1,(4,8))\n(1,(4,7))\n(1,(1,2))\n(1,(1,1))\n(2,(5,10))\n(2,(5,9))\n(2,(2,4))\n(2,(2,3))\n```  \n**HashPartitioner是如何分区的**： 国内很多说法都是有问题的，参考国外的一个说法 Uses Java’s Object.hashCodemethod to determine the partition as partition = key.hashCode() % numPartitions.  翻译过来就是使用java对象的hashCode来决定是哪个分区，对于piarRDD, 分区就是key.hashCode() % numPartitions, 3%3=0，所以 (3,6) 这个元素在0 分区， 4%3=1，所以元素(4,8) 在1 分区。 下面参考一张图  \n![spark分区](https://cdn.edureka.co/blog/wp-content/uploads/2016/03/11.png)  \n\n\n# **RangePartitioner**  \n我理解成范围分区器\n使用一个范围，将范围内的键分配给相应的分区。这种方法适用于键中有自然排序，键不为负。本文主要介绍如何使用，原理以后再仔细研究,以下代码片段显示了RangePartitioner的用法\n\n**RangePartitioner 分区 scala **\n```scala\n    var pairRdd = sc.parallelize(List((1,1), (5,10), (5,9), (2,4), (3,5), (3,6),(4,7), (4,8),(2,3), (1,2)))\n    printRDDPart(pairRdd)\n    println(\"=========================\")\n    val partitioned = pairRdd.partitionBy(new RangePartitioner(3,pairRdd))\n    printRDDPart(partitioned)\n  }\n-------------------输出------------------\n(0,(1,2))\n(0,(2,3))\n(0,(4,8))\n(0,(4,7))\n(0,(3,6))\n(0,(3,5))\n(0,(2,4))\n(0,(5,9))\n(0,(5,10))\n(0,(1,1))\n=========================\n(0,(1,2))\n(0,(2,3))\n(0,(2,4))\n(0,(1,1))\n(1,(4,8))\n(1,(4,7))\n(1,(3,6))\n(1,(3,5))\n(2,(5,9))\n(2,(5,10))\n\n```  \n上面的RDD生成的时候是乱的，但是我们让他分成三个范围，按照范围，key值为1,2的划分到第一个分区，key值为3，4的划分到第二个分区，key值为5的划分到第三个分区  \n\n\n\n# **自定义分区**  \n要实现自定义的分区器，你需要继承 org.apache.spark.Partitioner 类并实现下面三个方法  \n- **numPartitions: Int**：返回创建出来的分区数。  \n- **getPartition(key: Any): Int**：返回给定键的分区编号（ 0 到 numPartitions-1）。\n下面我自定义一个分区，让key大于等于4的落在第一个分区，key>=2并且key<4的落在第二个分区，其余的落在第一个分区。  \n**scala版本**\n自定义分区器\n```scala\nclass CustomPartitioner(numParts: Int) extends Partitioner{\n  override def numPartitions: Int = numParts\n\n  override def getPartition(key: Any): Int = {\n    if(key.toString.toInt>=4){\n       0\n    }else if(key.toString.toInt>=2&&key.toString.toInt<4){\n      1\n    }else{\n      2\n    }\n  }\n}\n```\n分区, 然后调用前面我们写的printRDDPart方法把各个分区中的RDD打印出来\n```scala\n    var pairRdd = sc.parallelize(List((1,1), (5,10), (5,9), (2,4), (3,5), (3,6),(4,7), (4,8),(2,3), (1,2)))\n    val partitionedRdd = pairRdd.partitionBy(new CustomPartitioner(3))\n    printRDDPart(partitionedRdd)\n----------输出-----------------\n(0,(4,8))\n(0,(4,7))\n(0,(5,9))\n(0,(5,10))\n(1,(2,3))\n(1,(3,6))\n(1,(3,5))\n(1,(2,4))\n(2,(1,2))\n(2,(1,1))\n```\n\n\n# ==**java 分区的用法**== \n\n同样，写个方法，该方法能打印RDD下的每个分区下的各个元素\n\n**打印每个分区下的各个元素的printPartRDD函数**\n```java\n    private static void printPartRDD(JavaPairRDD<Integer, Integer> pairRDD) {\n        JavaRDD<Tuple2<Integer, Tuple2<Integer, Integer>>> mapPartitionIndexRDD = pairRDD.mapPartitionsWithIndex(new Function2<Integer, Iterator<Tuple2<Integer, Integer>>, Iterator<Tuple2<Integer, Tuple2<Integer, Integer>>>>() {\n            @Override\n            public Iterator<Tuple2<Integer, Tuple2<Integer, Integer>>> call(Integer partIndex, Iterator<Tuple2<Integer, Integer>> tuple2Iterator) {\n                ArrayList<Tuple2<Integer, Tuple2<Integer, Integer>>> tuple2s = new ArrayList<>();\n\n                while (tuple2Iterator.hasNext()) {\n                    Tuple2<Integer, Integer> next = tuple2Iterator.next();\n                    tuple2s.add(new Tuple2<Integer, Tuple2<Integer, Integer>>(partIndex, next));\n                }\n                return tuple2s.iterator();\n            }\n        }, false);\n\n        mapPartitionIndexRDD.foreach(new VoidFunction<Tuple2<Integer, Tuple2<Integer, Integer>>>() {\n            @Override\n            public void call(Tuple2<Integer, Tuple2<Integer, Integer>> integerTuple2Tuple2) throws Exception {\n                System.out.println(integerTuple2Tuple2);\n            }\n        });\n    }\n```  \n**java HashPartitioner 分区**  \n\n```java\n\n\t\tJavaRDD<Tuple2<Integer, Integer>> tupRdd = sc.parallelize(Arrays.asList(new Tuple2<Integer, Integer>(1, 1), new Tuple2<Integer, Integer>(1, 2)\n                , new Tuple2<Integer, Integer>(2, 3), new Tuple2<Integer, Integer>(2, 4)\n                , new Tuple2<Integer, Integer>(3, 5), new Tuple2<Integer, Integer>(3, 6)\n                , new Tuple2<Integer, Integer>(4, 7), new Tuple2<Integer, Integer>(4, 8)\n                , new Tuple2<Integer, Integer>(5, 9), new Tuple2<Integer, Integer>(5, 10)\n        ), 3);\n        JavaPairRDD<Integer, Integer> pairRDD = JavaPairRDD.fromJavaRDD(tupRdd);\n\n        JavaPairRDD<Integer, Integer> partitioned = pairRDD.partitionBy(new HashPartitioner(3));\n        System.out.println(\"============HashPartitioner==================\");\n        printPartRDD(partitioned);\n--------------打印--------------------\n============HashPartitioner==================\n(0,(3,5))\n(0,(3,6))\n(1,(1,2))\n(1,(4,8))\n(1,(4,7))\n(1,(1,1))\n(2,(5,10))\n(2,(2,4))\n(2,(2,3))\n(2,(5,9))\n```\n**java 自定义分区**\n**自定义分区器** ，key大于4的落在第一个分区，[2,4)之间的落在第二个分区，其余的落在第三个分区\n```\npublic class JavaCustomPart  extends Partitioner {\n    int i = 1;\n    public JavaCustomPart(int i){\n        this.i=i;\n    }\n    public JavaCustomPart(){}\n    @Override\n    public int numPartitions() {\n        return i;\n    }\n\n    @Override\n    public int getPartition(Object key) {\n        int keyCode = Integer.parseInt(key.toString());\n        if(keyCode>=4){\n            return 0;\n        }else if(keyCode>=2&&keyCode<4){\n            return 1;\n        }else {\n            return 2;\n        }\n    }\n}\n\n```\n**分区并打印**  \n```java\n        System.out.println(\"============CustomPartition==================\");\n        JavaPairRDD<Integer, Integer> customPart = pairRDD.partitionBy(new JavaCustomPart(3));\n        printPartRDD(customPart);\n--------------打印---------------\n============CustomPartition==================\n(0,(5,10))\n(0,(4,8))\n(0,(4,7))\n(0,(5,9))\n(1,(2,4))\n(1,(3,5))\n(1,(3,6))\n(1,(2,3))\n(2,(1,2))\n(2,(1,1))\n```  \n**java RangePartitioner** \nTODO","source":"_posts/spark RDD算子（十三）之RDD 分区 HashPartitioner，RangePartitioner，自定义分区.md","raw":"---\ntitle: spark RDD算子（十三）之RDD 分区 HashPartitioner，RangePartitioner，自定义分区\ndate: 2017-04-22 21:25:21\ntags: [spark]\ncategories: [大数据,spark]\nauthor: kaishun\nid: 48\npermalink: spark-rdd-13\n---\n\n**关键字**: spark分区方式，java HashPartitioner分区，scala HashPartitioner分区， java RangePartitioner 分区，scala RangePartitioner分区， java 自定义分区，scala自定义分区 \n#  **默认分区和HashPartitioner分区**   \n默认的分区就是HashPartition分区,默认分区不再介绍，下面介绍HashPartition的使用\n\n通过上一章 [mapPartitionsWithIndex的例子](http://blog.csdn.net/t1dmzks/article/details/71336119#t1)，我们可以构建一个方法，用来查看RDD的分区  \n```scala\n    def mapPartIndexFunc(i1:Int,iter: Iterator[(Int,Int)]):Iterator[(Int,(Int,Int))]={\n      var res = List[(Int,(Int,Int))]()\n      while(iter.hasNext){\n        var next = iter.next()\n        res=res.::(i1,next)\n      }\n      res.iterator\n    }\n    def printRDDPart(rdd:RDD[(Int,Int)]): Unit ={\n      var mapPartIndexRDDs = rdd.mapPartitionsWithIndex(mapPartIndexFunc)\n      mapPartIndexRDDs.foreach(println( _))\n    }\n```  \n**HashPartitioner分区  scala**  \n使用pairRdd.partitionBy(new spark.HashPartitioner(n)), 可以分为n个区\n```scala\n    var pairRdd = sc.parallelize(List((1,1), (1,2), (2,3), (2,4), (3,5), (3,6),(4,7), (4,8),(5,9), (5,10)))\n    //未分区的输出\n    printRDDPart(pairRdd)\n    println(\"=========================\")\n    val partitioned = pairRdd.partitionBy(new spark.HashPartitioner(3))\n    //分区后的输出\n    printRDDPart(partitioned)\n-----------输出------------\n(0,(5,10))\n(0,(5,9))\n(0,(4,8))\n(0,(4,7))\n(0,(3,6))\n(0,(3,5))\n(0,(2,4))\n(0,(2,3))\n(0,(1,2))\n(0,(1,1))\n=========================\n(0,(3,6))\n(0,(3,5))\n(1,(4,8))\n(1,(4,7))\n(1,(1,2))\n(1,(1,1))\n(2,(5,10))\n(2,(5,9))\n(2,(2,4))\n(2,(2,3))\n```  \n**HashPartitioner是如何分区的**： 国内很多说法都是有问题的，参考国外的一个说法 Uses Java’s Object.hashCodemethod to determine the partition as partition = key.hashCode() % numPartitions.  翻译过来就是使用java对象的hashCode来决定是哪个分区，对于piarRDD, 分区就是key.hashCode() % numPartitions, 3%3=0，所以 (3,6) 这个元素在0 分区， 4%3=1，所以元素(4,8) 在1 分区。 下面参考一张图  \n![spark分区](https://cdn.edureka.co/blog/wp-content/uploads/2016/03/11.png)  \n\n\n# **RangePartitioner**  \n我理解成范围分区器\n使用一个范围，将范围内的键分配给相应的分区。这种方法适用于键中有自然排序，键不为负。本文主要介绍如何使用，原理以后再仔细研究,以下代码片段显示了RangePartitioner的用法\n\n**RangePartitioner 分区 scala **\n```scala\n    var pairRdd = sc.parallelize(List((1,1), (5,10), (5,9), (2,4), (3,5), (3,6),(4,7), (4,8),(2,3), (1,2)))\n    printRDDPart(pairRdd)\n    println(\"=========================\")\n    val partitioned = pairRdd.partitionBy(new RangePartitioner(3,pairRdd))\n    printRDDPart(partitioned)\n  }\n-------------------输出------------------\n(0,(1,2))\n(0,(2,3))\n(0,(4,8))\n(0,(4,7))\n(0,(3,6))\n(0,(3,5))\n(0,(2,4))\n(0,(5,9))\n(0,(5,10))\n(0,(1,1))\n=========================\n(0,(1,2))\n(0,(2,3))\n(0,(2,4))\n(0,(1,1))\n(1,(4,8))\n(1,(4,7))\n(1,(3,6))\n(1,(3,5))\n(2,(5,9))\n(2,(5,10))\n\n```  \n上面的RDD生成的时候是乱的，但是我们让他分成三个范围，按照范围，key值为1,2的划分到第一个分区，key值为3，4的划分到第二个分区，key值为5的划分到第三个分区  \n\n\n\n# **自定义分区**  \n要实现自定义的分区器，你需要继承 org.apache.spark.Partitioner 类并实现下面三个方法  \n- **numPartitions: Int**：返回创建出来的分区数。  \n- **getPartition(key: Any): Int**：返回给定键的分区编号（ 0 到 numPartitions-1）。\n下面我自定义一个分区，让key大于等于4的落在第一个分区，key>=2并且key<4的落在第二个分区，其余的落在第一个分区。  \n**scala版本**\n自定义分区器\n```scala\nclass CustomPartitioner(numParts: Int) extends Partitioner{\n  override def numPartitions: Int = numParts\n\n  override def getPartition(key: Any): Int = {\n    if(key.toString.toInt>=4){\n       0\n    }else if(key.toString.toInt>=2&&key.toString.toInt<4){\n      1\n    }else{\n      2\n    }\n  }\n}\n```\n分区, 然后调用前面我们写的printRDDPart方法把各个分区中的RDD打印出来\n```scala\n    var pairRdd = sc.parallelize(List((1,1), (5,10), (5,9), (2,4), (3,5), (3,6),(4,7), (4,8),(2,3), (1,2)))\n    val partitionedRdd = pairRdd.partitionBy(new CustomPartitioner(3))\n    printRDDPart(partitionedRdd)\n----------输出-----------------\n(0,(4,8))\n(0,(4,7))\n(0,(5,9))\n(0,(5,10))\n(1,(2,3))\n(1,(3,6))\n(1,(3,5))\n(1,(2,4))\n(2,(1,2))\n(2,(1,1))\n```\n\n\n# ==**java 分区的用法**== \n\n同样，写个方法，该方法能打印RDD下的每个分区下的各个元素\n\n**打印每个分区下的各个元素的printPartRDD函数**\n```java\n    private static void printPartRDD(JavaPairRDD<Integer, Integer> pairRDD) {\n        JavaRDD<Tuple2<Integer, Tuple2<Integer, Integer>>> mapPartitionIndexRDD = pairRDD.mapPartitionsWithIndex(new Function2<Integer, Iterator<Tuple2<Integer, Integer>>, Iterator<Tuple2<Integer, Tuple2<Integer, Integer>>>>() {\n            @Override\n            public Iterator<Tuple2<Integer, Tuple2<Integer, Integer>>> call(Integer partIndex, Iterator<Tuple2<Integer, Integer>> tuple2Iterator) {\n                ArrayList<Tuple2<Integer, Tuple2<Integer, Integer>>> tuple2s = new ArrayList<>();\n\n                while (tuple2Iterator.hasNext()) {\n                    Tuple2<Integer, Integer> next = tuple2Iterator.next();\n                    tuple2s.add(new Tuple2<Integer, Tuple2<Integer, Integer>>(partIndex, next));\n                }\n                return tuple2s.iterator();\n            }\n        }, false);\n\n        mapPartitionIndexRDD.foreach(new VoidFunction<Tuple2<Integer, Tuple2<Integer, Integer>>>() {\n            @Override\n            public void call(Tuple2<Integer, Tuple2<Integer, Integer>> integerTuple2Tuple2) throws Exception {\n                System.out.println(integerTuple2Tuple2);\n            }\n        });\n    }\n```  \n**java HashPartitioner 分区**  \n\n```java\n\n\t\tJavaRDD<Tuple2<Integer, Integer>> tupRdd = sc.parallelize(Arrays.asList(new Tuple2<Integer, Integer>(1, 1), new Tuple2<Integer, Integer>(1, 2)\n                , new Tuple2<Integer, Integer>(2, 3), new Tuple2<Integer, Integer>(2, 4)\n                , new Tuple2<Integer, Integer>(3, 5), new Tuple2<Integer, Integer>(3, 6)\n                , new Tuple2<Integer, Integer>(4, 7), new Tuple2<Integer, Integer>(4, 8)\n                , new Tuple2<Integer, Integer>(5, 9), new Tuple2<Integer, Integer>(5, 10)\n        ), 3);\n        JavaPairRDD<Integer, Integer> pairRDD = JavaPairRDD.fromJavaRDD(tupRdd);\n\n        JavaPairRDD<Integer, Integer> partitioned = pairRDD.partitionBy(new HashPartitioner(3));\n        System.out.println(\"============HashPartitioner==================\");\n        printPartRDD(partitioned);\n--------------打印--------------------\n============HashPartitioner==================\n(0,(3,5))\n(0,(3,6))\n(1,(1,2))\n(1,(4,8))\n(1,(4,7))\n(1,(1,1))\n(2,(5,10))\n(2,(2,4))\n(2,(2,3))\n(2,(5,9))\n```\n**java 自定义分区**\n**自定义分区器** ，key大于4的落在第一个分区，[2,4)之间的落在第二个分区，其余的落在第三个分区\n```\npublic class JavaCustomPart  extends Partitioner {\n    int i = 1;\n    public JavaCustomPart(int i){\n        this.i=i;\n    }\n    public JavaCustomPart(){}\n    @Override\n    public int numPartitions() {\n        return i;\n    }\n\n    @Override\n    public int getPartition(Object key) {\n        int keyCode = Integer.parseInt(key.toString());\n        if(keyCode>=4){\n            return 0;\n        }else if(keyCode>=2&&keyCode<4){\n            return 1;\n        }else {\n            return 2;\n        }\n    }\n}\n\n```\n**分区并打印**  \n```java\n        System.out.println(\"============CustomPartition==================\");\n        JavaPairRDD<Integer, Integer> customPart = pairRDD.partitionBy(new JavaCustomPart(3));\n        printPartRDD(customPart);\n--------------打印---------------\n============CustomPartition==================\n(0,(5,10))\n(0,(4,8))\n(0,(4,7))\n(0,(5,9))\n(1,(2,4))\n(1,(3,5))\n(1,(3,6))\n(1,(2,3))\n(2,(1,2))\n(2,(1,1))\n```  \n**java RangePartitioner** \nTODO","slug":"spark-rdd-13","published":1,"updated":"2018-01-22T15:35:10.859Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcrpnzrk003n2wv3x3j5krox","content":"<p><strong>关键字</strong>: spark分区方式，java HashPartitioner分区，scala HashPartitioner分区， java RangePartitioner 分区，scala RangePartitioner分区， java 自定义分区，scala自定义分区 </p>\n<h1 id=\"默认分区和HashPartitioner分区\"><a href=\"#默认分区和HashPartitioner分区\" class=\"headerlink\" title=\"默认分区和HashPartitioner分区\"></a><strong>默认分区和HashPartitioner分区</strong></h1><p>默认的分区就是HashPartition分区,默认分区不再介绍，下面介绍HashPartition的使用</p>\n<p>通过上一章 <a href=\"http://blog.csdn.net/t1dmzks/article/details/71336119#t1\" target=\"_blank\" rel=\"external\">mapPartitionsWithIndex的例子</a>，我们可以构建一个方法，用来查看RDD的分区<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div><div class=\"line\">106</div><div class=\"line\">107</div><div class=\"line\">108</div><div class=\"line\">109</div><div class=\"line\">110</div><div class=\"line\">111</div></pre></td><td class=\"code\"><pre><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">mapPartIndexFunc</span></span>(i1:<span class=\"type\">Int</span>,iter: <span class=\"type\">Iterator</span>[(<span class=\"type\">Int</span>,<span class=\"type\">Int</span>)]):<span class=\"type\">Iterator</span>[(<span class=\"type\">Int</span>,(<span class=\"type\">Int</span>,<span class=\"type\">Int</span>))]=&#123;</div><div class=\"line\">      <span class=\"keyword\">var</span> res = <span class=\"type\">List</span>[(<span class=\"type\">Int</span>,(<span class=\"type\">Int</span>,<span class=\"type\">Int</span>))]()</div><div class=\"line\">      <span class=\"keyword\">while</span>(iter.hasNext)&#123;</div><div class=\"line\">        <span class=\"keyword\">var</span> next = iter.next()</div><div class=\"line\">        res=res.::(i1,next)</div><div class=\"line\">      &#125;</div><div class=\"line\">      res.iterator</div><div class=\"line\">    &#125;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">printRDDPart</span></span>(rdd:<span class=\"type\">RDD</span>[(<span class=\"type\">Int</span>,<span class=\"type\">Int</span>)]): <span class=\"type\">Unit</span> =&#123;</div><div class=\"line\">      <span class=\"keyword\">var</span> mapPartIndexRDDs = rdd.mapPartitionsWithIndex(mapPartIndexFunc)</div><div class=\"line\">      mapPartIndexRDDs.foreach(println( _))</div><div class=\"line\">    &#125;</div><div class=\"line\">```  </div><div class=\"line\">**<span class=\"type\">HashPartitioner</span>分区  scala**  </div><div class=\"line\">使用pairRdd.partitionBy(<span class=\"keyword\">new</span> spark.<span class=\"type\">HashPartitioner</span>(n)), 可以分为n个区</div><div class=\"line\">```scala</div><div class=\"line\">    <span class=\"keyword\">var</span> pairRdd = sc.parallelize(<span class=\"type\">List</span>((<span class=\"number\">1</span>,<span class=\"number\">1</span>), (<span class=\"number\">1</span>,<span class=\"number\">2</span>), (<span class=\"number\">2</span>,<span class=\"number\">3</span>), (<span class=\"number\">2</span>,<span class=\"number\">4</span>), (<span class=\"number\">3</span>,<span class=\"number\">5</span>), (<span class=\"number\">3</span>,<span class=\"number\">6</span>),(<span class=\"number\">4</span>,<span class=\"number\">7</span>), (<span class=\"number\">4</span>,<span class=\"number\">8</span>),(<span class=\"number\">5</span>,<span class=\"number\">9</span>), (<span class=\"number\">5</span>,<span class=\"number\">10</span>)))</div><div class=\"line\">    <span class=\"comment\">//未分区的输出</span></div><div class=\"line\">    printRDDPart(pairRdd)</div><div class=\"line\">    println(<span class=\"string\">\"=========================\"</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> partitioned = pairRdd.partitionBy(<span class=\"keyword\">new</span> spark.<span class=\"type\">HashPartitioner</span>(<span class=\"number\">3</span>))</div><div class=\"line\">    <span class=\"comment\">//分区后的输出</span></div><div class=\"line\">    printRDDPart(partitioned)</div><div class=\"line\">-----------输出------------</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">5</span>,<span class=\"number\">10</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">5</span>,<span class=\"number\">9</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">4</span>,<span class=\"number\">8</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">4</span>,<span class=\"number\">7</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">3</span>,<span class=\"number\">6</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">3</span>,<span class=\"number\">5</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">2</span>,<span class=\"number\">4</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">2</span>,<span class=\"number\">3</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">1</span>,<span class=\"number\">2</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">1</span>,<span class=\"number\">1</span>))</div><div class=\"line\">=========================</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">3</span>,<span class=\"number\">6</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">3</span>,<span class=\"number\">5</span>))</div><div class=\"line\">(<span class=\"number\">1</span>,(<span class=\"number\">4</span>,<span class=\"number\">8</span>))</div><div class=\"line\">(<span class=\"number\">1</span>,(<span class=\"number\">4</span>,<span class=\"number\">7</span>))</div><div class=\"line\">(<span class=\"number\">1</span>,(<span class=\"number\">1</span>,<span class=\"number\">2</span>))</div><div class=\"line\">(<span class=\"number\">1</span>,(<span class=\"number\">1</span>,<span class=\"number\">1</span>))</div><div class=\"line\">(<span class=\"number\">2</span>,(<span class=\"number\">5</span>,<span class=\"number\">10</span>))</div><div class=\"line\">(<span class=\"number\">2</span>,(<span class=\"number\">5</span>,<span class=\"number\">9</span>))</div><div class=\"line\">(<span class=\"number\">2</span>,(<span class=\"number\">2</span>,<span class=\"number\">4</span>))</div><div class=\"line\">(<span class=\"number\">2</span>,(<span class=\"number\">2</span>,<span class=\"number\">3</span>))</div><div class=\"line\">```  </div><div class=\"line\">**<span class=\"type\">HashPartitioner</span>是如何分区的**： 国内很多说法都是有问题的，参考国外的一个说法 <span class=\"type\">Uses</span> <span class=\"type\">Java</span>’s <span class=\"type\">Object</span>.hashCodemethod to determine the partition as partition = key.hashCode() % numPartitions.  翻译过来就是使用java对象的hashCode来决定是哪个分区，对于piarRDD, 分区就是key.hashCode() % numPartitions, <span class=\"number\">3</span>%<span class=\"number\">3</span>=<span class=\"number\">0</span>，所以 (<span class=\"number\">3</span>,<span class=\"number\">6</span>) 这个元素在<span class=\"number\">0</span> 分区， <span class=\"number\">4</span>%<span class=\"number\">3</span>=<span class=\"number\">1</span>，所以元素(<span class=\"number\">4</span>,<span class=\"number\">8</span>) 在<span class=\"number\">1</span> 分区。 下面参考一张图  </div><div class=\"line\">![spark分区](https:<span class=\"comment\">//cdn.edureka.co/blog/wp-content/uploads/2016/03/11.png)  </span></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"># **<span class=\"type\">RangePartitioner</span>**  </div><div class=\"line\">我理解成范围分区器</div><div class=\"line\">使用一个范围，将范围内的键分配给相应的分区。这种方法适用于键中有自然排序，键不为负。本文主要介绍如何使用，原理以后再仔细研究,以下代码片段显示了<span class=\"type\">RangePartitioner</span>的用法</div><div class=\"line\"></div><div class=\"line\">**<span class=\"type\">RangePartitioner</span> 分区 scala **</div><div class=\"line\">```scala</div><div class=\"line\">    <span class=\"keyword\">var</span> pairRdd = sc.parallelize(<span class=\"type\">List</span>((<span class=\"number\">1</span>,<span class=\"number\">1</span>), (<span class=\"number\">5</span>,<span class=\"number\">10</span>), (<span class=\"number\">5</span>,<span class=\"number\">9</span>), (<span class=\"number\">2</span>,<span class=\"number\">4</span>), (<span class=\"number\">3</span>,<span class=\"number\">5</span>), (<span class=\"number\">3</span>,<span class=\"number\">6</span>),(<span class=\"number\">4</span>,<span class=\"number\">7</span>), (<span class=\"number\">4</span>,<span class=\"number\">8</span>),(<span class=\"number\">2</span>,<span class=\"number\">3</span>), (<span class=\"number\">1</span>,<span class=\"number\">2</span>)))</div><div class=\"line\">    printRDDPart(pairRdd)</div><div class=\"line\">    println(<span class=\"string\">\"=========================\"</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> partitioned = pairRdd.partitionBy(<span class=\"keyword\">new</span> <span class=\"type\">RangePartitioner</span>(<span class=\"number\">3</span>,pairRdd))</div><div class=\"line\">    printRDDPart(partitioned)</div><div class=\"line\">  &#125;</div><div class=\"line\">-------------------输出------------------</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">1</span>,<span class=\"number\">2</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">2</span>,<span class=\"number\">3</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">4</span>,<span class=\"number\">8</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">4</span>,<span class=\"number\">7</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">3</span>,<span class=\"number\">6</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">3</span>,<span class=\"number\">5</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">2</span>,<span class=\"number\">4</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">5</span>,<span class=\"number\">9</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">5</span>,<span class=\"number\">10</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">1</span>,<span class=\"number\">1</span>))</div><div class=\"line\">=========================</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">1</span>,<span class=\"number\">2</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">2</span>,<span class=\"number\">3</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">2</span>,<span class=\"number\">4</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">1</span>,<span class=\"number\">1</span>))</div><div class=\"line\">(<span class=\"number\">1</span>,(<span class=\"number\">4</span>,<span class=\"number\">8</span>))</div><div class=\"line\">(<span class=\"number\">1</span>,(<span class=\"number\">4</span>,<span class=\"number\">7</span>))</div><div class=\"line\">(<span class=\"number\">1</span>,(<span class=\"number\">3</span>,<span class=\"number\">6</span>))</div><div class=\"line\">(<span class=\"number\">1</span>,(<span class=\"number\">3</span>,<span class=\"number\">5</span>))</div><div class=\"line\">(<span class=\"number\">2</span>,(<span class=\"number\">5</span>,<span class=\"number\">9</span>))</div><div class=\"line\">(<span class=\"number\">2</span>,(<span class=\"number\">5</span>,<span class=\"number\">10</span>))</div><div class=\"line\"></div><div class=\"line\">```  </div><div class=\"line\">上面的<span class=\"type\">RDD</span>生成的时候是乱的，但是我们让他分成三个范围，按照范围，key值为<span class=\"number\">1</span>,<span class=\"number\">2</span>的划分到第一个分区，key值为<span class=\"number\">3</span>，<span class=\"number\">4</span>的划分到第二个分区，key值为<span class=\"number\">5</span>的划分到第三个分区  </div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"># **自定义分区**  </div><div class=\"line\">要实现自定义的分区器，你需要继承 org.apache.spark.<span class=\"type\">Partitioner</span> 类并实现下面三个方法  </div><div class=\"line\">- **numPartitions: <span class=\"type\">Int</span>**：返回创建出来的分区数。  </div><div class=\"line\">- **getPartition(key: <span class=\"type\">Any</span>): <span class=\"type\">Int</span>**：返回给定键的分区编号（ <span class=\"number\">0</span> 到 numPartitions<span class=\"number\">-1</span>）。</div><div class=\"line\">下面我自定义一个分区，让key大于等于<span class=\"number\">4</span>的落在第一个分区，key&gt;=<span class=\"number\">2</span>并且key&lt;<span class=\"number\">4</span>的落在第二个分区，其余的落在第一个分区。  </div><div class=\"line\">**scala版本**</div><div class=\"line\">自定义分区器</div><div class=\"line\">```scala</div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">CustomPartitioner</span>(<span class=\"params\">numParts: <span class=\"type\">Int</span></span>) <span class=\"keyword\">extends</span> <span class=\"title\">Partitioner</span></span>&#123;</div><div class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">numPartitions</span></span>: <span class=\"type\">Int</span> = numParts</div><div class=\"line\"></div><div class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getPartition</span></span>(key: <span class=\"type\">Any</span>): <span class=\"type\">Int</span> = &#123;</div><div class=\"line\">    <span class=\"keyword\">if</span>(key.toString.toInt&gt;=<span class=\"number\">4</span>)&#123;</div><div class=\"line\">       <span class=\"number\">0</span></div><div class=\"line\">    &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(key.toString.toInt&gt;=<span class=\"number\">2</span>&amp;&amp;key.toString.toInt&lt;<span class=\"number\">4</span>)&#123;</div><div class=\"line\">      <span class=\"number\">1</span></div><div class=\"line\">    &#125;<span class=\"keyword\">else</span>&#123;</div><div class=\"line\">      <span class=\"number\">2</span></div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>分区, 然后调用前面我们写的printRDDPart方法把各个分区中的RDD打印出来<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\">    <span class=\"keyword\">var</span> pairRdd = sc.parallelize(<span class=\"type\">List</span>((<span class=\"number\">1</span>,<span class=\"number\">1</span>), (<span class=\"number\">5</span>,<span class=\"number\">10</span>), (<span class=\"number\">5</span>,<span class=\"number\">9</span>), (<span class=\"number\">2</span>,<span class=\"number\">4</span>), (<span class=\"number\">3</span>,<span class=\"number\">5</span>), (<span class=\"number\">3</span>,<span class=\"number\">6</span>),(<span class=\"number\">4</span>,<span class=\"number\">7</span>), (<span class=\"number\">4</span>,<span class=\"number\">8</span>),(<span class=\"number\">2</span>,<span class=\"number\">3</span>), (<span class=\"number\">1</span>,<span class=\"number\">2</span>)))</div><div class=\"line\">    <span class=\"keyword\">val</span> partitionedRdd = pairRdd.partitionBy(<span class=\"keyword\">new</span> <span class=\"type\">CustomPartitioner</span>(<span class=\"number\">3</span>))</div><div class=\"line\">    printRDDPart(partitionedRdd)</div><div class=\"line\">----------输出-----------------</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">4</span>,<span class=\"number\">8</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">4</span>,<span class=\"number\">7</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">5</span>,<span class=\"number\">9</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">5</span>,<span class=\"number\">10</span>))</div><div class=\"line\">(<span class=\"number\">1</span>,(<span class=\"number\">2</span>,<span class=\"number\">3</span>))</div><div class=\"line\">(<span class=\"number\">1</span>,(<span class=\"number\">3</span>,<span class=\"number\">6</span>))</div><div class=\"line\">(<span class=\"number\">1</span>,(<span class=\"number\">3</span>,<span class=\"number\">5</span>))</div><div class=\"line\">(<span class=\"number\">1</span>,(<span class=\"number\">2</span>,<span class=\"number\">4</span>))</div><div class=\"line\">(<span class=\"number\">2</span>,(<span class=\"number\">1</span>,<span class=\"number\">2</span>))</div><div class=\"line\">(<span class=\"number\">2</span>,(<span class=\"number\">1</span>,<span class=\"number\">1</span>))</div></pre></td></tr></table></figure></p>\n<h1 id=\"java-分区的用法\"><a href=\"#java-分区的用法\" class=\"headerlink\" title=\"==java 分区的用法==\"></a>==<strong>java 分区的用法</strong>==</h1><p>同样，写个方法，该方法能打印RDD下的每个分区下的各个元素</p>\n<p><strong>打印每个分区下的各个元素的printPartRDD函数</strong><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div></pre></td><td class=\"code\"><pre><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">printPartRDD</span><span class=\"params\">(JavaPairRDD&lt;Integer, Integer&gt; pairRDD)</span> </span>&#123;</div><div class=\"line\">        JavaRDD&lt;Tuple2&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt;&gt; mapPartitionIndexRDD = pairRDD.mapPartitionsWithIndex(<span class=\"keyword\">new</span> Function2&lt;Integer, Iterator&lt;Tuple2&lt;Integer, Integer&gt;&gt;, Iterator&lt;Tuple2&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt;&gt;&gt;() &#123;</div><div class=\"line\">            <span class=\"meta\">@Override</span></div><div class=\"line\">            <span class=\"keyword\">public</span> Iterator&lt;Tuple2&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt;&gt; call(Integer partIndex, Iterator&lt;Tuple2&lt;Integer, Integer&gt;&gt; tuple2Iterator) &#123;</div><div class=\"line\">                ArrayList&lt;Tuple2&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt;&gt; tuple2s = <span class=\"keyword\">new</span> ArrayList&lt;&gt;();</div><div class=\"line\"></div><div class=\"line\">                <span class=\"keyword\">while</span> (tuple2Iterator.hasNext()) &#123;</div><div class=\"line\">                    Tuple2&lt;Integer, Integer&gt; next = tuple2Iterator.next();</div><div class=\"line\">                    tuple2s.add(<span class=\"keyword\">new</span> Tuple2&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt;(partIndex, next));</div><div class=\"line\">                &#125;</div><div class=\"line\">                <span class=\"keyword\">return</span> tuple2s.iterator();</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;, <span class=\"keyword\">false</span>);</div><div class=\"line\"></div><div class=\"line\">        mapPartitionIndexRDD.foreach(<span class=\"keyword\">new</span> VoidFunction&lt;Tuple2&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt;&gt;() &#123;</div><div class=\"line\">            <span class=\"meta\">@Override</span></div><div class=\"line\">            <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">call</span><span class=\"params\">(Tuple2&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt; integerTuple2Tuple2)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">                System.out.println(integerTuple2Tuple2);</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;);</div><div class=\"line\">    &#125;</div><div class=\"line\">```  </div><div class=\"line\">**java HashPartitioner 分区**  </div><div class=\"line\"></div><div class=\"line\">```java</div><div class=\"line\"></div><div class=\"line\">\t\tJavaRDD&lt;Tuple2&lt;Integer, Integer&gt;&gt; tupRdd = sc.parallelize(Arrays.asList(<span class=\"keyword\">new</span> Tuple2&lt;Integer, Integer&gt;(<span class=\"number\">1</span>, <span class=\"number\">1</span>), <span class=\"keyword\">new</span> Tuple2&lt;Integer, Integer&gt;(<span class=\"number\">1</span>, <span class=\"number\">2</span>)</div><div class=\"line\">                , <span class=\"keyword\">new</span> Tuple2&lt;Integer, Integer&gt;(<span class=\"number\">2</span>, <span class=\"number\">3</span>), <span class=\"keyword\">new</span> Tuple2&lt;Integer, Integer&gt;(<span class=\"number\">2</span>, <span class=\"number\">4</span>)</div><div class=\"line\">                , <span class=\"keyword\">new</span> Tuple2&lt;Integer, Integer&gt;(<span class=\"number\">3</span>, <span class=\"number\">5</span>), <span class=\"keyword\">new</span> Tuple2&lt;Integer, Integer&gt;(<span class=\"number\">3</span>, <span class=\"number\">6</span>)</div><div class=\"line\">                , <span class=\"keyword\">new</span> Tuple2&lt;Integer, Integer&gt;(<span class=\"number\">4</span>, <span class=\"number\">7</span>), <span class=\"keyword\">new</span> Tuple2&lt;Integer, Integer&gt;(<span class=\"number\">4</span>, <span class=\"number\">8</span>)</div><div class=\"line\">                , <span class=\"keyword\">new</span> Tuple2&lt;Integer, Integer&gt;(<span class=\"number\">5</span>, <span class=\"number\">9</span>), <span class=\"keyword\">new</span> Tuple2&lt;Integer, Integer&gt;(<span class=\"number\">5</span>, <span class=\"number\">10</span>)</div><div class=\"line\">        ), <span class=\"number\">3</span>);</div><div class=\"line\">        JavaPairRDD&lt;Integer, Integer&gt; pairRDD = JavaPairRDD.fromJavaRDD(tupRdd);</div><div class=\"line\"></div><div class=\"line\">        JavaPairRDD&lt;Integer, Integer&gt; partitioned = pairRDD.partitionBy(<span class=\"keyword\">new</span> HashPartitioner(<span class=\"number\">3</span>));</div><div class=\"line\">        System.out.println(<span class=\"string\">\"============HashPartitioner==================\"</span>);</div><div class=\"line\">        printPartRDD(partitioned);</div><div class=\"line\">--------------打印--------------------</div><div class=\"line\">============HashPartitioner==================</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">3</span>,<span class=\"number\">5</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">3</span>,<span class=\"number\">6</span>))</div><div class=\"line\">(<span class=\"number\">1</span>,(<span class=\"number\">1</span>,<span class=\"number\">2</span>))</div><div class=\"line\">(<span class=\"number\">1</span>,(<span class=\"number\">4</span>,<span class=\"number\">8</span>))</div><div class=\"line\">(<span class=\"number\">1</span>,(<span class=\"number\">4</span>,<span class=\"number\">7</span>))</div><div class=\"line\">(<span class=\"number\">1</span>,(<span class=\"number\">1</span>,<span class=\"number\">1</span>))</div><div class=\"line\">(<span class=\"number\">2</span>,(<span class=\"number\">5</span>,<span class=\"number\">10</span>))</div><div class=\"line\">(<span class=\"number\">2</span>,(<span class=\"number\">2</span>,<span class=\"number\">4</span>))</div><div class=\"line\">(<span class=\"number\">2</span>,(<span class=\"number\">2</span>,<span class=\"number\">3</span>))</div><div class=\"line\">(<span class=\"number\">2</span>,(<span class=\"number\">5</span>,<span class=\"number\">9</span>))</div></pre></td></tr></table></figure></p>\n<p><strong>java 自定义分区</strong><br><strong>自定义分区器</strong> ，key大于4的落在第一个分区，[2,4)之间的落在第二个分区，其余的落在第三个分区<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div></pre></td><td class=\"code\"><pre><div class=\"line\">public class JavaCustomPart  extends Partitioner &#123;</div><div class=\"line\">    int i = 1;</div><div class=\"line\">    public JavaCustomPart(int i)&#123;</div><div class=\"line\">        this.i=i;</div><div class=\"line\">    &#125;</div><div class=\"line\">    public JavaCustomPart()&#123;&#125;</div><div class=\"line\">    @Override</div><div class=\"line\">    public int numPartitions() &#123;</div><div class=\"line\">        return i;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    @Override</div><div class=\"line\">    public int getPartition(Object key) &#123;</div><div class=\"line\">        int keyCode = Integer.parseInt(key.toString());</div><div class=\"line\">        if(keyCode&gt;=4)&#123;</div><div class=\"line\">            return 0;</div><div class=\"line\">        &#125;else if(keyCode&gt;=2&amp;&amp;keyCode&lt;4)&#123;</div><div class=\"line\">            return 1;</div><div class=\"line\">        &#125;else &#123;</div><div class=\"line\">            return 2;</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p><strong>分区并打印</strong>  </p>\n<pre><code class=\"java\">        System.out.println(<span class=\"string\">\"============CustomPartition==================\"</span>);\n        JavaPairRDD&lt;Integer, Integer&gt; customPart = pairRDD.partitionBy(<span class=\"keyword\">new</span> JavaCustomPart(<span class=\"number\">3</span>));\n        printPartRDD(customPart);\n--------------打印---------------\n============CustomPartition==================\n(<span class=\"number\">0</span>,(<span class=\"number\">5</span>,<span class=\"number\">10</span>))\n(<span class=\"number\">0</span>,(<span class=\"number\">4</span>,<span class=\"number\">8</span>))\n(<span class=\"number\">0</span>,(<span class=\"number\">4</span>,<span class=\"number\">7</span>))\n(<span class=\"number\">0</span>,(<span class=\"number\">5</span>,<span class=\"number\">9</span>))\n(<span class=\"number\">1</span>,(<span class=\"number\">2</span>,<span class=\"number\">4</span>))\n(<span class=\"number\">1</span>,(<span class=\"number\">3</span>,<span class=\"number\">5</span>))\n(<span class=\"number\">1</span>,(<span class=\"number\">3</span>,<span class=\"number\">6</span>))\n(<span class=\"number\">1</span>,(<span class=\"number\">2</span>,<span class=\"number\">3</span>))\n(<span class=\"number\">2</span>,(<span class=\"number\">1</span>,<span class=\"number\">2</span>))\n(<span class=\"number\">2</span>,(<span class=\"number\">1</span>,<span class=\"number\">1</span>))\n</code></pre>\n<p><strong>java RangePartitioner</strong><br>TODO</p>\n","site":{"data":{}},"excerpt":"","more":"<p><strong>关键字</strong>: spark分区方式，java HashPartitioner分区，scala HashPartitioner分区， java RangePartitioner 分区，scala RangePartitioner分区， java 自定义分区，scala自定义分区 </p>\n<h1 id=\"默认分区和HashPartitioner分区\"><a href=\"#默认分区和HashPartitioner分区\" class=\"headerlink\" title=\"默认分区和HashPartitioner分区\"></a><strong>默认分区和HashPartitioner分区</strong></h1><p>默认的分区就是HashPartition分区,默认分区不再介绍，下面介绍HashPartition的使用</p>\n<p>通过上一章 <a href=\"http://blog.csdn.net/t1dmzks/article/details/71336119#t1\" target=\"_blank\" rel=\"external\">mapPartitionsWithIndex的例子</a>，我们可以构建一个方法，用来查看RDD的分区<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div><div class=\"line\">106</div><div class=\"line\">107</div><div class=\"line\">108</div><div class=\"line\">109</div><div class=\"line\">110</div><div class=\"line\">111</div></pre></td><td class=\"code\"><pre><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">mapPartIndexFunc</span></span>(i1:<span class=\"type\">Int</span>,iter: <span class=\"type\">Iterator</span>[(<span class=\"type\">Int</span>,<span class=\"type\">Int</span>)]):<span class=\"type\">Iterator</span>[(<span class=\"type\">Int</span>,(<span class=\"type\">Int</span>,<span class=\"type\">Int</span>))]=&#123;</div><div class=\"line\">      <span class=\"keyword\">var</span> res = <span class=\"type\">List</span>[(<span class=\"type\">Int</span>,(<span class=\"type\">Int</span>,<span class=\"type\">Int</span>))]()</div><div class=\"line\">      <span class=\"keyword\">while</span>(iter.hasNext)&#123;</div><div class=\"line\">        <span class=\"keyword\">var</span> next = iter.next()</div><div class=\"line\">        res=res.::(i1,next)</div><div class=\"line\">      &#125;</div><div class=\"line\">      res.iterator</div><div class=\"line\">    &#125;</div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">printRDDPart</span></span>(rdd:<span class=\"type\">RDD</span>[(<span class=\"type\">Int</span>,<span class=\"type\">Int</span>)]): <span class=\"type\">Unit</span> =&#123;</div><div class=\"line\">      <span class=\"keyword\">var</span> mapPartIndexRDDs = rdd.mapPartitionsWithIndex(mapPartIndexFunc)</div><div class=\"line\">      mapPartIndexRDDs.foreach(println( _))</div><div class=\"line\">    &#125;</div><div class=\"line\">```  </div><div class=\"line\">**<span class=\"type\">HashPartitioner</span>分区  scala**  </div><div class=\"line\">使用pairRdd.partitionBy(<span class=\"keyword\">new</span> spark.<span class=\"type\">HashPartitioner</span>(n)), 可以分为n个区</div><div class=\"line\">```scala</div><div class=\"line\">    <span class=\"keyword\">var</span> pairRdd = sc.parallelize(<span class=\"type\">List</span>((<span class=\"number\">1</span>,<span class=\"number\">1</span>), (<span class=\"number\">1</span>,<span class=\"number\">2</span>), (<span class=\"number\">2</span>,<span class=\"number\">3</span>), (<span class=\"number\">2</span>,<span class=\"number\">4</span>), (<span class=\"number\">3</span>,<span class=\"number\">5</span>), (<span class=\"number\">3</span>,<span class=\"number\">6</span>),(<span class=\"number\">4</span>,<span class=\"number\">7</span>), (<span class=\"number\">4</span>,<span class=\"number\">8</span>),(<span class=\"number\">5</span>,<span class=\"number\">9</span>), (<span class=\"number\">5</span>,<span class=\"number\">10</span>)))</div><div class=\"line\">    <span class=\"comment\">//未分区的输出</span></div><div class=\"line\">    printRDDPart(pairRdd)</div><div class=\"line\">    println(<span class=\"string\">\"=========================\"</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> partitioned = pairRdd.partitionBy(<span class=\"keyword\">new</span> spark.<span class=\"type\">HashPartitioner</span>(<span class=\"number\">3</span>))</div><div class=\"line\">    <span class=\"comment\">//分区后的输出</span></div><div class=\"line\">    printRDDPart(partitioned)</div><div class=\"line\">-----------输出------------</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">5</span>,<span class=\"number\">10</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">5</span>,<span class=\"number\">9</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">4</span>,<span class=\"number\">8</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">4</span>,<span class=\"number\">7</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">3</span>,<span class=\"number\">6</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">3</span>,<span class=\"number\">5</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">2</span>,<span class=\"number\">4</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">2</span>,<span class=\"number\">3</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">1</span>,<span class=\"number\">2</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">1</span>,<span class=\"number\">1</span>))</div><div class=\"line\">=========================</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">3</span>,<span class=\"number\">6</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">3</span>,<span class=\"number\">5</span>))</div><div class=\"line\">(<span class=\"number\">1</span>,(<span class=\"number\">4</span>,<span class=\"number\">8</span>))</div><div class=\"line\">(<span class=\"number\">1</span>,(<span class=\"number\">4</span>,<span class=\"number\">7</span>))</div><div class=\"line\">(<span class=\"number\">1</span>,(<span class=\"number\">1</span>,<span class=\"number\">2</span>))</div><div class=\"line\">(<span class=\"number\">1</span>,(<span class=\"number\">1</span>,<span class=\"number\">1</span>))</div><div class=\"line\">(<span class=\"number\">2</span>,(<span class=\"number\">5</span>,<span class=\"number\">10</span>))</div><div class=\"line\">(<span class=\"number\">2</span>,(<span class=\"number\">5</span>,<span class=\"number\">9</span>))</div><div class=\"line\">(<span class=\"number\">2</span>,(<span class=\"number\">2</span>,<span class=\"number\">4</span>))</div><div class=\"line\">(<span class=\"number\">2</span>,(<span class=\"number\">2</span>,<span class=\"number\">3</span>))</div><div class=\"line\">```  </div><div class=\"line\">**<span class=\"type\">HashPartitioner</span>是如何分区的**： 国内很多说法都是有问题的，参考国外的一个说法 <span class=\"type\">Uses</span> <span class=\"type\">Java</span>’s <span class=\"type\">Object</span>.hashCodemethod to determine the partition as partition = key.hashCode() % numPartitions.  翻译过来就是使用java对象的hashCode来决定是哪个分区，对于piarRDD, 分区就是key.hashCode() % numPartitions, <span class=\"number\">3</span>%<span class=\"number\">3</span>=<span class=\"number\">0</span>，所以 (<span class=\"number\">3</span>,<span class=\"number\">6</span>) 这个元素在<span class=\"number\">0</span> 分区， <span class=\"number\">4</span>%<span class=\"number\">3</span>=<span class=\"number\">1</span>，所以元素(<span class=\"number\">4</span>,<span class=\"number\">8</span>) 在<span class=\"number\">1</span> 分区。 下面参考一张图  </div><div class=\"line\">![spark分区](https:<span class=\"comment\">//cdn.edureka.co/blog/wp-content/uploads/2016/03/11.png)  </span></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"># **<span class=\"type\">RangePartitioner</span>**  </div><div class=\"line\">我理解成范围分区器</div><div class=\"line\">使用一个范围，将范围内的键分配给相应的分区。这种方法适用于键中有自然排序，键不为负。本文主要介绍如何使用，原理以后再仔细研究,以下代码片段显示了<span class=\"type\">RangePartitioner</span>的用法</div><div class=\"line\"></div><div class=\"line\">**<span class=\"type\">RangePartitioner</span> 分区 scala **</div><div class=\"line\">```scala</div><div class=\"line\">    <span class=\"keyword\">var</span> pairRdd = sc.parallelize(<span class=\"type\">List</span>((<span class=\"number\">1</span>,<span class=\"number\">1</span>), (<span class=\"number\">5</span>,<span class=\"number\">10</span>), (<span class=\"number\">5</span>,<span class=\"number\">9</span>), (<span class=\"number\">2</span>,<span class=\"number\">4</span>), (<span class=\"number\">3</span>,<span class=\"number\">5</span>), (<span class=\"number\">3</span>,<span class=\"number\">6</span>),(<span class=\"number\">4</span>,<span class=\"number\">7</span>), (<span class=\"number\">4</span>,<span class=\"number\">8</span>),(<span class=\"number\">2</span>,<span class=\"number\">3</span>), (<span class=\"number\">1</span>,<span class=\"number\">2</span>)))</div><div class=\"line\">    printRDDPart(pairRdd)</div><div class=\"line\">    println(<span class=\"string\">\"=========================\"</span>)</div><div class=\"line\">    <span class=\"keyword\">val</span> partitioned = pairRdd.partitionBy(<span class=\"keyword\">new</span> <span class=\"type\">RangePartitioner</span>(<span class=\"number\">3</span>,pairRdd))</div><div class=\"line\">    printRDDPart(partitioned)</div><div class=\"line\">  &#125;</div><div class=\"line\">-------------------输出------------------</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">1</span>,<span class=\"number\">2</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">2</span>,<span class=\"number\">3</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">4</span>,<span class=\"number\">8</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">4</span>,<span class=\"number\">7</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">3</span>,<span class=\"number\">6</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">3</span>,<span class=\"number\">5</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">2</span>,<span class=\"number\">4</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">5</span>,<span class=\"number\">9</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">5</span>,<span class=\"number\">10</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">1</span>,<span class=\"number\">1</span>))</div><div class=\"line\">=========================</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">1</span>,<span class=\"number\">2</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">2</span>,<span class=\"number\">3</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">2</span>,<span class=\"number\">4</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">1</span>,<span class=\"number\">1</span>))</div><div class=\"line\">(<span class=\"number\">1</span>,(<span class=\"number\">4</span>,<span class=\"number\">8</span>))</div><div class=\"line\">(<span class=\"number\">1</span>,(<span class=\"number\">4</span>,<span class=\"number\">7</span>))</div><div class=\"line\">(<span class=\"number\">1</span>,(<span class=\"number\">3</span>,<span class=\"number\">6</span>))</div><div class=\"line\">(<span class=\"number\">1</span>,(<span class=\"number\">3</span>,<span class=\"number\">5</span>))</div><div class=\"line\">(<span class=\"number\">2</span>,(<span class=\"number\">5</span>,<span class=\"number\">9</span>))</div><div class=\"line\">(<span class=\"number\">2</span>,(<span class=\"number\">5</span>,<span class=\"number\">10</span>))</div><div class=\"line\"></div><div class=\"line\">```  </div><div class=\"line\">上面的<span class=\"type\">RDD</span>生成的时候是乱的，但是我们让他分成三个范围，按照范围，key值为<span class=\"number\">1</span>,<span class=\"number\">2</span>的划分到第一个分区，key值为<span class=\"number\">3</span>，<span class=\"number\">4</span>的划分到第二个分区，key值为<span class=\"number\">5</span>的划分到第三个分区  </div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"># **自定义分区**  </div><div class=\"line\">要实现自定义的分区器，你需要继承 org.apache.spark.<span class=\"type\">Partitioner</span> 类并实现下面三个方法  </div><div class=\"line\">- **numPartitions: <span class=\"type\">Int</span>**：返回创建出来的分区数。  </div><div class=\"line\">- **getPartition(key: <span class=\"type\">Any</span>): <span class=\"type\">Int</span>**：返回给定键的分区编号（ <span class=\"number\">0</span> 到 numPartitions<span class=\"number\">-1</span>）。</div><div class=\"line\">下面我自定义一个分区，让key大于等于<span class=\"number\">4</span>的落在第一个分区，key&gt;=<span class=\"number\">2</span>并且key&lt;<span class=\"number\">4</span>的落在第二个分区，其余的落在第一个分区。  </div><div class=\"line\">**scala版本**</div><div class=\"line\">自定义分区器</div><div class=\"line\">```scala</div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">CustomPartitioner</span>(<span class=\"params\">numParts: <span class=\"type\">Int</span></span>) <span class=\"keyword\">extends</span> <span class=\"title\">Partitioner</span></span>&#123;</div><div class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">numPartitions</span></span>: <span class=\"type\">Int</span> = numParts</div><div class=\"line\"></div><div class=\"line\">  <span class=\"keyword\">override</span> <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">getPartition</span></span>(key: <span class=\"type\">Any</span>): <span class=\"type\">Int</span> = &#123;</div><div class=\"line\">    <span class=\"keyword\">if</span>(key.toString.toInt&gt;=<span class=\"number\">4</span>)&#123;</div><div class=\"line\">       <span class=\"number\">0</span></div><div class=\"line\">    &#125;<span class=\"keyword\">else</span> <span class=\"keyword\">if</span>(key.toString.toInt&gt;=<span class=\"number\">2</span>&amp;&amp;key.toString.toInt&lt;<span class=\"number\">4</span>)&#123;</div><div class=\"line\">      <span class=\"number\">1</span></div><div class=\"line\">    &#125;<span class=\"keyword\">else</span>&#123;</div><div class=\"line\">      <span class=\"number\">2</span></div><div class=\"line\">    &#125;</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>分区, 然后调用前面我们写的printRDDPart方法把各个分区中的RDD打印出来<br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div></pre></td><td class=\"code\"><pre><div class=\"line\">    <span class=\"keyword\">var</span> pairRdd = sc.parallelize(<span class=\"type\">List</span>((<span class=\"number\">1</span>,<span class=\"number\">1</span>), (<span class=\"number\">5</span>,<span class=\"number\">10</span>), (<span class=\"number\">5</span>,<span class=\"number\">9</span>), (<span class=\"number\">2</span>,<span class=\"number\">4</span>), (<span class=\"number\">3</span>,<span class=\"number\">5</span>), (<span class=\"number\">3</span>,<span class=\"number\">6</span>),(<span class=\"number\">4</span>,<span class=\"number\">7</span>), (<span class=\"number\">4</span>,<span class=\"number\">8</span>),(<span class=\"number\">2</span>,<span class=\"number\">3</span>), (<span class=\"number\">1</span>,<span class=\"number\">2</span>)))</div><div class=\"line\">    <span class=\"keyword\">val</span> partitionedRdd = pairRdd.partitionBy(<span class=\"keyword\">new</span> <span class=\"type\">CustomPartitioner</span>(<span class=\"number\">3</span>))</div><div class=\"line\">    printRDDPart(partitionedRdd)</div><div class=\"line\">----------输出-----------------</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">4</span>,<span class=\"number\">8</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">4</span>,<span class=\"number\">7</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">5</span>,<span class=\"number\">9</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">5</span>,<span class=\"number\">10</span>))</div><div class=\"line\">(<span class=\"number\">1</span>,(<span class=\"number\">2</span>,<span class=\"number\">3</span>))</div><div class=\"line\">(<span class=\"number\">1</span>,(<span class=\"number\">3</span>,<span class=\"number\">6</span>))</div><div class=\"line\">(<span class=\"number\">1</span>,(<span class=\"number\">3</span>,<span class=\"number\">5</span>))</div><div class=\"line\">(<span class=\"number\">1</span>,(<span class=\"number\">2</span>,<span class=\"number\">4</span>))</div><div class=\"line\">(<span class=\"number\">2</span>,(<span class=\"number\">1</span>,<span class=\"number\">2</span>))</div><div class=\"line\">(<span class=\"number\">2</span>,(<span class=\"number\">1</span>,<span class=\"number\">1</span>))</div></pre></td></tr></table></figure></p>\n<h1 id=\"java-分区的用法\"><a href=\"#java-分区的用法\" class=\"headerlink\" title=\"==java 分区的用法==\"></a>==<strong>java 分区的用法</strong>==</h1><p>同样，写个方法，该方法能打印RDD下的每个分区下的各个元素</p>\n<p><strong>打印每个分区下的各个元素的printPartRDD函数</strong><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div></pre></td><td class=\"code\"><pre><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">printPartRDD</span><span class=\"params\">(JavaPairRDD&lt;Integer, Integer&gt; pairRDD)</span> </span>&#123;</div><div class=\"line\">        JavaRDD&lt;Tuple2&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt;&gt; mapPartitionIndexRDD = pairRDD.mapPartitionsWithIndex(<span class=\"keyword\">new</span> Function2&lt;Integer, Iterator&lt;Tuple2&lt;Integer, Integer&gt;&gt;, Iterator&lt;Tuple2&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt;&gt;&gt;() &#123;</div><div class=\"line\">            <span class=\"meta\">@Override</span></div><div class=\"line\">            <span class=\"keyword\">public</span> Iterator&lt;Tuple2&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt;&gt; call(Integer partIndex, Iterator&lt;Tuple2&lt;Integer, Integer&gt;&gt; tuple2Iterator) &#123;</div><div class=\"line\">                ArrayList&lt;Tuple2&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt;&gt; tuple2s = <span class=\"keyword\">new</span> ArrayList&lt;&gt;();</div><div class=\"line\"></div><div class=\"line\">                <span class=\"keyword\">while</span> (tuple2Iterator.hasNext()) &#123;</div><div class=\"line\">                    Tuple2&lt;Integer, Integer&gt; next = tuple2Iterator.next();</div><div class=\"line\">                    tuple2s.add(<span class=\"keyword\">new</span> Tuple2&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt;(partIndex, next));</div><div class=\"line\">                &#125;</div><div class=\"line\">                <span class=\"keyword\">return</span> tuple2s.iterator();</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;, <span class=\"keyword\">false</span>);</div><div class=\"line\"></div><div class=\"line\">        mapPartitionIndexRDD.foreach(<span class=\"keyword\">new</span> VoidFunction&lt;Tuple2&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt;&gt;() &#123;</div><div class=\"line\">            <span class=\"meta\">@Override</span></div><div class=\"line\">            <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">call</span><span class=\"params\">(Tuple2&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt; integerTuple2Tuple2)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">                System.out.println(integerTuple2Tuple2);</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;);</div><div class=\"line\">    &#125;</div><div class=\"line\">```  </div><div class=\"line\">**java HashPartitioner 分区**  </div><div class=\"line\"></div><div class=\"line\">```java</div><div class=\"line\"></div><div class=\"line\">\t\tJavaRDD&lt;Tuple2&lt;Integer, Integer&gt;&gt; tupRdd = sc.parallelize(Arrays.asList(<span class=\"keyword\">new</span> Tuple2&lt;Integer, Integer&gt;(<span class=\"number\">1</span>, <span class=\"number\">1</span>), <span class=\"keyword\">new</span> Tuple2&lt;Integer, Integer&gt;(<span class=\"number\">1</span>, <span class=\"number\">2</span>)</div><div class=\"line\">                , <span class=\"keyword\">new</span> Tuple2&lt;Integer, Integer&gt;(<span class=\"number\">2</span>, <span class=\"number\">3</span>), <span class=\"keyword\">new</span> Tuple2&lt;Integer, Integer&gt;(<span class=\"number\">2</span>, <span class=\"number\">4</span>)</div><div class=\"line\">                , <span class=\"keyword\">new</span> Tuple2&lt;Integer, Integer&gt;(<span class=\"number\">3</span>, <span class=\"number\">5</span>), <span class=\"keyword\">new</span> Tuple2&lt;Integer, Integer&gt;(<span class=\"number\">3</span>, <span class=\"number\">6</span>)</div><div class=\"line\">                , <span class=\"keyword\">new</span> Tuple2&lt;Integer, Integer&gt;(<span class=\"number\">4</span>, <span class=\"number\">7</span>), <span class=\"keyword\">new</span> Tuple2&lt;Integer, Integer&gt;(<span class=\"number\">4</span>, <span class=\"number\">8</span>)</div><div class=\"line\">                , <span class=\"keyword\">new</span> Tuple2&lt;Integer, Integer&gt;(<span class=\"number\">5</span>, <span class=\"number\">9</span>), <span class=\"keyword\">new</span> Tuple2&lt;Integer, Integer&gt;(<span class=\"number\">5</span>, <span class=\"number\">10</span>)</div><div class=\"line\">        ), <span class=\"number\">3</span>);</div><div class=\"line\">        JavaPairRDD&lt;Integer, Integer&gt; pairRDD = JavaPairRDD.fromJavaRDD(tupRdd);</div><div class=\"line\"></div><div class=\"line\">        JavaPairRDD&lt;Integer, Integer&gt; partitioned = pairRDD.partitionBy(<span class=\"keyword\">new</span> HashPartitioner(<span class=\"number\">3</span>));</div><div class=\"line\">        System.out.println(<span class=\"string\">\"============HashPartitioner==================\"</span>);</div><div class=\"line\">        printPartRDD(partitioned);</div><div class=\"line\">--------------打印--------------------</div><div class=\"line\">============HashPartitioner==================</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">3</span>,<span class=\"number\">5</span>))</div><div class=\"line\">(<span class=\"number\">0</span>,(<span class=\"number\">3</span>,<span class=\"number\">6</span>))</div><div class=\"line\">(<span class=\"number\">1</span>,(<span class=\"number\">1</span>,<span class=\"number\">2</span>))</div><div class=\"line\">(<span class=\"number\">1</span>,(<span class=\"number\">4</span>,<span class=\"number\">8</span>))</div><div class=\"line\">(<span class=\"number\">1</span>,(<span class=\"number\">4</span>,<span class=\"number\">7</span>))</div><div class=\"line\">(<span class=\"number\">1</span>,(<span class=\"number\">1</span>,<span class=\"number\">1</span>))</div><div class=\"line\">(<span class=\"number\">2</span>,(<span class=\"number\">5</span>,<span class=\"number\">10</span>))</div><div class=\"line\">(<span class=\"number\">2</span>,(<span class=\"number\">2</span>,<span class=\"number\">4</span>))</div><div class=\"line\">(<span class=\"number\">2</span>,(<span class=\"number\">2</span>,<span class=\"number\">3</span>))</div><div class=\"line\">(<span class=\"number\">2</span>,(<span class=\"number\">5</span>,<span class=\"number\">9</span>))</div></pre></td></tr></table></figure></p>\n<p><strong>java 自定义分区</strong><br><strong>自定义分区器</strong> ，key大于4的落在第一个分区，[2,4)之间的落在第二个分区，其余的落在第三个分区<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div></pre></td><td class=\"code\"><pre><div class=\"line\">public class JavaCustomPart  extends Partitioner &#123;</div><div class=\"line\">    int i = 1;</div><div class=\"line\">    public JavaCustomPart(int i)&#123;</div><div class=\"line\">        this.i=i;</div><div class=\"line\">    &#125;</div><div class=\"line\">    public JavaCustomPart()&#123;&#125;</div><div class=\"line\">    @Override</div><div class=\"line\">    public int numPartitions() &#123;</div><div class=\"line\">        return i;</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">    @Override</div><div class=\"line\">    public int getPartition(Object key) &#123;</div><div class=\"line\">        int keyCode = Integer.parseInt(key.toString());</div><div class=\"line\">        if(keyCode&gt;=4)&#123;</div><div class=\"line\">            return 0;</div><div class=\"line\">        &#125;else if(keyCode&gt;=2&amp;&amp;keyCode&lt;4)&#123;</div><div class=\"line\">            return 1;</div><div class=\"line\">        &#125;else &#123;</div><div class=\"line\">            return 2;</div><div class=\"line\">        &#125;</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p><strong>分区并打印</strong>  </p>\n<pre><code class=\"java\">        System.out.println(<span class=\"string\">\"============CustomPartition==================\"</span>);\n        JavaPairRDD&lt;Integer, Integer&gt; customPart = pairRDD.partitionBy(<span class=\"keyword\">new</span> JavaCustomPart(<span class=\"number\">3</span>));\n        printPartRDD(customPart);\n--------------打印---------------\n============CustomPartition==================\n(<span class=\"number\">0</span>,(<span class=\"number\">5</span>,<span class=\"number\">10</span>))\n(<span class=\"number\">0</span>,(<span class=\"number\">4</span>,<span class=\"number\">8</span>))\n(<span class=\"number\">0</span>,(<span class=\"number\">4</span>,<span class=\"number\">7</span>))\n(<span class=\"number\">0</span>,(<span class=\"number\">5</span>,<span class=\"number\">9</span>))\n(<span class=\"number\">1</span>,(<span class=\"number\">2</span>,<span class=\"number\">4</span>))\n(<span class=\"number\">1</span>,(<span class=\"number\">3</span>,<span class=\"number\">5</span>))\n(<span class=\"number\">1</span>,(<span class=\"number\">3</span>,<span class=\"number\">6</span>))\n(<span class=\"number\">1</span>,(<span class=\"number\">2</span>,<span class=\"number\">3</span>))\n(<span class=\"number\">2</span>,(<span class=\"number\">1</span>,<span class=\"number\">2</span>))\n(<span class=\"number\">2</span>,(<span class=\"number\">1</span>,<span class=\"number\">1</span>))\n</code></pre>\n<p><strong>java RangePartitioner</strong><br>TODO</p>\n"},{"title":"spark RDD算子（十二）之RDD 分区操作上mapPartitions, mapPartitionsWithIndex保存操作saveAsTextFile,saveAsSequenceFile,saveAsObjectFile,saveAsHadoopFile 等","date":"2017-04-18T13:25:21.000Z","author":"kaishun","id":"46","_content":"\n# **mapPartitions** \nmapPartition可以倒过来理解，先partition，再把每个partition进行map函数，\n**适用场景**   \n如果在映射的过程中需要频繁创建额外的对象，使用mapPartitions要比map高效的过。\n\n比如，将RDD中的所有数据通过JDBC连接写入数据库，如果使用map函数，可能要为每一个元素都创建一个connection，这样开销很大，如果使用mapPartitions，那么只需要针对每一个分区建立一个connection。  \n下面的例子，**把每一个元素平方**\n**java 每一个元素平方**\n```java\n        JavaRDD<Integer> rdd = sc.parallelize(\n                Arrays.asList(1,2,3,4,5,6,7,8,9,10));\n        JavaRDD<Integer> mapPartitionRDD = rdd.mapPartitions(new FlatMapFunction<Iterator<Integer>, Integer>() {\n            @Override\n            public Iterable<Integer> call(Iterator<Integer> it) throws Exception {\n                ArrayList<Integer> results = new ArrayList<>();\n                while (it.hasNext()) {\n                    int i = it.next();\n                    results.add(i*i);\n                }\n                return results;\n            }\n        });\n        mapPartitionRDD.foreach(new VoidFunction<Integer>() {\n            @Override\n            public void call(Integer integer) throws Exception {\n                System.out.println(integer);\n            }\n        });\n----------输出-------------\n1\n4\n9\n16\n25\n36\n49\n64\n81\n100\n```  \n**把每一个数字i变成一个map(i,i*i)的形式**\n\n**java，把每一个元素变成map(i,i*i)**\n```java\n        JavaRDD<Tuple2<Integer, Integer>> tuple2JavaRDD = rdd.mapPartitions(new FlatMapFunction<Iterator<Integer>, Tuple2<Integer, Integer>>() {\n            @Override\n            public Iterable<Tuple2<Integer, Integer>> call(Iterator<Integer> it) throws Exception {\n                ArrayList<Tuple2<Integer, Integer>> tuple2s = new ArrayList<>();\n                while (it.hasNext()) {\n                    Integer next = it.next();\n                    tuple2s.add(new Tuple2<Integer, Integer>(next, next * next));\n                }\n                return tuple2s;\n            }\n        });\n        tuple2JavaRDD.foreach(new VoidFunction<Tuple2<Integer, Integer>>() {\n            @Override\n            public void call(Tuple2<Integer, Integer> tp2) throws Exception {\n                System.out.println(tp2);\n            }\n        });\n---------输出---------------\n(1,1)\n(2,4)\n(3,9)\n(4,16)\n(5,25)\n(6,36)\n(7,49)\n(8,64)\n(9,81)\n(10,100)\n```\n**scala 把每一个元素变成map(i,i*i)**  \n```\n    val rdd = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10),3)\n    def mapPartFunc(iter: Iterator[Int]):Iterator[(Int,Int)]={\n      var res = List[(Int,Int)]()\n      while (iter.hasNext){\n        val cur = iter.next\n        res=res.::(cur,cur*cur)\n      }\n       res.iterator\n    }\n    val mapPartRDD = rdd.mapPartitions(mapPartFunc)\n    mapPartRDD.foreach(maps=>println(maps))\n----------输出-----------\n(3,9)\n(2,4)\n(1,1)\n(6,36)\n(5,25)\n(4,16)\n(10,100)\n(9,81)\n(8,64)\n(7,49)\n``` \n**mapPartitions操作键值对 把(i,j) 变成(i,j*j)**\n**scala版本**\n```scala\n    var rdd = sc.parallelize(List((1,1), (1,2), (1,3), (2,1), (2,2), (2,3)))\n\n    def mapPartFunc(iter: Iterator[(Int,Int)]):Iterator[(Int,Int)]={\n      var res = List[(Int,Int)]()\n      while (iter.hasNext){\n        val cur = iter.next\n        res=res.::(cur._1,cur._2*cur._2)\n      }\n      res.iterator\n    }\n    val mapPartionsRDD = rdd.mapPartitions(mapPartFunc)\n    mapPartionsRDD.foreach(println( _))\n```  \n**java版本**\n```java\n        JavaRDD<Tuple2<Integer, Integer>> rdd = sc.parallelize(Arrays.asList(new Tuple2<Integer, Integer>(1, 1), new Tuple2<Integer, Integer>(1, 2)\n                , new Tuple2<Integer, Integer>(1, 3), new Tuple2<Integer, Integer>(2, 1)\n                , new Tuple2<Integer, Integer>(2, 2), new Tuple2<Integer, Integer>(2, 3)), 3);\n        JavaPairRDD<Integer, Integer> pairRDD = JavaPairRDD.fromJavaRDD(rdd);\n        JavaRDD<Tuple2<Integer, Integer>> tuple2JavaRDD = pairRDD.mapPartitions(new FlatMapFunction<Iterator<Tuple2<Integer, Integer>>, Tuple2<Integer, Integer>>() {\n            @Override\n            public Iterable<Tuple2<Integer, Integer>> call(Iterator<Tuple2<Integer, Integer>> tp2It) throws Exception {\n                ArrayList<Tuple2<Integer, Integer>> tuple2s = new ArrayList<>();\n                while (tp2It.hasNext()){\n                    Tuple2<Integer, Integer> next = tp2It.next();\n                    tuple2s.add(new Tuple2<Integer, Integer>(next._1,next._2*next._2));\n                }\n                return tuple2s;\n            }\n        });\n        tuple2JavaRDD.foreach(new VoidFunction<Tuple2<Integer, Integer>>() {\n            @Override\n            public void call(Tuple2<Integer, Integer> tp2) throws Exception {\n                System.out.println(tp2);\n            }\n        });\n-----------------输出---------------\n(1,1)\n(1,4)\n(1,9)\n(2,1)\n(2,4)\n(2,9)\n```  \n\n\n\n# **mapPartitionsWithIndex**  \n与mapPartitionWithIndex类似，也是按照分区进行的map操作，不过mapPartitionsWithIndex传入的参数多了一个分区的值，下面举个例子,为统计各个分区中的元素 (稍加修改可以做统计各个分区的数量)\n\n**java**\n```java\nJavaRDD<Integer> rdd = sc.parallelize(Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10), 3);\n        JavaRDD<Tuple2<Integer, Integer>> tuple2JavaRDD = rdd.mapPartitionsWithIndex(new Function2<Integer, Iterator<Integer>, Iterator<Tuple2<Integer, Integer>>>() {\n            @Override\n            public Iterator<Tuple2<Integer, Integer>> call(Integer partIndex, Iterator<Integer> it) throws Exception {\n                ArrayList<Tuple2<Integer, Integer>> tuple2s = new ArrayList<>();\n                while (it.hasNext()) {\n                    int next = it.next();\n                    tuple2s.add(new Tuple2<>(partIndex, next));\n                }\n                return tuple2s.iterator();\n            }\n        }, false);\n\n        tuple2JavaRDD.foreach(new VoidFunction<Tuple2<Integer, Integer>>() {\n            @Override\n            public void call(Tuple2<Integer, Integer> tp2) throws Exception {\n                System.out.println(tp2);\n            }\n        });\n-------输出-------------\n(0,1)\n(0,2)\n(0,3)\n(1,4)\n(1,5)\n(1,6)\n(2,7)\n(2,8)\n(2,9)\n(2,10)\n``` \n**scala**  \n```\n    val rdd = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10),3)\n\n    def mapPartIndexFunc(i1:Int,iter: Iterator[Int]):Iterator[(Int,Int)]={\n      var res = List[(Int,Int)]()\n      while(iter.hasNext){\n        var next = iter.next()\n        res=res.::(i1,next)\n      }\n      res.iterator\n    }\n    var mapPartIndexRDDs = rdd.mapPartitionsWithIndex(mapPartIndexFunc)\n\n    mapPartIndexRDDs.foreach(println( _))\n------------输出-------\n(0,3)\n(0,2)\n(0,1)\n(1,6)\n(1,5)\n(1,4)\n(2,10)\n(2,9)\n(2,8)\n(2,7)\n```  \n\n**mapPartitionsWithIndex 统计键值对中的各个分区的元素**  \n**scala版本**\n```scala\nvar rdd = sc.parallelize(List((1,1), (1,2), (2,3), (2,4), (3,5), (3,6),(4,7), (4,8),(5,9), (5,10)),3)\n\n    def mapPartIndexFunc(i1:Int,iter: Iterator[(Int,Int)]):Iterator[(Int,(Int,Int))]={\n      var res = List[(Int,(Int,Int))]()\n      while(iter.hasNext){\n        var next = iter.next()\n        res=res.::(i1,next)\n      }\n      res.iterator\n    }\n    val mapPartIndexRDD = rdd.mapPartitionsWithIndex(mapPartIndexFunc)\n\n    mapPartIndexRDD.foreach(println( _))\n-----------输出---------\n(0,(1,1))\n(0,(1,2))\n(0,(2,3))\n(1,(2,4))\n(1,(3,5))\n(1,(3,6))\n(2,(4,7))\n(2,(4,8))\n(2,(5,9))\n(2,(5,10))\n```\n**java版本**\n```\n        JavaRDD<Tuple2<Integer, Integer>> rdd = sc.parallelize(Arrays.asList(new Tuple2<Integer, Integer>(1, 1), new Tuple2<Integer, Integer>(1, 2)\n                , new Tuple2<Integer, Integer>(2, 3), new Tuple2<Integer, Integer>(2, 4)\n                , new Tuple2<Integer, Integer>(3, 5), new Tuple2<Integer, Integer>(3, 6)\n                , new Tuple2<Integer, Integer>(4, 7), new Tuple2<Integer, Integer>(4, 8)\n                , new Tuple2<Integer, Integer>(5, 9), new Tuple2<Integer, Integer>(5, 10)\n                ), 3);\n        JavaPairRDD<Integer, Integer> pairRDD = JavaPairRDD.fromJavaRDD(rdd);\n\n        JavaRDD<Tuple2<Integer, Tuple2<Integer, Integer>>> mapPartitionIndexRDD = pairRDD.mapPartitionsWithIndex(new Function2<Integer, Iterator<Tuple2<Integer, Integer>>, Iterator<Tuple2<Integer, Tuple2<Integer, Integer>>>>() {\n            @Override\n            public Iterator<Tuple2<Integer, Tuple2<Integer, Integer>>> call(Integer partIndex, Iterator<Tuple2<Integer, Integer>> tuple2Iterator) {\n                ArrayList<Tuple2<Integer, Tuple2<Integer, Integer>>> tuple2s = new ArrayList<>();\n\n                while (tuple2Iterator.hasNext()) {\n                    Tuple2<Integer, Integer> next = tuple2Iterator.next();\n                    tuple2s.add(new Tuple2<Integer, Tuple2<Integer, Integer>>(partIndex, next));\n                }\n                return tuple2s.iterator();\n            }\n        }, false);\n\n        mapPartitionIndexRDD.foreach(new VoidFunction<Tuple2<Integer, Tuple2<Integer, Integer>>>() {\n            @Override\n            public void call(Tuple2<Integer, Tuple2<Integer, Integer>> integerTuple2Tuple2) throws Exception {\n                System.out.println(integerTuple2Tuple2);\n            }\n        });\n--------------输出---------\n(0,(1,1))\n(0,(1,2))\n(0,(2,3))\n(1,(2,4))\n(1,(3,5))\n(1,(3,6))\n(2,(4,7))\n(2,(4,8))\n(2,(5,9))\n(2,(5,10))\n```\n\nmapPartitionsWithIndex 中 第二个参数，true还是false  \n这篇文章有些探讨,http://stackoverflow.com/questions/38048904/how-to-use-function-mappartitionswithindex-in-spark/38049239，个人还未理解 TODO  \n\n\n补充： 打印各个分区的操作，可以使用 glom 的方法\n```java\nJavaRDD<Tuple2<Integer, Integer>> rdd1 = sc.parallelize(Arrays.asList(new Tuple2<Integer, Integer>(1, 1), new Tuple2<Integer, Integer>(1, 2)\n                , new Tuple2<Integer, Integer>(2, 3), new Tuple2<Integer, Integer>(2, 4)\n                , new Tuple2<Integer, Integer>(3, 5), new Tuple2<Integer, Integer>(3, 6)\n                , new Tuple2<Integer, Integer>(4, 7), new Tuple2<Integer, Integer>(4, 8)\n                , new Tuple2<Integer, Integer>(5, 9), new Tuple2<Integer, Integer>(5, 10)\n        ), 3);\n        JavaPairRDD<Integer, Integer> pairRDD = JavaPairRDD.fromJavaRDD(rdd1);\n        /*补充：打印各个分区的操作，可以使用 glom 的方法*/\n        System.out.println(\"打印各个分区的操作，可以使用 glom 的方法\");\n        JavaRDD<List<Tuple2<Integer, Integer>>> glom = pairRDD.glom();\n        glom.foreach(new VoidFunction<List<Tuple2<Integer, Integer>>>() {\n            @Override\n            public void call(List<Tuple2<Integer, Integer>> tuple2s) throws Exception {\n                System.out.println(tuple2s);\n            }\n        });\n\n//************************* 输出 \n打印各个分区的操作，可以使用 glom 的方法\n[(1,1), (1,2), (2,3)]\n[(2,4), (3,5), (3,6)]\n[(4,7), (4,8), (5,9), (5,10)]\n```","source":"_posts/spark RDD算子（十二）之RDD 分区操作上mapPartitions, mapPartitionsWithIndex.md","raw":"---\ntitle: spark RDD算子（十二）之RDD 分区操作上mapPartitions, mapPartitionsWithIndex保存操作saveAsTextFile,saveAsSequenceFile,saveAsObjectFile,saveAsHadoopFile 等\ndate: 2017-04-18 21:25:21\ntags: [spark]\ncategories: [大数据,spark]\nauthor: kaishun\nid: 46\npermalink: spark-rdd-12\n---\n\n# **mapPartitions** \nmapPartition可以倒过来理解，先partition，再把每个partition进行map函数，\n**适用场景**   \n如果在映射的过程中需要频繁创建额外的对象，使用mapPartitions要比map高效的过。\n\n比如，将RDD中的所有数据通过JDBC连接写入数据库，如果使用map函数，可能要为每一个元素都创建一个connection，这样开销很大，如果使用mapPartitions，那么只需要针对每一个分区建立一个connection。  \n下面的例子，**把每一个元素平方**\n**java 每一个元素平方**\n```java\n        JavaRDD<Integer> rdd = sc.parallelize(\n                Arrays.asList(1,2,3,4,5,6,7,8,9,10));\n        JavaRDD<Integer> mapPartitionRDD = rdd.mapPartitions(new FlatMapFunction<Iterator<Integer>, Integer>() {\n            @Override\n            public Iterable<Integer> call(Iterator<Integer> it) throws Exception {\n                ArrayList<Integer> results = new ArrayList<>();\n                while (it.hasNext()) {\n                    int i = it.next();\n                    results.add(i*i);\n                }\n                return results;\n            }\n        });\n        mapPartitionRDD.foreach(new VoidFunction<Integer>() {\n            @Override\n            public void call(Integer integer) throws Exception {\n                System.out.println(integer);\n            }\n        });\n----------输出-------------\n1\n4\n9\n16\n25\n36\n49\n64\n81\n100\n```  \n**把每一个数字i变成一个map(i,i*i)的形式**\n\n**java，把每一个元素变成map(i,i*i)**\n```java\n        JavaRDD<Tuple2<Integer, Integer>> tuple2JavaRDD = rdd.mapPartitions(new FlatMapFunction<Iterator<Integer>, Tuple2<Integer, Integer>>() {\n            @Override\n            public Iterable<Tuple2<Integer, Integer>> call(Iterator<Integer> it) throws Exception {\n                ArrayList<Tuple2<Integer, Integer>> tuple2s = new ArrayList<>();\n                while (it.hasNext()) {\n                    Integer next = it.next();\n                    tuple2s.add(new Tuple2<Integer, Integer>(next, next * next));\n                }\n                return tuple2s;\n            }\n        });\n        tuple2JavaRDD.foreach(new VoidFunction<Tuple2<Integer, Integer>>() {\n            @Override\n            public void call(Tuple2<Integer, Integer> tp2) throws Exception {\n                System.out.println(tp2);\n            }\n        });\n---------输出---------------\n(1,1)\n(2,4)\n(3,9)\n(4,16)\n(5,25)\n(6,36)\n(7,49)\n(8,64)\n(9,81)\n(10,100)\n```\n**scala 把每一个元素变成map(i,i*i)**  \n```\n    val rdd = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10),3)\n    def mapPartFunc(iter: Iterator[Int]):Iterator[(Int,Int)]={\n      var res = List[(Int,Int)]()\n      while (iter.hasNext){\n        val cur = iter.next\n        res=res.::(cur,cur*cur)\n      }\n       res.iterator\n    }\n    val mapPartRDD = rdd.mapPartitions(mapPartFunc)\n    mapPartRDD.foreach(maps=>println(maps))\n----------输出-----------\n(3,9)\n(2,4)\n(1,1)\n(6,36)\n(5,25)\n(4,16)\n(10,100)\n(9,81)\n(8,64)\n(7,49)\n``` \n**mapPartitions操作键值对 把(i,j) 变成(i,j*j)**\n**scala版本**\n```scala\n    var rdd = sc.parallelize(List((1,1), (1,2), (1,3), (2,1), (2,2), (2,3)))\n\n    def mapPartFunc(iter: Iterator[(Int,Int)]):Iterator[(Int,Int)]={\n      var res = List[(Int,Int)]()\n      while (iter.hasNext){\n        val cur = iter.next\n        res=res.::(cur._1,cur._2*cur._2)\n      }\n      res.iterator\n    }\n    val mapPartionsRDD = rdd.mapPartitions(mapPartFunc)\n    mapPartionsRDD.foreach(println( _))\n```  \n**java版本**\n```java\n        JavaRDD<Tuple2<Integer, Integer>> rdd = sc.parallelize(Arrays.asList(new Tuple2<Integer, Integer>(1, 1), new Tuple2<Integer, Integer>(1, 2)\n                , new Tuple2<Integer, Integer>(1, 3), new Tuple2<Integer, Integer>(2, 1)\n                , new Tuple2<Integer, Integer>(2, 2), new Tuple2<Integer, Integer>(2, 3)), 3);\n        JavaPairRDD<Integer, Integer> pairRDD = JavaPairRDD.fromJavaRDD(rdd);\n        JavaRDD<Tuple2<Integer, Integer>> tuple2JavaRDD = pairRDD.mapPartitions(new FlatMapFunction<Iterator<Tuple2<Integer, Integer>>, Tuple2<Integer, Integer>>() {\n            @Override\n            public Iterable<Tuple2<Integer, Integer>> call(Iterator<Tuple2<Integer, Integer>> tp2It) throws Exception {\n                ArrayList<Tuple2<Integer, Integer>> tuple2s = new ArrayList<>();\n                while (tp2It.hasNext()){\n                    Tuple2<Integer, Integer> next = tp2It.next();\n                    tuple2s.add(new Tuple2<Integer, Integer>(next._1,next._2*next._2));\n                }\n                return tuple2s;\n            }\n        });\n        tuple2JavaRDD.foreach(new VoidFunction<Tuple2<Integer, Integer>>() {\n            @Override\n            public void call(Tuple2<Integer, Integer> tp2) throws Exception {\n                System.out.println(tp2);\n            }\n        });\n-----------------输出---------------\n(1,1)\n(1,4)\n(1,9)\n(2,1)\n(2,4)\n(2,9)\n```  \n\n\n\n# **mapPartitionsWithIndex**  \n与mapPartitionWithIndex类似，也是按照分区进行的map操作，不过mapPartitionsWithIndex传入的参数多了一个分区的值，下面举个例子,为统计各个分区中的元素 (稍加修改可以做统计各个分区的数量)\n\n**java**\n```java\nJavaRDD<Integer> rdd = sc.parallelize(Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10), 3);\n        JavaRDD<Tuple2<Integer, Integer>> tuple2JavaRDD = rdd.mapPartitionsWithIndex(new Function2<Integer, Iterator<Integer>, Iterator<Tuple2<Integer, Integer>>>() {\n            @Override\n            public Iterator<Tuple2<Integer, Integer>> call(Integer partIndex, Iterator<Integer> it) throws Exception {\n                ArrayList<Tuple2<Integer, Integer>> tuple2s = new ArrayList<>();\n                while (it.hasNext()) {\n                    int next = it.next();\n                    tuple2s.add(new Tuple2<>(partIndex, next));\n                }\n                return tuple2s.iterator();\n            }\n        }, false);\n\n        tuple2JavaRDD.foreach(new VoidFunction<Tuple2<Integer, Integer>>() {\n            @Override\n            public void call(Tuple2<Integer, Integer> tp2) throws Exception {\n                System.out.println(tp2);\n            }\n        });\n-------输出-------------\n(0,1)\n(0,2)\n(0,3)\n(1,4)\n(1,5)\n(1,6)\n(2,7)\n(2,8)\n(2,9)\n(2,10)\n``` \n**scala**  \n```\n    val rdd = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10),3)\n\n    def mapPartIndexFunc(i1:Int,iter: Iterator[Int]):Iterator[(Int,Int)]={\n      var res = List[(Int,Int)]()\n      while(iter.hasNext){\n        var next = iter.next()\n        res=res.::(i1,next)\n      }\n      res.iterator\n    }\n    var mapPartIndexRDDs = rdd.mapPartitionsWithIndex(mapPartIndexFunc)\n\n    mapPartIndexRDDs.foreach(println( _))\n------------输出-------\n(0,3)\n(0,2)\n(0,1)\n(1,6)\n(1,5)\n(1,4)\n(2,10)\n(2,9)\n(2,8)\n(2,7)\n```  \n\n**mapPartitionsWithIndex 统计键值对中的各个分区的元素**  \n**scala版本**\n```scala\nvar rdd = sc.parallelize(List((1,1), (1,2), (2,3), (2,4), (3,5), (3,6),(4,7), (4,8),(5,9), (5,10)),3)\n\n    def mapPartIndexFunc(i1:Int,iter: Iterator[(Int,Int)]):Iterator[(Int,(Int,Int))]={\n      var res = List[(Int,(Int,Int))]()\n      while(iter.hasNext){\n        var next = iter.next()\n        res=res.::(i1,next)\n      }\n      res.iterator\n    }\n    val mapPartIndexRDD = rdd.mapPartitionsWithIndex(mapPartIndexFunc)\n\n    mapPartIndexRDD.foreach(println( _))\n-----------输出---------\n(0,(1,1))\n(0,(1,2))\n(0,(2,3))\n(1,(2,4))\n(1,(3,5))\n(1,(3,6))\n(2,(4,7))\n(2,(4,8))\n(2,(5,9))\n(2,(5,10))\n```\n**java版本**\n```\n        JavaRDD<Tuple2<Integer, Integer>> rdd = sc.parallelize(Arrays.asList(new Tuple2<Integer, Integer>(1, 1), new Tuple2<Integer, Integer>(1, 2)\n                , new Tuple2<Integer, Integer>(2, 3), new Tuple2<Integer, Integer>(2, 4)\n                , new Tuple2<Integer, Integer>(3, 5), new Tuple2<Integer, Integer>(3, 6)\n                , new Tuple2<Integer, Integer>(4, 7), new Tuple2<Integer, Integer>(4, 8)\n                , new Tuple2<Integer, Integer>(5, 9), new Tuple2<Integer, Integer>(5, 10)\n                ), 3);\n        JavaPairRDD<Integer, Integer> pairRDD = JavaPairRDD.fromJavaRDD(rdd);\n\n        JavaRDD<Tuple2<Integer, Tuple2<Integer, Integer>>> mapPartitionIndexRDD = pairRDD.mapPartitionsWithIndex(new Function2<Integer, Iterator<Tuple2<Integer, Integer>>, Iterator<Tuple2<Integer, Tuple2<Integer, Integer>>>>() {\n            @Override\n            public Iterator<Tuple2<Integer, Tuple2<Integer, Integer>>> call(Integer partIndex, Iterator<Tuple2<Integer, Integer>> tuple2Iterator) {\n                ArrayList<Tuple2<Integer, Tuple2<Integer, Integer>>> tuple2s = new ArrayList<>();\n\n                while (tuple2Iterator.hasNext()) {\n                    Tuple2<Integer, Integer> next = tuple2Iterator.next();\n                    tuple2s.add(new Tuple2<Integer, Tuple2<Integer, Integer>>(partIndex, next));\n                }\n                return tuple2s.iterator();\n            }\n        }, false);\n\n        mapPartitionIndexRDD.foreach(new VoidFunction<Tuple2<Integer, Tuple2<Integer, Integer>>>() {\n            @Override\n            public void call(Tuple2<Integer, Tuple2<Integer, Integer>> integerTuple2Tuple2) throws Exception {\n                System.out.println(integerTuple2Tuple2);\n            }\n        });\n--------------输出---------\n(0,(1,1))\n(0,(1,2))\n(0,(2,3))\n(1,(2,4))\n(1,(3,5))\n(1,(3,6))\n(2,(4,7))\n(2,(4,8))\n(2,(5,9))\n(2,(5,10))\n```\n\nmapPartitionsWithIndex 中 第二个参数，true还是false  \n这篇文章有些探讨,http://stackoverflow.com/questions/38048904/how-to-use-function-mappartitionswithindex-in-spark/38049239，个人还未理解 TODO  \n\n\n补充： 打印各个分区的操作，可以使用 glom 的方法\n```java\nJavaRDD<Tuple2<Integer, Integer>> rdd1 = sc.parallelize(Arrays.asList(new Tuple2<Integer, Integer>(1, 1), new Tuple2<Integer, Integer>(1, 2)\n                , new Tuple2<Integer, Integer>(2, 3), new Tuple2<Integer, Integer>(2, 4)\n                , new Tuple2<Integer, Integer>(3, 5), new Tuple2<Integer, Integer>(3, 6)\n                , new Tuple2<Integer, Integer>(4, 7), new Tuple2<Integer, Integer>(4, 8)\n                , new Tuple2<Integer, Integer>(5, 9), new Tuple2<Integer, Integer>(5, 10)\n        ), 3);\n        JavaPairRDD<Integer, Integer> pairRDD = JavaPairRDD.fromJavaRDD(rdd1);\n        /*补充：打印各个分区的操作，可以使用 glom 的方法*/\n        System.out.println(\"打印各个分区的操作，可以使用 glom 的方法\");\n        JavaRDD<List<Tuple2<Integer, Integer>>> glom = pairRDD.glom();\n        glom.foreach(new VoidFunction<List<Tuple2<Integer, Integer>>>() {\n            @Override\n            public void call(List<Tuple2<Integer, Integer>> tuple2s) throws Exception {\n                System.out.println(tuple2s);\n            }\n        });\n\n//************************* 输出 \n打印各个分区的操作，可以使用 glom 的方法\n[(1,1), (1,2), (2,3)]\n[(2,4), (3,5), (3,6)]\n[(4,7), (4,8), (5,9), (5,10)]\n```","slug":"spark-rdd-12","published":1,"updated":"2018-01-22T15:30:48.134Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcrpnzs0003q2wv35ylijy8v","content":"<h1 id=\"mapPartitions\"><a href=\"#mapPartitions\" class=\"headerlink\" title=\"mapPartitions\"></a><strong>mapPartitions</strong></h1><p>mapPartition可以倒过来理解，先partition，再把每个partition进行map函数，<br><strong>适用场景</strong><br>如果在映射的过程中需要频繁创建额外的对象，使用mapPartitions要比map高效的过。</p>\n<p>比如，将RDD中的所有数据通过JDBC连接写入数据库，如果使用map函数，可能要为每一个元素都创建一个connection，这样开销很大，如果使用mapPartitions，那么只需要针对每一个分区建立一个connection。<br>下面的例子，<strong>把每一个元素平方</strong><br><strong>java 每一个元素平方</strong><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div></pre></td><td class=\"code\"><pre><div class=\"line\">        JavaRDD&lt;Integer&gt; rdd = sc.parallelize(</div><div class=\"line\">                Arrays.asList(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>,<span class=\"number\">6</span>,<span class=\"number\">7</span>,<span class=\"number\">8</span>,<span class=\"number\">9</span>,<span class=\"number\">10</span>));</div><div class=\"line\">        JavaRDD&lt;Integer&gt; mapPartitionRDD = rdd.mapPartitions(<span class=\"keyword\">new</span> FlatMapFunction&lt;Iterator&lt;Integer&gt;, Integer&gt;() &#123;</div><div class=\"line\">            <span class=\"meta\">@Override</span></div><div class=\"line\">            <span class=\"function\"><span class=\"keyword\">public</span> Iterable&lt;Integer&gt; <span class=\"title\">call</span><span class=\"params\">(Iterator&lt;Integer&gt; it)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">                ArrayList&lt;Integer&gt; results = <span class=\"keyword\">new</span> ArrayList&lt;&gt;();</div><div class=\"line\">                <span class=\"keyword\">while</span> (it.hasNext()) &#123;</div><div class=\"line\">                    <span class=\"keyword\">int</span> i = it.next();</div><div class=\"line\">                    results.add(i*i);</div><div class=\"line\">                &#125;</div><div class=\"line\">                <span class=\"keyword\">return</span> results;</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;);</div><div class=\"line\">        mapPartitionRDD.foreach(<span class=\"keyword\">new</span> VoidFunction&lt;Integer&gt;() &#123;</div><div class=\"line\">            <span class=\"meta\">@Override</span></div><div class=\"line\">            <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">call</span><span class=\"params\">(Integer integer)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">                System.out.println(integer);</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;);</div><div class=\"line\">----------输出-------------</div><div class=\"line\"><span class=\"number\">1</span></div><div class=\"line\"><span class=\"number\">4</span></div><div class=\"line\"><span class=\"number\">9</span></div><div class=\"line\"><span class=\"number\">16</span></div><div class=\"line\"><span class=\"number\">25</span></div><div class=\"line\"><span class=\"number\">36</span></div><div class=\"line\"><span class=\"number\">49</span></div><div class=\"line\"><span class=\"number\">64</span></div><div class=\"line\"><span class=\"number\">81</span></div><div class=\"line\"><span class=\"number\">100</span></div><div class=\"line\">```  </div><div class=\"line\">**把每一个数字i变成一个map(i,i*i)的形式**</div><div class=\"line\"></div><div class=\"line\">**java，把每一个元素变成map(i,i*i)**</div><div class=\"line\">```java</div><div class=\"line\">        JavaRDD&lt;Tuple2&lt;Integer, Integer&gt;&gt; tuple2JavaRDD = rdd.mapPartitions(<span class=\"keyword\">new</span> FlatMapFunction&lt;Iterator&lt;Integer&gt;, Tuple2&lt;Integer, Integer&gt;&gt;() &#123;</div><div class=\"line\">            <span class=\"meta\">@Override</span></div><div class=\"line\">            <span class=\"keyword\">public</span> Iterable&lt;Tuple2&lt;Integer, Integer&gt;&gt; call(Iterator&lt;Integer&gt; it) <span class=\"keyword\">throws</span> Exception &#123;</div><div class=\"line\">                ArrayList&lt;Tuple2&lt;Integer, Integer&gt;&gt; tuple2s = <span class=\"keyword\">new</span> ArrayList&lt;&gt;();</div><div class=\"line\">                <span class=\"keyword\">while</span> (it.hasNext()) &#123;</div><div class=\"line\">                    Integer next = it.next();</div><div class=\"line\">                    tuple2s.add(<span class=\"keyword\">new</span> Tuple2&lt;Integer, Integer&gt;(next, next * next));</div><div class=\"line\">                &#125;</div><div class=\"line\">                <span class=\"keyword\">return</span> tuple2s;</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;);</div><div class=\"line\">        tuple2JavaRDD.foreach(<span class=\"keyword\">new</span> VoidFunction&lt;Tuple2&lt;Integer, Integer&gt;&gt;() &#123;</div><div class=\"line\">            <span class=\"meta\">@Override</span></div><div class=\"line\">            <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">call</span><span class=\"params\">(Tuple2&lt;Integer, Integer&gt; tp2)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">                System.out.println(tp2);</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;);</div><div class=\"line\">---------输出---------------</div><div class=\"line\">(<span class=\"number\">1</span>,<span class=\"number\">1</span>)</div><div class=\"line\">(<span class=\"number\">2</span>,<span class=\"number\">4</span>)</div><div class=\"line\">(<span class=\"number\">3</span>,<span class=\"number\">9</span>)</div><div class=\"line\">(<span class=\"number\">4</span>,<span class=\"number\">16</span>)</div><div class=\"line\">(<span class=\"number\">5</span>,<span class=\"number\">25</span>)</div><div class=\"line\">(<span class=\"number\">6</span>,<span class=\"number\">36</span>)</div><div class=\"line\">(<span class=\"number\">7</span>,<span class=\"number\">49</span>)</div><div class=\"line\">(<span class=\"number\">8</span>,<span class=\"number\">64</span>)</div><div class=\"line\">(<span class=\"number\">9</span>,<span class=\"number\">81</span>)</div><div class=\"line\">(<span class=\"number\">10</span>,<span class=\"number\">100</span>)</div></pre></td></tr></table></figure></p>\n<p><strong>scala 把每一个元素变成map(i,i*i)</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div><div class=\"line\">106</div><div class=\"line\">107</div><div class=\"line\">108</div><div class=\"line\">109</div><div class=\"line\">110</div></pre></td><td class=\"code\"><pre><div class=\"line\">    val rdd = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10),3)</div><div class=\"line\">    def mapPartFunc(iter: Iterator[Int]):Iterator[(Int,Int)]=&#123;</div><div class=\"line\">      var res = List[(Int,Int)]()</div><div class=\"line\">      while (iter.hasNext)&#123;</div><div class=\"line\">        val cur = iter.next</div><div class=\"line\">        res=res.::(cur,cur*cur)</div><div class=\"line\">      &#125;</div><div class=\"line\">       res.iterator</div><div class=\"line\">    &#125;</div><div class=\"line\">    val mapPartRDD = rdd.mapPartitions(mapPartFunc)</div><div class=\"line\">    mapPartRDD.foreach(maps=&gt;println(maps))</div><div class=\"line\">----------输出-----------</div><div class=\"line\">(3,9)</div><div class=\"line\">(2,4)</div><div class=\"line\">(1,1)</div><div class=\"line\">(6,36)</div><div class=\"line\">(5,25)</div><div class=\"line\">(4,16)</div><div class=\"line\">(10,100)</div><div class=\"line\">(9,81)</div><div class=\"line\">(8,64)</div><div class=\"line\">(7,49)</div><div class=\"line\">``` </div><div class=\"line\">**mapPartitions操作键值对 把(i,j) 变成(i,j*j)**</div><div class=\"line\">**scala版本**</div><div class=\"line\">```scala</div><div class=\"line\">    var rdd = sc.parallelize(List((1,1), (1,2), (1,3), (2,1), (2,2), (2,3)))</div><div class=\"line\"></div><div class=\"line\">    def mapPartFunc(iter: Iterator[(Int,Int)]):Iterator[(Int,Int)]=&#123;</div><div class=\"line\">      var res = List[(Int,Int)]()</div><div class=\"line\">      while (iter.hasNext)&#123;</div><div class=\"line\">        val cur = iter.next</div><div class=\"line\">        res=res.::(cur._1,cur._2*cur._2)</div><div class=\"line\">      &#125;</div><div class=\"line\">      res.iterator</div><div class=\"line\">    &#125;</div><div class=\"line\">    val mapPartionsRDD = rdd.mapPartitions(mapPartFunc)</div><div class=\"line\">    mapPartionsRDD.foreach(println( _))</div><div class=\"line\">```  </div><div class=\"line\">**java版本**</div><div class=\"line\">```java</div><div class=\"line\">        JavaRDD&lt;Tuple2&lt;Integer, Integer&gt;&gt; rdd = sc.parallelize(Arrays.asList(new Tuple2&lt;Integer, Integer&gt;(1, 1), new Tuple2&lt;Integer, Integer&gt;(1, 2)</div><div class=\"line\">                , new Tuple2&lt;Integer, Integer&gt;(1, 3), new Tuple2&lt;Integer, Integer&gt;(2, 1)</div><div class=\"line\">                , new Tuple2&lt;Integer, Integer&gt;(2, 2), new Tuple2&lt;Integer, Integer&gt;(2, 3)), 3);</div><div class=\"line\">        JavaPairRDD&lt;Integer, Integer&gt; pairRDD = JavaPairRDD.fromJavaRDD(rdd);</div><div class=\"line\">        JavaRDD&lt;Tuple2&lt;Integer, Integer&gt;&gt; tuple2JavaRDD = pairRDD.mapPartitions(new FlatMapFunction&lt;Iterator&lt;Tuple2&lt;Integer, Integer&gt;&gt;, Tuple2&lt;Integer, Integer&gt;&gt;() &#123;</div><div class=\"line\">            @Override</div><div class=\"line\">            public Iterable&lt;Tuple2&lt;Integer, Integer&gt;&gt; call(Iterator&lt;Tuple2&lt;Integer, Integer&gt;&gt; tp2It) throws Exception &#123;</div><div class=\"line\">                ArrayList&lt;Tuple2&lt;Integer, Integer&gt;&gt; tuple2s = new ArrayList&lt;&gt;();</div><div class=\"line\">                while (tp2It.hasNext())&#123;</div><div class=\"line\">                    Tuple2&lt;Integer, Integer&gt; next = tp2It.next();</div><div class=\"line\">                    tuple2s.add(new Tuple2&lt;Integer, Integer&gt;(next._1,next._2*next._2));</div><div class=\"line\">                &#125;</div><div class=\"line\">                return tuple2s;</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;);</div><div class=\"line\">        tuple2JavaRDD.foreach(new VoidFunction&lt;Tuple2&lt;Integer, Integer&gt;&gt;() &#123;</div><div class=\"line\">            @Override</div><div class=\"line\">            public void call(Tuple2&lt;Integer, Integer&gt; tp2) throws Exception &#123;</div><div class=\"line\">                System.out.println(tp2);</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;);</div><div class=\"line\">-----------------输出---------------</div><div class=\"line\">(1,1)</div><div class=\"line\">(1,4)</div><div class=\"line\">(1,9)</div><div class=\"line\">(2,1)</div><div class=\"line\">(2,4)</div><div class=\"line\">(2,9)</div><div class=\"line\">```  </div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"># **mapPartitionsWithIndex**  </div><div class=\"line\">与mapPartitionWithIndex类似，也是按照分区进行的map操作，不过mapPartitionsWithIndex传入的参数多了一个分区的值，下面举个例子,为统计各个分区中的元素 (稍加修改可以做统计各个分区的数量)</div><div class=\"line\"></div><div class=\"line\">**java**</div><div class=\"line\">```java</div><div class=\"line\">JavaRDD&lt;Integer&gt; rdd = sc.parallelize(Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10), 3);</div><div class=\"line\">        JavaRDD&lt;Tuple2&lt;Integer, Integer&gt;&gt; tuple2JavaRDD = rdd.mapPartitionsWithIndex(new Function2&lt;Integer, Iterator&lt;Integer&gt;, Iterator&lt;Tuple2&lt;Integer, Integer&gt;&gt;&gt;() &#123;</div><div class=\"line\">            @Override</div><div class=\"line\">            public Iterator&lt;Tuple2&lt;Integer, Integer&gt;&gt; call(Integer partIndex, Iterator&lt;Integer&gt; it) throws Exception &#123;</div><div class=\"line\">                ArrayList&lt;Tuple2&lt;Integer, Integer&gt;&gt; tuple2s = new ArrayList&lt;&gt;();</div><div class=\"line\">                while (it.hasNext()) &#123;</div><div class=\"line\">                    int next = it.next();</div><div class=\"line\">                    tuple2s.add(new Tuple2&lt;&gt;(partIndex, next));</div><div class=\"line\">                &#125;</div><div class=\"line\">                return tuple2s.iterator();</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;, false);</div><div class=\"line\"></div><div class=\"line\">        tuple2JavaRDD.foreach(new VoidFunction&lt;Tuple2&lt;Integer, Integer&gt;&gt;() &#123;</div><div class=\"line\">            @Override</div><div class=\"line\">            public void call(Tuple2&lt;Integer, Integer&gt; tp2) throws Exception &#123;</div><div class=\"line\">                System.out.println(tp2);</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;);</div><div class=\"line\">-------输出-------------</div><div class=\"line\">(0,1)</div><div class=\"line\">(0,2)</div><div class=\"line\">(0,3)</div><div class=\"line\">(1,4)</div><div class=\"line\">(1,5)</div><div class=\"line\">(1,6)</div><div class=\"line\">(2,7)</div><div class=\"line\">(2,8)</div><div class=\"line\">(2,9)</div><div class=\"line\">(2,10)</div><div class=\"line\">``` </div><div class=\"line\">**scala**</div></pre></td></tr></table></figure></p>\n<pre><code>val rdd = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10),3)\n\ndef mapPartIndexFunc(i1:Int,iter: Iterator[Int]):Iterator[(Int,Int)]={\n  var res = List[(Int,Int)]()\n  while(iter.hasNext){\n    var next = iter.next()\n    res=res.::(i1,next)\n  }\n  res.iterator\n}\nvar mapPartIndexRDDs = rdd.mapPartitionsWithIndex(mapPartIndexFunc)\n\nmapPartIndexRDDs.foreach(println( _))\n</code></pre><p>————输出——-<br>(0,3)<br>(0,2)<br>(0,1)<br>(1,6)<br>(1,5)<br>(1,4)<br>(2,10)<br>(2,9)<br>(2,8)<br>(2,7)<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\">**mapPartitionsWithIndex 统计键值对中的各个分区的元素**  </div><div class=\"line\">**scala版本**</div><div class=\"line\">```scala</div><div class=\"line\">var rdd = sc.parallelize(List((1,1), (1,2), (2,3), (2,4), (3,5), (3,6),(4,7), (4,8),(5,9), (5,10)),3)</div><div class=\"line\"></div><div class=\"line\">    def mapPartIndexFunc(i1:Int,iter: Iterator[(Int,Int)]):Iterator[(Int,(Int,Int))]=&#123;</div><div class=\"line\">      var res = List[(Int,(Int,Int))]()</div><div class=\"line\">      while(iter.hasNext)&#123;</div><div class=\"line\">        var next = iter.next()</div><div class=\"line\">        res=res.::(i1,next)</div><div class=\"line\">      &#125;</div><div class=\"line\">      res.iterator</div><div class=\"line\">    &#125;</div><div class=\"line\">    val mapPartIndexRDD = rdd.mapPartitionsWithIndex(mapPartIndexFunc)</div><div class=\"line\"></div><div class=\"line\">    mapPartIndexRDD.foreach(println( _))</div><div class=\"line\">-----------输出---------</div><div class=\"line\">(0,(1,1))</div><div class=\"line\">(0,(1,2))</div><div class=\"line\">(0,(2,3))</div><div class=\"line\">(1,(2,4))</div><div class=\"line\">(1,(3,5))</div><div class=\"line\">(1,(3,6))</div><div class=\"line\">(2,(4,7))</div><div class=\"line\">(2,(4,8))</div><div class=\"line\">(2,(5,9))</div><div class=\"line\">(2,(5,10))</div></pre></td></tr></table></figure></p>\n<p><strong>java版本</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div></pre></td><td class=\"code\"><pre><div class=\"line\">        JavaRDD&lt;Tuple2&lt;Integer, Integer&gt;&gt; rdd = sc.parallelize(Arrays.asList(new Tuple2&lt;Integer, Integer&gt;(1, 1), new Tuple2&lt;Integer, Integer&gt;(1, 2)</div><div class=\"line\">                , new Tuple2&lt;Integer, Integer&gt;(2, 3), new Tuple2&lt;Integer, Integer&gt;(2, 4)</div><div class=\"line\">                , new Tuple2&lt;Integer, Integer&gt;(3, 5), new Tuple2&lt;Integer, Integer&gt;(3, 6)</div><div class=\"line\">                , new Tuple2&lt;Integer, Integer&gt;(4, 7), new Tuple2&lt;Integer, Integer&gt;(4, 8)</div><div class=\"line\">                , new Tuple2&lt;Integer, Integer&gt;(5, 9), new Tuple2&lt;Integer, Integer&gt;(5, 10)</div><div class=\"line\">                ), 3);</div><div class=\"line\">        JavaPairRDD&lt;Integer, Integer&gt; pairRDD = JavaPairRDD.fromJavaRDD(rdd);</div><div class=\"line\"></div><div class=\"line\">        JavaRDD&lt;Tuple2&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt;&gt; mapPartitionIndexRDD = pairRDD.mapPartitionsWithIndex(new Function2&lt;Integer, Iterator&lt;Tuple2&lt;Integer, Integer&gt;&gt;, Iterator&lt;Tuple2&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt;&gt;&gt;() &#123;</div><div class=\"line\">            @Override</div><div class=\"line\">            public Iterator&lt;Tuple2&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt;&gt; call(Integer partIndex, Iterator&lt;Tuple2&lt;Integer, Integer&gt;&gt; tuple2Iterator) &#123;</div><div class=\"line\">                ArrayList&lt;Tuple2&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt;&gt; tuple2s = new ArrayList&lt;&gt;();</div><div class=\"line\"></div><div class=\"line\">                while (tuple2Iterator.hasNext()) &#123;</div><div class=\"line\">                    Tuple2&lt;Integer, Integer&gt; next = tuple2Iterator.next();</div><div class=\"line\">                    tuple2s.add(new Tuple2&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt;(partIndex, next));</div><div class=\"line\">                &#125;</div><div class=\"line\">                return tuple2s.iterator();</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;, false);</div><div class=\"line\"></div><div class=\"line\">        mapPartitionIndexRDD.foreach(new VoidFunction&lt;Tuple2&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt;&gt;() &#123;</div><div class=\"line\">            @Override</div><div class=\"line\">            public void call(Tuple2&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt; integerTuple2Tuple2) throws Exception &#123;</div><div class=\"line\">                System.out.println(integerTuple2Tuple2);</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;);</div><div class=\"line\">--------------输出---------</div><div class=\"line\">(0,(1,1))</div><div class=\"line\">(0,(1,2))</div><div class=\"line\">(0,(2,3))</div><div class=\"line\">(1,(2,4))</div><div class=\"line\">(1,(3,5))</div><div class=\"line\">(1,(3,6))</div><div class=\"line\">(2,(4,7))</div><div class=\"line\">(2,(4,8))</div><div class=\"line\">(2,(5,9))</div><div class=\"line\">(2,(5,10))</div></pre></td></tr></table></figure></p>\n<p>mapPartitionsWithIndex 中 第二个参数，true还是false<br>这篇文章有些探讨,<a href=\"http://stackoverflow.com/questions/38048904/how-to-use-function-mappartitionswithindex-in-spark/38049239，个人还未理解\" target=\"_blank\" rel=\"external\">http://stackoverflow.com/questions/38048904/how-to-use-function-mappartitionswithindex-in-spark/38049239，个人还未理解</a> TODO  </p>\n<p>补充： 打印各个分区的操作，可以使用 glom 的方法<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div></pre></td><td class=\"code\"><pre><div class=\"line\">JavaRDD&lt;Tuple2&lt;Integer, Integer&gt;&gt; rdd1 = sc.parallelize(Arrays.asList(<span class=\"keyword\">new</span> Tuple2&lt;Integer, Integer&gt;(<span class=\"number\">1</span>, <span class=\"number\">1</span>), <span class=\"keyword\">new</span> Tuple2&lt;Integer, Integer&gt;(<span class=\"number\">1</span>, <span class=\"number\">2</span>)</div><div class=\"line\">                , <span class=\"keyword\">new</span> Tuple2&lt;Integer, Integer&gt;(<span class=\"number\">2</span>, <span class=\"number\">3</span>), <span class=\"keyword\">new</span> Tuple2&lt;Integer, Integer&gt;(<span class=\"number\">2</span>, <span class=\"number\">4</span>)</div><div class=\"line\">                , <span class=\"keyword\">new</span> Tuple2&lt;Integer, Integer&gt;(<span class=\"number\">3</span>, <span class=\"number\">5</span>), <span class=\"keyword\">new</span> Tuple2&lt;Integer, Integer&gt;(<span class=\"number\">3</span>, <span class=\"number\">6</span>)</div><div class=\"line\">                , <span class=\"keyword\">new</span> Tuple2&lt;Integer, Integer&gt;(<span class=\"number\">4</span>, <span class=\"number\">7</span>), <span class=\"keyword\">new</span> Tuple2&lt;Integer, Integer&gt;(<span class=\"number\">4</span>, <span class=\"number\">8</span>)</div><div class=\"line\">                , <span class=\"keyword\">new</span> Tuple2&lt;Integer, Integer&gt;(<span class=\"number\">5</span>, <span class=\"number\">9</span>), <span class=\"keyword\">new</span> Tuple2&lt;Integer, Integer&gt;(<span class=\"number\">5</span>, <span class=\"number\">10</span>)</div><div class=\"line\">        ), <span class=\"number\">3</span>);</div><div class=\"line\">        JavaPairRDD&lt;Integer, Integer&gt; pairRDD = JavaPairRDD.fromJavaRDD(rdd1);</div><div class=\"line\">        <span class=\"comment\">/*补充：打印各个分区的操作，可以使用 glom 的方法*/</span></div><div class=\"line\">        System.out.println(<span class=\"string\">\"打印各个分区的操作，可以使用 glom 的方法\"</span>);</div><div class=\"line\">        JavaRDD&lt;List&lt;Tuple2&lt;Integer, Integer&gt;&gt;&gt; glom = pairRDD.glom();</div><div class=\"line\">        glom.foreach(<span class=\"keyword\">new</span> VoidFunction&lt;List&lt;Tuple2&lt;Integer, Integer&gt;&gt;&gt;() &#123;</div><div class=\"line\">            <span class=\"meta\">@Override</span></div><div class=\"line\">            <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">call</span><span class=\"params\">(List&lt;Tuple2&lt;Integer, Integer&gt;&gt; tuple2s)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">                System.out.println(tuple2s);</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;);</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">//************************* 输出 </span></div><div class=\"line\">打印各个分区的操作，可以使用 glom 的方法</div><div class=\"line\">[(<span class=\"number\">1</span>,<span class=\"number\">1</span>), (<span class=\"number\">1</span>,<span class=\"number\">2</span>), (<span class=\"number\">2</span>,<span class=\"number\">3</span>)]</div><div class=\"line\">[(<span class=\"number\">2</span>,<span class=\"number\">4</span>), (<span class=\"number\">3</span>,<span class=\"number\">5</span>), (<span class=\"number\">3</span>,<span class=\"number\">6</span>)]</div><div class=\"line\">[(<span class=\"number\">4</span>,<span class=\"number\">7</span>), (<span class=\"number\">4</span>,<span class=\"number\">8</span>), (<span class=\"number\">5</span>,<span class=\"number\">9</span>), (<span class=\"number\">5</span>,<span class=\"number\">10</span>)]</div></pre></td></tr></table></figure></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"mapPartitions\"><a href=\"#mapPartitions\" class=\"headerlink\" title=\"mapPartitions\"></a><strong>mapPartitions</strong></h1><p>mapPartition可以倒过来理解，先partition，再把每个partition进行map函数，<br><strong>适用场景</strong><br>如果在映射的过程中需要频繁创建额外的对象，使用mapPartitions要比map高效的过。</p>\n<p>比如，将RDD中的所有数据通过JDBC连接写入数据库，如果使用map函数，可能要为每一个元素都创建一个connection，这样开销很大，如果使用mapPartitions，那么只需要针对每一个分区建立一个connection。<br>下面的例子，<strong>把每一个元素平方</strong><br><strong>java 每一个元素平方</strong><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div></pre></td><td class=\"code\"><pre><div class=\"line\">        JavaRDD&lt;Integer&gt; rdd = sc.parallelize(</div><div class=\"line\">                Arrays.asList(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">5</span>,<span class=\"number\">6</span>,<span class=\"number\">7</span>,<span class=\"number\">8</span>,<span class=\"number\">9</span>,<span class=\"number\">10</span>));</div><div class=\"line\">        JavaRDD&lt;Integer&gt; mapPartitionRDD = rdd.mapPartitions(<span class=\"keyword\">new</span> FlatMapFunction&lt;Iterator&lt;Integer&gt;, Integer&gt;() &#123;</div><div class=\"line\">            <span class=\"meta\">@Override</span></div><div class=\"line\">            <span class=\"function\"><span class=\"keyword\">public</span> Iterable&lt;Integer&gt; <span class=\"title\">call</span><span class=\"params\">(Iterator&lt;Integer&gt; it)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">                ArrayList&lt;Integer&gt; results = <span class=\"keyword\">new</span> ArrayList&lt;&gt;();</div><div class=\"line\">                <span class=\"keyword\">while</span> (it.hasNext()) &#123;</div><div class=\"line\">                    <span class=\"keyword\">int</span> i = it.next();</div><div class=\"line\">                    results.add(i*i);</div><div class=\"line\">                &#125;</div><div class=\"line\">                <span class=\"keyword\">return</span> results;</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;);</div><div class=\"line\">        mapPartitionRDD.foreach(<span class=\"keyword\">new</span> VoidFunction&lt;Integer&gt;() &#123;</div><div class=\"line\">            <span class=\"meta\">@Override</span></div><div class=\"line\">            <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">call</span><span class=\"params\">(Integer integer)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">                System.out.println(integer);</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;);</div><div class=\"line\">----------输出-------------</div><div class=\"line\"><span class=\"number\">1</span></div><div class=\"line\"><span class=\"number\">4</span></div><div class=\"line\"><span class=\"number\">9</span></div><div class=\"line\"><span class=\"number\">16</span></div><div class=\"line\"><span class=\"number\">25</span></div><div class=\"line\"><span class=\"number\">36</span></div><div class=\"line\"><span class=\"number\">49</span></div><div class=\"line\"><span class=\"number\">64</span></div><div class=\"line\"><span class=\"number\">81</span></div><div class=\"line\"><span class=\"number\">100</span></div><div class=\"line\">```  </div><div class=\"line\">**把每一个数字i变成一个map(i,i*i)的形式**</div><div class=\"line\"></div><div class=\"line\">**java，把每一个元素变成map(i,i*i)**</div><div class=\"line\">```java</div><div class=\"line\">        JavaRDD&lt;Tuple2&lt;Integer, Integer&gt;&gt; tuple2JavaRDD = rdd.mapPartitions(<span class=\"keyword\">new</span> FlatMapFunction&lt;Iterator&lt;Integer&gt;, Tuple2&lt;Integer, Integer&gt;&gt;() &#123;</div><div class=\"line\">            <span class=\"meta\">@Override</span></div><div class=\"line\">            <span class=\"keyword\">public</span> Iterable&lt;Tuple2&lt;Integer, Integer&gt;&gt; call(Iterator&lt;Integer&gt; it) <span class=\"keyword\">throws</span> Exception &#123;</div><div class=\"line\">                ArrayList&lt;Tuple2&lt;Integer, Integer&gt;&gt; tuple2s = <span class=\"keyword\">new</span> ArrayList&lt;&gt;();</div><div class=\"line\">                <span class=\"keyword\">while</span> (it.hasNext()) &#123;</div><div class=\"line\">                    Integer next = it.next();</div><div class=\"line\">                    tuple2s.add(<span class=\"keyword\">new</span> Tuple2&lt;Integer, Integer&gt;(next, next * next));</div><div class=\"line\">                &#125;</div><div class=\"line\">                <span class=\"keyword\">return</span> tuple2s;</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;);</div><div class=\"line\">        tuple2JavaRDD.foreach(<span class=\"keyword\">new</span> VoidFunction&lt;Tuple2&lt;Integer, Integer&gt;&gt;() &#123;</div><div class=\"line\">            <span class=\"meta\">@Override</span></div><div class=\"line\">            <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">call</span><span class=\"params\">(Tuple2&lt;Integer, Integer&gt; tp2)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">                System.out.println(tp2);</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;);</div><div class=\"line\">---------输出---------------</div><div class=\"line\">(<span class=\"number\">1</span>,<span class=\"number\">1</span>)</div><div class=\"line\">(<span class=\"number\">2</span>,<span class=\"number\">4</span>)</div><div class=\"line\">(<span class=\"number\">3</span>,<span class=\"number\">9</span>)</div><div class=\"line\">(<span class=\"number\">4</span>,<span class=\"number\">16</span>)</div><div class=\"line\">(<span class=\"number\">5</span>,<span class=\"number\">25</span>)</div><div class=\"line\">(<span class=\"number\">6</span>,<span class=\"number\">36</span>)</div><div class=\"line\">(<span class=\"number\">7</span>,<span class=\"number\">49</span>)</div><div class=\"line\">(<span class=\"number\">8</span>,<span class=\"number\">64</span>)</div><div class=\"line\">(<span class=\"number\">9</span>,<span class=\"number\">81</span>)</div><div class=\"line\">(<span class=\"number\">10</span>,<span class=\"number\">100</span>)</div></pre></td></tr></table></figure></p>\n<p><strong>scala 把每一个元素变成map(i,i*i)</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div><div class=\"line\">56</div><div class=\"line\">57</div><div class=\"line\">58</div><div class=\"line\">59</div><div class=\"line\">60</div><div class=\"line\">61</div><div class=\"line\">62</div><div class=\"line\">63</div><div class=\"line\">64</div><div class=\"line\">65</div><div class=\"line\">66</div><div class=\"line\">67</div><div class=\"line\">68</div><div class=\"line\">69</div><div class=\"line\">70</div><div class=\"line\">71</div><div class=\"line\">72</div><div class=\"line\">73</div><div class=\"line\">74</div><div class=\"line\">75</div><div class=\"line\">76</div><div class=\"line\">77</div><div class=\"line\">78</div><div class=\"line\">79</div><div class=\"line\">80</div><div class=\"line\">81</div><div class=\"line\">82</div><div class=\"line\">83</div><div class=\"line\">84</div><div class=\"line\">85</div><div class=\"line\">86</div><div class=\"line\">87</div><div class=\"line\">88</div><div class=\"line\">89</div><div class=\"line\">90</div><div class=\"line\">91</div><div class=\"line\">92</div><div class=\"line\">93</div><div class=\"line\">94</div><div class=\"line\">95</div><div class=\"line\">96</div><div class=\"line\">97</div><div class=\"line\">98</div><div class=\"line\">99</div><div class=\"line\">100</div><div class=\"line\">101</div><div class=\"line\">102</div><div class=\"line\">103</div><div class=\"line\">104</div><div class=\"line\">105</div><div class=\"line\">106</div><div class=\"line\">107</div><div class=\"line\">108</div><div class=\"line\">109</div><div class=\"line\">110</div></pre></td><td class=\"code\"><pre><div class=\"line\">    val rdd = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10),3)</div><div class=\"line\">    def mapPartFunc(iter: Iterator[Int]):Iterator[(Int,Int)]=&#123;</div><div class=\"line\">      var res = List[(Int,Int)]()</div><div class=\"line\">      while (iter.hasNext)&#123;</div><div class=\"line\">        val cur = iter.next</div><div class=\"line\">        res=res.::(cur,cur*cur)</div><div class=\"line\">      &#125;</div><div class=\"line\">       res.iterator</div><div class=\"line\">    &#125;</div><div class=\"line\">    val mapPartRDD = rdd.mapPartitions(mapPartFunc)</div><div class=\"line\">    mapPartRDD.foreach(maps=&gt;println(maps))</div><div class=\"line\">----------输出-----------</div><div class=\"line\">(3,9)</div><div class=\"line\">(2,4)</div><div class=\"line\">(1,1)</div><div class=\"line\">(6,36)</div><div class=\"line\">(5,25)</div><div class=\"line\">(4,16)</div><div class=\"line\">(10,100)</div><div class=\"line\">(9,81)</div><div class=\"line\">(8,64)</div><div class=\"line\">(7,49)</div><div class=\"line\">``` </div><div class=\"line\">**mapPartitions操作键值对 把(i,j) 变成(i,j*j)**</div><div class=\"line\">**scala版本**</div><div class=\"line\">```scala</div><div class=\"line\">    var rdd = sc.parallelize(List((1,1), (1,2), (1,3), (2,1), (2,2), (2,3)))</div><div class=\"line\"></div><div class=\"line\">    def mapPartFunc(iter: Iterator[(Int,Int)]):Iterator[(Int,Int)]=&#123;</div><div class=\"line\">      var res = List[(Int,Int)]()</div><div class=\"line\">      while (iter.hasNext)&#123;</div><div class=\"line\">        val cur = iter.next</div><div class=\"line\">        res=res.::(cur._1,cur._2*cur._2)</div><div class=\"line\">      &#125;</div><div class=\"line\">      res.iterator</div><div class=\"line\">    &#125;</div><div class=\"line\">    val mapPartionsRDD = rdd.mapPartitions(mapPartFunc)</div><div class=\"line\">    mapPartionsRDD.foreach(println( _))</div><div class=\"line\">```  </div><div class=\"line\">**java版本**</div><div class=\"line\">```java</div><div class=\"line\">        JavaRDD&lt;Tuple2&lt;Integer, Integer&gt;&gt; rdd = sc.parallelize(Arrays.asList(new Tuple2&lt;Integer, Integer&gt;(1, 1), new Tuple2&lt;Integer, Integer&gt;(1, 2)</div><div class=\"line\">                , new Tuple2&lt;Integer, Integer&gt;(1, 3), new Tuple2&lt;Integer, Integer&gt;(2, 1)</div><div class=\"line\">                , new Tuple2&lt;Integer, Integer&gt;(2, 2), new Tuple2&lt;Integer, Integer&gt;(2, 3)), 3);</div><div class=\"line\">        JavaPairRDD&lt;Integer, Integer&gt; pairRDD = JavaPairRDD.fromJavaRDD(rdd);</div><div class=\"line\">        JavaRDD&lt;Tuple2&lt;Integer, Integer&gt;&gt; tuple2JavaRDD = pairRDD.mapPartitions(new FlatMapFunction&lt;Iterator&lt;Tuple2&lt;Integer, Integer&gt;&gt;, Tuple2&lt;Integer, Integer&gt;&gt;() &#123;</div><div class=\"line\">            @Override</div><div class=\"line\">            public Iterable&lt;Tuple2&lt;Integer, Integer&gt;&gt; call(Iterator&lt;Tuple2&lt;Integer, Integer&gt;&gt; tp2It) throws Exception &#123;</div><div class=\"line\">                ArrayList&lt;Tuple2&lt;Integer, Integer&gt;&gt; tuple2s = new ArrayList&lt;&gt;();</div><div class=\"line\">                while (tp2It.hasNext())&#123;</div><div class=\"line\">                    Tuple2&lt;Integer, Integer&gt; next = tp2It.next();</div><div class=\"line\">                    tuple2s.add(new Tuple2&lt;Integer, Integer&gt;(next._1,next._2*next._2));</div><div class=\"line\">                &#125;</div><div class=\"line\">                return tuple2s;</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;);</div><div class=\"line\">        tuple2JavaRDD.foreach(new VoidFunction&lt;Tuple2&lt;Integer, Integer&gt;&gt;() &#123;</div><div class=\"line\">            @Override</div><div class=\"line\">            public void call(Tuple2&lt;Integer, Integer&gt; tp2) throws Exception &#123;</div><div class=\"line\">                System.out.println(tp2);</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;);</div><div class=\"line\">-----------------输出---------------</div><div class=\"line\">(1,1)</div><div class=\"line\">(1,4)</div><div class=\"line\">(1,9)</div><div class=\"line\">(2,1)</div><div class=\"line\">(2,4)</div><div class=\"line\">(2,9)</div><div class=\"line\">```  </div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"># **mapPartitionsWithIndex**  </div><div class=\"line\">与mapPartitionWithIndex类似，也是按照分区进行的map操作，不过mapPartitionsWithIndex传入的参数多了一个分区的值，下面举个例子,为统计各个分区中的元素 (稍加修改可以做统计各个分区的数量)</div><div class=\"line\"></div><div class=\"line\">**java**</div><div class=\"line\">```java</div><div class=\"line\">JavaRDD&lt;Integer&gt; rdd = sc.parallelize(Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10), 3);</div><div class=\"line\">        JavaRDD&lt;Tuple2&lt;Integer, Integer&gt;&gt; tuple2JavaRDD = rdd.mapPartitionsWithIndex(new Function2&lt;Integer, Iterator&lt;Integer&gt;, Iterator&lt;Tuple2&lt;Integer, Integer&gt;&gt;&gt;() &#123;</div><div class=\"line\">            @Override</div><div class=\"line\">            public Iterator&lt;Tuple2&lt;Integer, Integer&gt;&gt; call(Integer partIndex, Iterator&lt;Integer&gt; it) throws Exception &#123;</div><div class=\"line\">                ArrayList&lt;Tuple2&lt;Integer, Integer&gt;&gt; tuple2s = new ArrayList&lt;&gt;();</div><div class=\"line\">                while (it.hasNext()) &#123;</div><div class=\"line\">                    int next = it.next();</div><div class=\"line\">                    tuple2s.add(new Tuple2&lt;&gt;(partIndex, next));</div><div class=\"line\">                &#125;</div><div class=\"line\">                return tuple2s.iterator();</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;, false);</div><div class=\"line\"></div><div class=\"line\">        tuple2JavaRDD.foreach(new VoidFunction&lt;Tuple2&lt;Integer, Integer&gt;&gt;() &#123;</div><div class=\"line\">            @Override</div><div class=\"line\">            public void call(Tuple2&lt;Integer, Integer&gt; tp2) throws Exception &#123;</div><div class=\"line\">                System.out.println(tp2);</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;);</div><div class=\"line\">-------输出-------------</div><div class=\"line\">(0,1)</div><div class=\"line\">(0,2)</div><div class=\"line\">(0,3)</div><div class=\"line\">(1,4)</div><div class=\"line\">(1,5)</div><div class=\"line\">(1,6)</div><div class=\"line\">(2,7)</div><div class=\"line\">(2,8)</div><div class=\"line\">(2,9)</div><div class=\"line\">(2,10)</div><div class=\"line\">``` </div><div class=\"line\">**scala**</div></pre></td></tr></table></figure></p>\n<pre><code>val rdd = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10),3)\n\ndef mapPartIndexFunc(i1:Int,iter: Iterator[Int]):Iterator[(Int,Int)]={\n  var res = List[(Int,Int)]()\n  while(iter.hasNext){\n    var next = iter.next()\n    res=res.::(i1,next)\n  }\n  res.iterator\n}\nvar mapPartIndexRDDs = rdd.mapPartitionsWithIndex(mapPartIndexFunc)\n\nmapPartIndexRDDs.foreach(println( _))\n</code></pre><p>————输出——-<br>(0,3)<br>(0,2)<br>(0,1)<br>(1,6)<br>(1,5)<br>(1,4)<br>(2,10)<br>(2,9)<br>(2,8)<br>(2,7)<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div></pre></td><td class=\"code\"><pre><div class=\"line\"></div><div class=\"line\">**mapPartitionsWithIndex 统计键值对中的各个分区的元素**  </div><div class=\"line\">**scala版本**</div><div class=\"line\">```scala</div><div class=\"line\">var rdd = sc.parallelize(List((1,1), (1,2), (2,3), (2,4), (3,5), (3,6),(4,7), (4,8),(5,9), (5,10)),3)</div><div class=\"line\"></div><div class=\"line\">    def mapPartIndexFunc(i1:Int,iter: Iterator[(Int,Int)]):Iterator[(Int,(Int,Int))]=&#123;</div><div class=\"line\">      var res = List[(Int,(Int,Int))]()</div><div class=\"line\">      while(iter.hasNext)&#123;</div><div class=\"line\">        var next = iter.next()</div><div class=\"line\">        res=res.::(i1,next)</div><div class=\"line\">      &#125;</div><div class=\"line\">      res.iterator</div><div class=\"line\">    &#125;</div><div class=\"line\">    val mapPartIndexRDD = rdd.mapPartitionsWithIndex(mapPartIndexFunc)</div><div class=\"line\"></div><div class=\"line\">    mapPartIndexRDD.foreach(println( _))</div><div class=\"line\">-----------输出---------</div><div class=\"line\">(0,(1,1))</div><div class=\"line\">(0,(1,2))</div><div class=\"line\">(0,(2,3))</div><div class=\"line\">(1,(2,4))</div><div class=\"line\">(1,(3,5))</div><div class=\"line\">(1,(3,6))</div><div class=\"line\">(2,(4,7))</div><div class=\"line\">(2,(4,8))</div><div class=\"line\">(2,(5,9))</div><div class=\"line\">(2,(5,10))</div></pre></td></tr></table></figure></p>\n<p><strong>java版本</strong><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div></pre></td><td class=\"code\"><pre><div class=\"line\">        JavaRDD&lt;Tuple2&lt;Integer, Integer&gt;&gt; rdd = sc.parallelize(Arrays.asList(new Tuple2&lt;Integer, Integer&gt;(1, 1), new Tuple2&lt;Integer, Integer&gt;(1, 2)</div><div class=\"line\">                , new Tuple2&lt;Integer, Integer&gt;(2, 3), new Tuple2&lt;Integer, Integer&gt;(2, 4)</div><div class=\"line\">                , new Tuple2&lt;Integer, Integer&gt;(3, 5), new Tuple2&lt;Integer, Integer&gt;(3, 6)</div><div class=\"line\">                , new Tuple2&lt;Integer, Integer&gt;(4, 7), new Tuple2&lt;Integer, Integer&gt;(4, 8)</div><div class=\"line\">                , new Tuple2&lt;Integer, Integer&gt;(5, 9), new Tuple2&lt;Integer, Integer&gt;(5, 10)</div><div class=\"line\">                ), 3);</div><div class=\"line\">        JavaPairRDD&lt;Integer, Integer&gt; pairRDD = JavaPairRDD.fromJavaRDD(rdd);</div><div class=\"line\"></div><div class=\"line\">        JavaRDD&lt;Tuple2&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt;&gt; mapPartitionIndexRDD = pairRDD.mapPartitionsWithIndex(new Function2&lt;Integer, Iterator&lt;Tuple2&lt;Integer, Integer&gt;&gt;, Iterator&lt;Tuple2&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt;&gt;&gt;() &#123;</div><div class=\"line\">            @Override</div><div class=\"line\">            public Iterator&lt;Tuple2&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt;&gt; call(Integer partIndex, Iterator&lt;Tuple2&lt;Integer, Integer&gt;&gt; tuple2Iterator) &#123;</div><div class=\"line\">                ArrayList&lt;Tuple2&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt;&gt; tuple2s = new ArrayList&lt;&gt;();</div><div class=\"line\"></div><div class=\"line\">                while (tuple2Iterator.hasNext()) &#123;</div><div class=\"line\">                    Tuple2&lt;Integer, Integer&gt; next = tuple2Iterator.next();</div><div class=\"line\">                    tuple2s.add(new Tuple2&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt;(partIndex, next));</div><div class=\"line\">                &#125;</div><div class=\"line\">                return tuple2s.iterator();</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;, false);</div><div class=\"line\"></div><div class=\"line\">        mapPartitionIndexRDD.foreach(new VoidFunction&lt;Tuple2&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt;&gt;() &#123;</div><div class=\"line\">            @Override</div><div class=\"line\">            public void call(Tuple2&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt; integerTuple2Tuple2) throws Exception &#123;</div><div class=\"line\">                System.out.println(integerTuple2Tuple2);</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;);</div><div class=\"line\">--------------输出---------</div><div class=\"line\">(0,(1,1))</div><div class=\"line\">(0,(1,2))</div><div class=\"line\">(0,(2,3))</div><div class=\"line\">(1,(2,4))</div><div class=\"line\">(1,(3,5))</div><div class=\"line\">(1,(3,6))</div><div class=\"line\">(2,(4,7))</div><div class=\"line\">(2,(4,8))</div><div class=\"line\">(2,(5,9))</div><div class=\"line\">(2,(5,10))</div></pre></td></tr></table></figure></p>\n<p>mapPartitionsWithIndex 中 第二个参数，true还是false<br>这篇文章有些探讨,<a href=\"http://stackoverflow.com/questions/38048904/how-to-use-function-mappartitionswithindex-in-spark/38049239，个人还未理解\" target=\"_blank\" rel=\"external\">http://stackoverflow.com/questions/38048904/how-to-use-function-mappartitionswithindex-in-spark/38049239，个人还未理解</a> TODO  </p>\n<p>补充： 打印各个分区的操作，可以使用 glom 的方法<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div></pre></td><td class=\"code\"><pre><div class=\"line\">JavaRDD&lt;Tuple2&lt;Integer, Integer&gt;&gt; rdd1 = sc.parallelize(Arrays.asList(<span class=\"keyword\">new</span> Tuple2&lt;Integer, Integer&gt;(<span class=\"number\">1</span>, <span class=\"number\">1</span>), <span class=\"keyword\">new</span> Tuple2&lt;Integer, Integer&gt;(<span class=\"number\">1</span>, <span class=\"number\">2</span>)</div><div class=\"line\">                , <span class=\"keyword\">new</span> Tuple2&lt;Integer, Integer&gt;(<span class=\"number\">2</span>, <span class=\"number\">3</span>), <span class=\"keyword\">new</span> Tuple2&lt;Integer, Integer&gt;(<span class=\"number\">2</span>, <span class=\"number\">4</span>)</div><div class=\"line\">                , <span class=\"keyword\">new</span> Tuple2&lt;Integer, Integer&gt;(<span class=\"number\">3</span>, <span class=\"number\">5</span>), <span class=\"keyword\">new</span> Tuple2&lt;Integer, Integer&gt;(<span class=\"number\">3</span>, <span class=\"number\">6</span>)</div><div class=\"line\">                , <span class=\"keyword\">new</span> Tuple2&lt;Integer, Integer&gt;(<span class=\"number\">4</span>, <span class=\"number\">7</span>), <span class=\"keyword\">new</span> Tuple2&lt;Integer, Integer&gt;(<span class=\"number\">4</span>, <span class=\"number\">8</span>)</div><div class=\"line\">                , <span class=\"keyword\">new</span> Tuple2&lt;Integer, Integer&gt;(<span class=\"number\">5</span>, <span class=\"number\">9</span>), <span class=\"keyword\">new</span> Tuple2&lt;Integer, Integer&gt;(<span class=\"number\">5</span>, <span class=\"number\">10</span>)</div><div class=\"line\">        ), <span class=\"number\">3</span>);</div><div class=\"line\">        JavaPairRDD&lt;Integer, Integer&gt; pairRDD = JavaPairRDD.fromJavaRDD(rdd1);</div><div class=\"line\">        <span class=\"comment\">/*补充：打印各个分区的操作，可以使用 glom 的方法*/</span></div><div class=\"line\">        System.out.println(<span class=\"string\">\"打印各个分区的操作，可以使用 glom 的方法\"</span>);</div><div class=\"line\">        JavaRDD&lt;List&lt;Tuple2&lt;Integer, Integer&gt;&gt;&gt; glom = pairRDD.glom();</div><div class=\"line\">        glom.foreach(<span class=\"keyword\">new</span> VoidFunction&lt;List&lt;Tuple2&lt;Integer, Integer&gt;&gt;&gt;() &#123;</div><div class=\"line\">            <span class=\"meta\">@Override</span></div><div class=\"line\">            <span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title\">call</span><span class=\"params\">(List&lt;Tuple2&lt;Integer, Integer&gt;&gt; tuple2s)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">                System.out.println(tuple2s);</div><div class=\"line\">            &#125;</div><div class=\"line\">        &#125;);</div><div class=\"line\"></div><div class=\"line\"><span class=\"comment\">//************************* 输出 </span></div><div class=\"line\">打印各个分区的操作，可以使用 glom 的方法</div><div class=\"line\">[(<span class=\"number\">1</span>,<span class=\"number\">1</span>), (<span class=\"number\">1</span>,<span class=\"number\">2</span>), (<span class=\"number\">2</span>,<span class=\"number\">3</span>)]</div><div class=\"line\">[(<span class=\"number\">2</span>,<span class=\"number\">4</span>), (<span class=\"number\">3</span>,<span class=\"number\">5</span>), (<span class=\"number\">3</span>,<span class=\"number\">6</span>)]</div><div class=\"line\">[(<span class=\"number\">4</span>,<span class=\"number\">7</span>), (<span class=\"number\">4</span>,<span class=\"number\">8</span>), (<span class=\"number\">5</span>,<span class=\"number\">9</span>), (<span class=\"number\">5</span>,<span class=\"number\">10</span>)]</div></pre></td></tr></table></figure></p>\n"},{"title":"spark RDD算子（十）之PairRDD的Action操作countByKey, collectAsMap","date":"2017-04-05T13:25:21.000Z","author":"kaishun","id":"44","_content":"\n# **countByKey**\ndef countByKey(): Map[K, Long]  \n以RDD{(1, 2),(2,4),(2,5), (3, 4),(3,5), (3, 6)}为例  rdd.countByKey会返回{(1,1),(2,2),(3,3)}\n**scala例子**\n```scala\nscala> val rdd = sc.parallelize(Array((1, 2),(2,4),(2,5), (3, 4),(3,5), (3, 6)))\n\nscala> val countbyKeyRDD = rdd.countByKey()\ncountbyKeyRDD: scala.collection.Map[Int,Long] = Map(1 -> 1, 2 -> 2, 3 -> 3)\n```\n\n**java例子**  \n```java\n    JavaRDD<Tuple2<Integer, Integer>> tupleRDD =\n            sc.parallelize(Arrays.asList(new Tuple2<>(1, 2),\n            new Tuple2<>(2, 4),\n            new Tuple2<>(2, 5),\n            new Tuple2<>(3, 4),\n            new Tuple2<>(3, 5),\n            new Tuple2<>(3, 6)));\n    JavaPairRDD<Integer, Integer> mapRDD = JavaPairRDD.fromJavaRDD(tupleRDD);\n    //countByKey\n    Map<Integer, Object> countByKeyRDD = mapRDD.countByKey();\n    for (Integer i:countByKeyRDD.keySet()) {\n        System.out.println(\"(\"+i+\", \"+countByKeyRDD.get(i)+\")\");\n    }\n/*\n输出  \n(1, 1)\n(3, 3)\n(2, 2)\n\n*/\n```\n\n# **collectAsMap**\n将pair类型(键值对类型)的RDD转换成map, 还是上面的例子\n\n**scala例子**\n```scala\n    scala> val rdd = sc.parallelize(Array((1, 2),(2,4),(2,5), (3, 4),(3,5), (3, 6)))\n    \n    scala> rdd.collectAsMap()\n    res1: scala.collection.Map[Int,Int] = Map(2 -> 5, 1 -> 2, 3 -> 6)\n```\n\n**java例子**  \n```java\n    JavaRDD<Tuple2<Integer, Integer>> tupleRDD =\n            sc.parallelize(Arrays.asList(new Tuple2<>(1, 2),\n            new Tuple2<>(2, 4),\n            new Tuple2<>(2, 5),\n            new Tuple2<>(3, 4),\n            new Tuple2<>(3, 5),\n            new Tuple2<>(3, 6)));\n    JavaPairRDD<Integer, Integer> mapRDD = JavaPairRDD.fromJavaRDD(tupleRDD);\n\n    Map<Integer, Integer> collectMap = mapRDD.collectAsMap();\n```","source":"_posts/spark RDD算子（十）之PairRDD的Action操作countByKey, collectAsMap.md","raw":"---\ntitle: spark RDD算子（十）之PairRDD的Action操作countByKey, collectAsMap\ndate: 2017-04-05 21:25:21\ntags: [spark]\ncategories: [大数据,spark]\nauthor: kaishun\nid: 44\npermalink: spark-rdd-10\n---\n\n# **countByKey**\ndef countByKey(): Map[K, Long]  \n以RDD{(1, 2),(2,4),(2,5), (3, 4),(3,5), (3, 6)}为例  rdd.countByKey会返回{(1,1),(2,2),(3,3)}\n**scala例子**\n```scala\nscala> val rdd = sc.parallelize(Array((1, 2),(2,4),(2,5), (3, 4),(3,5), (3, 6)))\n\nscala> val countbyKeyRDD = rdd.countByKey()\ncountbyKeyRDD: scala.collection.Map[Int,Long] = Map(1 -> 1, 2 -> 2, 3 -> 3)\n```\n\n**java例子**  \n```java\n    JavaRDD<Tuple2<Integer, Integer>> tupleRDD =\n            sc.parallelize(Arrays.asList(new Tuple2<>(1, 2),\n            new Tuple2<>(2, 4),\n            new Tuple2<>(2, 5),\n            new Tuple2<>(3, 4),\n            new Tuple2<>(3, 5),\n            new Tuple2<>(3, 6)));\n    JavaPairRDD<Integer, Integer> mapRDD = JavaPairRDD.fromJavaRDD(tupleRDD);\n    //countByKey\n    Map<Integer, Object> countByKeyRDD = mapRDD.countByKey();\n    for (Integer i:countByKeyRDD.keySet()) {\n        System.out.println(\"(\"+i+\", \"+countByKeyRDD.get(i)+\")\");\n    }\n/*\n输出  \n(1, 1)\n(3, 3)\n(2, 2)\n\n*/\n```\n\n# **collectAsMap**\n将pair类型(键值对类型)的RDD转换成map, 还是上面的例子\n\n**scala例子**\n```scala\n    scala> val rdd = sc.parallelize(Array((1, 2),(2,4),(2,5), (3, 4),(3,5), (3, 6)))\n    \n    scala> rdd.collectAsMap()\n    res1: scala.collection.Map[Int,Int] = Map(2 -> 5, 1 -> 2, 3 -> 6)\n```\n\n**java例子**  \n```java\n    JavaRDD<Tuple2<Integer, Integer>> tupleRDD =\n            sc.parallelize(Arrays.asList(new Tuple2<>(1, 2),\n            new Tuple2<>(2, 4),\n            new Tuple2<>(2, 5),\n            new Tuple2<>(3, 4),\n            new Tuple2<>(3, 5),\n            new Tuple2<>(3, 6)));\n    JavaPairRDD<Integer, Integer> mapRDD = JavaPairRDD.fromJavaRDD(tupleRDD);\n\n    Map<Integer, Integer> collectMap = mapRDD.collectAsMap();\n```","slug":"spark-rdd-10","published":1,"updated":"2018-01-22T15:28:45.931Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcrpnzs0003v2wv335hsrh3u","content":"<h1 id=\"countByKey\"><a href=\"#countByKey\" class=\"headerlink\" title=\"countByKey\"></a><strong>countByKey</strong></h1><p>def countByKey(): Map[K, Long]<br>以RDD{(1, 2),(2,4),(2,5), (3, 4),(3,5), (3, 6)}为例  rdd.countByKey会返回{(1,1),(2,2),(3,3)}<br><strong>scala例子</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> rdd = sc.parallelize(<span class=\"type\">Array</span>((<span class=\"number\">1</span>, <span class=\"number\">2</span>),(<span class=\"number\">2</span>,<span class=\"number\">4</span>),(<span class=\"number\">2</span>,<span class=\"number\">5</span>), (<span class=\"number\">3</span>, <span class=\"number\">4</span>),(<span class=\"number\">3</span>,<span class=\"number\">5</span>), (<span class=\"number\">3</span>, <span class=\"number\">6</span>)))</div><div class=\"line\"></div><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> countbyKeyRDD = rdd.countByKey()</div><div class=\"line\">countbyKeyRDD: scala.collection.<span class=\"type\">Map</span>[<span class=\"type\">Int</span>,<span class=\"type\">Long</span>] = <span class=\"type\">Map</span>(<span class=\"number\">1</span> -&gt; <span class=\"number\">1</span>, <span class=\"number\">2</span> -&gt; <span class=\"number\">2</span>, <span class=\"number\">3</span> -&gt; <span class=\"number\">3</span>)</div></pre></td></tr></table></figure></p>\n<p><strong>java例子</strong><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div></pre></td><td class=\"code\"><pre><div class=\"line\">    JavaRDD&lt;Tuple2&lt;Integer, Integer&gt;&gt; tupleRDD =</div><div class=\"line\">            sc.parallelize(Arrays.asList(<span class=\"keyword\">new</span> Tuple2&lt;&gt;(<span class=\"number\">1</span>, <span class=\"number\">2</span>),</div><div class=\"line\">            <span class=\"keyword\">new</span> Tuple2&lt;&gt;(<span class=\"number\">2</span>, <span class=\"number\">4</span>),</div><div class=\"line\">            <span class=\"keyword\">new</span> Tuple2&lt;&gt;(<span class=\"number\">2</span>, <span class=\"number\">5</span>),</div><div class=\"line\">            <span class=\"keyword\">new</span> Tuple2&lt;&gt;(<span class=\"number\">3</span>, <span class=\"number\">4</span>),</div><div class=\"line\">            <span class=\"keyword\">new</span> Tuple2&lt;&gt;(<span class=\"number\">3</span>, <span class=\"number\">5</span>),</div><div class=\"line\">            <span class=\"keyword\">new</span> Tuple2&lt;&gt;(<span class=\"number\">3</span>, <span class=\"number\">6</span>)));</div><div class=\"line\">    JavaPairRDD&lt;Integer, Integer&gt; mapRDD = JavaPairRDD.fromJavaRDD(tupleRDD);</div><div class=\"line\">    <span class=\"comment\">//countByKey</span></div><div class=\"line\">    Map&lt;Integer, Object&gt; countByKeyRDD = mapRDD.countByKey();</div><div class=\"line\">    <span class=\"keyword\">for</span> (Integer i:countByKeyRDD.keySet()) &#123;</div><div class=\"line\">        System.out.println(<span class=\"string\">\"(\"</span>+i+<span class=\"string\">\", \"</span>+countByKeyRDD.get(i)+<span class=\"string\">\")\"</span>);</div><div class=\"line\">    &#125;</div><div class=\"line\"><span class=\"comment\">/*</span></div><div class=\"line\"><span class=\"comment\">输出  </span></div><div class=\"line\"><span class=\"comment\">(1, 1)</span></div><div class=\"line\"><span class=\"comment\">(3, 3)</span></div><div class=\"line\"><span class=\"comment\">(2, 2)</span></div><div class=\"line\"><span class=\"comment\"></span></div><div class=\"line\"><span class=\"comment\">*/</span></div></pre></td></tr></table></figure></p>\n<h1 id=\"collectAsMap\"><a href=\"#collectAsMap\" class=\"headerlink\" title=\"collectAsMap\"></a><strong>collectAsMap</strong></h1><p>将pair类型(键值对类型)的RDD转换成map, 还是上面的例子</p>\n<p><strong>scala例子</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> rdd = sc.parallelize(<span class=\"type\">Array</span>((<span class=\"number\">1</span>, <span class=\"number\">2</span>),(<span class=\"number\">2</span>,<span class=\"number\">4</span>),(<span class=\"number\">2</span>,<span class=\"number\">5</span>), (<span class=\"number\">3</span>, <span class=\"number\">4</span>),(<span class=\"number\">3</span>,<span class=\"number\">5</span>), (<span class=\"number\">3</span>, <span class=\"number\">6</span>)))</div><div class=\"line\"></div><div class=\"line\">scala&gt; rdd.collectAsMap()</div><div class=\"line\">res1: scala.collection.<span class=\"type\">Map</span>[<span class=\"type\">Int</span>,<span class=\"type\">Int</span>] = <span class=\"type\">Map</span>(<span class=\"number\">2</span> -&gt; <span class=\"number\">5</span>, <span class=\"number\">1</span> -&gt; <span class=\"number\">2</span>, <span class=\"number\">3</span> -&gt; <span class=\"number\">6</span>)</div></pre></td></tr></table></figure></p>\n<p><strong>java例子</strong><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">JavaRDD&lt;Tuple2&lt;Integer, Integer&gt;&gt; tupleRDD =</div><div class=\"line\">        sc.parallelize(Arrays.asList(<span class=\"keyword\">new</span> Tuple2&lt;&gt;(<span class=\"number\">1</span>, <span class=\"number\">2</span>),</div><div class=\"line\">        <span class=\"keyword\">new</span> Tuple2&lt;&gt;(<span class=\"number\">2</span>, <span class=\"number\">4</span>),</div><div class=\"line\">        <span class=\"keyword\">new</span> Tuple2&lt;&gt;(<span class=\"number\">2</span>, <span class=\"number\">5</span>),</div><div class=\"line\">        <span class=\"keyword\">new</span> Tuple2&lt;&gt;(<span class=\"number\">3</span>, <span class=\"number\">4</span>),</div><div class=\"line\">        <span class=\"keyword\">new</span> Tuple2&lt;&gt;(<span class=\"number\">3</span>, <span class=\"number\">5</span>),</div><div class=\"line\">        <span class=\"keyword\">new</span> Tuple2&lt;&gt;(<span class=\"number\">3</span>, <span class=\"number\">6</span>)));</div><div class=\"line\">JavaPairRDD&lt;Integer, Integer&gt; mapRDD = JavaPairRDD.fromJavaRDD(tupleRDD);</div><div class=\"line\"></div><div class=\"line\">Map&lt;Integer, Integer&gt; collectMap = mapRDD.collectAsMap();</div></pre></td></tr></table></figure></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"countByKey\"><a href=\"#countByKey\" class=\"headerlink\" title=\"countByKey\"></a><strong>countByKey</strong></h1><p>def countByKey(): Map[K, Long]<br>以RDD{(1, 2),(2,4),(2,5), (3, 4),(3,5), (3, 6)}为例  rdd.countByKey会返回{(1,1),(2,2),(3,3)}<br><strong>scala例子</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> rdd = sc.parallelize(<span class=\"type\">Array</span>((<span class=\"number\">1</span>, <span class=\"number\">2</span>),(<span class=\"number\">2</span>,<span class=\"number\">4</span>),(<span class=\"number\">2</span>,<span class=\"number\">5</span>), (<span class=\"number\">3</span>, <span class=\"number\">4</span>),(<span class=\"number\">3</span>,<span class=\"number\">5</span>), (<span class=\"number\">3</span>, <span class=\"number\">6</span>)))</div><div class=\"line\"></div><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> countbyKeyRDD = rdd.countByKey()</div><div class=\"line\">countbyKeyRDD: scala.collection.<span class=\"type\">Map</span>[<span class=\"type\">Int</span>,<span class=\"type\">Long</span>] = <span class=\"type\">Map</span>(<span class=\"number\">1</span> -&gt; <span class=\"number\">1</span>, <span class=\"number\">2</span> -&gt; <span class=\"number\">2</span>, <span class=\"number\">3</span> -&gt; <span class=\"number\">3</span>)</div></pre></td></tr></table></figure></p>\n<p><strong>java例子</strong><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div></pre></td><td class=\"code\"><pre><div class=\"line\">    JavaRDD&lt;Tuple2&lt;Integer, Integer&gt;&gt; tupleRDD =</div><div class=\"line\">            sc.parallelize(Arrays.asList(<span class=\"keyword\">new</span> Tuple2&lt;&gt;(<span class=\"number\">1</span>, <span class=\"number\">2</span>),</div><div class=\"line\">            <span class=\"keyword\">new</span> Tuple2&lt;&gt;(<span class=\"number\">2</span>, <span class=\"number\">4</span>),</div><div class=\"line\">            <span class=\"keyword\">new</span> Tuple2&lt;&gt;(<span class=\"number\">2</span>, <span class=\"number\">5</span>),</div><div class=\"line\">            <span class=\"keyword\">new</span> Tuple2&lt;&gt;(<span class=\"number\">3</span>, <span class=\"number\">4</span>),</div><div class=\"line\">            <span class=\"keyword\">new</span> Tuple2&lt;&gt;(<span class=\"number\">3</span>, <span class=\"number\">5</span>),</div><div class=\"line\">            <span class=\"keyword\">new</span> Tuple2&lt;&gt;(<span class=\"number\">3</span>, <span class=\"number\">6</span>)));</div><div class=\"line\">    JavaPairRDD&lt;Integer, Integer&gt; mapRDD = JavaPairRDD.fromJavaRDD(tupleRDD);</div><div class=\"line\">    <span class=\"comment\">//countByKey</span></div><div class=\"line\">    Map&lt;Integer, Object&gt; countByKeyRDD = mapRDD.countByKey();</div><div class=\"line\">    <span class=\"keyword\">for</span> (Integer i:countByKeyRDD.keySet()) &#123;</div><div class=\"line\">        System.out.println(<span class=\"string\">\"(\"</span>+i+<span class=\"string\">\", \"</span>+countByKeyRDD.get(i)+<span class=\"string\">\")\"</span>);</div><div class=\"line\">    &#125;</div><div class=\"line\"><span class=\"comment\">/*</span></div><div class=\"line\"><span class=\"comment\">输出  </span></div><div class=\"line\"><span class=\"comment\">(1, 1)</span></div><div class=\"line\"><span class=\"comment\">(3, 3)</span></div><div class=\"line\"><span class=\"comment\">(2, 2)</span></div><div class=\"line\"><span class=\"comment\"></span></div><div class=\"line\"><span class=\"comment\">*/</span></div></pre></td></tr></table></figure></p>\n<h1 id=\"collectAsMap\"><a href=\"#collectAsMap\" class=\"headerlink\" title=\"collectAsMap\"></a><strong>collectAsMap</strong></h1><p>将pair类型(键值对类型)的RDD转换成map, 还是上面的例子</p>\n<p><strong>scala例子</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">scala&gt; <span class=\"keyword\">val</span> rdd = sc.parallelize(<span class=\"type\">Array</span>((<span class=\"number\">1</span>, <span class=\"number\">2</span>),(<span class=\"number\">2</span>,<span class=\"number\">4</span>),(<span class=\"number\">2</span>,<span class=\"number\">5</span>), (<span class=\"number\">3</span>, <span class=\"number\">4</span>),(<span class=\"number\">3</span>,<span class=\"number\">5</span>), (<span class=\"number\">3</span>, <span class=\"number\">6</span>)))</div><div class=\"line\"></div><div class=\"line\">scala&gt; rdd.collectAsMap()</div><div class=\"line\">res1: scala.collection.<span class=\"type\">Map</span>[<span class=\"type\">Int</span>,<span class=\"type\">Int</span>] = <span class=\"type\">Map</span>(<span class=\"number\">2</span> -&gt; <span class=\"number\">5</span>, <span class=\"number\">1</span> -&gt; <span class=\"number\">2</span>, <span class=\"number\">3</span> -&gt; <span class=\"number\">6</span>)</div></pre></td></tr></table></figure></p>\n<p><strong>java例子</strong><br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">JavaRDD&lt;Tuple2&lt;Integer, Integer&gt;&gt; tupleRDD =</div><div class=\"line\">        sc.parallelize(Arrays.asList(<span class=\"keyword\">new</span> Tuple2&lt;&gt;(<span class=\"number\">1</span>, <span class=\"number\">2</span>),</div><div class=\"line\">        <span class=\"keyword\">new</span> Tuple2&lt;&gt;(<span class=\"number\">2</span>, <span class=\"number\">4</span>),</div><div class=\"line\">        <span class=\"keyword\">new</span> Tuple2&lt;&gt;(<span class=\"number\">2</span>, <span class=\"number\">5</span>),</div><div class=\"line\">        <span class=\"keyword\">new</span> Tuple2&lt;&gt;(<span class=\"number\">3</span>, <span class=\"number\">4</span>),</div><div class=\"line\">        <span class=\"keyword\">new</span> Tuple2&lt;&gt;(<span class=\"number\">3</span>, <span class=\"number\">5</span>),</div><div class=\"line\">        <span class=\"keyword\">new</span> Tuple2&lt;&gt;(<span class=\"number\">3</span>, <span class=\"number\">6</span>)));</div><div class=\"line\">JavaPairRDD&lt;Integer, Integer&gt; mapRDD = JavaPairRDD.fromJavaRDD(tupleRDD);</div><div class=\"line\"></div><div class=\"line\">Map&lt;Integer, Integer&gt; collectMap = mapRDD.collectAsMap();</div></pre></td></tr></table></figure></p>\n"},{"title":"spark 从1.x 转到2.x，编写程序的的一些区别","date":"2017-02-05T13:25:21.000Z","author":"kaishun","id":"32","_content":"\nspark 2.x 版本相对于1.x版本，有挺多地方的修改，一是类似于flatMapRDD 中 iteator iteatable之类的区别  \n2是类似于dataset的一些问题\n\n下面是2.x版本的iteatable和iteartor之类的区别，只举例了两个，其实只要和iteartor有关的都有了修改\n## flatMap\n```java\n        JavaRDD<String> flatMapRDD = lines.flatMap(new FlatMapFunction<String, String>() {\n            @Override\n            public Iterator<String> call(String s) throws Exception {\n                String[] split = s.split(\"\\\\s+\");\n                return Arrays.asList(split).iterator();\n            }\n        });\n```\n\n## flatMapToPair  java\n```java\n        JavaPairRDD<String, Integer> wordPairRDD = lines.flatMapToPair(new PairFlatMapFunction<String, String, Integer>() {\n            @Override\n            public Iterator<Tuple2<String, Integer>> call(String s) throws Exception {\n                ArrayList<Tuple2<String, Integer>> tpLists = new ArrayList<Tuple2<String, Integer>>();\n                String[] split = s.split(\"\\\\s+\");\n                for (int i = 0; i <split.length ; i++) {\n                    Tuple2 tp = new Tuple2<String,Integer>(split[i], 1);\n                    tpLists.add(tp);\n                }\n                return tpLists.iterator();\n            }\n        });\n```  \n\n## spark中初始化driver的区别\nspark2.0中，可以使用session来创建一个sparkContext作为一个新的入口，具体参考例子就可以了\n\n## jar包的区别\nspark2.x版本中不再有spark-assembly-xxx jar包，jar包全都在.jars 中 \n\n## scala的版本\nspark2.x版本的，对scala的版本最低要求是2.11\n## 下面是sql中的区别\n2.x 版本的 sparkSql中  \n1.x 版本的 DataFrame与Dataset 统一化了，只剩下DataSet了，具体的也可以直接参看官方给的spark sql 的例子即可 \n具体 todo\n","source":"_posts/spark 从1.x 转到2.x，编写程序的的一些区别.md","raw":"---\ntitle: spark 从1.x 转到2.x，编写程序的的一些区别\ndate: 2017-02-05 21:25:21\ntags: [spark]\ncategories: [大数据,spark]\nauthor: kaishun\nid: 32\npermalink: spark-v1-v2\n---\n\nspark 2.x 版本相对于1.x版本，有挺多地方的修改，一是类似于flatMapRDD 中 iteator iteatable之类的区别  \n2是类似于dataset的一些问题\n\n下面是2.x版本的iteatable和iteartor之类的区别，只举例了两个，其实只要和iteartor有关的都有了修改\n## flatMap\n```java\n        JavaRDD<String> flatMapRDD = lines.flatMap(new FlatMapFunction<String, String>() {\n            @Override\n            public Iterator<String> call(String s) throws Exception {\n                String[] split = s.split(\"\\\\s+\");\n                return Arrays.asList(split).iterator();\n            }\n        });\n```\n\n## flatMapToPair  java\n```java\n        JavaPairRDD<String, Integer> wordPairRDD = lines.flatMapToPair(new PairFlatMapFunction<String, String, Integer>() {\n            @Override\n            public Iterator<Tuple2<String, Integer>> call(String s) throws Exception {\n                ArrayList<Tuple2<String, Integer>> tpLists = new ArrayList<Tuple2<String, Integer>>();\n                String[] split = s.split(\"\\\\s+\");\n                for (int i = 0; i <split.length ; i++) {\n                    Tuple2 tp = new Tuple2<String,Integer>(split[i], 1);\n                    tpLists.add(tp);\n                }\n                return tpLists.iterator();\n            }\n        });\n```  \n\n## spark中初始化driver的区别\nspark2.0中，可以使用session来创建一个sparkContext作为一个新的入口，具体参考例子就可以了\n\n## jar包的区别\nspark2.x版本中不再有spark-assembly-xxx jar包，jar包全都在.jars 中 \n\n## scala的版本\nspark2.x版本的，对scala的版本最低要求是2.11\n## 下面是sql中的区别\n2.x 版本的 sparkSql中  \n1.x 版本的 DataFrame与Dataset 统一化了，只剩下DataSet了，具体的也可以直接参看官方给的spark sql 的例子即可 \n具体 todo\n","slug":"spark-v1-v2","published":1,"updated":"2018-01-22T15:20:17.654Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcrpnzsf003y2wv3izf8ugti","content":"<p>spark 2.x 版本相对于1.x版本，有挺多地方的修改，一是类似于flatMapRDD 中 iteator iteatable之类的区别<br>2是类似于dataset的一些问题</p>\n<p>下面是2.x版本的iteatable和iteartor之类的区别，只举例了两个，其实只要和iteartor有关的都有了修改</p>\n<h2 id=\"flatMap\"><a href=\"#flatMap\" class=\"headerlink\" title=\"flatMap\"></a>flatMap</h2><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">JavaRDD&lt;String&gt; flatMapRDD = lines.flatMap(<span class=\"keyword\">new</span> FlatMapFunction&lt;String, String&gt;() &#123;</div><div class=\"line\">    <span class=\"meta\">@Override</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> Iterator&lt;String&gt; <span class=\"title\">call</span><span class=\"params\">(String s)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">        String[] split = s.split(<span class=\"string\">\"\\\\s+\"</span>);</div><div class=\"line\">        <span class=\"keyword\">return</span> Arrays.asList(split).iterator();</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;);</div></pre></td></tr></table></figure>\n<h2 id=\"flatMapToPair-java\"><a href=\"#flatMapToPair-java\" class=\"headerlink\" title=\"flatMapToPair  java\"></a>flatMapToPair  java</h2><pre><code class=\"java\">JavaPairRDD&lt;String, Integer&gt; wordPairRDD = lines.flatMapToPair(<span class=\"keyword\">new</span> PairFlatMapFunction&lt;String, String, Integer&gt;() {\n    <span class=\"meta\">@Override</span>\n    <span class=\"keyword\">public</span> Iterator&lt;Tuple2&lt;String, Integer&gt;&gt; call(String s) <span class=\"keyword\">throws</span> Exception {\n        ArrayList&lt;Tuple2&lt;String, Integer&gt;&gt; tpLists = <span class=\"keyword\">new</span> ArrayList&lt;Tuple2&lt;String, Integer&gt;&gt;();\n        String[] split = s.split(<span class=\"string\">\"\\\\s+\"</span>);\n        <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt;split.length ; i++) {\n            Tuple2 tp = <span class=\"keyword\">new</span> Tuple2&lt;String,Integer&gt;(split[i], <span class=\"number\">1</span>);\n            tpLists.add(tp);\n        }\n        <span class=\"keyword\">return</span> tpLists.iterator();\n    }\n});\n</code></pre>\n<h2 id=\"spark中初始化driver的区别\"><a href=\"#spark中初始化driver的区别\" class=\"headerlink\" title=\"spark中初始化driver的区别\"></a>spark中初始化driver的区别</h2><p>spark2.0中，可以使用session来创建一个sparkContext作为一个新的入口，具体参考例子就可以了</p>\n<h2 id=\"jar包的区别\"><a href=\"#jar包的区别\" class=\"headerlink\" title=\"jar包的区别\"></a>jar包的区别</h2><p>spark2.x版本中不再有spark-assembly-xxx jar包，jar包全都在.jars 中 </p>\n<h2 id=\"scala的版本\"><a href=\"#scala的版本\" class=\"headerlink\" title=\"scala的版本\"></a>scala的版本</h2><p>spark2.x版本的，对scala的版本最低要求是2.11</p>\n<h2 id=\"下面是sql中的区别\"><a href=\"#下面是sql中的区别\" class=\"headerlink\" title=\"下面是sql中的区别\"></a>下面是sql中的区别</h2><p>2.x 版本的 sparkSql中<br>1.x 版本的 DataFrame与Dataset 统一化了，只剩下DataSet了，具体的也可以直接参看官方给的spark sql 的例子即可<br>具体 todo</p>\n","site":{"data":{}},"excerpt":"","more":"<p>spark 2.x 版本相对于1.x版本，有挺多地方的修改，一是类似于flatMapRDD 中 iteator iteatable之类的区别<br>2是类似于dataset的一些问题</p>\n<p>下面是2.x版本的iteatable和iteartor之类的区别，只举例了两个，其实只要和iteartor有关的都有了修改</p>\n<h2 id=\"flatMap\"><a href=\"#flatMap\" class=\"headerlink\" title=\"flatMap\"></a>flatMap</h2><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div></pre></td><td class=\"code\"><pre><div class=\"line\">JavaRDD&lt;String&gt; flatMapRDD = lines.flatMap(<span class=\"keyword\">new</span> FlatMapFunction&lt;String, String&gt;() &#123;</div><div class=\"line\">    <span class=\"meta\">@Override</span></div><div class=\"line\">    <span class=\"function\"><span class=\"keyword\">public</span> Iterator&lt;String&gt; <span class=\"title\">call</span><span class=\"params\">(String s)</span> <span class=\"keyword\">throws</span> Exception </span>&#123;</div><div class=\"line\">        String[] split = s.split(<span class=\"string\">\"\\\\s+\"</span>);</div><div class=\"line\">        <span class=\"keyword\">return</span> Arrays.asList(split).iterator();</div><div class=\"line\">    &#125;</div><div class=\"line\">&#125;);</div></pre></td></tr></table></figure>\n<h2 id=\"flatMapToPair-java\"><a href=\"#flatMapToPair-java\" class=\"headerlink\" title=\"flatMapToPair  java\"></a>flatMapToPair  java</h2><pre><code class=\"java\">JavaPairRDD&lt;String, Integer&gt; wordPairRDD = lines.flatMapToPair(<span class=\"keyword\">new</span> PairFlatMapFunction&lt;String, String, Integer&gt;() {\n    <span class=\"meta\">@Override</span>\n    <span class=\"keyword\">public</span> Iterator&lt;Tuple2&lt;String, Integer&gt;&gt; call(String s) <span class=\"keyword\">throws</span> Exception {\n        ArrayList&lt;Tuple2&lt;String, Integer&gt;&gt; tpLists = <span class=\"keyword\">new</span> ArrayList&lt;Tuple2&lt;String, Integer&gt;&gt;();\n        String[] split = s.split(<span class=\"string\">\"\\\\s+\"</span>);\n        <span class=\"keyword\">for</span> (<span class=\"keyword\">int</span> i = <span class=\"number\">0</span>; i &lt;split.length ; i++) {\n            Tuple2 tp = <span class=\"keyword\">new</span> Tuple2&lt;String,Integer&gt;(split[i], <span class=\"number\">1</span>);\n            tpLists.add(tp);\n        }\n        <span class=\"keyword\">return</span> tpLists.iterator();\n    }\n});\n</code></pre>\n<h2 id=\"spark中初始化driver的区别\"><a href=\"#spark中初始化driver的区别\" class=\"headerlink\" title=\"spark中初始化driver的区别\"></a>spark中初始化driver的区别</h2><p>spark2.0中，可以使用session来创建一个sparkContext作为一个新的入口，具体参考例子就可以了</p>\n<h2 id=\"jar包的区别\"><a href=\"#jar包的区别\" class=\"headerlink\" title=\"jar包的区别\"></a>jar包的区别</h2><p>spark2.x版本中不再有spark-assembly-xxx jar包，jar包全都在.jars 中 </p>\n<h2 id=\"scala的版本\"><a href=\"#scala的版本\" class=\"headerlink\" title=\"scala的版本\"></a>scala的版本</h2><p>spark2.x版本的，对scala的版本最低要求是2.11</p>\n<h2 id=\"下面是sql中的区别\"><a href=\"#下面是sql中的区别\" class=\"headerlink\" title=\"下面是sql中的区别\"></a>下面是sql中的区别</h2><p>2.x 版本的 sparkSql中<br>1.x 版本的 DataFrame与Dataset 统一化了，只剩下DataSet了，具体的也可以直接参看官方给的spark sql 的例子即可<br>具体 todo</p>\n"},{"title":"虚心竹有低头叶 傲骨梅无仰面花","date":"2017-04-23T13:25:21.000Z","author":"kaishun","id":"49","blogexcerpt":"虚心竹有低头叶，傲骨梅无仰面花.不讲大道理，只说工作事今天，2017年6月21日，曾经花了一两个月的时间，设计并且实现了一个数据处理的程序，做是做完了，可是一当用于生产，遍出现各种问题。主要问题有以下几点","_content":"\n 虚心竹有低头叶； \n傲骨梅无仰面花.\n——郑燮  \n\n\n不讲大道理，只说工作事  \n今天，2017年6月21日，曾经花了一两个月的时间，设计并且实现了一个数据处理的程序，做是做完了，可是一当用于生产，遍出现各种问题。主要问题有以下几点  \n## **问题的所在**\n**1. 程序处理的性能**  \n性能不堪入目。某当个大市的前期数据处理就需要两天。  \n**2. 代码各种bug**  \n总是有些深藏中的bug未被发现，在使用时才发现，从而需要急急忙忙的修改，并且数据又要从新生成\n**3. 代码的质量**  \n代码质量很低，代码挺乱，需要重构的方法很多  \n**4. 真的很难测试**  \n程序写出来，是否满足需求，需要大量的测试，确实是比较难\n\n## **曾做过的**\n**1. 针对程序处理的性能**\n**读写问题：**：发现在写的地方很花时间，有大量的小文件需要写，想进办法依然无法解决  \n**spark处理：**甚至想过使用spark完成，也做了一份spark的程序，使用集群处理，但是数据需要放在hdfs上读写，非常麻烦\n**2. 针对各种bug**\n平时一个人如何也发现不了问题，需要在使用过程中发现问题，解决问题\n**3. 代码质量**\n想过需要提高代码的可读性，想过重构，但是比较随便，只提取了部分方法。代码依然臃肿\n**4. 测试问题**  \n使用小数据测试，但是由于数据复杂性，对于某些数据可能成功，对于其他数据，若格式稍有变化，不保证一定可用\n\n## **这两天的碰到的人或事**\n**c哥：** 大神，我的项目负责人，现在带我的师傅。很多事都只有他才能处理，c哥做事极其负责，对于这个程序，  给予了非常多的建议与思路，可以说，这整个程序基本都是按照他的思路完成的，刚开始觉得，再难的问题，只要想通了，也就清晰了。   \n**sf兄：**高级工程师，低调大神，业务熟悉，善于优化， 协助我优化代码，sf听我思路，一下就能找到问题的所在，一看代码，立刻动手帮我把代码翻了个底儿朝天，刚开始挺不开心的，我的代码思路怎么的好，也不能从头到尾给我修改吧，逐渐的，我体会到了大神的用意，我之前的代码实在是太差了，一些最基本的性能的优化我都忽略，代码也不规范，比如map中，key能用int就不用string，经常需要拼接的字符串，千万不能随意使用string，尝试使用StringBuffer或者List，代码方法分散，做到真正的面相对象，主方法主思路，代码量不能多，多提炼方法出去。保证代码的清晰整洁，sf给我修改并且讲解大半天，真的是受益匪浅。  \n**y总：** 10多年的大神，总工之一，依然坚守在研发岗位，y总甚至提出了另外的一种处理方法，最主要的原因是我们对业务的不熟悉，y总使用的完全不一样的方法，目前觉得能极大的提高效率。\n**shihong：**同事，朋友，差不多同时进公司，安排的任务基本是一起完成，合作非常愉快，这个程序是我们合作完成的，遇到不清晰的，我们便相互帮助，1+1>2\n\n## **从这些人或事的感悟**\n1. 虚心竹有低头叶 傲骨梅无仰面花， 向优秀的人请教，多他人的的优缺点。\n2. 最基础的知识还不够牢固。需要一步一步，脚踏实地的积累。  \n3. 思维这种东西，需要使劲的去想，实在想不通，换一个方向使劲去想。\n4. 写程序之前，其实内心已经想好了需要如何写了，这时候，不要忙，先想想有没有什么问题，有没有更好的方法，真的全部想通后再开始码。做事也一样。\n5. 写程序，特别是做数据，先要知道数据源，先想好要如何测试，先想好可能的测试结果。\n6. 代码太乱，需要多看优秀代码，不断向前辈同事学习，不断思考自己写的代码，必须让自己的代码给其他人也能看懂。  \n7. 多多分享，分享其实也是学习与复习最有效的办法之一  \n8. 多感谢家人与同事，朋友。与同事愉快的合作，感谢他们提出的意见，不能太坚持自己的想法，先接受他人的意见，再看看和自己的想法有哪些不同，使用更好的，如果觉得自己的想法更好，可以提出来一起讨论。","source":"_posts/虚心竹有低头叶 傲骨梅无仰面花.md","raw":"---\ntitle: 虚心竹有低头叶 傲骨梅无仰面花\ndate: 2017-04-23 21:25:21\ntags: [随笔]\ncategories: [随笔]\nauthor: kaishun\nid: 49\npermalink: mylife1\nblogexcerpt: 虚心竹有低头叶，傲骨梅无仰面花.不讲大道理，只说工作事今天，2017年6月21日，曾经花了一两个月的时间，设计并且实现了一个数据处理的程序，做是做完了，可是一当用于生产，遍出现各种问题。主要问题有以下几点\n---\n\n 虚心竹有低头叶； \n傲骨梅无仰面花.\n——郑燮  \n\n\n不讲大道理，只说工作事  \n今天，2017年6月21日，曾经花了一两个月的时间，设计并且实现了一个数据处理的程序，做是做完了，可是一当用于生产，遍出现各种问题。主要问题有以下几点  \n## **问题的所在**\n**1. 程序处理的性能**  \n性能不堪入目。某当个大市的前期数据处理就需要两天。  \n**2. 代码各种bug**  \n总是有些深藏中的bug未被发现，在使用时才发现，从而需要急急忙忙的修改，并且数据又要从新生成\n**3. 代码的质量**  \n代码质量很低，代码挺乱，需要重构的方法很多  \n**4. 真的很难测试**  \n程序写出来，是否满足需求，需要大量的测试，确实是比较难\n\n## **曾做过的**\n**1. 针对程序处理的性能**\n**读写问题：**：发现在写的地方很花时间，有大量的小文件需要写，想进办法依然无法解决  \n**spark处理：**甚至想过使用spark完成，也做了一份spark的程序，使用集群处理，但是数据需要放在hdfs上读写，非常麻烦\n**2. 针对各种bug**\n平时一个人如何也发现不了问题，需要在使用过程中发现问题，解决问题\n**3. 代码质量**\n想过需要提高代码的可读性，想过重构，但是比较随便，只提取了部分方法。代码依然臃肿\n**4. 测试问题**  \n使用小数据测试，但是由于数据复杂性，对于某些数据可能成功，对于其他数据，若格式稍有变化，不保证一定可用\n\n## **这两天的碰到的人或事**\n**c哥：** 大神，我的项目负责人，现在带我的师傅。很多事都只有他才能处理，c哥做事极其负责，对于这个程序，  给予了非常多的建议与思路，可以说，这整个程序基本都是按照他的思路完成的，刚开始觉得，再难的问题，只要想通了，也就清晰了。   \n**sf兄：**高级工程师，低调大神，业务熟悉，善于优化， 协助我优化代码，sf听我思路，一下就能找到问题的所在，一看代码，立刻动手帮我把代码翻了个底儿朝天，刚开始挺不开心的，我的代码思路怎么的好，也不能从头到尾给我修改吧，逐渐的，我体会到了大神的用意，我之前的代码实在是太差了，一些最基本的性能的优化我都忽略，代码也不规范，比如map中，key能用int就不用string，经常需要拼接的字符串，千万不能随意使用string，尝试使用StringBuffer或者List，代码方法分散，做到真正的面相对象，主方法主思路，代码量不能多，多提炼方法出去。保证代码的清晰整洁，sf给我修改并且讲解大半天，真的是受益匪浅。  \n**y总：** 10多年的大神，总工之一，依然坚守在研发岗位，y总甚至提出了另外的一种处理方法，最主要的原因是我们对业务的不熟悉，y总使用的完全不一样的方法，目前觉得能极大的提高效率。\n**shihong：**同事，朋友，差不多同时进公司，安排的任务基本是一起完成，合作非常愉快，这个程序是我们合作完成的，遇到不清晰的，我们便相互帮助，1+1>2\n\n## **从这些人或事的感悟**\n1. 虚心竹有低头叶 傲骨梅无仰面花， 向优秀的人请教，多他人的的优缺点。\n2. 最基础的知识还不够牢固。需要一步一步，脚踏实地的积累。  \n3. 思维这种东西，需要使劲的去想，实在想不通，换一个方向使劲去想。\n4. 写程序之前，其实内心已经想好了需要如何写了，这时候，不要忙，先想想有没有什么问题，有没有更好的方法，真的全部想通后再开始码。做事也一样。\n5. 写程序，特别是做数据，先要知道数据源，先想好要如何测试，先想好可能的测试结果。\n6. 代码太乱，需要多看优秀代码，不断向前辈同事学习，不断思考自己写的代码，必须让自己的代码给其他人也能看懂。  \n7. 多多分享，分享其实也是学习与复习最有效的办法之一  \n8. 多感谢家人与同事，朋友。与同事愉快的合作，感谢他们提出的意见，不能太坚持自己的想法，先接受他人的意见，再看看和自己的想法有哪些不同，使用更好的，如果觉得自己的想法更好，可以提出来一起讨论。","slug":"mylife1","published":1,"updated":"2018-01-23T14:11:30.549Z","_id":"cjcrpnzsn00432wv36tlf5f62","comments":1,"layout":"post","photos":[],"link":"","content":"<p> 虚心竹有低头叶；<br>傲骨梅无仰面花.<br>——郑燮  </p>\n<p>不讲大道理，只说工作事<br>今天，2017年6月21日，曾经花了一两个月的时间，设计并且实现了一个数据处理的程序，做是做完了，可是一当用于生产，遍出现各种问题。主要问题有以下几点  </p>\n<h2 id=\"问题的所在\"><a href=\"#问题的所在\" class=\"headerlink\" title=\"问题的所在\"></a><strong>问题的所在</strong></h2><p><strong>1. 程序处理的性能</strong><br>性能不堪入目。某当个大市的前期数据处理就需要两天。<br><strong>2. 代码各种bug</strong><br>总是有些深藏中的bug未被发现，在使用时才发现，从而需要急急忙忙的修改，并且数据又要从新生成<br><strong>3. 代码的质量</strong><br>代码质量很低，代码挺乱，需要重构的方法很多<br><strong>4. 真的很难测试</strong><br>程序写出来，是否满足需求，需要大量的测试，确实是比较难</p>\n<h2 id=\"曾做过的\"><a href=\"#曾做过的\" class=\"headerlink\" title=\"曾做过的\"></a><strong>曾做过的</strong></h2><p><strong>1. 针对程序处理的性能</strong><br><strong>读写问题：</strong>：发现在写的地方很花时间，有大量的小文件需要写，想进办法依然无法解决<br><strong>spark处理：</strong>甚至想过使用spark完成，也做了一份spark的程序，使用集群处理，但是数据需要放在hdfs上读写，非常麻烦<br><strong>2. 针对各种bug</strong><br>平时一个人如何也发现不了问题，需要在使用过程中发现问题，解决问题<br><strong>3. 代码质量</strong><br>想过需要提高代码的可读性，想过重构，但是比较随便，只提取了部分方法。代码依然臃肿<br><strong>4. 测试问题</strong><br>使用小数据测试，但是由于数据复杂性，对于某些数据可能成功，对于其他数据，若格式稍有变化，不保证一定可用</p>\n<h2 id=\"这两天的碰到的人或事\"><a href=\"#这两天的碰到的人或事\" class=\"headerlink\" title=\"这两天的碰到的人或事\"></a><strong>这两天的碰到的人或事</strong></h2><p><strong>c哥：</strong> 大神，我的项目负责人，现在带我的师傅。很多事都只有他才能处理，c哥做事极其负责，对于这个程序，  给予了非常多的建议与思路，可以说，这整个程序基本都是按照他的思路完成的，刚开始觉得，再难的问题，只要想通了，也就清晰了。<br><strong>sf兄：</strong>高级工程师，低调大神，业务熟悉，善于优化， 协助我优化代码，sf听我思路，一下就能找到问题的所在，一看代码，立刻动手帮我把代码翻了个底儿朝天，刚开始挺不开心的，我的代码思路怎么的好，也不能从头到尾给我修改吧，逐渐的，我体会到了大神的用意，我之前的代码实在是太差了，一些最基本的性能的优化我都忽略，代码也不规范，比如map中，key能用int就不用string，经常需要拼接的字符串，千万不能随意使用string，尝试使用StringBuffer或者List，代码方法分散，做到真正的面相对象，主方法主思路，代码量不能多，多提炼方法出去。保证代码的清晰整洁，sf给我修改并且讲解大半天，真的是受益匪浅。<br><strong>y总：</strong> 10多年的大神，总工之一，依然坚守在研发岗位，y总甚至提出了另外的一种处理方法，最主要的原因是我们对业务的不熟悉，y总使用的完全不一样的方法，目前觉得能极大的提高效率。<br><strong>shihong：</strong>同事，朋友，差不多同时进公司，安排的任务基本是一起完成，合作非常愉快，这个程序是我们合作完成的，遇到不清晰的，我们便相互帮助，1+1&gt;2</p>\n<h2 id=\"从这些人或事的感悟\"><a href=\"#从这些人或事的感悟\" class=\"headerlink\" title=\"从这些人或事的感悟\"></a><strong>从这些人或事的感悟</strong></h2><ol>\n<li>虚心竹有低头叶 傲骨梅无仰面花， 向优秀的人请教，多他人的的优缺点。</li>\n<li>最基础的知识还不够牢固。需要一步一步，脚踏实地的积累。  </li>\n<li>思维这种东西，需要使劲的去想，实在想不通，换一个方向使劲去想。</li>\n<li>写程序之前，其实内心已经想好了需要如何写了，这时候，不要忙，先想想有没有什么问题，有没有更好的方法，真的全部想通后再开始码。做事也一样。</li>\n<li>写程序，特别是做数据，先要知道数据源，先想好要如何测试，先想好可能的测试结果。</li>\n<li>代码太乱，需要多看优秀代码，不断向前辈同事学习，不断思考自己写的代码，必须让自己的代码给其他人也能看懂。  </li>\n<li>多多分享，分享其实也是学习与复习最有效的办法之一  </li>\n<li>多感谢家人与同事，朋友。与同事愉快的合作，感谢他们提出的意见，不能太坚持自己的想法，先接受他人的意见，再看看和自己的想法有哪些不同，使用更好的，如果觉得自己的想法更好，可以提出来一起讨论。</li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<p> 虚心竹有低头叶；<br>傲骨梅无仰面花.<br>——郑燮  </p>\n<p>不讲大道理，只说工作事<br>今天，2017年6月21日，曾经花了一两个月的时间，设计并且实现了一个数据处理的程序，做是做完了，可是一当用于生产，遍出现各种问题。主要问题有以下几点  </p>\n<h2 id=\"问题的所在\"><a href=\"#问题的所在\" class=\"headerlink\" title=\"问题的所在\"></a><strong>问题的所在</strong></h2><p><strong>1. 程序处理的性能</strong><br>性能不堪入目。某当个大市的前期数据处理就需要两天。<br><strong>2. 代码各种bug</strong><br>总是有些深藏中的bug未被发现，在使用时才发现，从而需要急急忙忙的修改，并且数据又要从新生成<br><strong>3. 代码的质量</strong><br>代码质量很低，代码挺乱，需要重构的方法很多<br><strong>4. 真的很难测试</strong><br>程序写出来，是否满足需求，需要大量的测试，确实是比较难</p>\n<h2 id=\"曾做过的\"><a href=\"#曾做过的\" class=\"headerlink\" title=\"曾做过的\"></a><strong>曾做过的</strong></h2><p><strong>1. 针对程序处理的性能</strong><br><strong>读写问题：</strong>：发现在写的地方很花时间，有大量的小文件需要写，想进办法依然无法解决<br><strong>spark处理：</strong>甚至想过使用spark完成，也做了一份spark的程序，使用集群处理，但是数据需要放在hdfs上读写，非常麻烦<br><strong>2. 针对各种bug</strong><br>平时一个人如何也发现不了问题，需要在使用过程中发现问题，解决问题<br><strong>3. 代码质量</strong><br>想过需要提高代码的可读性，想过重构，但是比较随便，只提取了部分方法。代码依然臃肿<br><strong>4. 测试问题</strong><br>使用小数据测试，但是由于数据复杂性，对于某些数据可能成功，对于其他数据，若格式稍有变化，不保证一定可用</p>\n<h2 id=\"这两天的碰到的人或事\"><a href=\"#这两天的碰到的人或事\" class=\"headerlink\" title=\"这两天的碰到的人或事\"></a><strong>这两天的碰到的人或事</strong></h2><p><strong>c哥：</strong> 大神，我的项目负责人，现在带我的师傅。很多事都只有他才能处理，c哥做事极其负责，对于这个程序，  给予了非常多的建议与思路，可以说，这整个程序基本都是按照他的思路完成的，刚开始觉得，再难的问题，只要想通了，也就清晰了。<br><strong>sf兄：</strong>高级工程师，低调大神，业务熟悉，善于优化， 协助我优化代码，sf听我思路，一下就能找到问题的所在，一看代码，立刻动手帮我把代码翻了个底儿朝天，刚开始挺不开心的，我的代码思路怎么的好，也不能从头到尾给我修改吧，逐渐的，我体会到了大神的用意，我之前的代码实在是太差了，一些最基本的性能的优化我都忽略，代码也不规范，比如map中，key能用int就不用string，经常需要拼接的字符串，千万不能随意使用string，尝试使用StringBuffer或者List，代码方法分散，做到真正的面相对象，主方法主思路，代码量不能多，多提炼方法出去。保证代码的清晰整洁，sf给我修改并且讲解大半天，真的是受益匪浅。<br><strong>y总：</strong> 10多年的大神，总工之一，依然坚守在研发岗位，y总甚至提出了另外的一种处理方法，最主要的原因是我们对业务的不熟悉，y总使用的完全不一样的方法，目前觉得能极大的提高效率。<br><strong>shihong：</strong>同事，朋友，差不多同时进公司，安排的任务基本是一起完成，合作非常愉快，这个程序是我们合作完成的，遇到不清晰的，我们便相互帮助，1+1&gt;2</p>\n<h2 id=\"从这些人或事的感悟\"><a href=\"#从这些人或事的感悟\" class=\"headerlink\" title=\"从这些人或事的感悟\"></a><strong>从这些人或事的感悟</strong></h2><ol>\n<li>虚心竹有低头叶 傲骨梅无仰面花， 向优秀的人请教，多他人的的优缺点。</li>\n<li>最基础的知识还不够牢固。需要一步一步，脚踏实地的积累。  </li>\n<li>思维这种东西，需要使劲的去想，实在想不通，换一个方向使劲去想。</li>\n<li>写程序之前，其实内心已经想好了需要如何写了，这时候，不要忙，先想想有没有什么问题，有没有更好的方法，真的全部想通后再开始码。做事也一样。</li>\n<li>写程序，特别是做数据，先要知道数据源，先想好要如何测试，先想好可能的测试结果。</li>\n<li>代码太乱，需要多看优秀代码，不断向前辈同事学习，不断思考自己写的代码，必须让自己的代码给其他人也能看懂。  </li>\n<li>多多分享，分享其实也是学习与复习最有效的办法之一  </li>\n<li>多感谢家人与同事，朋友。与同事愉快的合作，感谢他们提出的意见，不能太坚持自己的想法，先接受他人的意见，再看看和自己的想法有哪些不同，使用更好的，如果觉得自己的想法更好，可以提出来一起讨论。</li>\n</ol>\n"},{"title":"时隔半年从wordpress再次转回hexo","date":"2018-01-15T15:13:21.000Z","slug":"dddddddddd","id":"1","_content":"转眼就到了18年了，新年新气象，准备再次用回hexo了\n","source":"_posts/时隔半年从wordpress再次转回hexo.md","raw":"---\ntitle: 时隔半年从wordpress再次转回hexo\ndate: 2018-01-15 23:13:21\ntags: [标签1,标签2]\ncategories: [测试一下目录,大数据]\nslug: return-to-hexo\nid: 1\npermalink: dddddddddd\n---\n转眼就到了18年了，新年新气象，准备再次用回hexo了\n","published":1,"updated":"2018-01-22T14:32:50.845Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcrpnzsu00462wv3hrjylfm2","content":"<p>转眼就到了18年了，新年新气象，准备再次用回hexo了</p>\n","site":{"data":{}},"excerpt":"","more":"<p>转眼就到了18年了，新年新气象，准备再次用回hexo了</p>\n"},{"title":"spark RDD算子（三） distinct，union，intersection，subtract，cartesian","date":"2017-03-03T13:25:21.000Z","author":"kaishun","id":"36","_content":"\n**spark伪集合**  \n尽管 RDD 本身不是严格意义上的集合，但它也支持许多数学上的集合操作，比如合并和相交操作, 下图展示了这四种操作  \n![RDD伪集合](http://i2.muimg.com/567571/cf473cf5e7f9f270.png)  \n# **distinct**\ndistinct用于去重， 我们生成的RDD可能有重复的元素，使用distinct方法可以去掉重复的元素, 不过此方法涉及到混洗，操作开销很大  \n**scala版本**\n```scala\n    scala> var RDD1 = sc.parallelize(List(\"aa\",\"aa\",\"bb\",\"cc\",\"dd\"))\n    \n    scala> RDD1.collect\n    res3: Array[String] = Array(aa, aa, bb, cc, dd)\n    \n    scala> var distinctRDD = RDD1.distinct\n    \n    scala> distinctRDD.collect\n    res5: Array[String] = Array(aa, dd, bb, cc)\n```  \n**java版本**\n```java\n    JavaRDD<String> RDD1 = sc.parallelize(Arrays.asList(\"aa\", \"aa\", \"bb\", \"cc\", \"dd\"));\n    JavaRDD<String> distinctRDD = RDD1.distinct();\n    List<String> collect = distinctRDD.collect();\n    for (String str:collect) {\n        System.out.print(str+\", \");\n    }\n---------输出----------\naa, dd, bb, cc,\n```  \n# **union**  \n两个RDD进行合并\n**scala版本**\n```\n    scala> var RDD1 = sc.parallelize(List(\"aa\",\"aa\",\"bb\",\"cc\",\"dd\"))\n    scala> var RDD2 = sc.parallelize(List(\"aa\",\"dd\",\"ff\"))\n    \n    scala> RDD1.collect\n    res6: Array[String] = Array(aa, aa, bb, cc, dd)\n    \n    scala> RDD2.collect\n    res7: Array[String] = Array(aa, dd, ff)\n    \n    scala> RDD1.union(RDD2).collect\n    res8: Array[String] = Array(aa, aa, bb, cc, dd, aa, dd, ff)\n\n```\n**java版本**\n```java\n    JavaRDD<String> RDD1 = sc.parallelize(Arrays.asList(\"aa\", \"aa\", \"bb\", \"cc\", \"dd\"));\n    JavaRDD<String> RDD2 = sc.parallelize(Arrays.asList(\"aa\",\"dd\",\"ff\"));\n    JavaRDD<String> unionRDD = RDD1.union(RDD2);\n    List<String> collect = unionRDD.collect();\n    for (String str:collect) {\n        System.out.print(str+\", \");\n    }\n-----------输出---------\naa, aa, bb, cc, dd, aa, dd, ff,\n``` \n# **intersection**\nRDD1.intersection(RDD2)  返回两个RDD的交集，并且去重  \nintersection 需要混洗数据，比较浪费性能  \n**scala版本**  \n```scala\n    scala> var RDD1 = sc.parallelize(List(\"aa\",\"aa\",\"bb\",\"cc\",\"dd\"))\n    scala> var RDD2 = sc.parallelize(List(\"aa\",\"dd\",\"ff\"))\n    \n    scala> RDD1.collect\n    res6: Array[String] = Array(aa, aa, bb, cc, dd)\n    \n    scala> RDD2.collect\n    res7: Array[String] = Array(aa, dd, ff)\n    \n    scala> var insertsectionRDD = RDD1.intersection(RDD2)\n    scala> insertsectionRDD.collect\n    \n    res9: Array[String] = Array(aa, dd)\n```  \n**java版本**  \n```java\n    JavaRDD<String> RDD1 = sc.parallelize(Arrays.asList(\"aa\", \"aa\", \"bb\", \"cc\", \"dd\"));\n    JavaRDD<String> RDD2 = sc.parallelize(Arrays.asList(\"aa\",\"dd\",\"ff\"));\n    JavaRDD<String> intersectionRDD = RDD1.intersection(RDD2);\n    List<String> collect = intersectionRDD.collect();\n    for (String str:collect) {\n        System.out.print(str+\" \");\n    }\n-------------输出-----------\naa dd\n```  \n# **subtract**\nRDD1.subtract(RDD2),返回在RDD1中出现，但是不在RDD2中出现的元素，不去重  \n**scala版本**\n```scala\n    JavaRDD<String> RDD1 = sc.parallelize(Arrays.asList(\"aa\", \"aa\",\"bb\", \"cc\", \"dd\"));\n    \n    JavaRDD<String> RDD2 = sc.parallelize(Arrays.asList(\"aa\",\"dd\",\"ff\"));\n    \n    scala> var substractRDD =RDD1.subtract(RDD2)\n    \n    scala>  substractRDD.collect\n    res10: Array[String] = Array(bb, cc)\n```\n**java版本**\n```java\n    JavaRDD<String> RDD1 = sc.parallelize(Arrays.asList(\"aa\", \"aa\", \"bb\",\"cc\", \"dd\"));\n    JavaRDD<String> RDD2 = sc.parallelize(Arrays.asList(\"aa\",\"dd\",\"ff\"));\n    JavaRDD<String> subtractRDD = RDD1.subtract(RDD2);\n    List<String> collect = subtractRDD.collect();\n    for (String str:collect) {\n        System.out.print(str+\" \");\n    }\n------------输出-----------------\nbb  cc \n```  \n# **cartesian**  \nRDD1.cartesian(RDD2)  返回RDD1和RDD2的笛卡儿积，这个开销非常大\n\n**scala版本**\n```scala\n    scala>  var RDD1 = sc.parallelize(List(\"1\",\"2\",\"3\"))\n    \n    scala> var RDD2 = sc.parallelize(List(\"a\",\"b\",\"c\"))\n    \n    scala> var cartesianRDD = RDD1.cartesian(RDD2)\n    \n    scala> cartesianRDD.collect\n    res11: Array[(String, String)] = Array((1,a), (1,b), (1,c), (2,a), (2,b), (2,c), (3,a), (3,b), (3,c))\n```  \n\n**java版本**\n```java\n    JavaRDD<String> RDD1 = sc.parallelize(Arrays.asList(\"1\", \"2\", \"3\"));\n    JavaRDD<String> RDD2 = sc.parallelize(Arrays.asList(\"a\",\"b\",\"c\"));\n    JavaPairRDD<String, String> cartesian = RDD1.cartesian(RDD2);\n\n    List<Tuple2<String, String>> collect1 = cartesian.collect();\n    for (Tuple2<String, String> tp:collect1) {\n        System.out.println(\"(\"+tp._1+\" \"+tp._2+\")\");\n    }\n------------输出-----------------\n(1 a)\n(1 b)\n(1 c)\n(2 a)\n(2 b)\n(2 c)\n(3 a)\n(3 b)\n(3 c)\n```  \n","source":"_posts/spark RDD算子（三） distinct，union，intersection，subtract，cartesian.md","raw":"---\ntitle: spark RDD算子（三） distinct，union，intersection，subtract，cartesian\ndate: 2017-03-03 21:25:21\ntags: [spark]\ncategories: [大数据,spark]\nauthor: kaishun\nid: 36\npermalink: spark-rdd-3\n---\n\n**spark伪集合**  \n尽管 RDD 本身不是严格意义上的集合，但它也支持许多数学上的集合操作，比如合并和相交操作, 下图展示了这四种操作  \n![RDD伪集合](http://i2.muimg.com/567571/cf473cf5e7f9f270.png)  \n# **distinct**\ndistinct用于去重， 我们生成的RDD可能有重复的元素，使用distinct方法可以去掉重复的元素, 不过此方法涉及到混洗，操作开销很大  \n**scala版本**\n```scala\n    scala> var RDD1 = sc.parallelize(List(\"aa\",\"aa\",\"bb\",\"cc\",\"dd\"))\n    \n    scala> RDD1.collect\n    res3: Array[String] = Array(aa, aa, bb, cc, dd)\n    \n    scala> var distinctRDD = RDD1.distinct\n    \n    scala> distinctRDD.collect\n    res5: Array[String] = Array(aa, dd, bb, cc)\n```  \n**java版本**\n```java\n    JavaRDD<String> RDD1 = sc.parallelize(Arrays.asList(\"aa\", \"aa\", \"bb\", \"cc\", \"dd\"));\n    JavaRDD<String> distinctRDD = RDD1.distinct();\n    List<String> collect = distinctRDD.collect();\n    for (String str:collect) {\n        System.out.print(str+\", \");\n    }\n---------输出----------\naa, dd, bb, cc,\n```  \n# **union**  \n两个RDD进行合并\n**scala版本**\n```\n    scala> var RDD1 = sc.parallelize(List(\"aa\",\"aa\",\"bb\",\"cc\",\"dd\"))\n    scala> var RDD2 = sc.parallelize(List(\"aa\",\"dd\",\"ff\"))\n    \n    scala> RDD1.collect\n    res6: Array[String] = Array(aa, aa, bb, cc, dd)\n    \n    scala> RDD2.collect\n    res7: Array[String] = Array(aa, dd, ff)\n    \n    scala> RDD1.union(RDD2).collect\n    res8: Array[String] = Array(aa, aa, bb, cc, dd, aa, dd, ff)\n\n```\n**java版本**\n```java\n    JavaRDD<String> RDD1 = sc.parallelize(Arrays.asList(\"aa\", \"aa\", \"bb\", \"cc\", \"dd\"));\n    JavaRDD<String> RDD2 = sc.parallelize(Arrays.asList(\"aa\",\"dd\",\"ff\"));\n    JavaRDD<String> unionRDD = RDD1.union(RDD2);\n    List<String> collect = unionRDD.collect();\n    for (String str:collect) {\n        System.out.print(str+\", \");\n    }\n-----------输出---------\naa, aa, bb, cc, dd, aa, dd, ff,\n``` \n# **intersection**\nRDD1.intersection(RDD2)  返回两个RDD的交集，并且去重  \nintersection 需要混洗数据，比较浪费性能  \n**scala版本**  \n```scala\n    scala> var RDD1 = sc.parallelize(List(\"aa\",\"aa\",\"bb\",\"cc\",\"dd\"))\n    scala> var RDD2 = sc.parallelize(List(\"aa\",\"dd\",\"ff\"))\n    \n    scala> RDD1.collect\n    res6: Array[String] = Array(aa, aa, bb, cc, dd)\n    \n    scala> RDD2.collect\n    res7: Array[String] = Array(aa, dd, ff)\n    \n    scala> var insertsectionRDD = RDD1.intersection(RDD2)\n    scala> insertsectionRDD.collect\n    \n    res9: Array[String] = Array(aa, dd)\n```  \n**java版本**  \n```java\n    JavaRDD<String> RDD1 = sc.parallelize(Arrays.asList(\"aa\", \"aa\", \"bb\", \"cc\", \"dd\"));\n    JavaRDD<String> RDD2 = sc.parallelize(Arrays.asList(\"aa\",\"dd\",\"ff\"));\n    JavaRDD<String> intersectionRDD = RDD1.intersection(RDD2);\n    List<String> collect = intersectionRDD.collect();\n    for (String str:collect) {\n        System.out.print(str+\" \");\n    }\n-------------输出-----------\naa dd\n```  \n# **subtract**\nRDD1.subtract(RDD2),返回在RDD1中出现，但是不在RDD2中出现的元素，不去重  \n**scala版本**\n```scala\n    JavaRDD<String> RDD1 = sc.parallelize(Arrays.asList(\"aa\", \"aa\",\"bb\", \"cc\", \"dd\"));\n    \n    JavaRDD<String> RDD2 = sc.parallelize(Arrays.asList(\"aa\",\"dd\",\"ff\"));\n    \n    scala> var substractRDD =RDD1.subtract(RDD2)\n    \n    scala>  substractRDD.collect\n    res10: Array[String] = Array(bb, cc)\n```\n**java版本**\n```java\n    JavaRDD<String> RDD1 = sc.parallelize(Arrays.asList(\"aa\", \"aa\", \"bb\",\"cc\", \"dd\"));\n    JavaRDD<String> RDD2 = sc.parallelize(Arrays.asList(\"aa\",\"dd\",\"ff\"));\n    JavaRDD<String> subtractRDD = RDD1.subtract(RDD2);\n    List<String> collect = subtractRDD.collect();\n    for (String str:collect) {\n        System.out.print(str+\" \");\n    }\n------------输出-----------------\nbb  cc \n```  \n# **cartesian**  \nRDD1.cartesian(RDD2)  返回RDD1和RDD2的笛卡儿积，这个开销非常大\n\n**scala版本**\n```scala\n    scala>  var RDD1 = sc.parallelize(List(\"1\",\"2\",\"3\"))\n    \n    scala> var RDD2 = sc.parallelize(List(\"a\",\"b\",\"c\"))\n    \n    scala> var cartesianRDD = RDD1.cartesian(RDD2)\n    \n    scala> cartesianRDD.collect\n    res11: Array[(String, String)] = Array((1,a), (1,b), (1,c), (2,a), (2,b), (2,c), (3,a), (3,b), (3,c))\n```  \n\n**java版本**\n```java\n    JavaRDD<String> RDD1 = sc.parallelize(Arrays.asList(\"1\", \"2\", \"3\"));\n    JavaRDD<String> RDD2 = sc.parallelize(Arrays.asList(\"a\",\"b\",\"c\"));\n    JavaPairRDD<String, String> cartesian = RDD1.cartesian(RDD2);\n\n    List<Tuple2<String, String>> collect1 = cartesian.collect();\n    for (Tuple2<String, String> tp:collect1) {\n        System.out.println(\"(\"+tp._1+\" \"+tp._2+\")\");\n    }\n------------输出-----------------\n(1 a)\n(1 b)\n(1 c)\n(2 a)\n(2 b)\n(2 c)\n(3 a)\n(3 b)\n(3 c)\n```  \n","slug":"spark-rdd-3","published":1,"updated":"2018-01-22T15:24:02.192Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcrpnzw5006p2wv3i2bhvb3w","content":"<p><strong>spark伪集合</strong><br>尽管 RDD 本身不是严格意义上的集合，但它也支持许多数学上的集合操作，比如合并和相交操作, 下图展示了这四种操作<br><img src=\"http://i2.muimg.com/567571/cf473cf5e7f9f270.png\" alt=\"RDD伪集合\">  </p>\n<h1 id=\"distinct\"><a href=\"#distinct\" class=\"headerlink\" title=\"distinct\"></a><strong>distinct</strong></h1><p>distinct用于去重， 我们生成的RDD可能有重复的元素，使用distinct方法可以去掉重复的元素, 不过此方法涉及到混洗，操作开销很大<br><strong>scala版本</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div></pre></td><td class=\"code\"><pre><div class=\"line\">    scala&gt; <span class=\"keyword\">var</span> <span class=\"type\">RDD1</span> = sc.parallelize(<span class=\"type\">List</span>(<span class=\"string\">\"aa\"</span>,<span class=\"string\">\"aa\"</span>,<span class=\"string\">\"bb\"</span>,<span class=\"string\">\"cc\"</span>,<span class=\"string\">\"dd\"</span>))</div><div class=\"line\">    </div><div class=\"line\">    scala&gt; <span class=\"type\">RDD1</span>.collect</div><div class=\"line\">    res3: <span class=\"type\">Array</span>[<span class=\"type\">String</span>] = <span class=\"type\">Array</span>(aa, aa, bb, cc, dd)</div><div class=\"line\">    </div><div class=\"line\">    scala&gt; <span class=\"keyword\">var</span> distinctRDD = <span class=\"type\">RDD1</span>.distinct</div><div class=\"line\">    </div><div class=\"line\">    scala&gt; distinctRDD.collect</div><div class=\"line\">    res5: <span class=\"type\">Array</span>[<span class=\"type\">String</span>] = <span class=\"type\">Array</span>(aa, dd, bb, cc)</div><div class=\"line\">```  </div><div class=\"line\">**java版本**</div><div class=\"line\">```java</div><div class=\"line\">    <span class=\"type\">JavaRDD</span>&lt;<span class=\"type\">String</span>&gt; <span class=\"type\">RDD1</span> = sc.parallelize(<span class=\"type\">Arrays</span>.asList(<span class=\"string\">\"aa\"</span>, <span class=\"string\">\"aa\"</span>, <span class=\"string\">\"bb\"</span>, <span class=\"string\">\"cc\"</span>, <span class=\"string\">\"dd\"</span>));</div><div class=\"line\">    <span class=\"type\">JavaRDD</span>&lt;<span class=\"type\">String</span>&gt; distinctRDD = <span class=\"type\">RDD1</span>.distinct();</div><div class=\"line\">    <span class=\"type\">List</span>&lt;<span class=\"type\">String</span>&gt; collect = distinctRDD.collect();</div><div class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">String</span> str:collect) &#123;</div><div class=\"line\">        <span class=\"type\">System</span>.out.print(str+<span class=\"string\">\", \"</span>);</div><div class=\"line\">    &#125;</div><div class=\"line\">---------输出----------</div><div class=\"line\">aa, dd, bb, cc,</div><div class=\"line\">```  </div><div class=\"line\"># **union**  </div><div class=\"line\">两个<span class=\"type\">RDD</span>进行合并</div><div class=\"line\">**scala版本**</div></pre></td></tr></table></figure></p>\n<pre><code>scala&gt; var RDD1 = sc.parallelize(List(&quot;aa&quot;,&quot;aa&quot;,&quot;bb&quot;,&quot;cc&quot;,&quot;dd&quot;))\nscala&gt; var RDD2 = sc.parallelize(List(&quot;aa&quot;,&quot;dd&quot;,&quot;ff&quot;))\n\nscala&gt; RDD1.collect\nres6: Array[String] = Array(aa, aa, bb, cc, dd)\n\nscala&gt; RDD2.collect\nres7: Array[String] = Array(aa, dd, ff)\n\nscala&gt; RDD1.union(RDD2).collect\nres8: Array[String] = Array(aa, aa, bb, cc, dd, aa, dd, ff)\n</code></pre><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div></pre></td><td class=\"code\"><pre><div class=\"line\">**java版本**</div><div class=\"line\">```java</div><div class=\"line\">    JavaRDD&lt;String&gt; RDD1 = sc.parallelize(Arrays.asList(&quot;aa&quot;, &quot;aa&quot;, &quot;bb&quot;, &quot;cc&quot;, &quot;dd&quot;));</div><div class=\"line\">    JavaRDD&lt;String&gt; RDD2 = sc.parallelize(Arrays.asList(&quot;aa&quot;,&quot;dd&quot;,&quot;ff&quot;));</div><div class=\"line\">    JavaRDD&lt;String&gt; unionRDD = RDD1.union(RDD2);</div><div class=\"line\">    List&lt;String&gt; collect = unionRDD.collect();</div><div class=\"line\">    for (String str:collect) &#123;</div><div class=\"line\">        System.out.print(str+&quot;, &quot;);</div><div class=\"line\">    &#125;</div><div class=\"line\">-----------输出---------</div><div class=\"line\">aa, aa, bb, cc, dd, aa, dd, ff,</div><div class=\"line\">``` </div><div class=\"line\"># **intersection**</div><div class=\"line\">RDD1.intersection(RDD2)  返回两个RDD的交集，并且去重  </div><div class=\"line\">intersection 需要混洗数据，比较浪费性能  </div><div class=\"line\">**scala版本**  </div><div class=\"line\">```scala</div><div class=\"line\">    scala&gt; var RDD1 = sc.parallelize(List(&quot;aa&quot;,&quot;aa&quot;,&quot;bb&quot;,&quot;cc&quot;,&quot;dd&quot;))</div><div class=\"line\">    scala&gt; var RDD2 = sc.parallelize(List(&quot;aa&quot;,&quot;dd&quot;,&quot;ff&quot;))</div><div class=\"line\">    </div><div class=\"line\">    scala&gt; RDD1.collect</div><div class=\"line\">    res6: Array[String] = Array(aa, aa, bb, cc, dd)</div><div class=\"line\">    </div><div class=\"line\">    scala&gt; RDD2.collect</div><div class=\"line\">    res7: Array[String] = Array(aa, dd, ff)</div><div class=\"line\">    </div><div class=\"line\">    scala&gt; var insertsectionRDD = RDD1.intersection(RDD2)</div><div class=\"line\">    scala&gt; insertsectionRDD.collect</div><div class=\"line\">    </div><div class=\"line\">    res9: Array[String] = Array(aa, dd)</div><div class=\"line\">```  </div><div class=\"line\">**java版本**  </div><div class=\"line\">```java</div><div class=\"line\">    JavaRDD&lt;String&gt; RDD1 = sc.parallelize(Arrays.asList(&quot;aa&quot;, &quot;aa&quot;, &quot;bb&quot;, &quot;cc&quot;, &quot;dd&quot;));</div><div class=\"line\">    JavaRDD&lt;String&gt; RDD2 = sc.parallelize(Arrays.asList(&quot;aa&quot;,&quot;dd&quot;,&quot;ff&quot;));</div><div class=\"line\">    JavaRDD&lt;String&gt; intersectionRDD = RDD1.intersection(RDD2);</div><div class=\"line\">    List&lt;String&gt; collect = intersectionRDD.collect();</div><div class=\"line\">    for (String str:collect) &#123;</div><div class=\"line\">        System.out.print(str+&quot; &quot;);</div><div class=\"line\">    &#125;</div><div class=\"line\">-------------输出-----------</div><div class=\"line\">aa dd</div><div class=\"line\">```  </div><div class=\"line\"># **subtract**</div><div class=\"line\">RDD1.subtract(RDD2),返回在RDD1中出现，但是不在RDD2中出现的元素，不去重  </div><div class=\"line\">**scala版本**</div><div class=\"line\">```scala</div><div class=\"line\">    JavaRDD&lt;String&gt; RDD1 = sc.parallelize(Arrays.asList(&quot;aa&quot;, &quot;aa&quot;,&quot;bb&quot;, &quot;cc&quot;, &quot;dd&quot;));</div><div class=\"line\">    </div><div class=\"line\">    JavaRDD&lt;String&gt; RDD2 = sc.parallelize(Arrays.asList(&quot;aa&quot;,&quot;dd&quot;,&quot;ff&quot;));</div><div class=\"line\">    </div><div class=\"line\">    scala&gt; var substractRDD =RDD1.subtract(RDD2)</div><div class=\"line\">    </div><div class=\"line\">    scala&gt;  substractRDD.collect</div><div class=\"line\">    res10: Array[String] = Array(bb, cc)</div></pre></td></tr></table></figure>\n<p><strong>java版本</strong></p>\n<pre><code class=\"java\">    JavaRDD&lt;String&gt; RDD1 = sc.parallelize(Arrays.asList(<span class=\"string\">\"aa\"</span>, <span class=\"string\">\"aa\"</span>, <span class=\"string\">\"bb\"</span>,<span class=\"string\">\"cc\"</span>, <span class=\"string\">\"dd\"</span>));\n    JavaRDD&lt;String&gt; RDD2 = sc.parallelize(Arrays.asList(<span class=\"string\">\"aa\"</span>,<span class=\"string\">\"dd\"</span>,<span class=\"string\">\"ff\"</span>));\n    JavaRDD&lt;String&gt; subtractRDD = RDD1.subtract(RDD2);\n    List&lt;String&gt; collect = subtractRDD.collect();\n    <span class=\"keyword\">for</span> (String str:collect) {\n        System.out.print(str+<span class=\"string\">\" \"</span>);\n    }\n------------输出-----------------\nbb  cc\n</code></pre>\n<h1 id=\"cartesian\"><a href=\"#cartesian\" class=\"headerlink\" title=\"cartesian\"></a><strong>cartesian</strong></h1><p>RDD1.cartesian(RDD2)  返回RDD1和RDD2的笛卡儿积，这个开销非常大</p>\n<p><strong>scala版本</strong></p>\n<pre><code class=\"scala\">scala&gt;  <span class=\"keyword\">var</span> <span class=\"type\">RDD1</span> = sc.parallelize(<span class=\"type\">List</span>(<span class=\"string\">\"1\"</span>,<span class=\"string\">\"2\"</span>,<span class=\"string\">\"3\"</span>))\n\nscala&gt; <span class=\"keyword\">var</span> <span class=\"type\">RDD2</span> = sc.parallelize(<span class=\"type\">List</span>(<span class=\"string\">\"a\"</span>,<span class=\"string\">\"b\"</span>,<span class=\"string\">\"c\"</span>))\n\nscala&gt; <span class=\"keyword\">var</span> cartesianRDD = <span class=\"type\">RDD1</span>.cartesian(<span class=\"type\">RDD2</span>)\n\nscala&gt; cartesianRDD.collect\nres11: <span class=\"type\">Array</span>[(<span class=\"type\">String</span>, <span class=\"type\">String</span>)] = <span class=\"type\">Array</span>((<span class=\"number\">1</span>,a), (<span class=\"number\">1</span>,b), (<span class=\"number\">1</span>,c), (<span class=\"number\">2</span>,a), (<span class=\"number\">2</span>,b), (<span class=\"number\">2</span>,c), (<span class=\"number\">3</span>,a), (<span class=\"number\">3</span>,b), (<span class=\"number\">3</span>,c))\n</code></pre>\n<p><strong>java版本</strong></p>\n<pre><code class=\"java\">    JavaRDD&lt;String&gt; RDD1 = sc.parallelize(Arrays.asList(<span class=\"string\">\"1\"</span>, <span class=\"string\">\"2\"</span>, <span class=\"string\">\"3\"</span>));\n    JavaRDD&lt;String&gt; RDD2 = sc.parallelize(Arrays.asList(<span class=\"string\">\"a\"</span>,<span class=\"string\">\"b\"</span>,<span class=\"string\">\"c\"</span>));\n    JavaPairRDD&lt;String, String&gt; cartesian = RDD1.cartesian(RDD2);\n\n    List&lt;Tuple2&lt;String, String&gt;&gt; collect1 = cartesian.collect();\n    <span class=\"keyword\">for</span> (Tuple2&lt;String, String&gt; tp:collect1) {\n        System.out.println(<span class=\"string\">\"(\"</span>+tp._1+<span class=\"string\">\" \"</span>+tp._2+<span class=\"string\">\")\"</span>);\n    }\n------------输出-----------------\n(<span class=\"number\">1</span> a)\n(<span class=\"number\">1</span> b)\n(<span class=\"number\">1</span> c)\n(<span class=\"number\">2</span> a)\n(<span class=\"number\">2</span> b)\n(<span class=\"number\">2</span> c)\n(<span class=\"number\">3</span> a)\n(<span class=\"number\">3</span> b)\n(<span class=\"number\">3</span> c)\n</code></pre>\n","site":{"data":{}},"excerpt":"","more":"<p><strong>spark伪集合</strong><br>尽管 RDD 本身不是严格意义上的集合，但它也支持许多数学上的集合操作，比如合并和相交操作, 下图展示了这四种操作<br><img src=\"http://i2.muimg.com/567571/cf473cf5e7f9f270.png\" alt=\"RDD伪集合\">  </p>\n<h1 id=\"distinct\"><a href=\"#distinct\" class=\"headerlink\" title=\"distinct\"></a><strong>distinct</strong></h1><p>distinct用于去重， 我们生成的RDD可能有重复的元素，使用distinct方法可以去掉重复的元素, 不过此方法涉及到混洗，操作开销很大<br><strong>scala版本</strong><br><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div></pre></td><td class=\"code\"><pre><div class=\"line\">    scala&gt; <span class=\"keyword\">var</span> <span class=\"type\">RDD1</span> = sc.parallelize(<span class=\"type\">List</span>(<span class=\"string\">\"aa\"</span>,<span class=\"string\">\"aa\"</span>,<span class=\"string\">\"bb\"</span>,<span class=\"string\">\"cc\"</span>,<span class=\"string\">\"dd\"</span>))</div><div class=\"line\">    </div><div class=\"line\">    scala&gt; <span class=\"type\">RDD1</span>.collect</div><div class=\"line\">    res3: <span class=\"type\">Array</span>[<span class=\"type\">String</span>] = <span class=\"type\">Array</span>(aa, aa, bb, cc, dd)</div><div class=\"line\">    </div><div class=\"line\">    scala&gt; <span class=\"keyword\">var</span> distinctRDD = <span class=\"type\">RDD1</span>.distinct</div><div class=\"line\">    </div><div class=\"line\">    scala&gt; distinctRDD.collect</div><div class=\"line\">    res5: <span class=\"type\">Array</span>[<span class=\"type\">String</span>] = <span class=\"type\">Array</span>(aa, dd, bb, cc)</div><div class=\"line\">```  </div><div class=\"line\">**java版本**</div><div class=\"line\">```java</div><div class=\"line\">    <span class=\"type\">JavaRDD</span>&lt;<span class=\"type\">String</span>&gt; <span class=\"type\">RDD1</span> = sc.parallelize(<span class=\"type\">Arrays</span>.asList(<span class=\"string\">\"aa\"</span>, <span class=\"string\">\"aa\"</span>, <span class=\"string\">\"bb\"</span>, <span class=\"string\">\"cc\"</span>, <span class=\"string\">\"dd\"</span>));</div><div class=\"line\">    <span class=\"type\">JavaRDD</span>&lt;<span class=\"type\">String</span>&gt; distinctRDD = <span class=\"type\">RDD1</span>.distinct();</div><div class=\"line\">    <span class=\"type\">List</span>&lt;<span class=\"type\">String</span>&gt; collect = distinctRDD.collect();</div><div class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">String</span> str:collect) &#123;</div><div class=\"line\">        <span class=\"type\">System</span>.out.print(str+<span class=\"string\">\", \"</span>);</div><div class=\"line\">    &#125;</div><div class=\"line\">---------输出----------</div><div class=\"line\">aa, dd, bb, cc,</div><div class=\"line\">```  </div><div class=\"line\"># **union**  </div><div class=\"line\">两个<span class=\"type\">RDD</span>进行合并</div><div class=\"line\">**scala版本**</div></pre></td></tr></table></figure></p>\n<pre><code>scala&gt; var RDD1 = sc.parallelize(List(&quot;aa&quot;,&quot;aa&quot;,&quot;bb&quot;,&quot;cc&quot;,&quot;dd&quot;))\nscala&gt; var RDD2 = sc.parallelize(List(&quot;aa&quot;,&quot;dd&quot;,&quot;ff&quot;))\n\nscala&gt; RDD1.collect\nres6: Array[String] = Array(aa, aa, bb, cc, dd)\n\nscala&gt; RDD2.collect\nres7: Array[String] = Array(aa, dd, ff)\n\nscala&gt; RDD1.union(RDD2).collect\nres8: Array[String] = Array(aa, aa, bb, cc, dd, aa, dd, ff)\n</code></pre><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div><div class=\"line\">44</div><div class=\"line\">45</div><div class=\"line\">46</div><div class=\"line\">47</div><div class=\"line\">48</div><div class=\"line\">49</div><div class=\"line\">50</div><div class=\"line\">51</div><div class=\"line\">52</div><div class=\"line\">53</div><div class=\"line\">54</div><div class=\"line\">55</div></pre></td><td class=\"code\"><pre><div class=\"line\">**java版本**</div><div class=\"line\">```java</div><div class=\"line\">    JavaRDD&lt;String&gt; RDD1 = sc.parallelize(Arrays.asList(&quot;aa&quot;, &quot;aa&quot;, &quot;bb&quot;, &quot;cc&quot;, &quot;dd&quot;));</div><div class=\"line\">    JavaRDD&lt;String&gt; RDD2 = sc.parallelize(Arrays.asList(&quot;aa&quot;,&quot;dd&quot;,&quot;ff&quot;));</div><div class=\"line\">    JavaRDD&lt;String&gt; unionRDD = RDD1.union(RDD2);</div><div class=\"line\">    List&lt;String&gt; collect = unionRDD.collect();</div><div class=\"line\">    for (String str:collect) &#123;</div><div class=\"line\">        System.out.print(str+&quot;, &quot;);</div><div class=\"line\">    &#125;</div><div class=\"line\">-----------输出---------</div><div class=\"line\">aa, aa, bb, cc, dd, aa, dd, ff,</div><div class=\"line\">``` </div><div class=\"line\"># **intersection**</div><div class=\"line\">RDD1.intersection(RDD2)  返回两个RDD的交集，并且去重  </div><div class=\"line\">intersection 需要混洗数据，比较浪费性能  </div><div class=\"line\">**scala版本**  </div><div class=\"line\">```scala</div><div class=\"line\">    scala&gt; var RDD1 = sc.parallelize(List(&quot;aa&quot;,&quot;aa&quot;,&quot;bb&quot;,&quot;cc&quot;,&quot;dd&quot;))</div><div class=\"line\">    scala&gt; var RDD2 = sc.parallelize(List(&quot;aa&quot;,&quot;dd&quot;,&quot;ff&quot;))</div><div class=\"line\">    </div><div class=\"line\">    scala&gt; RDD1.collect</div><div class=\"line\">    res6: Array[String] = Array(aa, aa, bb, cc, dd)</div><div class=\"line\">    </div><div class=\"line\">    scala&gt; RDD2.collect</div><div class=\"line\">    res7: Array[String] = Array(aa, dd, ff)</div><div class=\"line\">    </div><div class=\"line\">    scala&gt; var insertsectionRDD = RDD1.intersection(RDD2)</div><div class=\"line\">    scala&gt; insertsectionRDD.collect</div><div class=\"line\">    </div><div class=\"line\">    res9: Array[String] = Array(aa, dd)</div><div class=\"line\">```  </div><div class=\"line\">**java版本**  </div><div class=\"line\">```java</div><div class=\"line\">    JavaRDD&lt;String&gt; RDD1 = sc.parallelize(Arrays.asList(&quot;aa&quot;, &quot;aa&quot;, &quot;bb&quot;, &quot;cc&quot;, &quot;dd&quot;));</div><div class=\"line\">    JavaRDD&lt;String&gt; RDD2 = sc.parallelize(Arrays.asList(&quot;aa&quot;,&quot;dd&quot;,&quot;ff&quot;));</div><div class=\"line\">    JavaRDD&lt;String&gt; intersectionRDD = RDD1.intersection(RDD2);</div><div class=\"line\">    List&lt;String&gt; collect = intersectionRDD.collect();</div><div class=\"line\">    for (String str:collect) &#123;</div><div class=\"line\">        System.out.print(str+&quot; &quot;);</div><div class=\"line\">    &#125;</div><div class=\"line\">-------------输出-----------</div><div class=\"line\">aa dd</div><div class=\"line\">```  </div><div class=\"line\"># **subtract**</div><div class=\"line\">RDD1.subtract(RDD2),返回在RDD1中出现，但是不在RDD2中出现的元素，不去重  </div><div class=\"line\">**scala版本**</div><div class=\"line\">```scala</div><div class=\"line\">    JavaRDD&lt;String&gt; RDD1 = sc.parallelize(Arrays.asList(&quot;aa&quot;, &quot;aa&quot;,&quot;bb&quot;, &quot;cc&quot;, &quot;dd&quot;));</div><div class=\"line\">    </div><div class=\"line\">    JavaRDD&lt;String&gt; RDD2 = sc.parallelize(Arrays.asList(&quot;aa&quot;,&quot;dd&quot;,&quot;ff&quot;));</div><div class=\"line\">    </div><div class=\"line\">    scala&gt; var substractRDD =RDD1.subtract(RDD2)</div><div class=\"line\">    </div><div class=\"line\">    scala&gt;  substractRDD.collect</div><div class=\"line\">    res10: Array[String] = Array(bb, cc)</div></pre></td></tr></table></figure>\n<p><strong>java版本</strong></p>\n<pre><code class=\"java\">    JavaRDD&lt;String&gt; RDD1 = sc.parallelize(Arrays.asList(<span class=\"string\">\"aa\"</span>, <span class=\"string\">\"aa\"</span>, <span class=\"string\">\"bb\"</span>,<span class=\"string\">\"cc\"</span>, <span class=\"string\">\"dd\"</span>));\n    JavaRDD&lt;String&gt; RDD2 = sc.parallelize(Arrays.asList(<span class=\"string\">\"aa\"</span>,<span class=\"string\">\"dd\"</span>,<span class=\"string\">\"ff\"</span>));\n    JavaRDD&lt;String&gt; subtractRDD = RDD1.subtract(RDD2);\n    List&lt;String&gt; collect = subtractRDD.collect();\n    <span class=\"keyword\">for</span> (String str:collect) {\n        System.out.print(str+<span class=\"string\">\" \"</span>);\n    }\n------------输出-----------------\nbb  cc\n</code></pre>\n<h1 id=\"cartesian\"><a href=\"#cartesian\" class=\"headerlink\" title=\"cartesian\"></a><strong>cartesian</strong></h1><p>RDD1.cartesian(RDD2)  返回RDD1和RDD2的笛卡儿积，这个开销非常大</p>\n<p><strong>scala版本</strong></p>\n<pre><code class=\"scala\">scala&gt;  <span class=\"keyword\">var</span> <span class=\"type\">RDD1</span> = sc.parallelize(<span class=\"type\">List</span>(<span class=\"string\">\"1\"</span>,<span class=\"string\">\"2\"</span>,<span class=\"string\">\"3\"</span>))\n\nscala&gt; <span class=\"keyword\">var</span> <span class=\"type\">RDD2</span> = sc.parallelize(<span class=\"type\">List</span>(<span class=\"string\">\"a\"</span>,<span class=\"string\">\"b\"</span>,<span class=\"string\">\"c\"</span>))\n\nscala&gt; <span class=\"keyword\">var</span> cartesianRDD = <span class=\"type\">RDD1</span>.cartesian(<span class=\"type\">RDD2</span>)\n\nscala&gt; cartesianRDD.collect\nres11: <span class=\"type\">Array</span>[(<span class=\"type\">String</span>, <span class=\"type\">String</span>)] = <span class=\"type\">Array</span>((<span class=\"number\">1</span>,a), (<span class=\"number\">1</span>,b), (<span class=\"number\">1</span>,c), (<span class=\"number\">2</span>,a), (<span class=\"number\">2</span>,b), (<span class=\"number\">2</span>,c), (<span class=\"number\">3</span>,a), (<span class=\"number\">3</span>,b), (<span class=\"number\">3</span>,c))\n</code></pre>\n<p><strong>java版本</strong></p>\n<pre><code class=\"java\">    JavaRDD&lt;String&gt; RDD1 = sc.parallelize(Arrays.asList(<span class=\"string\">\"1\"</span>, <span class=\"string\">\"2\"</span>, <span class=\"string\">\"3\"</span>));\n    JavaRDD&lt;String&gt; RDD2 = sc.parallelize(Arrays.asList(<span class=\"string\">\"a\"</span>,<span class=\"string\">\"b\"</span>,<span class=\"string\">\"c\"</span>));\n    JavaPairRDD&lt;String, String&gt; cartesian = RDD1.cartesian(RDD2);\n\n    List&lt;Tuple2&lt;String, String&gt;&gt; collect1 = cartesian.collect();\n    <span class=\"keyword\">for</span> (Tuple2&lt;String, String&gt; tp:collect1) {\n        System.out.println(<span class=\"string\">\"(\"</span>+tp._1+<span class=\"string\">\" \"</span>+tp._2+<span class=\"string\">\")\"</span>);\n    }\n------------输出-----------------\n(<span class=\"number\">1</span> a)\n(<span class=\"number\">1</span> b)\n(<span class=\"number\">1</span> c)\n(<span class=\"number\">2</span> a)\n(<span class=\"number\">2</span> b)\n(<span class=\"number\">2</span> c)\n(<span class=\"number\">3</span> a)\n(<span class=\"number\">3</span> b)\n(<span class=\"number\">3</span> c)\n</code></pre>\n"},{"title":"异常Exception in thread -AWT-EventQueue-0- java.lang.NullPointerException","date":"2017-05-04T13:25:21.000Z","author":"kaishun","id":"55","blogexcerpt":"更新UI等操作时异常，类似于Android中的更新ui一样，ui线程相当于主线程，再对此进行直接更新，两个线程事物会冲突报这个异常，在Android中使用的是handle的方法，同理,类似于，在java 界面编程上，我们把其他的线程放在一个队列当中，我们称这个队列为消息队列，可以使用如下方法，把更新UI的操作，放在下面的run方法中去。 这样，就实现了消息队列的方式。最后由主线程有序的处理这些消息事件","_content":"\nException in thread \"AWT-EventQueue-0\" java.lang.NullPointerException  \n更新UI等操作时异常  \n**个人理解原因**：  \n类似于Android中的更新ui一样，ui线程相当于主线程，再对此进行直接更新，两个线程事物会冲突报这个异常，在Android中使用的是handle的方法，同理,类似于，在java 界面编程上，我们把其他的线程放在一个队列当中，我们称这个队列为消息队列，可以使用如下方法，把更新UI的操作，放在下面的run方法中去。 这样，就实现了消息队列的方式。 最后由主线程有序的处理这些消息事件\n```\nSwingUtilities.invokeLater(new   Runnable(){ \n\t\t\tpublic   void   run() { \n\t\t\t//放入方法\n\t\t\t} \n });\n```\n\n例子如下\n```\nSwingUtilities.invokeLater(new   Runnable(){ \n\t\tpublic   void   run() { \n\t\ttree.updateUI(); \n\t\t} \n });\n```\n```\nSwingUtilities.invokeLater(new   Runnable() { \n\t\tpublic   void   run() { \n\t\t\tSwingUtilities.updateComponentTreeUI(jf); \n\t\t} \n\t\t\t});\n```","source":"_posts/异常Exception in thread -AWT-EventQueue-0- java.lang.NullPointerException.md","raw":"---\ntitle: 异常Exception in thread -AWT-EventQueue-0- java.lang.NullPointerException\ndate: 2017-05-04 21:25:21\ntags: [java,Exception]\ncategories: [programme]\nauthor: kaishun\nid: 55\npermalink: java-awt-eventqueue-exception\nblogexcerpt: 更新UI等操作时异常，类似于Android中的更新ui一样，ui线程相当于主线程，再对此进行直接更新，两个线程事物会冲突报这个异常，在Android中使用的是handle的方法，同理,类似于，在java 界面编程上，我们把其他的线程放在一个队列当中，我们称这个队列为消息队列，可以使用如下方法，把更新UI的操作，放在下面的run方法中去。 这样，就实现了消息队列的方式。最后由主线程有序的处理这些消息事件 \n---\n\nException in thread \"AWT-EventQueue-0\" java.lang.NullPointerException  \n更新UI等操作时异常  \n**个人理解原因**：  \n类似于Android中的更新ui一样，ui线程相当于主线程，再对此进行直接更新，两个线程事物会冲突报这个异常，在Android中使用的是handle的方法，同理,类似于，在java 界面编程上，我们把其他的线程放在一个队列当中，我们称这个队列为消息队列，可以使用如下方法，把更新UI的操作，放在下面的run方法中去。 这样，就实现了消息队列的方式。 最后由主线程有序的处理这些消息事件\n```\nSwingUtilities.invokeLater(new   Runnable(){ \n\t\t\tpublic   void   run() { \n\t\t\t//放入方法\n\t\t\t} \n });\n```\n\n例子如下\n```\nSwingUtilities.invokeLater(new   Runnable(){ \n\t\tpublic   void   run() { \n\t\ttree.updateUI(); \n\t\t} \n });\n```\n```\nSwingUtilities.invokeLater(new   Runnable() { \n\t\tpublic   void   run() { \n\t\t\tSwingUtilities.updateComponentTreeUI(jf); \n\t\t} \n\t\t\t});\n```","slug":"java-awt-eventqueue-exception","published":1,"updated":"2018-01-23T14:11:46.514Z","_id":"cjcrpnzx1006t2wv3l0vrohpu","comments":1,"layout":"post","photos":[],"link":"","content":"<p>Exception in thread “AWT-EventQueue-0” java.lang.NullPointerException<br>更新UI等操作时异常<br><strong>个人理解原因</strong>：<br>类似于Android中的更新ui一样，ui线程相当于主线程，再对此进行直接更新，两个线程事物会冲突报这个异常，在Android中使用的是handle的方法，同理,类似于，在java 界面编程上，我们把其他的线程放在一个队列当中，我们称这个队列为消息队列，可以使用如下方法，把更新UI的操作，放在下面的run方法中去。 这样，就实现了消息队列的方式。 最后由主线程有序的处理这些消息事件<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">SwingUtilities.invokeLater(new   Runnable()&#123; </div><div class=\"line\">\t\t\tpublic   void   run() &#123; </div><div class=\"line\">\t\t\t//放入方法</div><div class=\"line\">\t\t\t&#125; </div><div class=\"line\"> &#125;);</div></pre></td></tr></table></figure></p>\n<p>例子如下<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">SwingUtilities.invokeLater(new   Runnable()&#123; </div><div class=\"line\">\t\tpublic   void   run() &#123; </div><div class=\"line\">\t\ttree.updateUI(); </div><div class=\"line\">\t\t&#125; </div><div class=\"line\"> &#125;);</div></pre></td></tr></table></figure></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">SwingUtilities.invokeLater(new   Runnable() &#123; </div><div class=\"line\">\t\tpublic   void   run() &#123; </div><div class=\"line\">\t\t\tSwingUtilities.updateComponentTreeUI(jf); </div><div class=\"line\">\t\t&#125; </div><div class=\"line\">\t\t\t&#125;);</div></pre></td></tr></table></figure>","site":{"data":{}},"excerpt":"","more":"<p>Exception in thread “AWT-EventQueue-0” java.lang.NullPointerException<br>更新UI等操作时异常<br><strong>个人理解原因</strong>：<br>类似于Android中的更新ui一样，ui线程相当于主线程，再对此进行直接更新，两个线程事物会冲突报这个异常，在Android中使用的是handle的方法，同理,类似于，在java 界面编程上，我们把其他的线程放在一个队列当中，我们称这个队列为消息队列，可以使用如下方法，把更新UI的操作，放在下面的run方法中去。 这样，就实现了消息队列的方式。 最后由主线程有序的处理这些消息事件<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">SwingUtilities.invokeLater(new   Runnable()&#123; </div><div class=\"line\">\t\t\tpublic   void   run() &#123; </div><div class=\"line\">\t\t\t//放入方法</div><div class=\"line\">\t\t\t&#125; </div><div class=\"line\"> &#125;);</div></pre></td></tr></table></figure></p>\n<p>例子如下<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">SwingUtilities.invokeLater(new   Runnable()&#123; </div><div class=\"line\">\t\tpublic   void   run() &#123; </div><div class=\"line\">\t\ttree.updateUI(); </div><div class=\"line\">\t\t&#125; </div><div class=\"line\"> &#125;);</div></pre></td></tr></table></figure></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">SwingUtilities.invokeLater(new   Runnable() &#123; </div><div class=\"line\">\t\tpublic   void   run() &#123; </div><div class=\"line\">\t\t\tSwingUtilities.updateComponentTreeUI(jf); </div><div class=\"line\">\t\t&#125; </div><div class=\"line\">\t\t\t&#125;);</div></pre></td></tr></table></figure>"},{"title":"spark lost task 异常笔记","date":"2017-02-05T13:25:21.000Z","author":"kaishun","id":"33","_content":"\n# Lost task java.lang.NullPointerException\nException in thread \"main\" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.NullPointerException  \n\n我之前的代码类似于\n```java\npublic Iterable<Tuple2<String, String>> call(String s) throws Exception{\n    Tuple2<String, String> tp=null;\n    try{\n        if(XXX){\n            tp=new Tuple2<>(xxx, xxx);\n        }\n        if(xxx){\n          tp=new Tuple2<>(xxx, xxx);  \n        }\n        return Arrays.asList(tp);\n    }\n    catch(Exception e){\n        e.printStackTrace();\n        return new ArrayList<Tuple2<String, String>>();\n    }\n\n}\n```\n发现没有进入catch，那应该没问题啊，最后才发现，原来是tp对于两个if都不满足，return Arrays.asList(tp);中，tp为null，导致程序出错,**更加要注意的是，对于容错，如果某条数据不要，千万不能返回null,可以返回类似于new ArrayList<Tuple2<String, String>>();的list即可,返回null同样会报如上的错误**  \n\n**特别是传过来的参数有可能导致空指针**  \n一个对象并不是空的，但是在spark的内部，却报空指针，即使实现了序列化也有可能出现这种情况，这种情况下，应该像mapreduce中的setUp函数,或者使用广播传进来，这两种方法该如何实现呢？  参考 TODO\n\n\n## Lost task java.lang.OutOfMemoryError: Java heap space   \njava.lang.OutOfMemoryError: Java heap space\n        at java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:76)\n        at java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:59)\n        at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2.<init>(UnsafeRowSerializer.scala:55)  \n        \n解决办法：\n\n     由于我们在执行Spark任务是，读取所需要的原数据，数据量太大，导致在Worker上面分配的任务执行数据时所需要的内存不够，直接导致内存溢出了，所以我们有必要增加Worker上面的内存来满足程序运行需要。\n\n在Spark Streaming或者其他spark任务中，会遇到在Spark中常见的问题，典型如Executor Lost 相关的问题(shuffle fetch 失败，Task失败重试等)。这就意味着发生了内存不足或者数据倾斜的问题。这个目前需要考虑如下几个点以获得解决方案：\n\nA、相同资源下，增加partition数可以减少内存问题。 原因如下：通过增加partition数，每个task要处理的数据少了，同一时间内，所有正在运行的task要处理的数量少了很多，所有Executor占用的内存也变小了。这可以缓解数据倾斜以及内存不足的压力。\n\nB、关注shuffle read 阶段的并行数。例如reduce,group 之类的函数，其实他们都有第二个参数，并行度(partition数)，只是大家一般都不设置。不过出了问题再设置一下，也不错。\n\nC、给一个Executor 核数设置的太多，也就意味着同一时刻，在该Executor 的内存压力会更大，GC也会更频繁。我一般会控制在3个左右。然后通过提高Executor数量来保持资源的总量不变。  \n\n# Lost task FileNotFoundException  \n```\nWARN TaskSetManager: Lost task 0.0 in stage 10.0 (TID 17, xx.xx.xx.xx): java.io.FileNotFoundException: File file:/usr/spark/test.txt does not exist\nat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:534)\nat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:747)\nat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:524)\nat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:409)\nat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:140)\nat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:341)\nat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:766)\nat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:108)\nat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\nat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:239)\nat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:216)\nat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101)\nat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\nat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\nat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)\nat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\nat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\nat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)\nat org.apache.spark.scheduler.Task.run(Task.scala:70)\nat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\nat java.lang.Thread.run(Thread.java:745)\n```  \n解决办法\n1. 确保文件路径没有错误\n2. 确保文件没有损坏\n\n# Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.ArrayIndexOutOfBoundsException:  \n数组越界,这种问题最好是本地先运行小数据可以成功，再尝试在集群运行大数据，一般会有提示，按照提示去看是为什么数组越界了   \n\n\n\n\n\n\n\n\n\n\n\n# 异常  Exception in thread \"main\" org.apache.spark.SparkException: Job cancelled because SparkContext was shut down  \n```\nException in thread \"main\" org.apache.spark.SparkException: Job cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:736)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:735)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:735)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1468)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1403)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:1642)\n\tat org.apache.spark.SparkContext$$anonfun$3.apply$mcV$sp(SparkContext.scala:559)\n\tat org.apache.spark.util.SparkShutdownHook.run(Utils.scala:2292)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(Utils.scala:2262)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2262)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2262)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(Utils.scala:2262)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2262)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2262)\n\tat scala.util.Try$.apply(Try.scala:161)\n\tat org.apache.spark.util.SparkShutdownHookManager.runAll(Utils.scala:2262)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anon$6.run(Utils.scala:2244)\n\tat org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)\n```\n原因是我当时测试的时候 ，在flatmap的iterator的方法中使用了System.exit(0),sparkcontext关闭取消,所以导致报此错误  \n\n\n##  Lost task 5.0 in stage 1.0 (TID 32, node003): java.lang.OutOfMemoryError: unable to create new native thread \n\n一般是内存溢出，内存溢出情况很多种，单个execute 负载太高等都可能导致，如果是yarn模式，execute-memory设置多一些\n\n\n##  Lost task 1.0 in stage 6.0 (TID 17, 192.168.xx.xx): org.apache.hadoop.ipc.RemoteException(java.io.IOException)  File xxx  could only be replicated to 0 nodes instead of minReplication (=1).  There are 4 datanode(s) running and 4 node(s) are excluded in this operation\n```\nat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1550)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3110)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3034)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:723)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)\n        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)\n\n        at org.apache.hadoop.ipc.Client.call(Client.java:1468)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1399)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)\n        at com.sun.proxy.$Proxy13.addBlock(Unknown Source)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:399)\n        at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n        at com.sun.proxy.$Proxy14.addBlock(Unknown Source)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1532)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1349)\n\n```\n\n可能的情况：参考 http://blog.csdn.net/zuiaituantuan/article/details/6533867\n\n\n##  WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 4, 192.168.x.xx): java.net.SocketException: Too many open files\n是否是在spark中，有大量打开文件操作，打开文件操作太多，可能会报这个错误\n\n\n\n\n\n\n\n","source":"_posts/spark lost task 异常 笔记.md","raw":"---\ntitle: spark lost task 异常笔记\ndate: 2017-02-05 21:25:21\ntags: [spark]\ncategories: [大数据,spark]\nauthor: kaishun\nid: 33\npermalink: spark-exception\n---\n\n# Lost task java.lang.NullPointerException\nException in thread \"main\" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.NullPointerException  \n\n我之前的代码类似于\n```java\npublic Iterable<Tuple2<String, String>> call(String s) throws Exception{\n    Tuple2<String, String> tp=null;\n    try{\n        if(XXX){\n            tp=new Tuple2<>(xxx, xxx);\n        }\n        if(xxx){\n          tp=new Tuple2<>(xxx, xxx);  \n        }\n        return Arrays.asList(tp);\n    }\n    catch(Exception e){\n        e.printStackTrace();\n        return new ArrayList<Tuple2<String, String>>();\n    }\n\n}\n```\n发现没有进入catch，那应该没问题啊，最后才发现，原来是tp对于两个if都不满足，return Arrays.asList(tp);中，tp为null，导致程序出错,**更加要注意的是，对于容错，如果某条数据不要，千万不能返回null,可以返回类似于new ArrayList<Tuple2<String, String>>();的list即可,返回null同样会报如上的错误**  \n\n**特别是传过来的参数有可能导致空指针**  \n一个对象并不是空的，但是在spark的内部，却报空指针，即使实现了序列化也有可能出现这种情况，这种情况下，应该像mapreduce中的setUp函数,或者使用广播传进来，这两种方法该如何实现呢？  参考 TODO\n\n\n## Lost task java.lang.OutOfMemoryError: Java heap space   \njava.lang.OutOfMemoryError: Java heap space\n        at java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:76)\n        at java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:59)\n        at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2.<init>(UnsafeRowSerializer.scala:55)  \n        \n解决办法：\n\n     由于我们在执行Spark任务是，读取所需要的原数据，数据量太大，导致在Worker上面分配的任务执行数据时所需要的内存不够，直接导致内存溢出了，所以我们有必要增加Worker上面的内存来满足程序运行需要。\n\n在Spark Streaming或者其他spark任务中，会遇到在Spark中常见的问题，典型如Executor Lost 相关的问题(shuffle fetch 失败，Task失败重试等)。这就意味着发生了内存不足或者数据倾斜的问题。这个目前需要考虑如下几个点以获得解决方案：\n\nA、相同资源下，增加partition数可以减少内存问题。 原因如下：通过增加partition数，每个task要处理的数据少了，同一时间内，所有正在运行的task要处理的数量少了很多，所有Executor占用的内存也变小了。这可以缓解数据倾斜以及内存不足的压力。\n\nB、关注shuffle read 阶段的并行数。例如reduce,group 之类的函数，其实他们都有第二个参数，并行度(partition数)，只是大家一般都不设置。不过出了问题再设置一下，也不错。\n\nC、给一个Executor 核数设置的太多，也就意味着同一时刻，在该Executor 的内存压力会更大，GC也会更频繁。我一般会控制在3个左右。然后通过提高Executor数量来保持资源的总量不变。  \n\n# Lost task FileNotFoundException  \n```\nWARN TaskSetManager: Lost task 0.0 in stage 10.0 (TID 17, xx.xx.xx.xx): java.io.FileNotFoundException: File file:/usr/spark/test.txt does not exist\nat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:534)\nat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:747)\nat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:524)\nat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:409)\nat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:140)\nat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:341)\nat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:766)\nat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:108)\nat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\nat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:239)\nat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:216)\nat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101)\nat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\nat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\nat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)\nat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\nat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\nat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)\nat org.apache.spark.scheduler.Task.run(Task.scala:70)\nat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\nat java.lang.Thread.run(Thread.java:745)\n```  \n解决办法\n1. 确保文件路径没有错误\n2. 确保文件没有损坏\n\n# Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.ArrayIndexOutOfBoundsException:  \n数组越界,这种问题最好是本地先运行小数据可以成功，再尝试在集群运行大数据，一般会有提示，按照提示去看是为什么数组越界了   \n\n\n\n\n\n\n\n\n\n\n\n# 异常  Exception in thread \"main\" org.apache.spark.SparkException: Job cancelled because SparkContext was shut down  \n```\nException in thread \"main\" org.apache.spark.SparkException: Job cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:736)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:735)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:735)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1468)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1403)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:1642)\n\tat org.apache.spark.SparkContext$$anonfun$3.apply$mcV$sp(SparkContext.scala:559)\n\tat org.apache.spark.util.SparkShutdownHook.run(Utils.scala:2292)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(Utils.scala:2262)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2262)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2262)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(Utils.scala:2262)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2262)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2262)\n\tat scala.util.Try$.apply(Try.scala:161)\n\tat org.apache.spark.util.SparkShutdownHookManager.runAll(Utils.scala:2262)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anon$6.run(Utils.scala:2244)\n\tat org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)\n```\n原因是我当时测试的时候 ，在flatmap的iterator的方法中使用了System.exit(0),sparkcontext关闭取消,所以导致报此错误  \n\n\n##  Lost task 5.0 in stage 1.0 (TID 32, node003): java.lang.OutOfMemoryError: unable to create new native thread \n\n一般是内存溢出，内存溢出情况很多种，单个execute 负载太高等都可能导致，如果是yarn模式，execute-memory设置多一些\n\n\n##  Lost task 1.0 in stage 6.0 (TID 17, 192.168.xx.xx): org.apache.hadoop.ipc.RemoteException(java.io.IOException)  File xxx  could only be replicated to 0 nodes instead of minReplication (=1).  There are 4 datanode(s) running and 4 node(s) are excluded in this operation\n```\nat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1550)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3110)\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3034)\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:723)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)\n        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)\n\n        at org.apache.hadoop.ipc.Client.call(Client.java:1468)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1399)\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)\n        at com.sun.proxy.$Proxy13.addBlock(Unknown Source)\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:399)\n        at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n        at com.sun.proxy.$Proxy14.addBlock(Unknown Source)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1532)\n        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1349)\n\n```\n\n可能的情况：参考 http://blog.csdn.net/zuiaituantuan/article/details/6533867\n\n\n##  WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 4, 192.168.x.xx): java.net.SocketException: Too many open files\n是否是在spark中，有大量打开文件操作，打开文件操作太多，可能会报这个错误\n\n\n\n\n\n\n\n","slug":"spark-exception","published":1,"updated":"2018-01-22T15:21:26.130Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjcrpnzx1006u2wv39u5pqn0a","content":"<h1 id=\"Lost-task-java-lang-NullPointerException\"><a href=\"#Lost-task-java-lang-NullPointerException\" class=\"headerlink\" title=\"Lost task java.lang.NullPointerException\"></a>Lost task java.lang.NullPointerException</h1><p>Exception in thread “main” org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.NullPointerException  </p>\n<p>我之前的代码类似于<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">public</span> Iterable&lt;Tuple2&lt;String, String&gt;&gt; call(String s) <span class=\"keyword\">throws</span> Exception&#123;</div><div class=\"line\">    Tuple2&lt;String, String&gt; tp=<span class=\"keyword\">null</span>;</div><div class=\"line\">    <span class=\"keyword\">try</span>&#123;</div><div class=\"line\">        <span class=\"keyword\">if</span>(XXX)&#123;</div><div class=\"line\">            tp=<span class=\"keyword\">new</span> Tuple2&lt;&gt;(xxx, xxx);</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">if</span>(xxx)&#123;</div><div class=\"line\">          tp=<span class=\"keyword\">new</span> Tuple2&lt;&gt;(xxx, xxx);  </div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">return</span> Arrays.asList(tp);</div><div class=\"line\">    &#125;</div><div class=\"line\">    <span class=\"keyword\">catch</span>(Exception e)&#123;</div><div class=\"line\">        e.printStackTrace();</div><div class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> ArrayList&lt;Tuple2&lt;String, String&gt;&gt;();</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>发现没有进入catch，那应该没问题啊，最后才发现，原来是tp对于两个if都不满足，return Arrays.asList(tp);中，tp为null，导致程序出错,<strong>更加要注意的是，对于容错，如果某条数据不要，千万不能返回null,可以返回类似于new ArrayList<tuple2<string, string=\"\">&gt;();的list即可,返回null同样会报如上的错误</tuple2<string,></strong>  </p>\n<p><strong>特别是传过来的参数有可能导致空指针</strong><br>一个对象并不是空的，但是在spark的内部，却报空指针，即使实现了序列化也有可能出现这种情况，这种情况下，应该像mapreduce中的setUp函数,或者使用广播传进来，这两种方法该如何实现呢？  参考 TODO</p>\n<h2 id=\"Lost-task-java-lang-OutOfMemoryError-Java-heap-space\"><a href=\"#Lost-task-java-lang-OutOfMemoryError-Java-heap-space\" class=\"headerlink\" title=\"Lost task java.lang.OutOfMemoryError: Java heap space\"></a>Lost task java.lang.OutOfMemoryError: Java heap space</h2><p>java.lang.OutOfMemoryError: Java heap space<br>        at java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:76)<br>        at java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:59)<br>        at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2.<init>(UnsafeRowSerializer.scala:55)  </init></init></init></p>\n<p>解决办法：</p>\n<pre><code>由于我们在执行Spark任务是，读取所需要的原数据，数据量太大，导致在Worker上面分配的任务执行数据时所需要的内存不够，直接导致内存溢出了，所以我们有必要增加Worker上面的内存来满足程序运行需要。\n</code></pre><p>在Spark Streaming或者其他spark任务中，会遇到在Spark中常见的问题，典型如Executor Lost 相关的问题(shuffle fetch 失败，Task失败重试等)。这就意味着发生了内存不足或者数据倾斜的问题。这个目前需要考虑如下几个点以获得解决方案：</p>\n<p>A、相同资源下，增加partition数可以减少内存问题。 原因如下：通过增加partition数，每个task要处理的数据少了，同一时间内，所有正在运行的task要处理的数量少了很多，所有Executor占用的内存也变小了。这可以缓解数据倾斜以及内存不足的压力。</p>\n<p>B、关注shuffle read 阶段的并行数。例如reduce,group 之类的函数，其实他们都有第二个参数，并行度(partition数)，只是大家一般都不设置。不过出了问题再设置一下，也不错。</p>\n<p>C、给一个Executor 核数设置的太多，也就意味着同一时刻，在该Executor 的内存压力会更大，GC也会更频繁。我一般会控制在3个左右。然后通过提高Executor数量来保持资源的总量不变。  </p>\n<h1 id=\"Lost-task-FileNotFoundException\"><a href=\"#Lost-task-FileNotFoundException\" class=\"headerlink\" title=\"Lost task FileNotFoundException\"></a>Lost task FileNotFoundException</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div></pre></td><td class=\"code\"><pre><div class=\"line\">WARN TaskSetManager: Lost task 0.0 in stage 10.0 (TID 17, xx.xx.xx.xx): java.io.FileNotFoundException: File file:/usr/spark/test.txt does not exist</div><div class=\"line\">at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:534)</div><div class=\"line\">at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:747)</div><div class=\"line\">at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:524)</div><div class=\"line\">at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:409)</div><div class=\"line\">at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.&lt;init&gt;(ChecksumFileSystem.java:140)</div><div class=\"line\">at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:341)</div><div class=\"line\">at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:766)</div><div class=\"line\">at org.apache.hadoop.mapred.LineRecordReader.&lt;init&gt;(LineRecordReader.java:108)</div><div class=\"line\">at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)</div><div class=\"line\">at org.apache.spark.rdd.HadoopRDD$$anon$1.&lt;init&gt;(HadoopRDD.scala:239)</div><div class=\"line\">at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:216)</div><div class=\"line\">at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101)</div><div class=\"line\">at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)</div><div class=\"line\">at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)</div><div class=\"line\">at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)</div><div class=\"line\">at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)</div><div class=\"line\">at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)</div><div class=\"line\">at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)</div><div class=\"line\">at org.apache.spark.scheduler.Task.run(Task.scala:70)</div><div class=\"line\">at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)</div><div class=\"line\">at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)</div><div class=\"line\">at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)</div><div class=\"line\">at java.lang.Thread.run(Thread.java:745)</div><div class=\"line\">```  </div><div class=\"line\">解决办法</div><div class=\"line\">1. 确保文件路径没有错误</div><div class=\"line\">2. 确保文件没有损坏</div><div class=\"line\"></div><div class=\"line\"># Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.ArrayIndexOutOfBoundsException:  </div><div class=\"line\">数组越界,这种问题最好是本地先运行小数据可以成功，再尝试在集群运行大数据，一般会有提示，按照提示去看是为什么数组越界了   </div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"># 异常  Exception in thread &quot;main&quot; org.apache.spark.SparkException: Job cancelled because SparkContext was shut down</div></pre></td></tr></table></figure>\n<p>Exception in thread “main” org.apache.spark.SparkException: Job cancelled because SparkContext was shut down<br>    at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:736)<br>    at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:735)<br>    at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)<br>    at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:735)<br>    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1468)<br>    at org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)<br>    at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1403)<br>    at org.apache.spark.SparkContext.stop(SparkContext.scala:1642)<br>    at org.apache.spark.SparkContext$$anonfun$3.apply$mcV$sp(SparkContext.scala:559)<br>    at org.apache.spark.util.SparkShutdownHook.run(Utils.scala:2292)<br>    at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(Utils.scala:2262)<br>    at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2262)<br>    at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2262)<br>    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)<br>    at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(Utils.scala:2262)<br>    at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2262)<br>    at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2262)<br>    at scala.util.Try$.apply(Try.scala:161)<br>    at org.apache.spark.util.SparkShutdownHookManager.runAll(Utils.scala:2262)<br>    at org.apache.spark.util.SparkShutdownHookManager$$anon$6.run(Utils.scala:2244)<br>    at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">原因是我当时测试的时候 ，在flatmap的iterator的方法中使用了System.exit(0),sparkcontext关闭取消,所以导致报此错误  </div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">##  Lost task 5.0 in stage 1.0 (TID 32, node003): java.lang.OutOfMemoryError: unable to create new native thread </div><div class=\"line\"></div><div class=\"line\">一般是内存溢出，内存溢出情况很多种，单个execute 负载太高等都可能导致，如果是yarn模式，execute-memory设置多一些</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">##  Lost task 1.0 in stage 6.0 (TID 17, 192.168.xx.xx): org.apache.hadoop.ipc.RemoteException(java.io.IOException)  File xxx  could only be replicated to 0 nodes instead of minReplication (=1).  There are 4 datanode(s) running and 4 node(s) are excluded in this operation</div></pre></td></tr></table></figure></p>\n<p>at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1550)<br>        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3110)<br>        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3034)<br>        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:723)<br>        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)<br>        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)<br>        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)<br>        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)<br>        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)<br>        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)<br>        at java.security.AccessController.doPrivileged(Native Method)<br>        at javax.security.auth.Subject.doAs(Subject.java:415)<br>        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)<br>        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)</p>\n<pre><code>at org.apache.hadoop.ipc.Client.call(Client.java:1468)\nat org.apache.hadoop.ipc.Client.call(Client.java:1399)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)\nat com.sun.proxy.$Proxy13.addBlock(Unknown Source)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:399)\nat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:606)\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\nat com.sun.proxy.$Proxy14.addBlock(Unknown Source)\nat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1532)\nat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1349)\n</code></pre><p>```</p>\n<p>可能的情况：参考 <a href=\"http://blog.csdn.net/zuiaituantuan/article/details/6533867\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/zuiaituantuan/article/details/6533867</a></p>\n<h2 id=\"WARN-TaskSetManager-Lost-task-0-0-in-stage-2-0-TID-4-192-168-x-xx-java-net-SocketException-Too-many-open-files\"><a href=\"#WARN-TaskSetManager-Lost-task-0-0-in-stage-2-0-TID-4-192-168-x-xx-java-net-SocketException-Too-many-open-files\" class=\"headerlink\" title=\"WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 4, 192.168.x.xx): java.net.SocketException: Too many open files\"></a>WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 4, 192.168.x.xx): java.net.SocketException: Too many open files</h2><p>是否是在spark中，有大量打开文件操作，打开文件操作太多，可能会报这个错误</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Lost-task-java-lang-NullPointerException\"><a href=\"#Lost-task-java-lang-NullPointerException\" class=\"headerlink\" title=\"Lost task java.lang.NullPointerException\"></a>Lost task java.lang.NullPointerException</h1><p>Exception in thread “main” org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.NullPointerException  </p>\n<p>我之前的代码类似于<br><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">public</span> Iterable&lt;Tuple2&lt;String, String&gt;&gt; call(String s) <span class=\"keyword\">throws</span> Exception&#123;</div><div class=\"line\">    Tuple2&lt;String, String&gt; tp=<span class=\"keyword\">null</span>;</div><div class=\"line\">    <span class=\"keyword\">try</span>&#123;</div><div class=\"line\">        <span class=\"keyword\">if</span>(XXX)&#123;</div><div class=\"line\">            tp=<span class=\"keyword\">new</span> Tuple2&lt;&gt;(xxx, xxx);</div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">if</span>(xxx)&#123;</div><div class=\"line\">          tp=<span class=\"keyword\">new</span> Tuple2&lt;&gt;(xxx, xxx);  </div><div class=\"line\">        &#125;</div><div class=\"line\">        <span class=\"keyword\">return</span> Arrays.asList(tp);</div><div class=\"line\">    &#125;</div><div class=\"line\">    <span class=\"keyword\">catch</span>(Exception e)&#123;</div><div class=\"line\">        e.printStackTrace();</div><div class=\"line\">        <span class=\"keyword\">return</span> <span class=\"keyword\">new</span> ArrayList&lt;Tuple2&lt;String, String&gt;&gt;();</div><div class=\"line\">    &#125;</div><div class=\"line\"></div><div class=\"line\">&#125;</div></pre></td></tr></table></figure></p>\n<p>发现没有进入catch，那应该没问题啊，最后才发现，原来是tp对于两个if都不满足，return Arrays.asList(tp);中，tp为null，导致程序出错,<strong>更加要注意的是，对于容错，如果某条数据不要，千万不能返回null,可以返回类似于new ArrayList<tuple2<string, string=\"\">&gt;();的list即可,返回null同样会报如上的错误</tuple2<string,></strong>  </p>\n<p><strong>特别是传过来的参数有可能导致空指针</strong><br>一个对象并不是空的，但是在spark的内部，却报空指针，即使实现了序列化也有可能出现这种情况，这种情况下，应该像mapreduce中的setUp函数,或者使用广播传进来，这两种方法该如何实现呢？  参考 TODO</p>\n<h2 id=\"Lost-task-java-lang-OutOfMemoryError-Java-heap-space\"><a href=\"#Lost-task-java-lang-OutOfMemoryError-Java-heap-space\" class=\"headerlink\" title=\"Lost task java.lang.OutOfMemoryError: Java heap space\"></a>Lost task java.lang.OutOfMemoryError: Java heap space</h2><p>java.lang.OutOfMemoryError: Java heap space<br>        at java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:76)<br>        at java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:59)<br>        at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2.<init>(UnsafeRowSerializer.scala:55)  </init></init></init></p>\n<p>解决办法：</p>\n<pre><code>由于我们在执行Spark任务是，读取所需要的原数据，数据量太大，导致在Worker上面分配的任务执行数据时所需要的内存不够，直接导致内存溢出了，所以我们有必要增加Worker上面的内存来满足程序运行需要。\n</code></pre><p>在Spark Streaming或者其他spark任务中，会遇到在Spark中常见的问题，典型如Executor Lost 相关的问题(shuffle fetch 失败，Task失败重试等)。这就意味着发生了内存不足或者数据倾斜的问题。这个目前需要考虑如下几个点以获得解决方案：</p>\n<p>A、相同资源下，增加partition数可以减少内存问题。 原因如下：通过增加partition数，每个task要处理的数据少了，同一时间内，所有正在运行的task要处理的数量少了很多，所有Executor占用的内存也变小了。这可以缓解数据倾斜以及内存不足的压力。</p>\n<p>B、关注shuffle read 阶段的并行数。例如reduce,group 之类的函数，其实他们都有第二个参数，并行度(partition数)，只是大家一般都不设置。不过出了问题再设置一下，也不错。</p>\n<p>C、给一个Executor 核数设置的太多，也就意味着同一时刻，在该Executor 的内存压力会更大，GC也会更频繁。我一般会控制在3个左右。然后通过提高Executor数量来保持资源的总量不变。  </p>\n<h1 id=\"Lost-task-FileNotFoundException\"><a href=\"#Lost-task-FileNotFoundException\" class=\"headerlink\" title=\"Lost task FileNotFoundException\"></a>Lost task FileNotFoundException</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div><div class=\"line\">38</div><div class=\"line\">39</div><div class=\"line\">40</div><div class=\"line\">41</div><div class=\"line\">42</div><div class=\"line\">43</div></pre></td><td class=\"code\"><pre><div class=\"line\">WARN TaskSetManager: Lost task 0.0 in stage 10.0 (TID 17, xx.xx.xx.xx): java.io.FileNotFoundException: File file:/usr/spark/test.txt does not exist</div><div class=\"line\">at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:534)</div><div class=\"line\">at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:747)</div><div class=\"line\">at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:524)</div><div class=\"line\">at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:409)</div><div class=\"line\">at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.&lt;init&gt;(ChecksumFileSystem.java:140)</div><div class=\"line\">at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:341)</div><div class=\"line\">at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:766)</div><div class=\"line\">at org.apache.hadoop.mapred.LineRecordReader.&lt;init&gt;(LineRecordReader.java:108)</div><div class=\"line\">at org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)</div><div class=\"line\">at org.apache.spark.rdd.HadoopRDD$$anon$1.&lt;init&gt;(HadoopRDD.scala:239)</div><div class=\"line\">at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:216)</div><div class=\"line\">at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101)</div><div class=\"line\">at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)</div><div class=\"line\">at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)</div><div class=\"line\">at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)</div><div class=\"line\">at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)</div><div class=\"line\">at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)</div><div class=\"line\">at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)</div><div class=\"line\">at org.apache.spark.scheduler.Task.run(Task.scala:70)</div><div class=\"line\">at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)</div><div class=\"line\">at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)</div><div class=\"line\">at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)</div><div class=\"line\">at java.lang.Thread.run(Thread.java:745)</div><div class=\"line\">```  </div><div class=\"line\">解决办法</div><div class=\"line\">1. 确保文件路径没有错误</div><div class=\"line\">2. 确保文件没有损坏</div><div class=\"line\"></div><div class=\"line\"># Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.ArrayIndexOutOfBoundsException:  </div><div class=\"line\">数组越界,这种问题最好是本地先运行小数据可以成功，再尝试在集群运行大数据，一般会有提示，按照提示去看是为什么数组越界了   </div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\"># 异常  Exception in thread &quot;main&quot; org.apache.spark.SparkException: Job cancelled because SparkContext was shut down</div></pre></td></tr></table></figure>\n<p>Exception in thread “main” org.apache.spark.SparkException: Job cancelled because SparkContext was shut down<br>    at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:736)<br>    at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:735)<br>    at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)<br>    at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:735)<br>    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1468)<br>    at org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)<br>    at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1403)<br>    at org.apache.spark.SparkContext.stop(SparkContext.scala:1642)<br>    at org.apache.spark.SparkContext$$anonfun$3.apply$mcV$sp(SparkContext.scala:559)<br>    at org.apache.spark.util.SparkShutdownHook.run(Utils.scala:2292)<br>    at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(Utils.scala:2262)<br>    at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2262)<br>    at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2262)<br>    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1772)<br>    at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(Utils.scala:2262)<br>    at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2262)<br>    at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2262)<br>    at scala.util.Try$.apply(Try.scala:161)<br>    at org.apache.spark.util.SparkShutdownHookManager.runAll(Utils.scala:2262)<br>    at org.apache.spark.util.SparkShutdownHookManager$$anon$6.run(Utils.scala:2244)<br>    at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div></pre></td><td class=\"code\"><pre><div class=\"line\">原因是我当时测试的时候 ，在flatmap的iterator的方法中使用了System.exit(0),sparkcontext关闭取消,所以导致报此错误  </div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">##  Lost task 5.0 in stage 1.0 (TID 32, node003): java.lang.OutOfMemoryError: unable to create new native thread </div><div class=\"line\"></div><div class=\"line\">一般是内存溢出，内存溢出情况很多种，单个execute 负载太高等都可能导致，如果是yarn模式，execute-memory设置多一些</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">##  Lost task 1.0 in stage 6.0 (TID 17, 192.168.xx.xx): org.apache.hadoop.ipc.RemoteException(java.io.IOException)  File xxx  could only be replicated to 0 nodes instead of minReplication (=1).  There are 4 datanode(s) running and 4 node(s) are excluded in this operation</div></pre></td></tr></table></figure></p>\n<p>at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1550)<br>        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3110)<br>        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3034)<br>        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:723)<br>        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)<br>        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)<br>        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)<br>        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)<br>        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)<br>        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)<br>        at java.security.AccessController.doPrivileged(Native Method)<br>        at javax.security.auth.Subject.doAs(Subject.java:415)<br>        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)<br>        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)</p>\n<pre><code>at org.apache.hadoop.ipc.Client.call(Client.java:1468)\nat org.apache.hadoop.ipc.Client.call(Client.java:1399)\nat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)\nat com.sun.proxy.$Proxy13.addBlock(Unknown Source)\nat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:399)\nat sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:606)\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\nat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\nat com.sun.proxy.$Proxy14.addBlock(Unknown Source)\nat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1532)\nat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1349)\n</code></pre><p>```</p>\n<p>可能的情况：参考 <a href=\"http://blog.csdn.net/zuiaituantuan/article/details/6533867\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/zuiaituantuan/article/details/6533867</a></p>\n<h2 id=\"WARN-TaskSetManager-Lost-task-0-0-in-stage-2-0-TID-4-192-168-x-xx-java-net-SocketException-Too-many-open-files\"><a href=\"#WARN-TaskSetManager-Lost-task-0-0-in-stage-2-0-TID-4-192-168-x-xx-java-net-SocketException-Too-many-open-files\" class=\"headerlink\" title=\"WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 4, 192.168.x.xx): java.net.SocketException: Too many open files\"></a>WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 4, 192.168.x.xx): java.net.SocketException: Too many open files</h2><p>是否是在spark中，有大量打开文件操作，打开文件操作太多，可能会报这个错误</p>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"cjcrpnznn00082wv354a8qrsu","category_id":"cjcrpnzn800042wv37h6lvkzr","_id":"cjcrpnzoy000z2wv3zr8mteh6"},{"post_id":"cjcrpnznn00082wv354a8qrsu","category_id":"cjcrpnzoj000n2wv3t8z6zp0g","_id":"cjcrpnzoy00132wv39qmqbo81"},{"post_id":"cjcrpnzms00002wv3fyr62dhb","category_id":"cjcrpnzn800042wv37h6lvkzr","_id":"cjcrpnzoy00172wv35lfmvggg"},{"post_id":"cjcrpnzms00002wv3fyr62dhb","category_id":"cjcrpnzoj000n2wv3t8z6zp0g","_id":"cjcrpnzpe001b2wv3ml5tcfl1"},{"post_id":"cjcrpnznn000a2wv33us3gtup","category_id":"cjcrpnzn800042wv37h6lvkzr","_id":"cjcrpnzpe001f2wv39kk02g81"},{"post_id":"cjcrpnznn000a2wv33us3gtup","category_id":"cjcrpnzoj000n2wv3t8z6zp0g","_id":"cjcrpnzpe001j2wv3t91s23fs"},{"post_id":"cjcrpnzo3000e2wv3uwcepjae","category_id":"cjcrpnzn800042wv37h6lvkzr","_id":"cjcrpnzpe001m2wv3jqeeg00z"},{"post_id":"cjcrpnzo3000e2wv3uwcepjae","category_id":"cjcrpnzoj000n2wv3t8z6zp0g","_id":"cjcrpnzpt001r2wv35pb47ei0"},{"post_id":"cjcrpnzn800022wv3iyxu0l7v","category_id":"cjcrpnzn800042wv37h6lvkzr","_id":"cjcrpnzpt001u2wv39prkcj8c"},{"post_id":"cjcrpnzn800022wv3iyxu0l7v","category_id":"cjcrpnzoj000n2wv3t8z6zp0g","_id":"cjcrpnzpt001z2wv3atlfvnaw"},{"post_id":"cjcrpnzo3000g2wv3h7fg1qlx","category_id":"cjcrpnzn800042wv37h6lvkzr","_id":"cjcrpnzq900222wv32pmbuhvv"},{"post_id":"cjcrpnzo3000g2wv3h7fg1qlx","category_id":"cjcrpnzoj000n2wv3t8z6zp0g","_id":"cjcrpnzq900262wv3t0o4rdb4"},{"post_id":"cjcrpnzo3000k2wv30sqlt08c","category_id":"cjcrpnzn800042wv37h6lvkzr","_id":"cjcrpnzq900282wv3faxbwpun"},{"post_id":"cjcrpnzo3000k2wv30sqlt08c","category_id":"cjcrpnzpt001v2wv3aqr3z0fa","_id":"cjcrpnzq9002d2wv37eqyyt10"},{"post_id":"cjcrpnzq900252wv3iyji4c5t","category_id":"cjcrpnzn800042wv37h6lvkzr","_id":"cjcrpnzqp002g2wv3zi1h11lv"},{"post_id":"cjcrpnzq900252wv3iyji4c5t","category_id":"cjcrpnzpt001v2wv3aqr3z0fa","_id":"cjcrpnzqp002l2wv3qeeqfpw9"},{"post_id":"cjcrpnzn800062wv3n98yv5ai","category_id":"cjcrpnzn800042wv37h6lvkzr","_id":"cjcrpnzqp002o2wv33oc4zjvd"},{"post_id":"cjcrpnzn800062wv3n98yv5ai","category_id":"cjcrpnzoj000n2wv3t8z6zp0g","_id":"cjcrpnzr4002s2wv3mggf0bzm"},{"post_id":"cjcrpnzoj000m2wv3db66k2bm","category_id":"cjcrpnzn800042wv37h6lvkzr","_id":"cjcrpnzr4002w2wv3ufaqtu2m"},{"post_id":"cjcrpnzoj000m2wv3db66k2bm","category_id":"cjcrpnzoj000n2wv3t8z6zp0g","_id":"cjcrpnzr400302wv3zxtl82f0"},{"post_id":"cjcrpnzoj000q2wv3j1q8mhvm","category_id":"cjcrpnzn800042wv37h6lvkzr","_id":"cjcrpnzr400342wv31ujpnmu7"},{"post_id":"cjcrpnzoj000q2wv3j1q8mhvm","category_id":"cjcrpnzpt001v2wv3aqr3z0fa","_id":"cjcrpnzr400382wv34zv2pc44"},{"post_id":"cjcrpnzoj000s2wv35yqdy8iu","category_id":"cjcrpnzn800042wv37h6lvkzr","_id":"cjcrpnzrk003c2wv3vs2fnsdt"},{"post_id":"cjcrpnzoj000s2wv35yqdy8iu","category_id":"cjcrpnzpt001v2wv3aqr3z0fa","_id":"cjcrpnzrk003g2wv3orrvnxnn"},{"post_id":"cjcrpnzoj000v2wv3tosc38eg","category_id":"cjcrpnzn800042wv37h6lvkzr","_id":"cjcrpnzrk003k2wv32x20pv28"},{"post_id":"cjcrpnzoj000v2wv3tosc38eg","category_id":"cjcrpnzpt001v2wv3aqr3z0fa","_id":"cjcrpnzs0003o2wv3qwtba7y1"},{"post_id":"cjcrpnzoy000y2wv3gr4tilph","category_id":"cjcrpnzr400352wv3ky4aitoj","_id":"cjcrpnzs0003r2wv3zuwdazxu"},{"post_id":"cjcrpnzrk003a2wv3qdyuepl2","category_id":"cjcrpnzn800042wv37h6lvkzr","_id":"cjcrpnzs0003w2wv3tgyxw3or"},{"post_id":"cjcrpnzrk003a2wv3qdyuepl2","category_id":"cjcrpnzpt001v2wv3aqr3z0fa","_id":"cjcrpnzsk003z2wv3rkdw0n45"},{"post_id":"cjcrpnzoy00122wv33zzvehxb","category_id":"cjcrpnzn800042wv37h6lvkzr","_id":"cjcrpnzss00442wv365nm66w8"},{"post_id":"cjcrpnzoy00122wv33zzvehxb","category_id":"cjcrpnzpt001v2wv3aqr3z0fa","_id":"cjcrpnzsx00472wv3a4wtdyzh"},{"post_id":"cjcrpnzoy00162wv33yp7xp5w","category_id":"cjcrpnzn800042wv37h6lvkzr","_id":"cjcrpnzt6004c2wv311zwqrj5"},{"post_id":"cjcrpnzoy00162wv33yp7xp5w","category_id":"cjcrpnzpt001v2wv3aqr3z0fa","_id":"cjcrpnzta004e2wv3e4gs63yi"},{"post_id":"cjcrpnzpe001a2wv3tgbg7ew3","category_id":"cjcrpnzr400352wv3ky4aitoj","_id":"cjcrpnztd004i2wv35jwzcag0"},{"post_id":"cjcrpnzpe001d2wv38jzjpdkh","category_id":"cjcrpnzsm00422wv3sa1xu923","_id":"cjcrpnztd004k2wv37gn4wgoy"},{"post_id":"cjcrpnzpe001i2wv39qwh8u1g","category_id":"cjcrpnzn800042wv37h6lvkzr","_id":"cjcrpnztk004n2wv3ysho0boy"},{"post_id":"cjcrpnzpe001i2wv39qwh8u1g","category_id":"cjcrpnzt4004b2wv3rpm0qt0r","_id":"cjcrpnztk004q2wv3j9qt17ab"},{"post_id":"cjcrpnzpt001t2wv3lhxm2rvi","category_id":"cjcrpnzn800042wv37h6lvkzr","_id":"cjcrpnztk004y2wv3mfg8xwo9"},{"post_id":"cjcrpnzpt001t2wv3lhxm2rvi","category_id":"cjcrpnzpt001v2wv3aqr3z0fa","_id":"cjcrpnztk00512wv3nrp0rce7"},{"post_id":"cjcrpnzpt001y2wv3r9verdd9","category_id":"cjcrpnzsm00422wv3sa1xu923","_id":"cjcrpnztz00532wv3jrtg9lw9"},{"post_id":"cjcrpnzq900212wv3t0trcu0c","category_id":"cjcrpnzsm00422wv3sa1xu923","_id":"cjcrpnztz00582wv3l24s173m"},{"post_id":"cjcrpnzq900272wv3dnci1z2c","category_id":"cjcrpnzn800042wv37h6lvkzr","_id":"cjcrpnztz005c2wv3x7r4enec"},{"post_id":"cjcrpnzq900272wv3dnci1z2c","category_id":"cjcrpnzt4004b2wv3rpm0qt0r","_id":"cjcrpnztz005f2wv3t6323w1u"},{"post_id":"cjcrpnzq9002b2wv3jey3i00t","category_id":"cjcrpnzsm00422wv3sa1xu923","_id":"cjcrpnztz005h2wv3b569shx5"},{"post_id":"cjcrpnzqp002j2wv3o0j8upj7","category_id":"cjcrpnzn800042wv37h6lvkzr","_id":"cjcrpnztz005j2wv38fl5pc6u"},{"post_id":"cjcrpnzqp002j2wv3o0j8upj7","category_id":"cjcrpnzt4004b2wv3rpm0qt0r","_id":"cjcrpnzuf005l2wv3byaukif1"},{"post_id":"cjcrpnzqp002n2wv35gyujqmp","category_id":"cjcrpnzn800042wv37h6lvkzr","_id":"cjcrpnzuf005m2wv395gm4nx9"},{"post_id":"cjcrpnzqp002n2wv35gyujqmp","category_id":"cjcrpnzt4004b2wv3rpm0qt0r","_id":"cjcrpnzuf005o2wv3ws8bl8p8"},{"post_id":"cjcrpnzqp002r2wv3ubopczd5","category_id":"cjcrpnzsm00422wv3sa1xu923","_id":"cjcrpnzuf005p2wv30gt6wcjc"},{"post_id":"cjcrpnzr4002v2wv3m8ffyugv","category_id":"cjcrpnzn800042wv37h6lvkzr","_id":"cjcrpnzuf005r2wv3zjwko53w"},{"post_id":"cjcrpnzr4002v2wv3m8ffyugv","category_id":"cjcrpnzt4004b2wv3rpm0qt0r","_id":"cjcrpnzuf005t2wv30wsjxxfm"},{"post_id":"cjcrpnzr4002z2wv3ier8eqtg","category_id":"cjcrpnzn800042wv37h6lvkzr","_id":"cjcrpnzuf005u2wv37s8t1mi2"},{"post_id":"cjcrpnzr4002z2wv3ier8eqtg","category_id":"cjcrpnzt4004b2wv3rpm0qt0r","_id":"cjcrpnzuf005w2wv3hqsbjhsx"},{"post_id":"cjcrpnzr400332wv3r2tzmuji","category_id":"cjcrpnzn800042wv37h6lvkzr","_id":"cjcrpnzuf005x2wv38tq17s9y"},{"post_id":"cjcrpnzr400332wv3r2tzmuji","category_id":"cjcrpnzt4004b2wv3rpm0qt0r","_id":"cjcrpnzuf005z2wv3h5rtc2rz"},{"post_id":"cjcrpnzr400372wv3t8mb5g9y","category_id":"cjcrpnzn800042wv37h6lvkzr","_id":"cjcrpnzuf00602wv3t44qmbug"},{"post_id":"cjcrpnzr400372wv3t8mb5g9y","category_id":"cjcrpnzt4004b2wv3rpm0qt0r","_id":"cjcrpnzuf00622wv3e7ag2xif"},{"post_id":"cjcrpnzrk003f2wv37l6ul63e","category_id":"cjcrpnzn800042wv37h6lvkzr","_id":"cjcrpnzuu00632wv3kfvwthed"},{"post_id":"cjcrpnzrk003f2wv37l6ul63e","category_id":"cjcrpnzt4004b2wv3rpm0qt0r","_id":"cjcrpnzuu00652wv3nqx8rdvn"},{"post_id":"cjcrpnzrk003i2wv36xp0leea","category_id":"cjcrpnzn800042wv37h6lvkzr","_id":"cjcrpnzuu00662wv3l6jeuqd5"},{"post_id":"cjcrpnzrk003i2wv36xp0leea","category_id":"cjcrpnzt4004b2wv3rpm0qt0r","_id":"cjcrpnzuu00682wv3vnf296h8"},{"post_id":"cjcrpnzrk003n2wv3x3j5krox","category_id":"cjcrpnzn800042wv37h6lvkzr","_id":"cjcrpnzuu00692wv3jdqczqms"},{"post_id":"cjcrpnzrk003n2wv3x3j5krox","category_id":"cjcrpnzt4004b2wv3rpm0qt0r","_id":"cjcrpnzuu006b2wv39fh5vigx"},{"post_id":"cjcrpnzs0003q2wv35ylijy8v","category_id":"cjcrpnzn800042wv37h6lvkzr","_id":"cjcrpnzuu006c2wv3g84tjgvo"},{"post_id":"cjcrpnzs0003q2wv35ylijy8v","category_id":"cjcrpnzt4004b2wv3rpm0qt0r","_id":"cjcrpnzuu006e2wv3ulw5t7k0"},{"post_id":"cjcrpnzs0003v2wv335hsrh3u","category_id":"cjcrpnzn800042wv37h6lvkzr","_id":"cjcrpnzuu006f2wv3nbictjpk"},{"post_id":"cjcrpnzs0003v2wv335hsrh3u","category_id":"cjcrpnzt4004b2wv3rpm0qt0r","_id":"cjcrpnzuu006h2wv39l1d5l1w"},{"post_id":"cjcrpnzsf003y2wv3izf8ugti","category_id":"cjcrpnzn800042wv37h6lvkzr","_id":"cjcrpnzuu006i2wv3lg2kv7wu"},{"post_id":"cjcrpnzsf003y2wv3izf8ugti","category_id":"cjcrpnzt4004b2wv3rpm0qt0r","_id":"cjcrpnzuu006k2wv3l8m9yyug"},{"post_id":"cjcrpnzsn00432wv36tlf5f62","category_id":"cjcrpnzuu006g2wv386v09ito","_id":"cjcrpnzuu006l2wv3z6sc9cnk"},{"post_id":"cjcrpnzsu00462wv3hrjylfm2","category_id":"cjcrpnzuu006j2wv3im8bnerx","_id":"cjcrpnzva006n2wv37p2cvc81"},{"post_id":"cjcrpnzsu00462wv3hrjylfm2","category_id":"cjcrpnzuu006m2wv3mj5sij65","_id":"cjcrpnzva006o2wv3moiqa5rp"},{"post_id":"cjcrpnzw5006p2wv3i2bhvb3w","category_id":"cjcrpnzn800042wv37h6lvkzr","_id":"cjcrpnzw5006r2wv3jg0162k5"},{"post_id":"cjcrpnzw5006p2wv3i2bhvb3w","category_id":"cjcrpnzt4004b2wv3rpm0qt0r","_id":"cjcrpnzw5006s2wv306r59ydd"},{"post_id":"cjcrpnzx1006t2wv3l0vrohpu","category_id":"cjcrpnzsm00422wv3sa1xu923","_id":"cjcrpnzxg006x2wv3hk3nmbnx"},{"post_id":"cjcrpnzx1006u2wv39u5pqn0a","category_id":"cjcrpnzn800042wv37h6lvkzr","_id":"cjcrpnzxg006y2wv3dwch7tda"},{"post_id":"cjcrpnzx1006u2wv39u5pqn0a","category_id":"cjcrpnzt4004b2wv3rpm0qt0r","_id":"cjcrpnzxg00702wv31qq3wtvz"}],"PostTag":[{"post_id":"cjcrpnznn00082wv354a8qrsu","tag_id":"cjcrpnzn800052wv3apxlqblr","_id":"cjcrpnzo3000d2wv34ydsslaj"},{"post_id":"cjcrpnzms00002wv3fyr62dhb","tag_id":"cjcrpnzn800052wv3apxlqblr","_id":"cjcrpnzo3000f2wv3dfgwpfcb"},{"post_id":"cjcrpnznn000a2wv33us3gtup","tag_id":"cjcrpnzn800052wv3apxlqblr","_id":"cjcrpnzo3000j2wv3uq3hofqx"},{"post_id":"cjcrpnzo3000e2wv3uwcepjae","tag_id":"cjcrpnzn800052wv3apxlqblr","_id":"cjcrpnzoj000l2wv3aiybpr69"},{"post_id":"cjcrpnzn800022wv3iyxu0l7v","tag_id":"cjcrpnzn800052wv3apxlqblr","_id":"cjcrpnzoj000p2wv3a99ybcwi"},{"post_id":"cjcrpnzo3000g2wv3h7fg1qlx","tag_id":"cjcrpnzn800052wv3apxlqblr","_id":"cjcrpnzoj000r2wv3fpm05yo2"},{"post_id":"cjcrpnzn800062wv3n98yv5ai","tag_id":"cjcrpnzn800052wv3apxlqblr","_id":"cjcrpnzoj000u2wv3387rqy92"},{"post_id":"cjcrpnzoj000m2wv3db66k2bm","tag_id":"cjcrpnzn800052wv3apxlqblr","_id":"cjcrpnzoy000x2wv3hn4jwx9v"},{"post_id":"cjcrpnzoj000s2wv35yqdy8iu","tag_id":"cjcrpnzoj000o2wv33zo8wqf0","_id":"cjcrpnzoy00112wv33idj7xft"},{"post_id":"cjcrpnzo3000k2wv30sqlt08c","tag_id":"cjcrpnzoj000o2wv33zo8wqf0","_id":"cjcrpnzoy00152wv3yqvmu366"},{"post_id":"cjcrpnzoj000v2wv3tosc38eg","tag_id":"cjcrpnzoj000o2wv33zo8wqf0","_id":"cjcrpnzpe00192wv3n5er87uj"},{"post_id":"cjcrpnzoj000q2wv3j1q8mhvm","tag_id":"cjcrpnzoj000o2wv33zo8wqf0","_id":"cjcrpnzpe001c2wv367sir5hs"},{"post_id":"cjcrpnzoy00122wv33zzvehxb","tag_id":"cjcrpnzoj000o2wv33zo8wqf0","_id":"cjcrpnzpe001h2wv3lb8y8hay"},{"post_id":"cjcrpnzoy00162wv33yp7xp5w","tag_id":"cjcrpnzoj000o2wv33zo8wqf0","_id":"cjcrpnzpe001k2wv30so38ozx"},{"post_id":"cjcrpnzpe001a2wv3tgbg7ew3","tag_id":"cjcrpnzoy00142wv3umdvap28","_id":"cjcrpnzpe001p2wv3gx8agdo6"},{"post_id":"cjcrpnzoy000y2wv3gr4tilph","tag_id":"cjcrpnzoy00142wv3umdvap28","_id":"cjcrpnzpt001s2wv3wtwz4yo9"},{"post_id":"cjcrpnzoy000y2wv3gr4tilph","tag_id":"cjcrpnzpe001e2wv37ksjszmy","_id":"cjcrpnzpt001x2wv31e6pgupt"},{"post_id":"cjcrpnzpt001t2wv3lhxm2rvi","tag_id":"cjcrpnzoj000o2wv33zo8wqf0","_id":"cjcrpnzq900202wv3j7ikjltq"},{"post_id":"cjcrpnzq900252wv3iyji4c5t","tag_id":"cjcrpnzoj000o2wv33zo8wqf0","_id":"cjcrpnzq9002a2wv3aj4xv4ia"},{"post_id":"cjcrpnzpe001d2wv38jzjpdkh","tag_id":"cjcrpnzpe001n2wv3ykcl9cyx","_id":"cjcrpnzq9002e2wv333xyhiay"},{"post_id":"cjcrpnzpe001d2wv38jzjpdkh","tag_id":"cjcrpnzpt001w2wv36skbdrzp","_id":"cjcrpnzqp002i2wv3we2vi7ns"},{"post_id":"cjcrpnzpe001d2wv38jzjpdkh","tag_id":"cjcrpnzq900242wv354agagn0","_id":"cjcrpnzqp002m2wv3f9m50my9"},{"post_id":"cjcrpnzpe001i2wv39qwh8u1g","tag_id":"cjcrpnzq9002c2wv3i377zhe5","_id":"cjcrpnzqp002q2wv3bjlgtbvj"},{"post_id":"cjcrpnzqp002j2wv3o0j8upj7","tag_id":"cjcrpnzq9002c2wv3i377zhe5","_id":"cjcrpnzr4002u2wv3npripsta"},{"post_id":"cjcrpnzqp002n2wv35gyujqmp","tag_id":"cjcrpnzq9002c2wv3i377zhe5","_id":"cjcrpnzr4002y2wv3745al1li"},{"post_id":"cjcrpnzr4002v2wv3m8ffyugv","tag_id":"cjcrpnzq9002c2wv3i377zhe5","_id":"cjcrpnzr400322wv3zdonpoah"},{"post_id":"cjcrpnzr4002z2wv3ier8eqtg","tag_id":"cjcrpnzq9002c2wv3i377zhe5","_id":"cjcrpnzr400362wv3lup3dj9i"},{"post_id":"cjcrpnzr400332wv3r2tzmuji","tag_id":"cjcrpnzq9002c2wv3i377zhe5","_id":"cjcrpnzr400392wv3n21oxfdq"},{"post_id":"cjcrpnzr400372wv3t8mb5g9y","tag_id":"cjcrpnzq9002c2wv3i377zhe5","_id":"cjcrpnzrk003d2wv3jfqcf2zv"},{"post_id":"cjcrpnzrk003a2wv3qdyuepl2","tag_id":"cjcrpnzoj000o2wv33zo8wqf0","_id":"cjcrpnzrk003h2wv3mkg6rgj3"},{"post_id":"cjcrpnzrk003f2wv37l6ul63e","tag_id":"cjcrpnzq9002c2wv3i377zhe5","_id":"cjcrpnzrk003l2wv3xdddb8gf"},{"post_id":"cjcrpnzrk003i2wv36xp0leea","tag_id":"cjcrpnzq9002c2wv3i377zhe5","_id":"cjcrpnzs0003p2wv3kvbc9ug1"},{"post_id":"cjcrpnzrk003n2wv3x3j5krox","tag_id":"cjcrpnzq9002c2wv3i377zhe5","_id":"cjcrpnzs0003t2wv304zof3yj"},{"post_id":"cjcrpnzs0003q2wv35ylijy8v","tag_id":"cjcrpnzq9002c2wv3i377zhe5","_id":"cjcrpnztb004f2wv3rv7o6exr"},{"post_id":"cjcrpnzs0003v2wv335hsrh3u","tag_id":"cjcrpnzq9002c2wv3i377zhe5","_id":"cjcrpnztd004j2wv3yjh3fikc"},{"post_id":"cjcrpnzpt001y2wv3r9verdd9","tag_id":"cjcrpnzs0003s2wv3e1phr6id","_id":"cjcrpnztd004l2wv3jseq7nn8"},{"post_id":"cjcrpnzsf003y2wv3izf8ugti","tag_id":"cjcrpnzq9002c2wv3i377zhe5","_id":"cjcrpnztk004p2wv3dvj9a9jl"},{"post_id":"cjcrpnzq900212wv3t0trcu0c","tag_id":"cjcrpnzpe001n2wv3ykcl9cyx","_id":"cjcrpnztk004r2wv3v2jb9387"},{"post_id":"cjcrpnzq900212wv3t0trcu0c","tag_id":"cjcrpnzsk00402wv377l8trs9","_id":"cjcrpnztk004v2wv392xn28ac"},{"post_id":"cjcrpnzq900272wv3dnci1z2c","tag_id":"cjcrpnzq9002c2wv3i377zhe5","_id":"cjcrpnztz00562wv3eda0kn77"},{"post_id":"cjcrpnzq9002b2wv3jey3i00t","tag_id":"cjcrpnzs0003s2wv3e1phr6id","_id":"cjcrpnztz00572wv36bkz1eow"},{"post_id":"cjcrpnzqp002r2wv3ubopczd5","tag_id":"cjcrpnzs0003s2wv3e1phr6id","_id":"cjcrpnztz005a2wv3eo168vgt"},{"post_id":"cjcrpnzsn00432wv36tlf5f62","tag_id":"cjcrpnztk004s2wv319oyb4h4","_id":"cjcrpnztz005b2wv3dtnyptyt"},{"post_id":"cjcrpnzsu00462wv3hrjylfm2","tag_id":"cjcrpnztk004x2wv38vi0vq4q","_id":"cjcrpnztz005e2wv3d18530i4"},{"post_id":"cjcrpnzsu00462wv3hrjylfm2","tag_id":"cjcrpnztz00542wv3bon9mk9q","_id":"cjcrpnztz005g2wv3m1sfjd90"},{"post_id":"cjcrpnzw5006p2wv3i2bhvb3w","tag_id":"cjcrpnzq9002c2wv3i377zhe5","_id":"cjcrpnzw5006q2wv3al12gjas"},{"post_id":"cjcrpnzx1006u2wv39u5pqn0a","tag_id":"cjcrpnzq9002c2wv3i377zhe5","_id":"cjcrpnzxg006w2wv3hcacphs7"},{"post_id":"cjcrpnzx1006t2wv3l0vrohpu","tag_id":"cjcrpnzpe001n2wv3ykcl9cyx","_id":"cjcrpnzxg006z2wv31uk75uki"},{"post_id":"cjcrpnzx1006t2wv3l0vrohpu","tag_id":"cjcrpnzxg006v2wv369nevbrb","_id":"cjcrpnzxg00712wv35a9yt9qs"}],"Tag":[{"name":"hdfs","_id":"cjcrpnzn800052wv3apxlqblr"},{"name":"hadoop","_id":"cjcrpnzoj000o2wv33zo8wqf0"},{"name":"linux","_id":"cjcrpnzoy00142wv3umdvap28"},{"name":"crontab","_id":"cjcrpnzpe001e2wv37ksjszmy"},{"name":"java","_id":"cjcrpnzpe001n2wv3ykcl9cyx"},{"name":"JDBC","_id":"cjcrpnzpt001w2wv36skbdrzp"},{"name":"工具类","_id":"cjcrpnzq900242wv354agagn0"},{"name":"spark","_id":"cjcrpnzq9002c2wv3i377zhe5"},{"name":"npm","_id":"cjcrpnzqp002k2wv38bd2htrm"},{"name":"hexo","_id":"cjcrpnzr4002t2wv3a7ac2i5x"},{"name":"github","_id":"cjcrpnzr400312wv3ijszr7tp"},{"name":"tags1","_id":"cjcrpnzrk003b2wv3ordtzw7i"},{"name":"tags2","_id":"cjcrpnzrk003j2wv32iv6xbq7"},{"name":"scala","_id":"cjcrpnzs0003s2wv3e1phr6id"},{"name":"曲线拟合","_id":"cjcrpnzsk00402wv377l8trs9"},{"name":"随笔","_id":"cjcrpnztk004s2wv319oyb4h4"},{"name":"标签1","_id":"cjcrpnztk004x2wv38vi0vq4q"},{"name":"标签2","_id":"cjcrpnztz00542wv3bon9mk9q"},{"name":"Exception","_id":"cjcrpnzxg006v2wv369nevbrb"}]}}