<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[java线上程序排错经验5 -linux及其集群环境的分析]]></title>
    <url>%2Fjava%2Fjvm06.html</url>
    <content type="text"><![CDATA[1. top命令查看整体情况top命令和灵活，具体可自行搜索图转自： www.codesheep.cn 2. 查看内存free -m分析系统内存，看是否足够程序运行 3. 磁盘占用情况3.1. 查看文件夹中各文件（夹）的大小举例1du -h --max-depth=1 /home/ubuntu/ 3.2. 查看磁盘占用情况1df -h 4. 查看磁盘IO1dd if=/dev/zero of=dd.file bs=100M count=10 conv=fdatasync 上述命令会在当前目录生成一个dd.file文件，每次往这个文件写100M，一共写10次，conv=fdatasync表示实际写盘而不是用缓存，测试磁盘IO的时候，必须加上这个才准 5 网络能否ping通ping ip如果是同一个集群系统，查看是否ping通，是否丢包情况，ping速度如何 6. 查看网卡以及测试集群中SCP复制速度ifconfig找到本机Ip，对应的即为本IP网卡假如你看到的网卡是 eth0， 查看网卡速度的命令为1ethtool eth0 有时候网卡会有问题，导致网络有问题，排除后，我们可以测试节点间数据复制情况scp 命令将上面生成的文件复制到另外一个机器，查看其速度 7 查看cpu报告与设备利用率报告iostat, linux需要先安装使用方法：iostat123456789ubuntu@VM-0-12-ubuntu:~/sample/testio$ iostatLinux 4.4.0-91-generic (VM-0-12-ubuntu) 09/16/2018 _x86_64_ (1 CPU)avg-cpu: %user %nice %system %iowait %steal %idle 0.63 0.01 0.41 0.34 0.00 98.62Device: tps kB_read/s kB_wrtn/s kB_read kB_wrtnvda 1.86 5.31 15.11 27627271 78590297scd0 0.00 0.00 0.00 208 0 第一部分包含了CPU报告 %user : 显示了在执行用户(应用)层时的CPU利用率 %nice : 显示了在以nice优先级运行用户层的CPU利用率 %system : 显示了在执行系统(内核)层时的CPU利用率 %iowait : 显示了CPU在I/O请求挂起时空闲时间的百分比 %steal : 显示了当hypervisor正服务于另外一个虚拟处理器时无意识地等待虚拟CPU所占有的时间百分比。 %idle : 显示了CPU在I/O没有挂起请求时空闲时间的百分比第二部分包含了设备利用率报告 Device : 列出的/dev 目录下的设备/分区名称 tps : 显示每秒传输给设备的数量。更高的tps意味着处理器更忙。 Blk_read/s : 显示了每秒从设备上读取的块的数量(KB,MB) Blk_wrtn/s : 显示了每秒写入设备上块的数量(KB,MB) Blk_read : 显示所有已读取的块 Blk_wrtn : 显示所有已写入的块 8. 查看目前程序的IO情况iotop需要先安装使用方法(如果没设置权限，可能需要在root权限才能使用)1iotop 会显示1TID PRIO USER DISK READ DISK WRITE SWAPIN IO&gt; COMMAND 其中 DISK READ DISK WRITE 读写速度 IO io占比 COMMAND 运行的命令 未完待续： 以后会逐渐补充]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java线上程序排错经验4 -Btrace了解一下]]></title>
    <url>%2Fjava%2Fjvm04.html</url>
    <content type="text"><![CDATA[简介在生产环境中经常遇到格式各样的问题，如OOM或者莫名其妙的进程死掉。一般情况下是通过修改程序，添加打印日志；然后重新发布程序来完成。然而，这不仅麻烦，而且带来很多不可控的因素。有没有一种方式，在不修改原有运行程序的情况下获取运行时的数据信息呢？如方法参数、返回值、全局变量、堆栈信息等。Btrace就是这样一个工具，它可以在不修改原有代码的情况下动态地追踪java运行程序，通过hotswap技术，动态将跟踪字节码注入到运行类中，进行监控甚至到达修改程序的目的 1. 下载https://github.com/btraceio/btrace/releases/latest 2. 安装配置环境变量 本地jar包依赖 本地写脚本的准备Btrace是脚本，不过也是.java的格式，写脚本前建议引入以下几个jar包，方便于代码编写的提示jar包引用： 安装目录下build中有三个jar包123btrace-agent.jarbtrace-boot.jarbtrace-client.jar 三个核心jar直接拷贝到工程中临时使用即可123456789101112131415161718192021&lt;dependency&gt; &lt;groupId&gt;com.sun.tools.btrace&lt;/groupId&gt; &lt;artifactId&gt;btrace-agent&lt;/artifactId&gt; &lt;version&gt;1.3.11&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;D:/programmesoft/btrace/build/btrace-agent.jar&lt;/systemPath&gt;&lt;dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.sun.tools.btrace&lt;/groupId&gt; &lt;artifactId&gt;btrace-boot&lt;/artifactId&gt; &lt;version&gt;1.3.11&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;D:/programmesoft/btrace/build/btrace-boot.jar&lt;/systemPath&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.sun.tools.btrace&lt;/groupId&gt; &lt;artifactId&gt;btrace-client&lt;/artifactId&gt; &lt;version&gt;1.3.11&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;D:/programmesoft/btrace/build/btrace-client.jar&lt;/systemPath&gt;&lt;/dependency&gt; 4. 简单入门4.1 待测试代码如下FirstSample该程序有三个方法，main, fun1, fun2main方法一个while循环不断的调用fun1方法，传入了两个字符串”aa”,”bb”，fun1调用fun2方法，传入一个字符串”bb”，然后休眠3秒钟，fun2方法会返回一个”fun2”+传入的参数12345678910111213141516171819202122import java.util.concurrent.TimeUnit;public class FirstSample &#123; public static void main(String[] args) &#123; FirstSample firstSample = new FirstSample(); while (true)&#123; firstSample.fun1(&quot;aa&quot;,&quot;bb&quot;); &#125; &#125; public void fun1(String str1,String str2)&#123; try &#123; String result = fun2(str2); TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; private String fun2(String str2) &#123; System.out.println(&quot;fun2 参数为:&quot;+str2); return &quot;fun2&quot;+str2; &#125;&#125; 4.2 Btrace入门案例假如我们想要知道每次调用fun1都是一些什么参数，我们得脚本如下12345678910111213import com.sun.btrace.BTraceUtils;import com.sun.btrace.annotations.*;@BTracepublic class FirstSampleBtrace &#123; @OnMethod( clazz = &quot;me.zks.jvm.troubleshoot.btrace.FirstSample&quot; ,method = &quot;fun1&quot; ,location = @Location(value = Kind.ENTRY)) public static void m(@Self Object self,String str1,String str2)&#123; BTraceUtils.print(&quot;fun1 str1:&quot;+str1+&quot; str2: &quot;+str2+&quot; &quot;); &#125;&#125; 其中第4行@BTrace注解表示使用Btrace来监控@OnMethod里面定义了内容如下 clazz = “me.zks.jvm.troubleshoot.btrace.FirstSample”定义了哪个对象 method = “fun1”定义了哪个方法 location = @Location(value = Kind.ENTRY))定义了拦截的时机，我这里定义的是进入程序的时候进行拦截。更多拦截的请参考:m方法： m取名是任意取得里面的参数 @Self Object String str1, String str2 这个是传入的参数，这个参数必须要和我们定义的方法参数一致，如果方法有重载，我们会根据这个参数的来判断去拦截哪个方法 BTraceUtils.print(“fun1 str1:”+str1+” str2: “+str2); BtraceUtil的输出方法，用来输出结果 将上面的FIrstSample运行起来程序每三秒执行一次123456789fun1参数为:aa,bbfun2参数为:bbfun1参数为:aa,bbfun2参数为:bbfun1参数为:aa,bbfun2参数为:bbfun1参数为:aa,bbfun2参数为:bb... 用jps查看此程序的PID,发现PID为5044123456C:\Users\Administrator&gt;jps24042420 RemoteMavenServer3540 Launcher5044 FirstSample3372 Jps 启动脚本1btrace -v 5044 FirstSampleBtrace.java 其中 -v 代表可以输出到控制台，我们方便控制台查看 5044是PID FirstSampleBtrace.java是我们得脚本名其中输入-h 可以看使用方法12345678910111213141516C:\Users\Administrator&gt;btrace -hUsage: btrace &lt;options&gt; &lt;pid&gt; &lt;btrace source or .class file&gt; &lt;btrace arguments&gt;where possible options include: --version Show the version -v Run in verbose mode -o &lt;file&gt; The path to store the probe output (will disable showing the output in console)-u Run in trusted mode -d &lt;path&gt; Dump the instrumented classes to the specified path -pd &lt;path&gt; The search path for the probe XML descriptors -classpath &lt;path&gt; Specify where to find user class files and annotation processors -cp &lt;path&gt; Specify where to find user class files and annotation processors -I &lt;path&gt; Specify where to find include files -p &lt;port&gt; Specify port to which the btrace agent listens for clients -statsd &lt;host[:port]&gt; Specify the statsd server, if anyC:\Users\Administrator&gt; 显示1234567891011121314151617181920212223242526DEBUG: loading D:\programmesoft\btrace\build\btrace-agent.jarDEBUG: agent args: port=2020,statsd=,debug=true,bootClassPath=.,systemClassPath=C:\Program Files\Java\jdk1.8.0_92\jre/../lib/tools.jar,probeDescPath=.DEBUG: loaded D:\programmesoft\btrace\build\btrace-agent.jarDEBUG: registering shutdown hookDEBUG: registering signal handler for SIGINTDEBUG: submitting the BTrace programDEBUG: opening socket to 2020DEBUG: setting up client settingsDEBUG: sending instrument command: []DEBUG: entering into command loopDEBUG: received com.sun.btrace.comm.OkayCommand@74a10858DEBUG: received com.sun.btrace.comm.RetransformationStartNotification@23fe1d71DEBUG: received com.sun.btrace.comm.OkayCommand@28ac3dc3DEBUG: received com.sun.btrace.comm.MessageCommand@1d371b2dfun1 str1:aa str2: bb DEBUG: received com.sun.btrace.comm.MessageCommand@543c6f6dfun1 str1:aa str2: bb DEBUG: received com.sun.btrace.comm.MessageCommand@13eb8acffun1 str1:aa str2: bb DEBUG: received com.sun.btrace.comm.MessageCommand@51c8530ffun1 str1:aa str2: bb DEBUG: received com.sun.btrace.comm.MessageCommand@7403c468fun1 str1:aa str2: bb DEBUG: received com.sun.btrace.comm.MessageCommand@43738a82fun1 str1:aa str2: bb 我们通过打印的输出可以看到程序开启了2020端口，fun1 str1:aa str2: bb 达到了我们监控的目的 4.3 Btrace案例2-返回时拦截得到执行的时间假如我们想要知道每次调用fun2都调用了什么参数，并且fun1使用了多长的时间，代码如下FirstSampleBtrace123456789101112131415161718192021222324252627import com.sun.btrace.BTraceUtils;import com.sun.btrace.annotations.*;@BTracepublic class FirstSampleBtrace &#123;// @OnMethod(// clazz = &quot;me.zks.jvm.troubleshoot.btrace.FirstSample&quot;// ,method = &quot;fun1&quot;// ,location = @Location(value = Kind.ENTRY))// public static void m(@Self Object self,String str1,String str2)&#123;//,String str2// BTraceUtils.print(&quot;fun1 str1:&quot;+str1+&quot; str2: &quot;+str2);// &#125; @OnMethod(clazz = &quot;me.zks.jvm.troubleshoot.btrace.FirstSample&quot;, method = &quot;fun2&quot;, location = @Location(value = Kind.ENTRY)) public static void m1(@Self Object self,String str1)&#123; BTraceUtils.print(&quot;fun2 str1:&quot;+str1+&quot; &quot;); &#125; @OnMethod(clazz = &quot;me.zks.jvm.troubleshoot.btrace.FirstSample&quot;, method = &quot;fun1&quot;, location = @Location(value = Kind.RETURN)) public static void m2(@Self Object self,String str1,String str2,@Return Void a,@Duration long time)&#123; BTraceUtils.print(&quot;fun1 str1:&quot;+str1+&quot; str2:&quot;+str2+&quot; Duration is: &quot;+time+&quot; &quot;); &#125;&#125; 运行后即可知道打印出拦截的时间123456789101112131415161718192021222324252627D:\git\jvmtroubleshoot\src\main\java\me\zks\jvm\troubleshoot\btrace&gt;btrace -v 6392 FirstSampleBtrace.javaDEBUG: assuming default port 2020DEBUG: assuming default classpath &apos;.&apos;DEBUG: compiling FirstSampleBtrace.javaDEBUG: compiled FirstSampleBtrace.javaDEBUG: attaching to 6392DEBUG: checking port availability: 2020DEBUG: attached to 6392DEBUG: loading D:\programmesoft\btrace\build\btrace-agent.jarDEBUG: agent args: port=2020,statsd=,debug=true,bootClassPath=.,systemClassPath=C:\Program Files\Java\jdk1.8.0_92\jre/../lib/tools.jar,probeDescPatDEBUG: loaded D:\programmesoft\btrace\build\btrace-agent.jarDEBUG: registering shutdown hookDEBUG: registering signal handler for SIGINTDEBUG: submitting the BTrace programDEBUG: opening socket to 2020DEBUG: setting up client settingsDEBUG: sending instrument command: []DEBUG: entering into command loopDEBUG: received com.sun.btrace.comm.OkayCommand@fcd6521DEBUG: received com.sun.btrace.comm.RetransformationStartNotification@27d415d9DEBUG: received com.sun.btrace.comm.OkayCommand@5c18298fDEBUG: received com.sun.btrace.comm.MessageCommand@5204062dfun2 str1:bb DEBUG: received com.sun.btrace.comm.MessageCommand@4fcd19b3fun1 str1:aa str2:bb Duration is: 3001048056 DEBUG: received com.sun.btrace.comm.MessageCommand@376b4233fun2 str1:bb DEBUG: received com.sun.btrace.comm.MessageCommand@2fd66ad3fun1 str1:aa str2:bb Duration is: 2999950327 DEBUG: received com.sun.btrace.comm.MessageCommand@5d11346afun2 str1:bb DEBUG: received com.sun.btrace.comm.MessageCommand@7a36aefa 4.4 Btrace退出方法btrace退出时按ctrl+c 然后选择1进行退出，然后确认Y12345678fun1 str1:aa str2:bb Duration is: 2999939243 Please enter your option: 1. exit 2. send an event 3. send a named event 4. flush console output1DEBUG: sending exit command终止批处理操作吗(Y/N)? y 发现Btrace的一个bug，版本1.3.09~1.3.11都有，return的时候，当ctrl+c 然后选择1时会导致入侵的程序crash，具体报错Btrace java.lang.NoSuchMethodError123Exception in thread &quot;main&quot; java.lang.NoSuchMethodError: me.zks.jvm.troubleshoot.btrace.FirstSample.$btrace$me$zks$jvm$troubleshoot$btrace$FirstSampleBtrace$m2(Ljava/lang/Object;Ljava/lang/String;Ljava/lang/String;Ljava/lang/Void;)V at me.zks.jvm.troubleshoot.btrace.FirstSample.fun1(Unknown Source) at me.zks.jvm.troubleshoot.btrace.FirstSample.main(Unknown Source) 这个应该是Btrace的一个bug，我已经向作者提了issues原因可能是由于TimeUnit.SECONDS.sleep(3)和Thread.sleep(3)导致，我猜测对于线程执行sleep()或join()方法时，Btrace有bug导致所以说，我们得平时学习的时候，用Object.wait的等待阻塞来测试，而不应用Thread.sleep来测试，并且注意，如果我们拦截的方法中有Thread.sleep，这几个版本最好是做好jvm crash的打算，这点需要注意1234Object o = new Object();synchronized (o)&#123; o.wait(1000);&#125; 以后的测试，我将会使用上面的方法进行停顿。 4.5 Btrace案例4 异常抛出和异常捕获Kind.ERROR待测试代码如下代码在不断的调用divide方法，随机传一个数字进去, divide123456789101112131415161718192021222324252627package me.zks.jvm.troubleshoot.btrace;import java.util.Random;public class BtraceThrow &#123; public static void main(String[] args) &#123; Random random = new Random(); BtraceThrow btraceThrow = new BtraceThrow(); while (true)&#123; int randomNum =random.nextInt(4); try &#123; int result = btraceThrow.divide(randomNum); System.out.println(&quot;result: &quot;+result); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; public int divide(int a) throws Exception&#123; Object object = new Object(); //每次暂停1秒 synchronized (object)&#123; object.wait(1000); &#125; return 100/a; &#125;&#125; btrace脚本如下其中需要有一个@TargetInstance Throwable参数123456789101112@BTracepublic class BtraceThrowScript &#123; @OnMethod( clazz = &quot;me.zks.jvm.troubleshoot.btrace.BtraceThrow&quot;, method = &quot;divide&quot;, location = @Location(Kind.ERROR) ) public static void testThrow(@Self Object self, @TargetInstance Throwable err , int a)&#123; BTraceUtils.print(&quot;a: &quot;); BTraceUtils.Threads.jstack(err); &#125;&#125; 监听后输出123456789101112131415161718192021222324252627282930DEBUG: received com.sun.btrace.comm.OkayCommand@3ecd23d9DEBUG: received com.sun.btrace.comm.RetransformationStartNotification@569cfc36DEBUG: received com.sun.btrace.comm.OkayCommand@43bd930aDEBUG: received com.sun.btrace.comm.MessageCommand@553a3d88a: DEBUG: received com.sun.btrace.comm.MessageCommand@7a30d1e6java.lang.ArithmeticException: / by zeroDEBUG: received com.sun.btrace.comm.MessageCommand@5891e32e me.zks.jvm.troubleshoot.btrace.BtraceThrow.divide(BtraceThrow.java:24) me.zks.jvm.troubleshoot.btrace.BtraceThrow.main(BtraceThrow.java:10)DEBUG: received com.sun.btrace.comm.MessageCommand@cb0ed20a: DEBUG: received com.sun.btrace.comm.MessageCommand@8e24743java.lang.ArithmeticException: / by zeroDEBUG: received com.sun.btrace.comm.MessageCommand@74a10858 me.zks.jvm.troubleshoot.btrace.BtraceThrow.divide(BtraceThrow.java:24) me.zks.jvm.troubleshoot.btrace.BtraceThrow.main(BtraceThrow.java:10)DEBUG: received com.sun.btrace.comm.MessageCommand@23fe1d71a: DEBUG: received com.sun.btrace.comm.MessageCommand@28ac3dc3java.lang.ArithmeticException: / by zeroDEBUG: received com.sun.btrace.comm.MessageCommand@32eebfca me.zks.jvm.troubleshoot.btrace.BtraceThrow.divide(BtraceThrow.java:24) me.zks.jvm.troubleshoot.btrace.BtraceThrow.main(BtraceThrow.java:10)DEBUG: received com.sun.btrace.comm.MessageCommand@4e718207a: DEBUG: received com.sun.btrace.comm.MessageCommand@1d371b2djava.lang.ArithmeticException: / by zeroPlease enter your option: 1. exit 2. send an event 3. send a named event 4. flush console outputDEBUG: received com.sun.btrace.comm.MessageCommand@543c6f6d Kind.Cache Kind.Throwcatch感觉有bug了，Throw感觉用不上，一般用ERROR代替Throw即可，所以这里不说了。具体读者自己去尝试。 未完待续。。。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java线上程序排错经验2 - 线程堆栈分析]]></title>
    <url>%2Fjava%2Fjvm02.html</url>
    <content type="text"><![CDATA[1.前言在线上的程序中，我们可能经常会碰到程序卡死或者执行很慢的情况，这时候我们希望知道是代码哪里的问题，我们或许迫切希望得到代码运行到哪里了，是哪一步很慢，是否是进入了死循环，或者是否哪一段代码有问题导致程序很慢，或者出现了线程不安全的情况，或者是某些连接数或者打开文件数太多等问题，总之我们想知道程序卡在哪里了，哪块占用了大量的资源。此时，或许通过线程堆栈的分析就能定位出问题。如果能深入掌握堆栈分析的技术，很多问题都能迎刃而解，但是线程堆栈分析并不简单，设计到线上的排错问题，需要有一定的知识的广度，对某些知识也要有一定的深度，本人不才，无论是广度还是深度，都感觉略有欠缺，工作两年多来，碰到了很多问题，在解决问题时，并不是一帆风顺的，其中可没少折腾，但我相信，正确的理论可以指导实践，本文也算是现学现卖。 注： 本文主要针对于Linux的，不过大多数在windows上也适用 2. 什么是线程堆栈线程栈：Java线程堆栈是某个时间对所有线程的一个快照，其中主要记录了如下信息– 线程的名称 线程的ID 原生线程ID 线程的运行状态 锁的状态 调用堆栈,也就是每个线程在各个方法调用的栈上面描述的信息，接下来我们会具体的介绍与分析 3. 如何分析首先我们得知道如何去得到线程堆栈工具有很多，我这里只介绍最原始的先得到当前运行java程序的PID这时候就要用到jps命令jps常用的几个命令1234jps #显示所有java程序和线程ID jps -m #输出main method的参数 jps -l #输出完全的包名，应用主类名，jar的完全路径名 jps -v #输出jvm参数 一般直接使用jps即可，如果分不清哪个pid是哪个程序，可以直接1jps -mlv 下面举个简单的线程堆栈分析的例子123456789101112131415161718192021package me.zks.jvm.troubleshoot.threadhump;public class FirstSample &#123; public static void main(String[] args) &#123; FirstSample firstSample = new FirstSample(); firstSample.fun1(); &#125; public void fun1()&#123; //执行某些不耗时操作，然后直接进入fun2方法 fun2(); &#125; public void fun2()&#123; //执行某些不耗时的操作，然后直接进入fun3方法 fun3(); &#125; public void fun3()&#123; //一个死循环，或者耗时特别大的操作 while (true)&#123; System.out.println(&quot;&quot;); &#125; &#125;&#125; 上面程序打成jar包，名为jvm-troubleshoot-1.0-SNAPSHOT.jar，在linux上运行1java -cp jvm-troubleshoot-1.0-SNAPSHOT.jar me.zks.jvm.troubleshoot.threadhump.FirstSample 另外开一个session输入jps显示如下123456ubuntu@VM-0-12-ubuntu:~$ jps27831 Jps27819 FirstSampleubuntu@VM-0-12-ubuntu:~$``` 输入jps -mlv显示如下 ubuntu@VM-0-12-ubuntu:~$ jps -mlv27874 sun.tools.jps.Jps -mlv -Dapplication.home=/usr/lib/jvm/java-8-oracle -Xms8m27819 me.zks.jvm.troubleshoot.threadhump.FirstSampleubuntu@VM-0-12-ubuntu:~$12jstack 27819显示如下 ubuntu@VM-0-12-ubuntu:~$ jstack 278192018-09-02 13:56:14Full thread dump Java HotSpot(TM) 64-Bit Server VM (25.171-b11 mixed mode): “Attach Listener” #8 daemon prio=9 os_prio=0 tid=0x00007fd350001000 nid=0x6d27 waiting on condition [0x0000000000000000] java.lang.Thread.State: RUNNABLE “Service Thread” #7 daemon prio=9 os_prio=0 tid=0x00007fd3700c6800 nid=0x6cb3 runnable [0x0000000000000000] java.lang.Thread.State: RUNNABLE “C1 CompilerThread1” #6 daemon prio=9 os_prio=0 tid=0x00007fd3700b1800 nid=0x6cb2 waiting on condition [0x0000000000000000] java.lang.Thread.State: RUNNABLE “C2 CompilerThread0” #5 daemon prio=9 os_prio=0 tid=0x00007fd3700af800 nid=0x6cb1 waiting on condition [0x0000000000000000] java.lang.Thread.State: RUNNABLE “Signal Dispatcher” #4 daemon prio=9 os_prio=0 tid=0x00007fd3700ae000 nid=0x6cb0 runnable [0x0000000000000000] java.lang.Thread.State: RUNNABLE “Finalizer” #3 daemon prio=8 os_prio=0 tid=0x00007fd37007b000 nid=0x6caf in Object.wait() [0x00007fd35befd000] java.lang.Thread.State: WAITING (on object monitor) at java.lang.Object.wait(Native Method) - waiting on &lt;0x00000000ec6b2c98&gt; (a java.lang.ref.ReferenceQueue$Lock) at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143) - locked &lt;0x00000000ec6b2c98&gt; (a java.lang.ref.ReferenceQueue$Lock) at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:164) at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:212) “Reference Handler” #2 daemon prio=10 os_prio=0 tid=0x00007fd370076800 nid=0x6cae in Object.wait() [0x00007fd35bffe000] java.lang.Thread.State: WAITING (on object monitor) at java.lang.Object.wait(Native Method) - waiting on &lt;0x00000000ec6b2e50&gt; (a java.lang.ref.Reference$Lock) at java.lang.Object.wait(Object.java:502) at java.lang.ref.Reference.tryHandlePending(Reference.java:191) - locked &lt;0x00000000ec6b2e50&gt; (a java.lang.ref.Reference$Lock) at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153) “main” #1 prio=5 os_prio=0 tid=0x00007fd37000a800 nid=0x6cac runnable [0x00007fd377121000] java.lang.Thread.State: RUNNABLE at java.io.FileOutputStream.writeBytes(Native Method) at java.io.FileOutputStream.write(FileOutputStream.java:326) at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82) at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140) - locked &lt;0x00000000ec6bfe58&gt; (a java.io.BufferedOutputStream) at java.io.PrintStream.write(PrintStream.java:482) - locked &lt;0x00000000ec6b6ee0&gt; (a java.io.PrintStream) at sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:221) at sun.nio.cs.StreamEncoder.implFlushBuffer(StreamEncoder.java:291) at sun.nio.cs.StreamEncoder.flushBuffer(StreamEncoder.java:104) - locked &lt;0x00000000ec6b6e98&gt; (a java.io.OutputStreamWriter) at java.io.OutputStreamWriter.flushBuffer(OutputStreamWriter.java:185) at java.io.PrintStream.newLine(PrintStream.java:546) - eliminated &lt;0x00000000ec6b6ee0&gt; (a java.io.PrintStream) at java.io.PrintStream.println(PrintStream.java:807) - locked &lt;0x00000000ec6b6ee0&gt; (a java.io.PrintStream) at me.zks.jvm.troubleshoot.threadhump.FirstSample.fun3(FirstSample.java:22) at me.zks.jvm.troubleshoot.threadhump.FirstSample.fun2(FirstSample.java:17) at me.zks.jvm.troubleshoot.threadhump.FirstSample.fun1(FirstSample.java:13) at me.zks.jvm.troubleshoot.threadhump.FirstSample.main(FirstSample.java:9) “VM Thread” os_prio=0 tid=0x00007fd37006f000 nid=0x6cad runnable “VM Periodic Task Thread” os_prio=0 tid=0x00007fd3700cc000 nid=0x6cb4 waiting on condition JNI global references: 5 1通过上面的jstack我们可以指导，我们的这个简单的程序，竟然有这么多线程，分别是 “Attach Listener”,”Service Thread”,”C1 CompilerThread1”,”C2 CompilerThread0”,”Signal Dispatcher”,”Finalizer”,”Reference Handler”,”main”,”VM Thread”,”VM Periodic Task Thread”,1其中，只有main线程是我们写的，其他的线程是jvm启动的时候，自己会创建一些辅助的线程，我们一般分析我们所写的线程（也不一定，有时候也是需要分析其他线程的），我们来仔细分析一下我们得这个main线程,我给每一行标了个号 01 “main” #1 prio=5 os_prio=0 tid=0x00007fd37000a800 nid=0x6cac runnable [0x00007fd377121000]02 java.lang.Thread.State: RUNNABLE03 at java.io.FileOutputStream.writeBytes(Native Method)04 at java.io.FileOutputStream.write(FileOutputStream.java:326)05 at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)06 at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)07 - locked (a java.io.BufferedOutputStream)08 at java.io.PrintStream.write(PrintStream.java:482)09 - locked (a java.io.PrintStream)10 at sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:221)11 at sun.nio.cs.StreamEncoder.implFlushBuffer(StreamEncoder.java:291)12 at sun.nio.cs.StreamEncoder.flushBuffer(StreamEncoder.java:104)13 - locked (a java.io.OutputStreamWriter)14 at java.io.OutputStreamWriter.flushBuffer(OutputStreamWriter.java:185)15 at java.io.PrintStream.newLine(PrintStream.java:546)16 - eliminated (a java.io.PrintStream)17 at java.io.PrintStream.println(PrintStream.java:807)18 - locked (a java.io.PrintStream)19 at me.zks.jvm.troubleshoot.threadhump.FirstSample.fun3(FirstSample.java:22)20 at me.zks.jvm.troubleshoot.threadhump.FirstSample.fun2(FirstSample.java:17)21 at me.zks.jvm.troubleshoot.threadhump.FirstSample.fun1(FirstSample.java:13)22 at me.zks.jvm.troubleshoot.threadhump.FirstSample.main(FirstSample.java:9)1先分析第一行 “main” #1 prio=5 os_prio=0 tid=0x00007fd37000a800 nid=0x6cac runnable [0x00007fd377121000]1234567891011- &quot;main&quot; 即为线程的名称，所以给线程去个好的名字很重要，方便于我们得分析- #1 我也不知道 - prio=5 线程优先级，java 有1--10个优先级，当有多个线程处于可运行状态时，运行系统总是挑选一个优先级最高的线程执行，两个优先级相同的线程同时等待执行时，那么运行系统会以round-robin的方式选择一个线程执行，Java的优先级策略是抢占式调度。- tid=0x00007fd37000a800 这个是java虚拟机内部的ID，与什么操作系统没关系，我们可以通过 java.lang.Thread.getId() 或者 java.lang.management.ThreadMXBean.getAllThreadIds()获取到,在Oracle / Sun的JDK实现中，此ID只是一个自动递增的long类型的变量。 - nid=0x6cac linux或者windows等操作系统的线程ID，jvm作为一个虚拟机，里面的所有线程ID，其实都是映射到linux上面，这个ID就是linux机器实际运行的线程ID，这个很有用，我们可以通过这个来linux上查一些比较重要的信息，这个以后会有详细说明。 - runnable 线程的状态，线程的状态，这是从java虚拟机的角度来看的，表示线程正在运行，**但是处于Runnable状态的线程不一定真地消耗CPU**，这个以后会说，线程状态以后也会详细说明。 分析第二行 java.lang.Thread.State: RUNNABLE， 描述了线程的状态 分析第3-22行。 是方法的调用站的关系，我们一般从下往上看 例如先看第22行 at me.zks.jvm.troubleshoot.threadhump.FirstSample.main(FirstSample.java:9)1234567891011这行显示了正在调用的包名，类名，方法名称，以及代码中的第几行 从上可知，main方法调用了fun1,fun1调用了fun2,fun2调用了fun3,fun3调用了java.io.PrintStream.println ..... 使用top命令，看看是哪个线程占用的cpu最多,一般使用top -Hp 27819top -Hp 27819 [top-hp pid](https://raw.githubusercontent.com/zhaikaishun/blog_img/master/blog/jvm-thread-dump/top-hp-pid.png)!## 线程锁分析 代码 jps ubuntu@VM-0-12-ubuntu:~$ jps21786 LockAnalysics21804 Jps 1jstack ubuntu@VM-0-12-ubuntu:~$ jstack 217862018-09-02 22:06:31Full thread dump Java HotSpot(TM) 64-Bit Server VM (25.171-b11 mixed mode): “Attach Listener” #12 daemon prio=9 os_prio=0 tid=0x00007f0bac001000 nid=0x5563 waiting on condition [0x0000000000000000] java.lang.Thread.State: RUNNABLE “DestroyJavaVM” #11 prio=5 os_prio=0 tid=0x00007f0bd000a800 nid=0x551b waiting on condition [0x0000000000000000] java.lang.Thread.State: RUNNABLE “ThreadWaitTo” #10 prio=5 os_prio=0 tid=0x00007f0bd00ef800 nid=0x5526 waiting for monitor entry [0x00007f0bd4d17000] java.lang.Thread.State: BLOCKED (on object monitor) at me.zks.jvm.troubleshoot.threadhump.LockAnalysics$ThreadWaitTo.funWaitTo(LockAnalysics.java:76) - waiting to lock &lt;0x00000000e2a78dc0&gt; (a java.lang.Object) at me.zks.jvm.troubleshoot.threadhump.LockAnalysics$ThreadWaitTo.run(LockAnalysics.java:70) at java.lang.Thread.run(Thread.java:748) “ThreadWaitOn” #9 prio=5 os_prio=0 tid=0x00007f0bd00ee000 nid=0x5525 in Object.wait() [0x00007f0bd4e18000] java.lang.Thread.State: WAITING (on object monitor) at java.lang.Object.wait(Native Method) - waiting on &lt;0x00000000e2a7eda8&gt; (a java.lang.Object) at java.lang.Object.wait(Object.java:502) at me.zks.jvm.troubleshoot.threadhump.LockAnalysics$ThreadWaitOn.funWaitOn(LockAnalysics.java:59) - locked &lt;0x00000000e2a7eda8&gt; (a java.lang.Object) at me.zks.jvm.troubleshoot.threadhump.LockAnalysics$ThreadWaitOn.run(LockAnalysics.java:52) at java.lang.Thread.run(Thread.java:748) “ThreadLocked” #8 prio=5 os_prio=0 tid=0x00007f0bd00e5800 nid=0x5524 waiting on condition [0x00007f0bd4f19000] java.lang.Thread.State: TIMED_WAITING (sleeping) at java.lang.Thread.sleep(Native Method) at me.zks.jvm.troubleshoot.threadhump.LockAnalysics$ThreadLocked.fun1(LockAnalysics.java:40) at me.zks.jvm.troubleshoot.threadhump.LockAnalysics$ThreadLocked.run(LockAnalysics.java:34) - locked &lt;0x00000000e2a78dc0&gt; (a java.lang.Object) at java.lang.Thread.run(Thread.java:748) “Service Thread” #7 daemon prio=9 os_prio=0 tid=0x00007f0bd00be800 nid=0x5522 runnable [0x0000000000000000] java.lang.Thread.State: RUNNABLE “C1 CompilerThread1” #6 daemon prio=9 os_prio=0 tid=0x00007f0bd00b1800 nid=0x5521 waiting on condition [0x0000000000000000] java.lang.Thread.State: RUNNABLE “C2 CompilerThread0” #5 daemon prio=9 os_prio=0 tid=0x00007f0bd00af800 nid=0x5520 waiting on condition [0x0000000000000000] java.lang.Thread.State: RUNNABLE “Signal Dispatcher” #4 daemon prio=9 os_prio=0 tid=0x00007f0bd00ae000 nid=0x551f runnable [0x0000000000000000] java.lang.Thread.State: RUNNABLE “Finalizer” #3 daemon prio=8 os_prio=0 tid=0x00007f0bd007b000 nid=0x551e in Object.wait() [0x00007f0bd56ff000] java.lang.Thread.State: WAITING (on object monitor) at java.lang.Object.wait(Native Method) - waiting on &lt;0x00000000e2a08ed0&gt; (a java.lang.ref.ReferenceQueue$Lock) at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143) - locked &lt;0x00000000e2a08ed0&gt; (a java.lang.ref.ReferenceQueue$Lock) at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:164) at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:212) “Reference Handler” #2 daemon prio=10 os_prio=0 tid=0x00007f0bd0076800 nid=0x551d in Object.wait() [0x00007f0bd5800000] java.lang.Thread.State: WAITING (on object monitor) at java.lang.Object.wait(Native Method) - waiting on &lt;0x00000000e2a06bf8&gt; (a java.lang.ref.Reference$Lock) at java.lang.Object.wait(Object.java:502) at java.lang.ref.Reference.tryHandlePending(Reference.java:191) - locked &lt;0x00000000e2a06bf8&gt; (a java.lang.ref.Reference$Lock) at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153) “VM Thread” os_prio=0 tid=0x00007f0bd006f000 nid=0x551c runnable “VM Periodic Task Thread” os_prio=0 tid=0x00007f0bd00c4000 nid=0x5523 waiting on condition JNI global references: 5 1先看一下ThreadLocked线程, 线程状态处于TIMED_WAITING(sleep)，锁特征为waiting on condition。由这个堆栈可知该线程抢占了锁0x00000000e2a78dc0 “ThreadLocked” #8 prio=5 os_prio=0 tid=0x00007f0bd00e5800 nid=0x5524 waiting on condition [0x00007f0bd4f19000] //!! 锁特征为waiting on condition的 java.lang.Thread.State: TIMED_WAITING (sleeping) //线程状态是TIMED_WAITING(sleeping) at java.lang.Thread.sleep(Native Method) at me.zks.jvm.troubleshoot.threadhump.LockAnalysics$ThreadLocked.fun1(LockAnalysics.java:40) at me.zks.jvm.troubleshoot.threadhump.LockAnalysics$ThreadLocked.run(LockAnalysics.java:34) - locked &lt;0x00000000e2a78dc0&gt; (a java.lang.Object) // 该线程占有了锁0x00000000e2a78dc0, 进入了同步代码块的方法 at java.lang.Thread.run(Thread.java:748) 1再看一下ThreadWaitTo线程,可以发现线程状态为BLOCKED，锁特征为waiting for monitor entry, waiting to lock &lt;0x00000000e2a78dc0&gt;说明锁0x00000000e2a78dc0被其他线程占用了，本线程正在等待抢占这把锁。 “ThreadWaitTo” #10 prio=5 os_prio=0 tid=0x00007f0bd00ef800 nid=0x5526 waiting for monitor entry [0x00007f0bd4d17000] java.lang.Thread.State: BLOCKED (on object monitor) at me.zks.jvm.troubleshoot.threadhump.LockAnalysics$ThreadWaitTo.funWaitTo(LockAnalysics.java:76) - waiting to lock &lt;0x00000000e2a78dc0&gt; (a java.lang.Object) at me.zks.jvm.troubleshoot.threadhump.LockAnalysics$ThreadWaitTo.run(LockAnalysics.java:70) at java.lang.Thread.run(Thread.java:748) 1看一下ThreadWaitOn线程, 线程处于WAITING状态，锁特征为Object.wait()，其中waiting on &lt;0x00000000e2a7eda8&gt;会释放自己锁定的锁0x00000000e2a7eda8，其他线程可以占有这把锁，并且自己处于等待唤醒的状态 “ThreadWaitOn” #9 prio=5 os_prio=0 tid=0x00007f0bd00ee000 nid=0x5525 in Object.wait() [0x00007f0bd4e18000] java.lang.Thread.State: WAITING (on object monitor) at java.lang.Object.wait(Native Method) - waiting on &lt;0x00000000e2a7eda8&gt; (a java.lang.Object) //此时wait方法会导致该锁被释放,其它线程又可以占有该锁 at java.lang.Object.wait(Object.java:502) at me.zks.jvm.troubleshoot.threadhump.LockAnalysics$ThreadWaitOn.funWaitOn(LockAnalysics.java:59) - locked &lt;0x00000000e2a7eda8&gt; (a java.lang.Object) at me.zks.jvm.troubleshoot.threadhump.LockAnalysics$ThreadWaitOn.run(LockAnalysics.java:52) at java.lang.Thread.run(Thread.java:748) 1234567891011121314151617181920212223242526272829303132333435通过上面这个例子，我们看到了有locked,waiting to lock,waiting on这三个锁特征 - locked表示已经占有了锁 - waiting to lock表示这把锁目前还没抢到（可能别别的线程抢占了），正在等待这把锁- waiting on 表示线程处于Object.lock()状态，已经释放了锁，其他线程可以占用这把锁，同事本线程等待被唤醒. ### 3. 线程状态的解读 从上面的例子中，我们看到了线程状态有BLOCKED,WAITING,TIMED_WAITING。 实际上线程状态有如下几种 ![线程状态](https://raw.githubusercontent.com/zhaikaishun/blog_img/master/blog/jvm-thread-dump/thread-states.png) 1. 新建状态（New）：新创建了一个线程对象。2. 就绪状态（Runnable）：线程对象创建后，其他线程调用了该对象的start()方法。该状态的线程位于可运行线程池中，变得可运行，等待获取CPU的使用权。3. 运行状态（Running）：就绪状态的线程获取了CPU，执行程序代码。4. 阻塞状态（Blocked）：阻塞状态是线程因为某种原因放弃CPU使用权，暂时停止运行。直到线程进入就绪状态，才有机会转到运行状态。阻塞的情况分三种：（一）、WAITING (on object monitor) 等待阻塞：运行的线程执行wait()方法，JVM会把该线程放入等待池中。（二）、BLOCKED (on object monitor) 同步阻塞：运行的线程在获取对象的同步锁时，若该同步锁被别的线程占用，则JVM会把该线程放入锁池中。区分同步阻塞和等待阻塞也可以看锁的特征，例如同步阻塞锁的特征是waiting for monitor, 等待阻塞锁的特征是object.wait()（三）、TIMED_WAITING(sleeping) 其他阻塞：运行的线程执行sleep()或join()方法，或者发出了I/O请求时，JVM会把该线程置为阻塞状态。当sleep()状态超时、join()等待线程终止或者超时、或者I/O处理完毕时，线程重新转入就绪状态。 5. 死亡状态（Dead）：线程执行完了或者因异常退出了run()方法，该线程结束生命周期。**实际上我们jvm线程栈中，几乎是不会出现NEW,RUNNING,DEAD 这些状态，其中Runnable就算是正在运行了****处于WAITING, BLOCKED, TIME_WAITING的是不消耗CPU的，处于Runnable状态的线程，是否消耗cpu要看具体的上下文情况** - 如果是纯Java运算代码， 则消耗CPU. - 如果是网络IO,很少消耗CPU,这点在分布式程序中经常碰到 - 如果是本地代码， 结合本地代码的性质判断(可以通过pstack/gstack获取本地线程堆栈)，如果是纯运算代码， 则消耗CPU, 如果被挂起， 则不消耗CPU,如果是IO,则不怎么消耗CPU### 4. 线程死锁 ![死锁](https://raw.githubusercontent.com/zhaikaishun/blog_img/master/blog/jvm-thread-dump/%E6%AD%BB%E9%94%81.jpg) 死锁比较少见，而且难于调试：所谓死锁： 是指两个或两个以上的进程在执行过程中，由于竞争资源或者由于彼此通信而造成的一种阻塞的现象，若无外力作用，它们都将无法推进下去。此时称系统处于死锁状态或系统产生了死锁，这些永远在互相等待的进程称为死锁进程。其实很久之前学习数字电路，经常会遇到一些锁，这也是自动化的一些常见的问题，在计算机中，也有类似的东西，请看下图 R1 和R2，都只能被一个进程使用T1在使用R1，同时没有使用完R1的情况下，想使用R2T2在使用R2，同时在没有使用完R2的情况下，想使用R1这时，T1等待T2放弃使用R2，同时T2等待T1放弃使用R1，他们都不会放弃自己所使用的，于是产生了等待，将会一直僵持下去。 代码如下 package me.zks.jvm.troubleshoot.threadhump; public class DeadLock implements Runnable{ private int flag = 1; // 两个static的对象，静态变量 public static Object obj1=new Object(); public static Object obj2=new Object(); @Override public void run() { System.out.println(“flag=”+flag); if(flag==1){ fun1(); } if(flag==0){ fun2(); } } private void fun1() { synchronized (obj1){ System.out.println(&quot;我已经锁定了obj1,休息0.5秒后再锁定obj2,但是估计进不了obj2，因为obj2估计也被锁定了&quot;); try { Thread.sleep(500); } catch (InterruptedException e) { e.printStackTrace(); } synchronized (obj2){ System.out.println(&quot;进入了obj2&quot;); } } } private void fun2() { synchronized (obj2){ System.out.println(&quot;我已经锁定了obj2,休息0.5秒后再锁定obj1,但是估计进不了obj1，因为obj1估计也被锁定了&quot;); try { Thread.sleep(500); } catch (InterruptedException e) { e.printStackTrace(); } synchronized (obj1){ System.out.println(&quot;进入了obj1&quot;); } } } public static void main(String[] args) { DeadLock deadLock1 = new DeadLock(); DeadLock deadLock2 = new DeadLock(); deadLock1.flag=1; deadLock2.flag=0; System.out.println(&quot;线程开始了&quot;); new Thread(deadLock1).start(); new Thread(deadLock2).start(); } }1运行后显示 线程开始了flag=1我已经锁定了obj1,休息0.5秒后再锁定obj2,但是估计进不了obj2，因为obj2估计也被锁定了flag=0我已经锁定了obj2,休息0.5秒后再锁定obj1,但是估计进不了obj1，因为obj1估计也被锁定了1jstack显示(删除了一些信息，只留了用户线程的) Found one Java-level deadlock:“Thread-1”: waiting to lock monitor 0x00007f37680062c8 (object 0x00000000e2a794f8, a java.lang.Object), which is held by “Thread-0”“Thread-0”: waiting to lock monitor 0x00007f3768004e28 (object 0x00000000e2a79508, a java.lang.Object), which is held by “Thread-1” Java stack information for the threads listed above:“Thread-1”: at me.zks.jvm.troubleshoot.threadhump.DeadLock.fun2(DeadLock.java:41) - waiting to lock &lt;0x00000000e2a794f8&gt; (a java.lang.Object) - locked &lt;0x00000000e2a79508&gt; (a java.lang.Object) at me.zks.jvm.troubleshoot.threadhump.DeadLock.run(DeadLock.java:15) at java.lang.Thread.run(Thread.java:748) “Thread-0”: at me.zks.jvm.troubleshoot.threadhump.DeadLock.fun1(DeadLock.java:28) - waiting to lock &lt;0x00000000e2a79508&gt; (a java.lang.Object) - locked &lt;0x00000000e2a794f8&gt; (a java.lang.Object) at me.zks.jvm.troubleshoot.threadhump.DeadLock.run(DeadLock.java:12) at java.lang.Thread.run(Thread.java:748) Found 1 deadlock.123456789101112131415161718从线程中我们可以看到Found one Java-level deadlock: 这就说明进入了死锁，分析Thread-1和Thread-2 Thread-1锁定了0x00000000e2a79508, waiting to lock 0x00000000e2a794f8Thread-0锁定了0x00000000e2a794f8, waiting to lock 0x00000000e2a79508，于是这两个线程就进入了死锁。 如果通过这种方式发现了死锁，那没办法，只有该代码了，将并发做的更安全点才是王道。 ## 5. 大数据中死循环或者复杂度高的方法判断 首先，如果是进入了死循环，程序肯定是在某个方法直接卡死，我们将线程堆栈分析下来，多分析几次，比如20秒一次（具体情况而定），如果多次都在同一个方法栈的调用，但是根据我们得预估，这个代码并不需要这么多的时间，并且这个方法的线程占用的cpu很高(前面提到的TOP -Hp pid, 然后根据这个方法的nid 转成10进制就可以找到对应的线程)，那么我们就怀疑是这个cpu高引起的问题了。 这时候就需要分析代码了。 **需要注意**的是,我们分析还是要根据代码客观的时间来分析，特别是在spark等大数据处理中，有的方法是需要很久的时间，合理的判断，找出死循环或者复杂度高的方法，然后再对代码进行修改。 ## 6. IO或者网络问题 有时候，大数据在shuffle过程中，或者web程序在传输数据过程中，可能会由于网络等问题，导致程序会很慢，这个分析方法也同样是通过看线程堆栈，查看到是java.io.，或者java.nio等问题，那就可能就是网络问题了，网络问题可以参考使用iotop 和iostat 来进行分析 ## 7. 连接数瓶颈问题 有时候，linux机器的频繁连接，同时连接数过多，或者IO的频繁打开而不关闭，连接数过多，又或者是socketRead的并发太多，就容易发生连接数瓶颈问题。 最比较常见的就是在web做压力测试（例如ab test），再并发数一多的时候，就容易发现瓶颈，像mysql有类似于MYTOP的工具，同样，我们也可以通过jstack获取方法栈来进行，例如下面这个例子 代码参考自《java 问题定位技术》 “Thread-248” prio=1 tid=0xa58f2048 nid=0x7ac2 runnable[0xaeedb000..0xaeedc480]at java.net.SocketInputStream.socketRead0(Native Method)at java.net.SocketInputStream.read(SocketInputStream.java:129)at oracle.net.ns.Packet.receive(Unknown Source)… …at oracle.jdbc.driver.LongRawAccessor.getBytes()“Thread-429” prio=1 tid=0xa58f2048 nid=0x7ac2 runnable[0xaeedb000..0xaeedc480]at java.net.SocketInputStream.socketRead0(Native Method)at java.net.SocketInputStream.read(SocketInputStream.java:129)at oracle.net.ns.Packet.receive(Unknown Source)… …at oracle.jdbc.driver.LongRawAccessor.getBytes()“Thread-250” prio=1 tid=0xa58f2048 nid=0x7ac2 runnable[0xaeedb000..0xaeedc480]at java.net.SocketInputStream.socketRead0(Native Method)at java.net.SocketInputStream.read(SocketInputStream.java:129)at oracle.net.ns.Packet.receive(Unknown Source)… …at oracle.jdbc.driver.LongRawAccessor.getBytes()…..```假如有太多的这类线程，那么就可以得出是jdbc访问过多，这时候就要优化资源和优化程序了。 7. 频繁GC导致的cpu速度慢的问题下回讲解， 内存堆栈分析。 参考文献：HotSpotVM故障排除指南]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java线上程序排错经验3 - jvm内存分析]]></title>
    <url>%2Fjava%2Fjvm03.html</url>
    <content type="text"><![CDATA[前言堆分析工具很多，这里只介绍一种分析的方法，也是最原始的一种，以后会在这篇文字里面慢慢补充 补充： 最简单的方法，比较直观1jmap -histo：live &lt;PID&gt; 1. 先得到堆1.1 jmap得到堆直接jmap查看使用方法1234567891011121314151617181920212223242526272829ubuntu@VM-0-12-ubuntu:~$ jmapUsage: jmap [option] &lt;pid&gt; (to connect to running process) jmap [option] &lt;executable &lt;core&gt; (to connect to a core file) jmap [option] [server_id@]&lt;remote server IP or hostname&gt; (to connect to remote debug server)where &lt;option&gt; is one of: &lt;none&gt; to print same info as Solaris pmap -heap to print java heap summary -histo[:live] to print histogram of java object heap; if the &quot;live&quot; suboption is specified, only count live objects -clstats to print class loader statistics -finalizerinfo to print information on objects awaiting finalization -dump:&lt;dump-options&gt; to dump java heap in hprof binary format dump-options: live dump only live objects; if not specified, all objects in the heap are dumped. format=b binary format file=&lt;file&gt; dump heap to &lt;file&gt; Example: jmap -dump:live,format=b,file=heap.bin &lt;pid&gt; -F force. Use with -dump:&lt;dump-options&gt; &lt;pid&gt; or -histo to force a heap dump or histogram when &lt;pid&gt; does not respond. The &quot;live&quot; suboption is not supported in this mode. -h | -help to print this help message -J&lt;flag&gt; to pass &lt;flag&gt; directly to the runtime system 上面的说法中有个例子，我也一般使用这个例子1jmap -dump:live,format=b,file=heap.bin &lt;pid&gt; 上述命令会生成一个heap.bin文件，是个二进制的（当然也可以不输出二进制的，不过输出二进制的是为了jhat等堆分析工具的查看） 1.2 堆溢出时自动堆转存线上的程序什么时候堆溢出很难捕捉，我们可以设置在堆内存时自动进行堆转存，这样方便我们进行查看，操作方法1-XX:+HeapDumpOnOutOfMemoryError 举个例子代码1234567891011121314package me.zks.jvm.troubleshoot.heaphump;import java.util.ArrayList;public class FirstHeapSample &#123; public static void main(String[] args) &#123; ArrayList&lt;Sample&gt; samples = new ArrayList&lt;&gt;(); while (true)&#123; for (int i = 0; i &lt;1000 ; i++) &#123; samples.add(new Sample(i,String.valueOf(i))); &#125; &#125; &#125;&#125; 运行时VM options中添加-XX:+HeapDumpOnOutOfMemoryError1-Xms20m -Xmx20m -XX:+HeapDumpOnOutOfMemoryError 运行后，不一会儿就报OOM错误1234567891011java.lang.OutOfMemoryError: Java heap spaceDumping heap to java_pid3048.hprof ...Exception in thread &quot;main&quot; java.lang.OutOfMemoryError: Java heap space at java.util.Arrays.copyOf(Arrays.java:3210) at java.util.Arrays.copyOf(Arrays.java:3181) at java.util.ArrayList.grow(ArrayList.java:261) at java.util.ArrayList.ensureExplicitCapacity(ArrayList.java:235) at java.util.ArrayList.ensureCapacityInternal(ArrayList.java:227) at java.util.ArrayList.add(ArrayList.java:458) at me.zks.jvm.troubleshoot.heaphump.FirstHeapSample.main(FirstHeapSample.java:13)Heap dump file created [26835640 bytes in 0.188 secs] 并且程序生成了一个堆文件1java_pid3048.hprof 分析jvm dump工具的有很多，比如eclipse或者idea的一些插件，jprofile,jvisualVM等。这里先不介绍，这里只介绍一种最原始的工具jhat jhat貌似在jdk1.9后就没了或者不支持了 jhat 先查看使用方法jhat -help1234567891011121314151617181920212223242526ubuntu@VM-0-12-ubuntu:~$ jhat -helpUsage: jhat [-stack &lt;bool&gt;] [-refs &lt;bool&gt;] [-port &lt;port&gt;] [-baseline &lt;file&gt;] [-debug &lt;int&gt;] [-version] [-h|-help] &lt;file&gt; -J&lt;flag&gt; Pass &lt;flag&gt; directly to the runtime system. For example, -J-mx512m to use a maximum heap size of 512MB -stack false: Turn off tracking object allocation call stack. -refs false: Turn off tracking of references to objects -port &lt;port&gt;: Set the port for the HTTP server. Defaults to 7000 -exclude &lt;file&gt;: Specify a file that lists data members that should be excluded from the reachableFrom query. -baseline &lt;file&gt;: Specify a baseline object dump. Objects in both heap dumps with the same ID and same class will be marked as not being &quot;new&quot;. -debug &lt;int&gt;: Set debug level. 0: No debug output 1: Debug hprof file parsing 2: Debug hprof file parsing, no server -version Report version number -h|-help Print this help and exit &lt;file&gt; The file to readFor a dump file that contains multiple heap dumps,you may specify which dump in the fileby appending &quot;#&lt;number&gt;&quot; to the file name, i.e. &quot;foo.hprof#3&quot;.All boolean options default to &quot;true&quot; 一般这么多我也用不上，我一般只用1jhat -port &lt;port&gt; jmap生成的文件 有时候文件比较大，需要比较大的内存可以这样使用1jhat -J-Xmx2G jmap生成的文件 一段时间后会提示server开启成功，12345678910111213Reading from java_pid3048.hprof...Dump file created Wed Sep 05 21:25:13 CST 2018Snapshot read, resolving...Resolving 729071 objects...Chasing references, expect 145 dots.................................................................................................................................................Eliminating duplicate references.................................................................................................................................................Snapshot resolved.Started HTTP server on port 7000Server is ready. 这时候就可以在7000端口上进行查看了比较重要的是： Show instance counts for all classes (excluding platform) 和 Show heap histogram，显示堆实例的列表这个可以看到所有的除了平台（jdk内部的）的所有的class的实例，例如如图这有240098个me.zks.troubleshoot.heaphump.Sample的实例当然，如果对象种类太多，这样看起来比较麻烦的话，可以使用查询 当我们点击class me.zks.jvm.troubleshoot.heaphump.Sample 时，会进入到更详细的说明 我们可以通过对程序的理解，对各个对象的大小做一个比较客观的预估，如果显示的各个堆远大于 2. 频繁GC的分析TODO 20180916 以后会专门分析相关]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java程序线上排错经验1之了解JVM相关知识]]></title>
    <url>%2Fjava%2Fjvm01.html</url>
    <content type="text"><![CDATA[原文： https://github.com/CyC2018/CS-Notes/blob/master/notes/Java%20%E8%99%9A%E6%8B%9F%E6%9C%BA.md本文主要参考与《深入理解java虚拟机》 一、运行时数据区域 程序计数器 Java 虚拟机栈 本地方法栈 堆 方法区 运行时常量池 直接内存 二、垃圾收集 判断一个对象是否可被回收 引用类型 垃圾收集算法 垃圾收集器 三、内存分配与回收策略 Minor GC 和 Full GC 内存分配策略 Full GC 的触发条件 四、类加载机制 类的生命周期 类加载过程 类初始化时机 类与类加载器 类加载器分类 双亲委派模型 自定义类加载器实现 参考资料 一、运行时数据区域 程序计数器记录正在执行的虚拟机字节码指令的地址（如果正在执行的是本地方法则为空）。 Java 虚拟机栈每个 Java 方法在执行的同时会创建一个栈帧用于存储局部变量表、操作数栈、常量池引用等信息。从方法调用直至执行完成的过程，就对应着一个栈帧在 Java 虚拟机栈中入栈和出栈的过程。 可以通过 -Xss 这个虚拟机参数来指定每个线程的 Java 虚拟机栈内存大小： 1java -Xss512M HackTheJava 该区域可能抛出以下异常： 当线程请求的栈深度超过最大值，会抛出 StackOverflowError 异常； 栈进行动态扩展时如果无法申请到足够内存，会抛出 OutOfMemoryError 异常。 本地方法栈本地方法栈与 Java 虚拟机栈类似，它们之间的区别只不过是本地方法栈为本地方法服务。 本地方法一般是用其它语言（C、C++ 或汇编语言等）编写的，并且被编译为基于本机硬件和操作系统的程序，对待这些方法需要特别处理。 堆所有对象都在这里分配内存，是垃圾收集的主要区域（”GC 堆”）。 现代的垃圾收集器基本都是采用分代收集算法，其主要的思想是针对不同类型的对象采取不同的垃圾回收算法，可以将堆分成两块： 新生代（Young Generation） 老年代（Old Generation） 堆不需要连续内存，并且可以动态增加其内存，增加失败会抛出 OutOfMemoryError 异常。 可以通过 -Xms 和 -Xmx 两个虚拟机参数来指定一个程序的堆内存大小，第一个参数设置初始值，第二个参数设置最大值。 1java -Xms1M -Xmx2M HackTheJava 方法区用于存放已被加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。 和堆一样不需要连续的内存，并且可以动态扩展，动态扩展失败一样会抛出 OutOfMemoryError 异常。 对这块区域进行垃圾回收的主要目标是对常量池的回收和对类的卸载，但是一般比较难实现。 HotSpot 虚拟机把它当成永久代来进行垃圾回收。但是很难确定永久代的大小，因为它受到很多因素影响，并且每次 Full GC 之后永久代的大小都会改变，所以经常会抛出 OutOfMemoryError 异常。为了更容易管理方法区，从 JDK 1.8 开始，移除永久代，并把方法区移至元空间，它位于本地内存中，而不是虚拟机内存中。 运行时常量池运行时常量池是方法区的一部分。 Class 文件中的常量池（编译器生成的各种字面量和符号引用）会在类加载后被放入这个区域。 除了在编译期生成的常量，还允许动态生成，例如 String 类的 intern()。 直接内存在 JDK 1.4 中新加入了 NIO 类，它可以使用 Native 函数库直接分配堆外内存（Native 堆），然后通过一个存储在 Java 堆里的 DirectByteBuffer 对象作为这块内存的引用进行操作。 这样能在一些场景中显著提高性能，因为避免了在 Java 堆和 Native 堆中来回复制数据。 二、垃圾收集垃圾收集主要是针对堆和方法区进行。 程序计数器、虚拟机栈和本地方法栈这三个区域属于线程私有的，只存在于线程的生命周期内，线程结束之后也会消失，因此不需要对这三个区域进行垃圾回收。 判断一个对象是否可被回收1. 引用计数算法给对象添加一个引用计数器，当对象增加一个引用时计数器加 1，引用失效时计数器减 1。引用计数为 0 的对象可被回收。 两个对象出现循环引用的情况下，此时引用计数器永远不为 0，导致无法对它们进行回收。 正因为循环引用的存在，因此 Java 虚拟机不使用引用计数算法。 1234567891011public class ReferenceCountingGC &#123; public Object instance = null; public static void main(String[] args) &#123; ReferenceCountingGC objectA = new ReferenceCountingGC(); ReferenceCountingGC objectB = new ReferenceCountingGC(); objectA.instance = objectB; objectB.instance = objectA; &#125;&#125; 2. 可达性分析算法通过 GC Roots 作为起始点进行搜索，能够到达到的对象都是存活的，不可达的对象可被回收。 Java 虚拟机使用该算法来判断对象是否可被回收，在 Java 中 GC Roots 一般包含以下内容： 虚拟机栈中局部变量表中引用的对象 本地方法栈中 JNI 中引用的对象 方法区中类静态属性引用的对象 方法区中的常量引用的对象 3. 方法区的回收因为方法区主要存放永久代对象，而永久代对象的回收率比新生代低很多，因此在方法区上进行回收性价比不高。 主要是对常量池的回收和对类的卸载。 在大量使用反射、动态代理、CGLib 等 ByteCode 框架、动态生成 JSP 以及 OSGi 这类频繁自定义 ClassLoader 的场景都需要虚拟机具备类卸载功能，以保证不会出现内存溢出。 类的卸载条件很多，需要满足以下三个条件，并且满足了也不一定会被卸载： 该类所有的实例都已经被回收，也就是堆中不存在该类的任何实例。 加载该类的 ClassLoader 已经被回收。 该类对应的 Class 对象没有在任何地方被引用，也就无法在任何地方通过反射访问该类方法。 可以通过 -Xnoclassgc 参数来控制是否对类进行卸载。 4. finalize()finalize() 类似 C++ 的析构函数，用来做关闭外部资源等工作。但是 try-finally 等方式可以做的更好，并且该方法运行代价高昂，不确定性大，无法保证各个对象的调用顺序，因此最好不要使用。 当一个对象可被回收时，如果需要执行该对象的 finalize() 方法，那么就有可能在该方法中让对象重新被引用，从而实现自救。自救只能进行一次，如果回收的对象之前调用了 finalize() 方法自救，后面回收时不会调用 finalize() 方法。 引用类型无论是通过引用计算算法判断对象的引用数量，还是通过可达性分析算法判断对象是否可达，判定对象是否可被回收都与引用有关。 Java 提供了四种强度不同的引用类型。 1. 强引用被强引用关联的对象不会被回收。 使用 new 一个新对象的方式来创建强引用。 1Object obj = new Object(); 2. 软引用被软引用关联的对象只有在内存不够的情况下才会被回收。 使用 SoftReference 类来创建软引用。 123Object obj = new Object();SoftReference&lt;Object&gt; sf = new SoftReference&lt;Object&gt;(obj);obj = null; // 使对象只被软引用关联 3. 弱引用被弱引用关联的对象一定会被回收，也就是说它只能存活到下一次垃圾回收发生之前。 使用 WeakReference 类来实现弱引用。 123Object obj = new Object();WeakReference&lt;Object&gt; wf = new WeakReference&lt;Object&gt;(obj);obj = null; 4. 虚引用又称为幽灵引用或者幻影引用。一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用取得一个对象。 为一个对象设置虚引用关联的唯一目的就是能在这个对象被回收时收到一个系统通知。 使用 PhantomReference 来实现虚引用。 123Object obj = new Object();PhantomReference&lt;Object&gt; pf = new PhantomReference&lt;Object&gt;(obj);obj = null; 垃圾收集算法1. 标记 - 清除 将存活的对象进行标记，然后清理掉未被标记的对象。 不足： 标记和清除过程效率都不高； 会产生大量不连续的内存碎片，导致无法给大对象分配内存。 2. 标记 - 整理 让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存。 3. 复制 将内存划分为大小相等的两块，每次只使用其中一块，当这一块内存用完了就将还存活的对象复制到另一块上面，然后再把使用过的内存空间进行一次清理。 主要不足是只使用了内存的一半。 现在的商业虚拟机都采用这种收集算法来回收新生代，但是并不是将新生代划分为大小相等的两块，而是分为一块较大的 Eden 空间和两块较小的 Survivor 空间，每次使用 Eden 空间和其中一块 Survivor。在回收时，将 Eden 和 Survivor 中还存活着的对象一次性复制到另一块 Survivor 空间上，最后清理 Eden 和使用过的那一块 Survivor。 HotSpot 虚拟机的 Eden 和 Survivor 的大小比例默认为 8:1，保证了内存的利用率达到 90%。如果每次回收有多于 10% 的对象存活，那么一块 Survivor 空间就不够用了，此时需要依赖于老年代进行分配担保，也就是借用老年代的空间存储放不下的对象。 4. 分代收集现在的商业虚拟机采用分代收集算法，它根据对象存活周期将内存划分为几块，不同块采用适当的收集算法。 一般将堆分为新生代和老年代。 新生代使用：复制算法 老年代使用：标记 - 清除 或者 标记 - 整理 算法 垃圾收集器 以上是 HotSpot 虚拟机中的 7 个垃圾收集器，连线表示垃圾收集器可以配合使用。 单线程与多线程：单线程指的是垃圾收集器只使用一个线程进行收集，而多线程使用多个线程； 串行与并行：串行指的是垃圾收集器与用户程序交替执行，这意味着在执行垃圾收集的时候需要停顿用户程序；并行指的是垃圾收集器和用户程序同时执行。除了 CMS 和 G1 之外，其它垃圾收集器都是以串行的方式执行。 1. Serial 收集器 Serial 翻译为串行，也就是说它以串行的方式执行。 它是单线程的收集器，只会使用一个线程进行垃圾收集工作。 它的优点是简单高效，对于单个 CPU 环境来说，由于没有线程交互的开销，因此拥有最高的单线程收集效率。 它是 Client 模式下的默认新生代收集器，因为在该应用场景下，分配给虚拟机管理的内存一般来说不会很大。Serial 收集器收集几十兆甚至一两百兆的新生代停顿时间可以控制在一百多毫秒以内，只要不是太频繁，这点停顿是可以接受的。 2. ParNew 收集器 它是 Serial 收集器的多线程版本。 是 Server 模式下的虚拟机首选新生代收集器，除了性能原因外，主要是因为除了 Serial 收集器，只有它能与 CMS 收集器配合工作。 默认开启的线程数量与 CPU 数量相同，可以使用 -XX:ParallelGCThreads 参数来设置线程数。 3. Parallel Scavenge 收集器与 ParNew 一样是多线程收集器。 其它收集器关注点是尽可能缩短垃圾收集时用户线程的停顿时间，而它的目标是达到一个可控制的吞吐量，它被称为“吞吐量优先”收集器。这里的吞吐量指 CPU 用于运行用户代码的时间占总时间的比值。 停顿时间越短就越适合需要与用户交互的程序，良好的响应速度能提升用户体验。而高吞吐量则可以高效率地利用 CPU 时间，尽快完成程序的运算任务，适合在后台运算而不需要太多交互的任务。 缩短停顿时间是以牺牲吞吐量和新生代空间来换取的：新生代空间变小，垃圾回收变得频繁，导致吞吐量下降。 可以通过一个开关参数打开 GC 自适应的调节策略（GC Ergonomics），就不需要手工指定新生代的大小（-Xmn）、Eden 和 Survivor 区的比例、晋升老年代对象年龄等细节参数了。虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或者最大的吞吐量。 4. Serial Old 收集器 是 Serial 收集器的老年代版本，也是给 Client 模式下的虚拟机使用。如果用在 Server 模式下，它有两大用途： 在 JDK 1.5 以及之前版本（Parallel Old 诞生以前）中与 Parallel Scavenge 收集器搭配使用。 作为 CMS 收集器的后备预案，在并发收集发生 Concurrent Mode Failure 时使用。 5. Parallel Old 收集器 是 Parallel Scavenge 收集器的老年代版本。 在注重吞吐量以及 CPU 资源敏感的场合，都可以优先考虑 Parallel Scavenge 加 Parallel Old 收集器。 6. CMS 收集器 CMS（Concurrent Mark Sweep），Mark Sweep 指的是标记 - 清除算法。 分为以下四个流程： 初始标记：仅仅只是标记一下 GC Roots 能直接关联到的对象，速度很快，需要停顿。 并发标记：进行 GC Roots Tracing 的过程，它在整个回收过程中耗时最长，不需要停顿。 重新标记：为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，需要停顿。 并发清除：不需要停顿。 在整个过程中耗时最长的并发标记和并发清除过程中，收集器线程都可以与用户线程一起工作，不需要进行停顿。 具有以下缺点： 吞吐量低：低停顿时间是以牺牲吞吐量为代价的，导致 CPU 利用率不够高。 无法处理浮动垃圾，可能出现 Concurrent Mode Failure。浮动垃圾是指并发清除阶段由于用户线程继续运行而产生的垃圾，这部分垃圾只能到下一次 GC 时才能进行回收。由于浮动垃圾的存在，因此需要预留出一部分内存，意味着 CMS 收集不能像其它收集器那样等待老年代快满的时候再回收。如果预留的内存不够存放浮动垃圾，就会出现 Concurrent Mode Failure，这时虚拟机将临时启用 Serial Old 来替代 CMS。 标记 - 清除算法导致的空间碎片，往往出现老年代空间剩余，但无法找到足够大连续空间来分配当前对象，不得不提前触发一次 Full GC。 7. G1 收集器G1（Garbage-First），它是一款面向服务端应用的垃圾收集器，在多 CPU 和大内存的场景下有很好的性能。HotSpot 开发团队赋予它的使命是未来可以替换掉 CMS 收集器。 堆被分为新生代和老年代，其它收集器进行收集的范围都是整个新生代或者老年代，而 G1 可以直接对新生代和老年代一起回收。 G1 把堆划分成多个大小相等的独立区域（Region），新生代和老年代不再物理隔离。 通过引入 Region 的概念，从而将原来的一整块内存空间划分成多个的小空间，使得每个小空间可以单独进行垃圾回收。这种划分方法带来了很大的灵活性，使得可预测的停顿时间模型成为可能。通过记录每个 Region 垃圾回收时间以及回收所获得的空间（这两个值是通过过去回收的经验获得），并维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的 Region。 每个 Region 都有一个 Remembered Set，用来记录该 Region 对象的引用对象所在的 Region。通过使用 Remembered Set，在做可达性分析的时候就可以避免全堆扫描。 如果不计算维护 Remembered Set 的操作，G1 收集器的运作大致可划分为以下几个步骤： 初始标记 并发标记 最终标记：为了修正在并发标记期间因用户程序继续运作而导致标记产生变动的那一部分标记记录，虚拟机将这段时间对象变化记录在线程的 Remembered Set Logs 里面，最终标记阶段需要把 Remembered Set Logs 的数据合并到 Remembered Set 中。这阶段需要停顿线程，但是可并行执行。 筛选回收：首先对各个 Region 中的回收价值和成本进行排序，根据用户所期望的 GC 停顿时间来制定回收计划。此阶段其实也可以做到与用户程序一起并发执行，但是因为只回收一部分 Region，时间是用户可控制的，而且停顿用户线程将大幅度提高收集效率。 具备如下特点： 空间整合：整体来看是基于“标记 - 整理”算法实现的收集器，从局部（两个 Region 之间）上来看是基于“复制”算法实现的，这意味着运行期间不会产生内存空间碎片。 可预测的停顿：能让使用者明确指定在一个长度为 M 毫秒的时间片段内，消耗在 GC 上的时间不得超过 N 毫秒。 三、内存分配与回收策略Minor GC 和 Full GC Minor GC：发生在新生代上，因为新生代对象存活时间很短，因此 Minor GC 会频繁执行，执行的速度一般也会比较快。 Full GC：发生在老年代上，老年代对象其存活时间长，因此 Full GC 很少执行，执行速度会比 Minor GC 慢很多。 内存分配策略1. 对象优先在 Eden 分配大多数情况下，对象在新生代 Eden 区分配，当 Eden 区空间不够时，发起 Minor GC。 2. 大对象直接进入老年代大对象是指需要连续内存空间的对象，最典型的大对象是那种很长的字符串以及数组。 经常出现大对象会提前触发垃圾收集以获取足够的连续空间分配给大对象。 -XX:PretenureSizeThreshold，大于此值的对象直接在老年代分配，避免在 Eden 区和 Survivor 区之间的大量内存复制。 3. 长期存活的对象进入老年代为对象定义年龄计数器，对象在 Eden 出生并经过 Minor GC 依然存活，将移动到 Survivor 中，年龄就增加 1 岁，增加到一定年龄则移动到老年代中。 -XX:MaxTenuringThreshold 用来定义年龄的阈值。 4. 动态对象年龄判定虚拟机并不是永远地要求对象的年龄必须达到 MaxTenuringThreshold 才能晋升老年代，如果在 Survivor 中相同年龄所有对象大小的总和大于 Survivor 空间的一半，则年龄大于或等于该年龄的对象可以直接进入老年代，无需等到 MaxTenuringThreshold 中要求的年龄。 5. 空间分配担保在发生 Minor GC 之前，虚拟机先检查老年代最大可用的连续空间是否大于新生代所有对象总空间，如果条件成立的话，那么 Minor GC 可以确认是安全的。 如果不成立的话虚拟机会查看 HandlePromotionFailure 设置值是否允许担保失败，如果允许那么就会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试着进行一次 Minor GC；如果小于，或者 HandlePromotionFailure 设置不允许冒险，那么就要进行一次 Full GC。 Full GC 的触发条件对于 Minor GC，其触发条件非常简单，当 Eden 空间满时，就将触发一次 Minor GC。而 Full GC 则相对复杂，有以下条件： 1. 调用 System.gc()只是建议虚拟机执行 Full GC，但是虚拟机不一定真正去执行。不建议使用这种方式，而是让虚拟机管理内存。 2. 老年代空间不足老年代空间不足的常见场景为前文所讲的大对象直接进入老年代、长期存活的对象进入老年代等。 为了避免以上原因引起的 Full GC，应当尽量不要创建过大的对象以及数组。除此之外，可以通过 -Xmn 虚拟机参数调大新生代的大小，让对象尽量在新生代被回收掉，不进入老年代。还可以通过 -XX:MaxTenuringThreshold 调大对象进入老年代的年龄，让对象在新生代多存活一段时间。 3. 空间分配担保失败使用复制算法的 Minor GC 需要老年代的内存空间作担保，如果担保失败会执行一次 Full GC。具体内容请参考上面的第五小节。 4. JDK 1.7 及以前的永久代空间不足在 JDK 1.7 及以前，HotSpot 虚拟机中的方法区是用永久代实现的，永久代中存放的为一些 Class 的信息、常量、静态变量等数据。 当系统中要加载的类、反射的类和调用的方法较多时，永久代可能会被占满，在未配置为采用 CMS GC 的情况下也会执行 Full GC。如果经过 Full GC 仍然回收不了，那么虚拟机会抛出 java.lang.OutOfMemoryError。 为避免以上原因引起的 Full GC，可采用的方法为增大永久代空间或转为使用 CMS GC。 5. Concurrent Mode Failure执行 CMS GC 的过程中同时有对象要放入老年代，而此时老年代空间不足（可能是 GC 过程中浮动垃圾过多导致暂时性的空间不足），便会报 Concurrent Mode Failure 错误，并触发 Full GC。 四、类加载机制类是在运行期间第一次使用时动态加载的，而不是编译时期一次性加载。因为如果在编译时期一次性加载，那么会占用很多的内存。 类的生命周期 包括以下 7 个阶段： 加载（Loading） 验证（Verification） 准备（Preparation） 解析（Resolution） 初始化（Initialization） 使用（Using） 卸载（Unloading） 类加载过程包含了加载、验证、准备、解析和初始化这 5 个阶段。 1. 加载加载是类加载的一个阶段，注意不要混淆。 加载过程完成以下三件事： 通过一个类的全限定名来获取定义此类的二进制字节流。 将这个字节流所代表的静态存储结构转化为方法区的运行时存储结构。 在内存中生成一个代表这个类的 Class 对象，作为方法区这个类的各种数据的访问入口。 其中二进制字节流可以从以下方式中获取： 从 ZIP 包读取，成为 JAR、EAR、WAR 格式的基础。 从网络中获取，最典型的应用是 Applet。 运行时计算生成，例如动态代理技术，在 java.lang.reflect.Proxy 使用 ProxyGenerator.generateProxyClass 的代理类的二进制字节流。 由其他文件生成，例如由 JSP 文件生成对应的 Class 类。 2. 验证确保 Class 文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全。 3. 准备类变量是被 static 修饰的变量，准备阶段为类变量分配内存并设置初始值，使用的是方法区的内存。 实例变量不会在这阶段分配内存，它将会在对象实例化时随着对象一起分配在堆中。 注意，实例化不是类加载的一个过程，类加载发生在所有实例化操作之前，并且类加载只进行一次，实例化可以进行多次。 初始值一般为 0 值，例如下面的类变量 value 被初始化为 0 而不是 123。 1public static int value = 123; 如果类变量是常量，那么会按照表达式来进行初始化，而不是赋值为 0。 1public static final int value = 123; 4. 解析将常量池的符号引用替换为直接引用的过程。 其中解析过程在某些情况下可以在初始化阶段之后再开始，这是为了支持 Java 的动态绑定。 5. 初始化初始化阶段才真正开始执行类中定义的 Java 程序代码。初始化阶段即虚拟机执行类构造器 &lt;clinit&gt;() 方法的过程。 在准备阶段，类变量已经赋过一次系统要求的初始值，而在初始化阶段，根据程序员通过程序制定的主观计划去初始化类变量和其它资源。 &lt;clinit&gt;() 方法具有以下特点： 是由编译器自动收集类中所有类变量的赋值动作和静态语句块中的语句合并产生的，编译器收集的顺序由语句在源文件中出现的顺序决定。特别注意的是，静态语句块只能访问到定义在它之前的类变量，定义在它之后的类变量只能赋值，不能访问。例如以下代码： 1234567public class Test &#123; static &#123; i = 0; // 给变量赋值可以正常编译通过 System.out.print(i); // 这句编译器会提示“非法向前引用” &#125; static int i = 1;&#125; 与类的构造函数（或者说实例构造器 &lt;init&gt;()）不同，不需要显式的调用父类的构造器。虚拟机会自动保证在子类的 &lt;clinit&gt;() 方法运行之前，父类的 &lt;clinit&gt;() 方法已经执行结束。因此虚拟机中第一个执行 &lt;clinit&gt;() 方法的类肯定为 java.lang.Object。 由于父类的 &lt;clinit&gt;() 方法先执行，也就意味着父类中定义的静态语句块的执行要优先于子类。例如以下代码： 1234567891011121314static class Parent &#123; public static int A = 1; static &#123; A = 2; &#125;&#125;static class Sub extends Parent &#123; public static int B = A;&#125;public static void main(String[] args) &#123; System.out.println(Sub.B); // 2&#125; &lt;clinit&gt;() 方法对于类或接口不是必须的，如果一个类中不包含静态语句块，也没有对类变量的赋值操作，编译器可以不为该类生成 &lt;clinit&gt;() 方法。 接口中不可以使用静态语句块，但仍然有类变量初始化的赋值操作，因此接口与类一样都会生成 &lt;clinit&gt;() 方法。但接口与类不同的是，执行接口的 &lt;clinit&gt;() 方法不需要先执行父接口的 &lt;clinit&gt;() 方法。只有当父接口中定义的变量使用时，父接口才会初始化。另外，接口的实现类在初始化时也一样不会执行接口的 &lt;clinit&gt;() 方法。 虚拟机会保证一个类的 &lt;clinit&gt;() 方法在多线程环境下被正确的加锁和同步，如果多个线程同时初始化一个类，只会有一个线程执行这个类的 &lt;clinit&gt;() 方法，其它线程都会阻塞等待，直到活动线程执行 &lt;clinit&gt;() 方法完毕。如果在一个类的 &lt;clinit&gt;() 方法中有耗时的操作，就可能造成多个线程阻塞，在实际过程中此种阻塞很隐蔽。 类初始化时机1. 主动引用虚拟机规范中并没有强制约束何时进行加载，但是规范严格规定了有且只有下列五种情况必须对类进行初始化（加载、验证、准备都会随之发生）： 遇到 new、getstatic、putstatic、invokestatic 这四条字节码指令时，如果类没有进行过初始化，则必须先触发其初始化。最常见的生成这 4 条指令的场景是：使用 new 关键字实例化对象的时候；读取或设置一个类的静态字段（被 final 修饰、已在编译期把结果放入常量池的静态字段除外）的时候；以及调用一个类的静态方法的时候。 使用 java.lang.reflect 包的方法对类进行反射调用的时候，如果类没有进行初始化，则需要先触发其初始化。 当初始化一个类的时候，如果发现其父类还没有进行过初始化，则需要先触发其父类的初始化。 当虚拟机启动时，用户需要指定一个要执行的主类（包含 main() 方法的那个类），虚拟机会先初始化这个主类； 当使用 JDK 1.7 的动态语言支持时，如果一个 java.lang.invoke.MethodHandle 实例最后的解析结果为 REF_getStatic, REF_putStatic, REF_invokeStatic 的方法句柄，并且这个方法句柄所对应的类没有进行过初始化，则需要先触发其初始化； 2. 被动引用以上 5 种场景中的行为称为对一个类进行主动引用。除此之外，所有引用类的方式都不会触发初始化，称为被动引用。被动引用的常见例子包括： 通过子类引用父类的静态字段，不会导致子类初始化。 1System.out.println(SubClass.value); // value 字段在 SuperClass 中定义 通过数组定义来引用类，不会触发此类的初始化。该过程会对数组类进行初始化，数组类是一个由虚拟机自动生成的、直接继承自 Object 的子类，其中包含了数组的属性和方法。 1SuperClass[] sca = new SuperClass[10]; 常量在编译阶段会存入调用类的常量池中，本质上并没有直接引用到定义常量的类，因此不会触发定义常量的类的初始化。 1System.out.println(ConstClass.HELLOWORLD); 类与类加载器两个类相等需要类本身相等，并且使用同一个类加载器进行加载。这是因为每一个类加载器都拥有一个独立的类名称空间。 这里的相等，包括类的 Class 对象的 equals() 方法、isAssignableFrom() 方法、isInstance() 方法的返回结果为 true，也包括使用 instanceof 关键字做对象所属关系判定结果为 true。 类加载器分类从 Java 虚拟机的角度来讲，只存在以下两种不同的类加载器： 启动类加载器（Bootstrap ClassLoader），这个类加载器用 C++ 实现，是虚拟机自身的一部分； 所有其他类的加载器，这些类由 Java 实现，独立于虚拟机外部，并且全都继承自抽象类 java.lang.ClassLoader。 从 Java 开发人员的角度看，类加载器可以划分得更细致一些： 启动类加载器（Bootstrap ClassLoader）此类加载器负责将存放在 &lt;JRE_HOME&gt;\lib 目录中的，或者被 -Xbootclasspath 参数所指定的路径中的，并且是虚拟机识别的（仅按照文件名识别，如 rt.jar，名字不符合的类库即使放在 lib 目录中也不会被加载）类库加载到虚拟机内存中。启动类加载器无法被 Java 程序直接引用，用户在编写自定义类加载器时，如果需要把加载请求委派给启动类加载器，直接使用 null 代替即可。 扩展类加载器（Extension ClassLoader）这个类加载器是由 ExtClassLoader（sun.misc.Launcher$ExtClassLoader）实现的。它负责将 &lt;JAVA_HOME&gt;/lib/ext 或者被 java.ext.dir 系统变量所指定路径中的所有类库加载到内存中，开发者可以直接使用扩展类加载器。 应用程序类加载器（Application ClassLoader）这个类加载器是由 AppClassLoader（sun.misc.Launcher$AppClassLoader）实现的。由于这个类加载器是 ClassLoader 中的 getSystemClassLoader() 方法的返回值，因此一般称为系统类加载器。它负责加载用户类路径（ClassPath）上所指定的类库，开发者可以直接使用这个类加载器，如果应用程序中没有自定义过自己的类加载器，一般情况下这个就是程序中默认的类加载器。 双亲委派模型应用程序都是由三种类加载器相互配合进行加载的，如果有必要，还可以加入自己定义的类加载器。 下图展示的类加载器之间的层次关系，称为类加载器的双亲委派模型（Parents Delegation Model）。该模型要求除了顶层的启动类加载器外，其余的类加载器都应有自己的父类加载器。这里类加载器之间的父子关系一般通过组合（Composition）关系来实现，而不是通过继承（Inheritance）的关系实现。 1. 工作过程一个类加载器首先将类加载请求传送到父类加载器，只有当父类加载器无法完成类加载请求时才尝试加载。 2. 好处使得 Java 类随着它的类加载器一起具有一种带有优先级的层次关系，从而使得基础类得到统一。 例如 java.lang.Object 存放在 rt.jar 中，如果编写另外一个 java.lang.Object 的类并放到 ClassPath 中，程序可以编译通过。由于双亲委派模型的存在，所以在 rt.jar 中的 Object 比在 ClassPath 中的 Object 优先级更高，这是因为 rt.jar 中的 Object 使用的是启动类加载器，而 ClassPath 中的 Object 使用的是应用程序类加载器。rt.jar 中的 Object 优先级更高，那么程序中所有的 Object 都是这个 Object。 3. 实现以下是抽象类 java.lang.ClassLoader 的代码片段，其中的 loadClass() 方法运行过程如下：先检查类是否已经加载过，如果没有则让父类加载器去加载。当父类加载器加载失败时抛出 ClassNotFoundException，此时尝试自己去加载。 1234567891011121314151617181920212223242526272829303132333435363738394041public abstract class ClassLoader &#123; // The parent class loader for delegation private final ClassLoader parent; public Class&lt;?&gt; loadClass(String name) throws ClassNotFoundException &#123; return loadClass(name, false); &#125; protected Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException &#123; synchronized (getClassLoadingLock(name)) &#123; // First, check if the class has already been loaded Class&lt;?&gt; c = findLoadedClass(name); if (c == null) &#123; try &#123; if (parent != null) &#123; c = parent.loadClass(name, false); &#125; else &#123; c = findBootstrapClassOrNull(name); &#125; &#125; catch (ClassNotFoundException e) &#123; // ClassNotFoundException thrown if class not found // from the non-null parent class loader &#125; if (c == null) &#123; // If still not found, then invoke findClass in order // to find the class. c = findClass(name); &#125; &#125; if (resolve) &#123; resolveClass(c); &#125; return c; &#125; &#125; protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; throw new ClassNotFoundException(name); &#125;&#125; 自定义类加载器实现FileSystemClassLoader 是自定义类加载器，继承自 java.lang.ClassLoader，用于加载文件系统上的类。它首先根据类的全名在文件系统上查找类的字节代码文件（.class 文件），然后读取该文件内容，最后通过 defineClass() 方法来把这些字节代码转换成 java.lang.Class 类的实例。 java.lang.ClassLoader 的 loadClass() 实现了双亲委派模型的逻辑，因此自定义类加载器一般不去重写它，但是需要重写 findClass() 方法。 12345678910111213141516171819202122232425262728293031323334353637383940public class FileSystemClassLoader extends ClassLoader &#123; private String rootDir; public FileSystemClassLoader(String rootDir) &#123; this.rootDir = rootDir; &#125; protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; byte[] classData = getClassData(name); if (classData == null) &#123; throw new ClassNotFoundException(); &#125; else &#123; return defineClass(name, classData, 0, classData.length); &#125; &#125; private byte[] getClassData(String className) &#123; String path = classNameToPath(className); try &#123; InputStream ins = new FileInputStream(path); ByteArrayOutputStream baos = new ByteArrayOutputStream(); int bufferSize = 4096; byte[] buffer = new byte[bufferSize]; int bytesNumRead; while ((bytesNumRead = ins.read(buffer)) != -1) &#123; baos.write(buffer, 0, bytesNumRead); &#125; return baos.toByteArray(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; private String classNameToPath(String className) &#123; return rootDir + File.separatorChar + className.replace('.', File.separatorChar) + ".class"; &#125;&#125; 参考资料 周志明. 深入理解 Java 虚拟机 [M]. 机械工业出版社, 2011. Chapter 2. The Structure of the Java Virtual Machine Jvm memoryGetting Started with the G1 Garbage Collector JNI Part1: Java Native Interface Introduction and “Hello World” application Memory Architecture Of JVM(Runtime Data Areas) JVM Run-Time Data Areas Android on x86: Java Native Interface and the Android Native Development Kit 深入理解 JVM(2)——GC 算法与内存分配策略 深入理解 JVM(3)——7 种垃圾收集器 JVM Internals 深入探讨 Java 类加载器 Guide to WeakHashMap in Java Tomcat example source code file (ConcurrentCache.java)]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mapreduce的map阶段中某几个task非常慢的一次排错过程与总结]]></title>
    <url>%2Fhadoop%2Fmapreduce-trouble-shotting.html</url>
    <content type="text"><![CDATA[发现问题：在家里的测试集群测试数据，发现如下问题：程序map阶段很慢，然后通过hadoop的集群界面，几乎大多数的task都是在几分钟就执行完，看到有几个task非常慢，执行了4个多小时还不到一半。 分析原因要么数据和代码问题，要么测试集群问题 初步查看测试集群问题通过hadoop UI 和hdfs UI 并没有发现有任何问题，测试集群11个节点都是Active。 初步检查代码和数据问题排除数据倾斜问题： map阶段是不会数据倾斜的，而且我们一个map是设置了跑多少数据的，我们所设置的是6G，也就是一个map跑6G的数据查看代码问题： 因为我们的程序之前都运行过，并没发现什么问题，但是也是有点担心最近有改动或者某些异常数据导致程序出问题了。代码问题可能的情况： 某个地方try cache了，try cache肯定会导致很慢，但是也不至于这么慢，基本排除这个问题。 进入了死循环或者某个大循环（复杂度很高的运算），通过task查看，确实好几分钟都没有变化了 程序等原因等可能导致MapContainer内存不足，或许大量GC 进一步分析问题通过界面这几个慢的task可以分析出，并不是说某一台机器慢，他们分别分散在不同的节点中，那么我就随便取了一个慢的节点进行查看。jvm分析，我还是建议使用命令行的摸索，例如jmap，jstack等命令，占用的内存会比较少，我这里是比较懒，直接用的界面程序查看。1. 使用xstart远程桌面连接 因为我对jvm的界面工具比较熟悉2. 找到jdk按照目录，先打开jvisualVM, 监控某个container的cpu和内存 如下所示 3. 找到我们的这个container：如何找是哪个container呢？ 我们其实可以通过每个container运行的时间去找，一个一个找，找到一个时间和我们hadoop ui上这个task运行时间一样的，那么就可以确定是这个container了，然后我找到的是这个container 。（当然，在linux界面上，查看ps -ef |grep pid，可以看到全名，其中有个attmpt_task_id， 这个也可以确定是哪个container） 4. 监控container 分析问题：从图中可以看出，cpu占用率很低： 那么排除是死循环或者大循环问题右边堆内存最大3个G，我们程序占用1G不到，而且右边和左边都可以看出，没有发生GC， 说明内存充足。 这时候我就暂时卡住了，感觉很诡异，不是cpu，也不是内存，到底是什么问题呢？看看线程栈，将线程dump下来查看：通过分析，发现是maptask再等待拉的操作，我们主线程是runnable状态，但是cpu利用很低。是在线程的wait阶段。 OK，这里很容易就猜到是在进行IO等操作导致的了，由于这个稍微比较卡，我又用回jconsole了，因为为了给同事看，我就没用jstack工具。 打开jconsole工具具体查看线程栈发现是在读取hdfs的时候，由于是同步IO，main方法一直在等待读取hdfs的数据然后同时查看TCP传输的方法， 也同样卡在io流的地方，占用了一个锁，一直不释放（占用锁不释放是正常的，但是这个读取的操作，每个文件都会是一个锁，我dump多次都发现是一样的锁，那么就发现可能是io问题了） IO问题： 我们可以基本猜测（确定）是IO问题了。 登录到刚才有问题的这个container所在的节点，查看一下IO负载，因为担心是不是说某些程序在占用IO，发现后并不是，并且读取hdfs的速度很慢，一秒钟才1M，后来想了想那一小时才3个多G的读取速度，肯定慢啊。 查看是哪个节点比较慢界面中看log知道这个mapTask跑的是这些数据12345678910111213141516171819202122232425/mt_wlyh/Data/BeiJing/mroxdrmerge/xdr_loc/data_01_180125/XDR_LOCATION_01_180125/xdrLocation-r-00235:805306368+130232698,/mt_wlyh/Data/BeiJing/mroxdrmerge/xdr_loc/data_01_180125/XDR_LOCATION_01_180125/xdrLocation-r-00239:0+134217728,/mt_wlyh/Data/BeiJing/mroxdrmerge/xdr_loc/data_01_180125/XDR_LOCATION_01_180125/xdrLocation-r-00239:134217728+134217728,/mt_wlyh/Data/BeiJing/mroxdrmerge/xdr_loc/data_01_180125/XDR_LOCATION_01_180125/xdrLocation-r-00239:268435456+134217728,/mt_wlyh/Data/BeiJing/mroxdrmerge/xdr_loc/data_01_180125/XDR_LOCATION_01_180125/xdrLocation-r-00239:402653184+134217728,/mt_wlyh/Data/BeiJing/mroxdrmerge/xdr_loc/data_01_180125/XDR_LOCATION_01_180125/xdrLocation-r-00239:536870912+134217728,/mt_wlyh/Data/BeiJing/mroxdrmerge/xdr_loc/data_01_180125/XDR_LOCATION_01_180125/xdrLocation-r-00239:671088640+134217728,/mt_wlyh/Data/BeiJing/mroxdrmerge/xdr_loc/data_01_180125/XDR_LOCATION_01_180125/xdrLocation-r-00239:805306368+110006092,/mt_wlyh/Data/BeiJing/mroxdrmerge/xdr_loc/data_01_180125/XDR_LOCATION_01_180125/xdrLocation-r-00277:0+134217728,/mt_wlyh/Data/BeiJing/mroxdrmerge/xdr_loc/data_01_180125/XDR_LOCATION_01_180125/xdrLocation-r-00277:134217728+134217728,/mt_wlyh/Data/BeiJing/mroxdrmerge/xdr_loc/data_01_180125/XDR_LOCATION_01_180125/xdrLocation-r-00277:268435456+134217728,/mt_wlyh/Data/BeiJing/mroxdrmerge/xdr_loc/data_01_180125/XDR_LOCATION_01_180125/xdrLocation-r-00277:402653184+134217728,/mt_wlyh/Data/BeiJing/mroxdrmerge/xdr_loc/data_01_180125/XDR_LOCATION_01_180125/xdrLocation-r-00277:536870912+134217728,/mt_wlyh/Data/BeiJing/mroxdrmerge/xdr_loc/data_01_180125/XDR_LOCATION_01_180125/xdrLocation-r-00277:671088640+134217728,/mt_wlyh/Data/BeiJing/mroxdrmerge/xdr_loc/data_01_180125/XDR_LOCATION_01_180125/xdrLocation-r-00277:805306368+111362066,/mt_wlyh/Data/BeiJing/mroxdrmerge/xdr_loc/data_01_180125/XDR_LOCATION_01_180125/xdrLocation-r-00278:0+134217728,/mt_wlyh/Data/BeiJing/mroxdrmerge/xdr_loc/data_01_180125/XDR_LOCATION_01_180125/xdrLocation-r-00278:134217728+134217728``` **在界面中可以知道跑到了百分之16**， ok，这时候估计是在第二个文件（因为每个文件的大小都差不多） 查看第二个文件在哪个节点 hadoop fsck /mt_wlyh/Data/BeiJing/mroxdrmerge/xdr_loc/data_01_180125/XDR_LOCATION_01_180125/xdrLocation-r-00239 -files -locations -blocks 显示 DEPRECATED: Use of this script to execute hdfs command is deprecated.Instead use the hdfs command for it. SLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/home/hmaster/MyCloudera/APP/hadoop/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/home/hmaster/MyCloudera/APP/hbase/hbase-1.2.5/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]Connecting to namenode via http://master:50070/fsck?ugi=hmaster&amp;files=1&amp;locations=1&amp;blocks=1&amp;path=%2Fmt_wlyh%2FData%2FBeiJing%2Fmroxdrmerge%2Fxdr_loc%2Fdata_01_180125%2FXDR_LOCATION_01_180125%2FxdrLocation-r-00239FSCK started by hmaster (auth:SIMPLE) from /192.168.1.31 for path /mt_wlyh/Data/BeiJing/mroxdrmerge/xdr_loc/data_01_180125/XDR_LOCATION_01_180125/xdrLocation-r-00239 at Thu Aug 02 19:10:29 CST 2018/mt_wlyh/Data/BeiJing/mroxdrmerge/xdr_loc/data_01_180125/XDR_LOCATION_01_180125/xdrLocation-r-00239 915312460 bytes, 7 block(s): OK BP-369656756-192.168.1.65-1441938871593:blk_1091923684_18278024 len=134217728 repl=1 [DatanodeInfoWithStorage[192.168.1.72:50010,DS-5ce4cd2a-bda2-4c02-9fd6-cc008f589945,DISK]] BP-369656756-192.168.1.65-1441938871593:blk_1091923705_18278045 len=134217728 repl=1 [DatanodeInfoWithStorage[192.168.1.72:50010,DS-5ce4cd2a-bda2-4c02-9fd6-cc008f589945,DISK]] BP-369656756-192.168.1.65-1441938871593:blk_1091923724_18278064 len=134217728 repl=1 [DatanodeInfoWithStorage[192.168.1.72:50010,DS-5ce4cd2a-bda2-4c02-9fd6-cc008f589945,DISK]] BP-369656756-192.168.1.65-1441938871593:blk_1091923736_18278076 len=134217728 repl=1 [DatanodeInfoWithStorage[192.168.1.72:50010,DS-5ce4cd2a-bda2-4c02-9fd6-cc008f589945,DISK]] BP-369656756-192.168.1.65-1441938871593:blk_1091923744_18278084 len=134217728 repl=1 [DatanodeInfoWithStorage[192.168.1.72:50010,DS-5ce4cd2a-bda2-4c02-9fd6-cc008f589945,DISK]] BP-369656756-192.168.1.65-1441938871593:blk_1091923751_18278091 len=134217728 repl=1 [DatanodeInfoWithStorage[192.168.1.72:50010,DS-5ce4cd2a-bda2-4c02-9fd6-cc008f589945,DISK]] BP-369656756-192.168.1.65-1441938871593:blk_1091923762_18278102 len=110006092 repl=1 [DatanodeInfoWithStorage[192.168.1.72:50010,DS-5ce4cd2a-bda2-4c02-9fd6-cc008f589945,DISK]] Status: HEALTHY Total size: 915312460 B Total dirs: 0 Total files: 1 Total symlinks: 0 Total blocks (validated): 7 (avg. block size 130758922 B) Minimally replicated blocks: 7 (100.0 %) Over-replicated blocks: 0 (0.0 %) Under-replicated blocks: 0 (0.0 %) Mis-replicated blocks: 0 (0.0 %) Default replication factor: 1 Average block replication: 1.0 Corrupt blocks: 0 Missing replicas: 0 (0.0 %) Number of data-nodes: 11 Number of racks: 1FSCK ended at Thu Aug 02 19:10:29 CST 2018 in 0 milliseconds123456789 **说明这个数据在65和72节点** **进一步查看**，确实发现72节点（node002）有问题，ping很慢 ![ping速度](https://raw.githubusercontent.com/zhaikaishun/blog_img/master/blog/map%E4%B8%AD%E6%9F%90%E4%B8%AAtask%E5%8D%A1%E4%BD%8F%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/ping速度.png) ping node002发现很慢，但是ping其他的发现还好再测试一下node002的网络传输性能 dd if=/dev/zero of=/dev/null bs=50 count=100M```SCP到master节点的时候，发现速度很慢，平均就是1.2M每秒。 找来一位运维的同事，然后他发现了问题其他的节点都是1000M的网卡，而这个节点确实10M的网卡，肯定慢啊，也证实了前面的1.2M的scp和1M的hdfs读取速度就是这个问题引起的。 后来发现可能是网线还是什么的问题，后来解决后，集群也就恢复了正常，程序也能正常运行了。 排错总结： 冷静，这是测试集群，操作起来还比较方便，如果是正式环境，操作起来会比较麻烦，这时候更要冷静思考，不要放弃 理论指导实践，需要对jvm，hadoop等原理有所了解，需要对分布式，IO流，网络，linux等有所了解。即使没有经验，也能依靠自己的理论知识来指导实践 合理猜测中进行，最后验证猜测，除非很有经验，否则还是要在猜测中不断的证实自己的猜测，要大胆的猜测，有规划的验证 总结排错经验总结，对以后很有帮助 jvm定位技巧还是需要点火候的，堆内存溢出还比较简单，堆栈dump转存是比较考验一个人能力的。 感觉在spark中，界面显示更友好一些，mapreduce中的排错，可能是经常会用上述到这种方法的]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[maven-打包的几种方式笔记]]></title>
    <url>%2Ftool%2Fmaven-install.html</url>
    <content type="text"><![CDATA[指定打的jar包使用的jdk版本 1234567891011&lt;plugin&gt; &lt;!-- MAVEN 编译使用的JDK版本 --&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.2&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.7&lt;/source&gt; &lt;target&gt;1.7&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt;&lt;/plugin&gt; 将依赖打入到jar包内部 123456789101112131415161718192021222324&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;version&gt;2.5.5&lt;/version&gt; &lt;configuration&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;cn.mastercom.locspark.LocFill&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt; 将jar包打入到外面 1234567891011121314151617181920212223242526272829&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;archive&gt; &lt;manifest&gt; &lt;addClasspath&gt;true&lt;/addClasspath&gt; &lt;classpathPrefix&gt;lib/&lt;/classpathPrefix&gt; &lt;mainClass&gt;cn.mastercom.locspark.LocFill&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;/configuration&gt;&lt;/plugin&gt;&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;copy&lt;/id&gt; &lt;phase&gt;install&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;copy-dependencies&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;outputDirectory&gt;$&#123;project.build.directory&#125;/lib&lt;/outputDirectory&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt; 部分依赖打入到jar包内部 123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;2.1&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;!-- 自动将所有不使用的类全部排除掉 --&gt; &lt;minimizeJar&gt;false&lt;/minimizeJar&gt; &lt;transformers&gt; &lt;transformer implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;&gt; &lt;mainClass&gt;cn.mastercom.locspark.LocFill&lt;/mainClass&gt; &lt;/transformer&gt; &lt;/transformers&gt; &lt;artifactSet&gt; &lt;includes&gt; &lt;!-- 包括哪些类，自定义 --&gt; &lt;include&gt;xxx.xxx.xx*&lt;/include&gt; &lt;/includes&gt; &lt;/artifactSet&gt; &lt;filters&gt; &lt;filter&gt; &lt;artifact&gt;*:*&lt;/artifact&gt; &lt;!-- 排除哪些类，自定义 --&gt; &lt;excludes&gt; &lt;!-- 排除naive.io --&gt; &lt;exclude&gt;org/**&lt;/exclude&gt; &lt;!-- 排除不需要的配置 自定义 --&gt; &lt;exclude&gt;remote/**&lt;/exclude&gt; &lt;exclude&gt;local/**&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt; &lt;/excludes&gt; &lt;/filter&gt; &lt;/filters&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt;]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[yarn mapreduce参数最佳实践]]></title>
    <url>%2Fhadoop%2Fhadoop%2Fyarn-mapreduce-best-practice.html</url>
    <content type="text"><![CDATA[mapreduce.job.queuename设置队列名 小文件整合 mapreduce.input.fileinputformat.split.minsize mapreduce.input.fileinputformat.split.maxsize mapreduce.input.fileinputformat.split.minsize.per.node mapreduce.input.fileinputformat.split.minsize.per.rack mapreduce中map的个数和两个有关，一个是文件的个数，一个是大小，默认split是128M， 如果一个文件大于128M，例如129M，那么会有两个map，一个是128M，一个是1M。又例如有10个文件，每个都是1M，那么map是会有10个的。对于这种小文件太多，或者是我们想讲每一个map处理的数据量大一些，就应该设置上面的几个参数，上面几个参数是byte的单位。例如我们想设置一次处理1G，那么就设置成1234mapreduce.input.fileinputformat.split.minsize = 1024*1024*1024mapreduce.input.fileinputformat.split.maxsize = 2048*1024*1024mapreduce.input.fileinputformat.split.minsize.per.node = 512*1024*1024mapreduce.input.fileinputformat.split.minsize.per.rack = 512*1024*1024 推测功能mapreduce.reduce.speculative默认是true，有时候需要设置成false。参考： http://itfish.net/article/60389.html或者搜索 container大小设置最佳实践mapreduce.map.memory.mb 和 mapreduce.reduce.memory.mb 和mapreduce.map.cpu.vcores和 mapreduce.reduce.cpu.vcores mapreduce.map.memory.mb 默认1024M，一个map启动的container是1024Mmapreduce.reduce.memory.mb 默认3072M，一个map启动的container是3072mapreduce.map.cpu.vcores默认1个vcore，一个map任务默认使用一个虚拟核运行mapreduce.reduce.cpu.vcores默认1个vcore，一个reduce任务默认使用一个虚拟核运行。 调优就是尽可能的让集群资源充分利用，这里需要根据具体的需求和集群资源情况来定。例如不考虑内存溢出等情况, 集群资源如下 Memory Total VCores Total 320G 80 如果数据比较均匀，应该尽可能的设置成如下： mapreduce.map.memory.mb mapreduce.reduce.memory.mb mapreduce.map.cpu.vcores mapreduce.reduce.cpu.vcores 4096 4096 1 1 这样并发数能到 max(320G/4G，80vcore/1vcore)=80上面是map核reduce都到了最大的80的并发，集群利用最充分。 一般来说，我们默认mapreduce.map.cpu.vcores和mapreduce.reduce.cpu.vcores为1个就好了，但是对于一个map和一个reduce的container的内存大小设置成了4G，如果一个map和一个reduce处理的任务很小，那又会很浪费资源，这时，对于map来说，可以用前面说的小文件整合，设置mapreduce.input.fileinputformat.split来解决map的大小，尽可能接近4G，但是又要注意可能出现的内存溢出的情况。对于reduce，1个container启动用了4G内存，那这4G内存也应尽可能的充分使用，这时候，我们尽量的评估进入到reduce的数据大小有多少，合理的设置reduceTask数，这一步是比较麻烦的，因为这里如果出现数据倾斜将会导致oom内存溢出错误。 前面说到了，并发数受到集群总内存/container的限制，同时，并发数也会受到集群vcore的限制，还是上面那个例子，例如集群资源为320G，80vcore，我一个map任务为2G，由于受到cpu的限制，最多同时80个vcore的限制，那么内存使用只能使用160G。这显然是浪费资源了。 对于mapreduce.map.cpu.vcores和mapreduce.reduce.cpu.vcores，为什么默认是1呢，在集群的内存/cpu很小的情况下，能否一个map端将这两个值设置成2或者更大呢。这是当然可以的，但是，即使我们将这个设置成2，任务的并发并不会是 Vcores Total/2的关系，并发仍然将是上面两条决定的。举个例子，还是320G，80vcore集群。我们设置mapreduce.map.memory.mb为4G，mapreduce.map.cpu.vcores为2， 很多人以为我一个map需要两个核，那么80vcore/2vcore=40，那么我们并发最大只能用到40*4=160G的内存，其实这是错误的，这种情况，我们任然基本上能将内存占满，并发数任然能到80个。这个时候， mapreduce.map.cpu.vcores基本就失效了。后来仔细想了想，一个map或者reduce任务，里面的数据应该并不可能会有多线程并发，但是mapreduce.map.cpu.vcores为什么会有这个参数呢，后来想了一下，一个map或者reduce任务，代码的执行肯定是一个线程，但是任务的状态等监控，垃圾回收等，是可以使用另外一个线程来运行的，这也是将mapreduce.map.cpu.vcores设置成2可能会快一点的效果。我曾经碰到一个cpu十分充足的集群，vcore和内存比例是1比1，但是为了让数据不倾斜，我们的mapreduce.reduce.memory.mb至少要到4G，那么这时候，其实cpu就只能利用1/4了，这时候cpu很充足，我便尝试将mapreduce.map.cpu.vcores设置成2.其实这样也并不是说我一定每个map都能使用到2个vcore，只不过有时候，有的任务状态监控，jvm垃圾回收等，就有了另外一个vcore来运行了。 mapreduce.task.io.sort.mb这个参数理解需要理解mapreduce的shuffle过程，mapreduce的shuffle中，有一个环形缓冲区（就是一个带有前后两个指针的数组，shuffle过程自行搜索），这个值默认是100兆，配合上有个参数mapreduce.task.io.sort.spill.percent，一般这个参数默认为0.8，那么就意味着，这个数组到了80M，我就要开始进行排序了，然后要往磁盘写数据了。所以这个值越大，就不用导致频繁的溢出了。按照经验，一般这个值和map的输出，reduce的输入的大小相关比较好，但是这个值最好别超过2046，假如一个reduce处理的数据量为1G，那么这个值可以设置成200M， 一般的经验是reduce处理的数据量/5的关系比较好。 mapreduce.map.java.opts就是一个map container中jvm虚拟机的内存一般设置成mapreduce.map.memory.mb的0.8倍比较合适例如mapreduce.map.memory.mb=4096mapreduce.map.java.opts 设置成 -Xmx3276M mapreduce.reduce.java.opts就是一个reduce container中jvm虚拟机的内存一般设置成mapreduce.reduce.memory.mb的0.8倍比较合适例如mapreduce.reduce.memory.mb=4096mapreduce.reduce.java.opts 设置成 -Xmx3276M yarn.app.mapreduce.am.resource.mbMR ApplicationMaster占用的内存量，具体设置TODO，记得有时候小文件太多，超过多少万，这个太小了任务不会运行 mapreduce.task.timeoutmapreduce任务默认超时时间，有时候抢队列的时候，这个会用上，默认值600000就好，不用管 mapred.max.map.failures.percent允许map失败的比例，默认是0，可以根据自己需求，合理设置 mapred.max.reduce.failures.percent允许reduce失败的比例，默认是0，可以根据自己需求，合理设置 mapreduce.job.reduce.slowstart.completedmapsmap不用跑完就可以开始reduce了的比例，默认是0.95（网上说的0.05感觉不对啊），也就是map完成到百分之95时就可以开始reduce了，这样的好处是到了map最后几个，其实大多数资源都空闲了，这时候就先进行reduce吧，不然等全部跑完map有点浪费资源了。但是我之前碰到过一次资源死锁饿死的情况，就是map还有几个没跑完，reduce已经起来了，然而reduce需要等待map跑完的数据，reduce端拉不到，然后map端也没完成，并且整个集群的资源都被利用完了，这样map跑不完，reduce也跑不完，就这样相互等待卡着 HADOOP_CLIENT_OPTShadoop jar启动的时候，client端的jvm内存大小。太小的话，如果跑的文件个数比较多,JOB还未起来就会报OOM错误 此配置在hadoop-env.sh中1export HADOOP_CLIENT_OPTS=&quot;-Xmx1024m&quot;]]></content>
      <categories>
        <category>大数据</category>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[兔子递归想到的一些道理]]></title>
    <url>%2F%E9%9A%8F%E7%AC%94%2F%E7%AE%97%E6%B3%95%2Frabbit-recursive-life.html</url>
    <content type="text"><![CDATA[兔子递归问题前几天有个同事考我一个有关兔子递归的题目：题目如下 有一对兔子，生长到第3个月时。开始生第一对兔子，并且以后每月生一对兔子，小兔子生长三个月后，也开始生兔子，问N个月后兔子的总数量.刚开始我一想，这不就是大一就做过的吗，这个就是经典的斐波拉契啊。于是在草稿纸上画着 12月份 1 2 3 4 5 6 7对数 1 1 2 3 5 8 13 哦，然后觉得，可以使用递推法。也可以使用递归法。不过看我那同事，估计是说要用递归了。 然后就用递归写程序了。其实都知道不就是f(n)=f(n-1)+f(n-2) 吗(n&gt;2时)， 边界就是f1=1, f2=1 但是我并没有写，这貌似和我所理解的递归思维不一样。我所理解的，不是说通过规律找到1f(n)=f(n-1)+f(n-2)。 大家都知道，递归就是自己调用自己，但是一个抽象的方法，如何自己调用自己呢。我所理解的是将一种重复问题，分别为同类子问题的方法，这个子问题也是一种重复问题。而我们应该是要有这种思想。 分析回到这个问题， f(n)=f(n-1)+f(n-2),如果从一般的情况来说，这三个函数分别代表什么意思呢。 f(n)代表第n月时兔子总数 f(n-1)代表第n-1个月时的兔子总数 f(n-2)代表第n-2个月时的兔子总数. 可这个公式到底是怎么来的呢。后来想了想，才想清楚这里先给这个题目说清晰一点，第三个月开始生兔子，要说明是第三个月初。然后问第n个月后有多少只兔子，其实问的是第n个月底题目清晰后，分析如下12第n个月的兔子总数 = 第n-1个月的兔子总数+第n月初新生的兔子总数 而第n月初新生的兔子总数，其实就是第n-2个月时的兔子的总数，这是因为n-2个月时的兔子总数，在第n月初都将会生兔子。 所以得出1第n个月的兔子总数 = 第n-1个月的兔子总数 + 第n-2个月时的兔子的总数. 也就是1f(n)=f(n-1)+f(n-2). 这里我们可以看出，我们并不是想要通过找规律得到这个递归的过程，而是应该根据一种划分子问题的思维，去寻找这个规律。如果有这种思维，这道题可以将这个生长到第3个月改为第k个月还是一样的分析方法12第n个月的兔子总数 = 第n-1个月的兔子总数+第n月初新生的兔子总数 第n月初新生的兔子总数 = 第n-k+1时兔子的总数.因为第n-k+1时的兔子总数，在第n个月时，肯定都会生兔子。 所以1f(n)=f(n-1)+f(n-k+1)。 补充几个类似思想的问题然后再补充几个可以用划分子问题的方式，这是一种递归问题 跳台阶问题一只青蛙一次可以跳上1级台阶，也可以跳上2级。求该青蛙跳上一个n级的台阶总共有多少种跳法。 矩形覆盖问题我们可以用21的小矩形横着或者竖着去覆盖更大的矩形, 请问用n个21的小矩形无重叠地覆盖一个2*n的大矩形，总共有多少种方法？ 二叉树的前序遍历，中序遍历，后续遍历等问题 其实这几种问题，都不是说找规律来的，都是划分为同类子问题的方式 要有自己的思考所以，网上的挺多内容，都需要有自己的思考，多想想为什么，有时候即使自己想错了也可以，但毕竟还是自己思考过，而不是一味的参考网上的方法。其实从小到大，我思考都不会直接看答案，天生就对答案有一种怀疑的态度。之前刷数学题， 物理题，基本都是自己去做，如果和答案不一样，我一般都是先怀疑答案有问题，然后一个一个对比，如果是自己的问题，那OK。自己也会对比为什么要这样做，正是由于有这种思维，如果一个你理解了的问题，无论这个问题再怎么变化，你也是会做的。做算法也是一样，如果去刷leetcode，或者编程中，一有问题就百度google，然后网上说什么，就认同什么，其实这对自己提升是非常小的，甚至，大多时候先看完答案，总有一种先入为主的思维，让你不得不往答案的这个思路去想，但是有时候这并不是最好的，也有时也没有涉及到问题的本质，只有多思考，多怀疑，这样，才能对事物理解的更加深刻。（不过这样就只有一个不好的，也是大多理工科的一个通病，容易钻牛角尖，其实怀疑也需要合理，该相信的还是要相信，只不过更多要有自己的想法，并且尽量去客观，辩证的看待）最后再补充一句，看新闻别看腾讯今日头条百度等新闻，更不要看在腾讯百度今日头条等看大家的评论。]]></content>
      <categories>
        <category>随笔</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[idea多maven依赖模块的导入]]></title>
    <url>%2Ftool%2Fidea-multi-import.html</url>
    <content type="text"><![CDATA[idea多maven依赖模块的导入 先任意导入一个项目File -&gt; New -&gt;module from exitited source -&gt;选择maven 下一步勾选Search for projects recurisivery 一定要把不要的去掉, 其实如果项目是干净的，没过多导入项目，不会有问题 然后一路next就行了。 打包：右侧MavenProject, 调不出就百度选中module，在Lifecyle中直接顺序 clean，install即可]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper Curator框架简单使用]]></title>
    <url>%2F%E6%9E%B6%E6%9E%84%2Fzookeeper9.html</url>
    <content type="text"><![CDATA[github： https://github.com/zhaikaishun/zookeeper_tutorial Curator框架的目的官网首页介绍是 Guava is to Java what Curator is to Xookeeper ，为了更好的实现java操作zookeeper服务器，后来出现了Curator框架，非常的强大，目前已经是Apache的顶级项目，里面提供了更多丰富的操作，例如session超时重连、主从选举、分布式计算器、分布式锁等等适用于各种复杂zookeeper场景的API封装。Maven依赖jar包下载都去官网下载，http://curator.apache.org/ Curatot框架使用(一)Curatir框架使用链式编程风格，易读性更强，使用工程方法创建连接对象。1 使用CuratorFrameworkFactory的两个静态工厂方法（参数不同）来实现： 参数1： connectString，连接串 参数2： retyPolicy，重试连接策略。有四中实现分别为：ExponentialBackoffRetry、RetryTimes、RetryOneTimes、RetryUntilElapsed（具体参数的意思以后会讲解，也可先上网查看） 参数3：sessionTimeoutMs 会话超时时间 默认为60000ms 参数4：connectionTimeoutMs 连接超时时间，默认为15000ms注意：对于retryPolicy策略通过一个接口来让用户自定义实现。代码在package bjsxt.curator.base; 代码示例前面的设置123456789101112131415161718192021 /** zookeeper地址 */ static final String CONNECT_ADDR = "192.168.1.31:2181,192.168.1.32:2181,192.168.1.33:2181"; /** session超时时间 */ static final int SESSION_OUTTIME = 5000;//ms public static void main(String[] args) throws Exception &#123; //1 重试策略：初试时间为1s 重试10次 RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 10); //2 通过工厂创建连接 CuratorFramework cf = CuratorFrameworkFactory.builder() .connectString(CONNECT_ADDR) .sessionTimeoutMs(SESSION_OUTTIME) .retryPolicy(retryPolicy)// .namespace("super") .build(); //3 开启连接 cf.start(); System.out.println(States.CONNECTED); System.out.println(cf.getState()); Curator的基本方法1.创建连接2.Curator创建节点Create方法，可选链式项：creatingParentslfNeeded、withMode、forPath、withACL等。例如123456789101112131415//4 建立节点 指定节点类型（不加withMode默认为持久类型节点）、路径、数据内容cf.create().creatingParentsIfNeeded().withMode(CreateMode.PERSISTENT).forPath(&quot;/super/c1&quot;,&quot;c1内容&quot;.getBytes());或者 // cf.create().creatingParentsIfNeeded().withMode(CreateMode.PERSISTENT).forPath(&quot;/super/c1&quot;,&quot;c1内容&quot;.getBytes());// cf.create().creatingParentsIfNeeded().withMode(CreateMode.PERSISTENT).forPath(&quot;/super/c2&quot;,&quot;c2内容&quot;.getBytes());``` ### 3.删除节点 delete方法，可选链式项： deletingChildrenIfNeeded、guranteed、withVersion、forPath等。 例如```javacf.delete().guaranteed().deletingChildrenIfNeeded().forPath(&quot;/super&quot;); 4.读取和修改数据getData、setData方法 1234567//读取节点String ret1 = new String(cf.getData().forPath("/super/c2"));System.out.println(ret1);//修改节点cf.setData().forPath("/super/c2", "修改c2内容".getBytes());String ret2 = new String(cf.getData().forPath("/super/c2"));System.out.println(ret2); 5.异步回调方法。比如创建节点时绑定一个回调函数，该回调函数可以输出服务器的状态码以及服务器事件类型。还可以加入一个线程池进行优化操作。123456789101112ExecutorService pool = Executors.newCachedThreadPool();cf.create().creatingParentsIfNeeded().withMode(CreateMode.PERSISTENT).inBackground(new BackgroundCallback() &#123; @Override public void processResult(CuratorFramework cf, CuratorEvent ce) throws Exception &#123; System.out.println("code:" + ce.getResultCode()); System.out.println("type:" + ce.getType()); System.out.println("线程为:" + Thread.currentThread().getName()); &#125;&#125;, pool).forPath("/super/c3","c3内容".getBytes());Thread.sleep(Integer.MAX_VALUE); 6.读取子节点方法getChildren1234List&lt;String&gt; list = cf.getChildren().forPath(&quot;/super&quot;);for(String p : list)&#123; System.out.println(p);&#125; 7.判断子节点是否存在checkExists方法12Stat stat = cf.checkExists().forPath(&quot;/super/c3&quot;);System.out.println(stat); 讲上面异步回调的那个线程池的作用比如某个操作一次性要创建500个节点，不可能一次用500个线程去处理。所以这里使用的是一个线程池来进行控制 CuratorWatcher原理，使用缓存的判断的方式，不需要重复注册！！！最牛的地方，估计可以想到那个宕机订阅问题。具体的原理，建议深入了解一下，感觉挺厉害的。 1. 方法1注意最后一个参数，这个是是否压缩 ， 注意那个cache.star的时候的那个模式 POST_INITALLZED_EVENT直接上代码看即可123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class CuratorWatcher1 &#123; /** zookeeper地址 */ static final String CONNECT_ADDR = "192.168.1.31:2181,192.168.1.32:2181,192.168.1.33:2181"; /** session超时时间 */ static final int SESSION_OUTTIME = 5000;//ms public static void main(String[] args) throws Exception &#123; //1 重试策略：初试时间为1s 重试10次 RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 10); //2 通过工厂创建连接 CuratorFramework cf = CuratorFrameworkFactory.builder() .connectString(CONNECT_ADDR) .sessionTimeoutMs(SESSION_OUTTIME) .retryPolicy(retryPolicy) .build(); //3 建立连接 cf.start(); //4 建立一个cache缓存 final NodeCache cache = new NodeCache(cf, "/super", false); cache.start(true); cache.getListenable().addListener(new NodeCacheListener() &#123; /** * &lt;B&gt;方法名称：&lt;/B&gt;nodeChanged&lt;BR&gt; * &lt;B&gt;概要说明：&lt;/B&gt;触发事件为创建节点和更新节点，在删除节点的时候并不触发此操作。&lt;BR&gt; * @see org.apache.curator.framework.recipes.cache.NodeCacheListener#nodeChanged() */ @Override public void nodeChanged() throws Exception &#123; System.out.println("路径为：" + cache.getCurrentData().getPath()); System.out.println("数据为：" + new String(cache.getCurrentData().getData())); System.out.println("状态为：" + cache.getCurrentData().getStat()); System.out.println("---------------------------------------"); &#125; &#125;); Thread.sleep(1000); cf.create().forPath("/super", "123".getBytes()); Thread.sleep(1000); cf.setData().forPath("/super", "456".getBytes()); Thread.sleep(1000); cf.delete().forPath("/super"); Thread.sleep(Integer.MAX_VALUE); &#125;&#125;``` **输出** 路径为：/super数据为：123状态为：38654705677,38654705677,1509971265443,1509971265443,0,0,0,0,3,0,38654705677 路径为：/super数据为：456状态为：38654705677,38654705678,1509971265443,1509971266479,1,0,0,0,3,0,38654705677 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283###2. 方法2 注意第三个参数，表示是否接受节点数据内容，如果为false则不接受 ```javapublic class CuratorWatcher2 &#123; /** zookeeper地址 */ static final String CONNECT_ADDR = &quot;192.168.1.31:2181,192.168.1.32:2181&quot;; /** session超时时间 */ static final int SESSION_OUTTIME = 10000;//ms public static void main(String[] args) throws Exception &#123; //1 重试策略：初试时间为1s 重试10次 RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 10); //2 通过工厂创建连接 CuratorFramework cf = CuratorFrameworkFactory.builder() .connectString(CONNECT_ADDR) .sessionTimeoutMs(SESSION_OUTTIME) .retryPolicy(retryPolicy) .build(); //3 建立连接 cf.start(); //4 建立一个PathChildrenCache缓存,第三个参数为是否接受节点数据内容 如果为false则不接受 PathChildrenCache cache = new PathChildrenCache(cf, &quot;/super&quot;, true); //5 在初始化的时候就进行缓存监听 cache.start(StartMode.POST_INITIALIZED_EVENT); cache.getListenable().addListener(new PathChildrenCacheListener() &#123; /** * &lt;B&gt;方法名称：&lt;/B&gt;监听子节点变更&lt;BR&gt; * &lt;B&gt;概要说明：&lt;/B&gt;新建、修改、删除&lt;BR&gt; * @see org.apache.curator.framework.recipes.cache.PathChildrenCacheListener#childEvent(org.apache.curator.framework.CuratorFramework, org.apache.curator.framework.recipes.cache.PathChildrenCacheEvent) */ @Override public void childEvent(CuratorFramework cf, PathChildrenCacheEvent event) throws Exception &#123; switch (event.getType()) &#123; case CHILD_ADDED: System.out.println(&quot;CHILD_ADDED :&quot; + event.getData().getPath()); //也可以获取内容 System.out.println(&quot;CHILD_ADDED 内容 :&quot; + new String(event.getData().getData(),&quot;utf-8&quot;)); break; case CHILD_UPDATED: System.out.println(&quot;CHILD_UPDATED :&quot; + event.getData().getPath()); System.out.println(&quot;CHILD_UPDATED 内容 :&quot; + new String(event.getData().getData(),&quot;utf-8&quot;)); break; case CHILD_REMOVED: System.out.println(&quot;CHILD_REMOVED :&quot; + event.getData().getPath()); break; default: break; &#125; &#125; &#125;); //创建本身节点不发生变化 cf.create().forPath(&quot;/super&quot;, &quot;init&quot;.getBytes()); //添加子节点 Thread.sleep(1000); cf.create().forPath(&quot;/super/c1&quot;, &quot;c1内容&quot;.getBytes()); Thread.sleep(1000); cf.create().forPath(&quot;/super/c2&quot;, &quot;c2内容&quot;.getBytes()); //修改子节点 Thread.sleep(1000); cf.setData().forPath(&quot;/super/c1&quot;, &quot;c1更新内容&quot;.getBytes()); //删除子节点 Thread.sleep(1000); cf.delete().forPath(&quot;/super/c2&quot;); //删除本身节点 Thread.sleep(1000); cf.delete().deletingChildrenIfNeeded().forPath(&quot;/super&quot;); System.out.println(&quot;------end------&quot;); Thread.sleep(Integer.MAX_VALUE); &#125;&#125; 运行结果123456789CHILD_ADDED :/super/c1CHILD_ADDED 内容 :c1内容CHILD_ADDED :/super/c2CHILD_ADDED 内容 :c2内容CHILD_UPDATED :/super/c1CHILD_UPDATED 内容 :c1更新内容CHILD_REMOVED :/super/c2CHILD_REMOVED :/super/c1------end------ Curator场景应用(一)分布式锁功能在分布式场景中，我们为了保证数据的一致性，经常在程序运行的某一个点需要进行同步操作（java可提供synchronized或者Reentrantlock实现）比如我们看一个小示例，这个示例会出现分布式不同步的问题：因为我们之前所说的是再高并发下访问一个程序，现在我们则是在高并发下访问多个服务器节点（分布式）我们使用Curator基于Zookeeper的特性提供的分布式锁来处理分布式场景的数据一致性，zookeeper原生的写分布式比较麻烦，我们这里强烈推荐使用Curator的分布式锁！Curator主要使用 InterProcessMutex 来进行分布式锁的控制123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869public class Lock2 &#123; /** zookeeper地址 */ static final String CONNECT_ADDR = "192.168.1.31:2181,192.168.1.32:2181"; /** session超时时间 */ static final int SESSION_OUTTIME = 20000;//ms static int count = 10; public static void genarNo()&#123; try &#123; count--; System.out.println(count); &#125; finally &#123; &#125; &#125; public static void main(String[] args) throws Exception &#123; //1 重试策略：初试时间为1s 重试10次 RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 10); //2 通过工厂创建连接 CuratorFramework cf = CuratorFrameworkFactory.builder() .connectString(CONNECT_ADDR) .sessionTimeoutMs(SESSION_OUTTIME) .retryPolicy(retryPolicy)// .namespace("super") .build(); //3 开启连接 cf.start(); //4 分布式锁 final CountDownLatch countdown = new CountDownLatch(1); for(int i = 0; i &lt; 10; i++)&#123; new Thread(new Runnable() &#123; @Override public void run() &#123; InterProcessMutex lock = new InterProcessMutex(cf, "/super"); try &#123; countdown.await(); //加锁 lock.acquire(); //-------------业务处理开始 genarNo(); SimpleDateFormat sdf = new SimpleDateFormat("HH:mm:ss|SSS"); Thread.sleep(500); System.out.println(Thread.currentThread().getName()+"执行此操作"); //-------------业务处理结束 &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; //释放 lock.release(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;,"t" + i).start(); &#125; Thread.sleep(100); countdown.countDown(); &#125;&#125;``` 输出 9t9执行此操作8t8执行此操作7t3执行此操作6t0执行此操作5t6执行此操作4t7执行此操作3t2执行此操作2t5执行此操作1t4执行此操作0t1执行此操作123456789101112131415161718192021222324252627282930313233343536373839404142434445我们可以看到，这里new了10个线程，但是每个线程里面都有各自的锁，按照道理来说，他们各部干扰，但是从结果可以看出来，这个程序还是同步的，也实现了锁的原理。（相当于不同的程序放在不同的机器上，也有类似的效果）。 ## 分布式计数器功能 一说到分布式计数器，你可能脑海里想到了AtomicInteger这种经典的方式，如果针对一个jvm的场景当然没有问题，但是我们现在是在分布式场景下，就需要利用Curator框架的DistributedAtomicInteger了 代码 ```javapublic class CuratorAtomicInteger &#123; /** zookeeper地址 */ static final String CONNECT_ADDR = &quot;192.168.1.31:2181,192.168.1.32:2181&quot;; /** session超时时间 */ static final int SESSION_OUTTIME = 5000;//ms public static void main(String[] args) throws Exception &#123; //1 重试策略：初试时间为1s 重试10次 RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 10); //2 通过工厂创建连接 CuratorFramework cf = CuratorFrameworkFactory.builder() .connectString(CONNECT_ADDR) .sessionTimeoutMs(SESSION_OUTTIME) .retryPolicy(retryPolicy) .build(); //3 开启连接 cf.start(); //cf.delete().forPath(&quot;/super&quot;); //4 使用DistributedAtomicInteger DistributedAtomicInteger atomicIntger = new DistributedAtomicInteger(cf, &quot;/super&quot;, new RetryNTimes(3, 1000)); //atomicIntger.forceSet(0); //第一次需要有吧？ AtomicValue&lt;Integer&gt; value = atomicIntger.add(1);// atomicIntger.increment();// AtomicValue&lt;Integer&gt; value = atomicIntger.get(); System.out.println(value.succeeded()); System.out.println(value.postValue()); //最新值 System.out.println(value.preValue()); //原始值 &#125;&#125; 第一次运行123true10 第二次运行123true21 第三次运行123true32 其实这也就模拟了分布式的计数功能 barrier功能有这样的场景，多个程序在不同的机器中，需要等待同时都准备好了，再一起运行有两种方式，一种是等所有的都准备好在一起跑，一种是有一个开关，这个开关打开就跑，直接看代码方式一： 估计不常用1234567891011121314151617181920212223242526272829303132333435363738394041public class CuratorBarrier1 &#123; /** zookeeper地址 */ static final String CONNECT_ADDR = "192.168.1.31:2181,192.168.1.32:2181"; /** session超时时间 */ static final int SESSION_OUTTIME = 5000;//ms public static void main(String[] args) throws Exception &#123; for(int i = 0; i &lt; 5; i++)&#123; new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 10); CuratorFramework cf = CuratorFrameworkFactory.builder() .connectString(CONNECT_ADDR) .retryPolicy(retryPolicy) .build(); cf.start(); DistributedDoubleBarrier barrier = new DistributedDoubleBarrier(cf, "/super", 5); Thread.sleep(1000 * (new Random()).nextInt(3)); System.out.println(Thread.currentThread().getName() + "已经准备"); barrier.enter(); System.out.println("同时开始运行..."); Thread.sleep(1000 * (new Random()).nextInt(3)); System.out.println(Thread.currentThread().getName() + "运行完毕"); barrier.leave(); System.out.println("同时退出运行..."); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;,"t" + i).start(); &#125; &#125;&#125; 运行结果1234567891011121314151617181920t4已经准备t2已经准备t0已经准备t1已经准备t3已经准备同时开始运行...同时开始运行...t2运行完毕同时开始运行...t4运行完毕同时开始运行...t0运行完毕同时开始运行...t3运行完毕t1运行完毕同时退出运行...同时退出运行...同时退出运行...同时退出运行...同时退出运行... 方式二： 可能用的多一些，切近实际一些代码如下1234567891011121314151617181920212223242526272829303132333435363738394041public class CuratorBarrier2 &#123; /** zookeeper地址 */ static final String CONNECT_ADDR = "192.168.1.31:2181,192.168.1.32:2181"; /** session超时时间 */ static final int SESSION_OUTTIME = 50000;//ms static DistributedBarrier barrier = null; public static void main(String[] args) throws Exception &#123; for(int i = 0; i &lt; 5; i++)&#123; new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 10); CuratorFramework cf = CuratorFrameworkFactory.builder() .connectString(CONNECT_ADDR) .sessionTimeoutMs(SESSION_OUTTIME) .retryPolicy(retryPolicy) .build(); cf.start(); barrier = new DistributedBarrier(cf, "/super"); System.out.println(Thread.currentThread().getName() + "设置barrier!"); barrier.setBarrier(); //设置 barrier.waitOnBarrier(); //等待 System.out.println("---------开始执行程序----------"); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;,"t" + i).start(); &#125; Thread.sleep(5000); barrier.removeBarrier(); //释放 &#125;&#125; 运行结果123456t0设置barrier!t4设置barrier!t2设置barrier!t3设置barrier!t1设置barrier!---------开始执行程序---------- 集群的功能管理配置等注意，这个之后订阅后，宕机后再次打开，也会接受节点变更的信号。 我估计是由于缓存？ 不太明白，具体再看了。 TODO 本文大多来自于笔记，好记性不如烂笔头，烂笔头这年头比不上云笔记了]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper的zkclient的使用简介]]></title>
    <url>%2F%E6%9E%B6%E6%9E%84%2Fzookeeper8.html</url>
    <content type="text"><![CDATA[github： https://github.com/zhaikaishun/zookeeper_tutorial 前言Zookeeper的原生API，就之前的那一些，用起来还是比较麻烦的，所以，有些工程师对原生的API接口进行了封装，简化了ZK的复杂性。 创建客户端的方法： ZKClient(Arguments) 参数1：zkServer zookeeper服务器的地址，用”,”分割 参数2：sessionTimeout超时回话，为毫秒，默认是30000ms 参数3：connectionTimeOut 连接超时会话 参数4：IZKConnection接口的实现类 参数5：zkSerializer 兹定于序列化实现 需要引入zkclient的jar包，自行百度即可 ZKClient的基础操作ZKClient建立连接直接new即可12String CONNECT_ADDR = &quot;192.168.1.171:2181,192.168.1.172:2181,192.168.1.173:2181&quot;; ZkClient zkc = new ZkClient(new ZkConnection(CONNECT_ADDR), 10000); ZKClient创建节点方法1 和原生的差不多1zkc.create(final String path, Object data, final CreateMode mode) 方法2123zkc.createEphemeral("/temp");// 可以支持递归的创建，但是不能递归赋值zkc.createPersistent("/super/c1", true); 删除节点的操作支持递归的删除1234删除 /temp节点zkc.delete("/temp");//递归删除/superzkc.deleteRecursive("/super"); 获取子节点和阅读节点内容操作12345678910List&lt;String&gt; list = zkc.getChildren("/super");for(String p : list)&#123; System.out.println(p); String rp = "/super/" + p; String data = zkc.readData(rp); System.out.println("节点为：" + rp + "，内容为: " + data);&#125;``` ## 更新节点 zkc.writeData(“/super/c1”, “新内容”);12345678910111213141516171819202122232425262728293031323334353637383940414243## 一个简单的例子 代码在 bjsxt.zkclient.base```javapublic class ZkClientBase &#123; /** zookeeper地址 */ static final String CONNECT_ADDR = &quot;192.168.1.31:2181,192.168.1.32:2181,192.168.1.33:2181&quot;; /** session超时时间 */ static final int SESSION_OUTTIME = 10000;//ms public static void main(String[] args) throws Exception &#123; ZkClient zkc = new ZkClient(new ZkConnection(CONNECT_ADDR), SESSION_OUTTIME); //1. create and delete方法 zkc.createEphemeral(&quot;/temp&quot;); zkc.createPersistent(&quot;/super/c1&quot;, true); Thread.sleep(10000); zkc.delete(&quot;/temp&quot;); zkc.deleteRecursive(&quot;/super&quot;); //2. 设置path和data 并且读取子节点和每个节点的内容 zkc.createPersistent(&quot;/super&quot;, &quot;1234&quot;); zkc.createPersistent(&quot;/super/c1&quot;, &quot;c1内容&quot;); zkc.createPersistent(&quot;/super/c2&quot;, &quot;c2内容&quot;); List&lt;String&gt; list = zkc.getChildren(&quot;/super&quot;); for(String p : list)&#123; System.out.println(p); String rp = &quot;/super/&quot; + p; String data = zkc.readData(rp); System.out.println(&quot;节点为：&quot; + rp + &quot;，内容为: &quot; + data); &#125; //3. 更新和判断节点是否存在 zkc.writeData(&quot;/super/c1&quot;, &quot;新内容&quot;); System.out.println(zkc.readData(&quot;/super/c1&quot;).toString()); System.out.println(zkc.exists(&quot;/super/c1&quot;)); // 4.递归删除/super内容 zkc.deleteRecursive(&quot;/super&quot;); &#125;&#125; 输出123456c1节点为：/super/c1，内容为: c1内容c2节点为：/super/c2，内容为: c2内容新内容true ZKClient Watch的使用我们发现，上述ZKClient里面并没有类似的watcher、watch参数，这也就是说我们开发人员无需关心反复注册watcher的问题，zkclient给我们提供了一套监听方式，我们可以使用监听节点的方式进行操作，剔除了繁琐的反复watcher操作、简化了代码的复杂程度 subscribeChildChanges方法 订阅子节点变化参数1：path路径参数2：实现了IZKChildListener接口的类（如：实例化IZKClientListener类） 只需要重写其handleChildChanges(String parentPath, List currentChild)方法。其中 parentPath 为监听节点全路径 currentChilds为新的子节点列表IZKChildListener事件说明针对于下面三个事件触发： 新增子节点 减少子节点 删除节点注意： 不监听节点内容的变化举例 123456789101112131415161718192021222324252627282930313233343536373839public class ZkClientWatcher1 &#123; /** zookeeper地址 */ static final String CONNECT_ADDR = "192.168.1.31:2181,192.168.1.32:2181,192.168.1.33:2181"; /** session超时时间 */ static final int SESSION_OUTTIME = 10000;//ms public static void main(String[] args) throws Exception &#123; ZkClient zkc = new ZkClient(new ZkConnection(CONNECT_ADDR), SESSION_OUTTIME); //对父节点添加监听子节点变化。 zkc.subscribeChildChanges("/super", new IZkChildListener() &#123; @Override public void handleChildChange(String parentPath, List&lt;String&gt; currentChilds) throws Exception &#123; System.out.println("parentPath: " + parentPath); System.out.println("currentChilds: " + currentChilds); &#125; &#125;); Thread.sleep(3000); zkc.createPersistent("/super"); Thread.sleep(1000); zkc.createPersistent("/super" + "/" + "c1", "c1内容"); Thread.sleep(1000); zkc.createPersistent("/super" + "/" + "c2", "c2内容"); Thread.sleep(1000); zkc.delete("/super/c2"); Thread.sleep(1000); zkc.deleteRecursive("/super"); Thread.sleep(Integer.MAX_VALUE); &#125;&#125; 输出123456789101112parentPath: /supercurrentChilds: []parentPath: /supercurrentChilds: [c1]parentPath: /supercurrentChilds: [c1, c2]parentPath: /supercurrentChilds: [c1]parentPath: /supercurrentChilds: nullparentPath: /supercurrentChilds: null subscribeDataChanges 订阅内容变化和前面的subscribeChildChanges类似 参数1 路径 参数2 IZkDataListener对象，重写handleDataDeleted(String path) 方法，可以得到删除节点的path 重写handleDataChange(String path, Object data)可以得到变更的节点和变更的内容 举例12345678910111213141516171819202122232425262728293031323334public class ZkClientWatcher2 &#123; /** zookeeper地址 */ static final String CONNECT_ADDR = "192.168.1.31:2181,192.168.1.32:2181,192.168.1.33:2181"; /** session超时时间 */ static final int SESSION_OUTTIME = 10000;//ms public static void main(String[] args) throws Exception &#123; ZkClient zkc = new ZkClient(new ZkConnection(CONNECT_ADDR), SESSION_OUTTIME); zkc.createPersistent("/super", "1234"); //对父节点添加监听子节点变化。 zkc.subscribeDataChanges("/super", new IZkDataListener() &#123; @Override public void handleDataDeleted(String path) throws Exception &#123; System.out.println("删除的节点为:" + path); &#125; @Override public void handleDataChange(String path, Object data) throws Exception &#123; System.out.println("变更的节点为:" + path + ", 变更内容为:" + data); &#125; &#125;); Thread.sleep(3000); zkc.writeData("/super", "456", -1); Thread.sleep(1000); zkc.delete("/super"); Thread.sleep(Integer.MAX_VALUE); &#125;&#125; 输出12变更的节点为:/super, 变更内容为:456删除的节点为:/super 前言Zookeeper的原生API，就之前的那一些，用起来还是比较麻烦的，所以，有些工程师对原生的API接口进行了封装，简化了ZK的复杂性。 创建客户端的方法： ZKClient(Arguments) 参数1：zkServer zookeeper服务器的地址，用”,”分割 参数2：sessionTimeout超时回话，为毫秒，默认是30000ms 参数3：connectionTimeOut 连接超时会话 参数4：IZKConnection接口的实现类 参数5：zkSerializer 兹定于序列化实现 需要引入zkclient的jar包，自行百度即可 ZKClient的基础操作ZKClient建立连接直接new即可12String CONNECT_ADDR = &quot;192.168.1.171:2181,192.168.1.172:2181,192.168.1.173:2181&quot;; ZkClient zkc = new ZkClient(new ZkConnection(CONNECT_ADDR), 10000); ZKClient创建节点方法1 和原生的差不多1zkc.create(final String path, Object data, final CreateMode mode) 方法2123zkc.createEphemeral("/temp");// 可以支持递归的创建，但是不能递归赋值zkc.createPersistent("/super/c1", true); 删除节点的操作支持递归的删除1234删除 /temp节点zkc.delete("/temp");//递归删除/superzkc.deleteRecursive("/super"); 获取子节点和阅读节点内容操作12345678910List&lt;String&gt; list = zkc.getChildren("/super");for(String p : list)&#123; System.out.println(p); String rp = "/super/" + p; String data = zkc.readData(rp); System.out.println("节点为：" + rp + "，内容为: " + data);&#125;``` ## 更新节点 zkc.writeData(“/super/c1”, “新内容”);12345678910111213141516171819202122232425262728293031323334353637383940414243## 一个简单的例子 代码在 bjsxt.zkclient.base```javapublic class ZkClientBase &#123; /** zookeeper地址 */ static final String CONNECT_ADDR = &quot;192.168.1.31:2181,192.168.1.32:2181,192.168.1.33:2181&quot;; /** session超时时间 */ static final int SESSION_OUTTIME = 10000;//ms public static void main(String[] args) throws Exception &#123; ZkClient zkc = new ZkClient(new ZkConnection(CONNECT_ADDR), SESSION_OUTTIME); //1. create and delete方法 zkc.createEphemeral(&quot;/temp&quot;); zkc.createPersistent(&quot;/super/c1&quot;, true); Thread.sleep(10000); zkc.delete(&quot;/temp&quot;); zkc.deleteRecursive(&quot;/super&quot;); //2. 设置path和data 并且读取子节点和每个节点的内容 zkc.createPersistent(&quot;/super&quot;, &quot;1234&quot;); zkc.createPersistent(&quot;/super/c1&quot;, &quot;c1内容&quot;); zkc.createPersistent(&quot;/super/c2&quot;, &quot;c2内容&quot;); List&lt;String&gt; list = zkc.getChildren(&quot;/super&quot;); for(String p : list)&#123; System.out.println(p); String rp = &quot;/super/&quot; + p; String data = zkc.readData(rp); System.out.println(&quot;节点为：&quot; + rp + &quot;，内容为: &quot; + data); &#125; //3. 更新和判断节点是否存在 zkc.writeData(&quot;/super/c1&quot;, &quot;新内容&quot;); System.out.println(zkc.readData(&quot;/super/c1&quot;).toString()); System.out.println(zkc.exists(&quot;/super/c1&quot;)); // 4.递归删除/super内容 zkc.deleteRecursive(&quot;/super&quot;); &#125;&#125; 输出123456c1节点为：/super/c1，内容为: c1内容c2节点为：/super/c2，内容为: c2内容新内容true ZKClient Watch的使用我们发现，上述ZKClient里面并没有类似的watcher、watch参数，这也就是说我们开发人员无需关心反复注册watcher的问题，zkclient给我们提供了一套监听方式，我们可以使用监听节点的方式进行操作，剔除了繁琐的反复watcher操作、简化了代码的复杂程度 subscribeChildChanges方法 订阅子节点变化参数1：path路径参数2：实现了IZKChildListener接口的类（如：实例化IZKClientListener类） 只需要重写其handleChildChanges(String parentPath, List currentChild)方法。其中 parentPath 为监听节点全路径 currentChilds为新的子节点列表IZKChildListener事件说明针对于下面三个事件触发： 新增子节点 减少子节点 删除节点注意： 不监听节点内容的变化举例 123456789101112131415161718192021222324252627282930313233343536373839public class ZkClientWatcher1 &#123; /** zookeeper地址 */ static final String CONNECT_ADDR = "192.168.1.31:2181,192.168.1.32:2181,192.168.1.33:2181"; /** session超时时间 */ static final int SESSION_OUTTIME = 10000;//ms public static void main(String[] args) throws Exception &#123; ZkClient zkc = new ZkClient(new ZkConnection(CONNECT_ADDR), SESSION_OUTTIME); //对父节点添加监听子节点变化。 zkc.subscribeChildChanges("/super", new IZkChildListener() &#123; @Override public void handleChildChange(String parentPath, List&lt;String&gt; currentChilds) throws Exception &#123; System.out.println("parentPath: " + parentPath); System.out.println("currentChilds: " + currentChilds); &#125; &#125;); Thread.sleep(3000); zkc.createPersistent("/super"); Thread.sleep(1000); zkc.createPersistent("/super" + "/" + "c1", "c1内容"); Thread.sleep(1000); zkc.createPersistent("/super" + "/" + "c2", "c2内容"); Thread.sleep(1000); zkc.delete("/super/c2"); Thread.sleep(1000); zkc.deleteRecursive("/super"); Thread.sleep(Integer.MAX_VALUE); &#125;&#125; 输出123456789101112parentPath: /supercurrentChilds: []parentPath: /supercurrentChilds: [c1]parentPath: /supercurrentChilds: [c1, c2]parentPath: /supercurrentChilds: [c1]parentPath: /supercurrentChilds: nullparentPath: /supercurrentChilds: null subscribeDataChanges 订阅内容变化和前面的subscribeChildChanges类似 参数1 路径 参数2 IZkDataListener对象，重写handleDataDeleted(String path) 方法，可以得到删除节点的path 重写handleDataChange(String path, Object data)可以得到变更的节点和变更的内容 举例12345678910111213141516171819202122232425262728293031323334public class ZkClientWatcher2 &#123; /** zookeeper地址 */ static final String CONNECT_ADDR = "192.168.1.31:2181,192.168.1.32:2181,192.168.1.33:2181"; /** session超时时间 */ static final int SESSION_OUTTIME = 10000;//ms public static void main(String[] args) throws Exception &#123; ZkClient zkc = new ZkClient(new ZkConnection(CONNECT_ADDR), SESSION_OUTTIME); zkc.createPersistent("/super", "1234"); //对父节点添加监听子节点变化。 zkc.subscribeDataChanges("/super", new IZkDataListener() &#123; @Override public void handleDataDeleted(String path) throws Exception &#123; System.out.println("删除的节点为:" + path); &#125; @Override public void handleDataChange(String path, Object data) throws Exception &#123; System.out.println("变更的节点为:" + path + ", 变更内容为:" + data); &#125; &#125;); Thread.sleep(3000); zkc.writeData("/super", "456", -1); Thread.sleep(1000); zkc.delete("/super"); Thread.sleep(Integer.MAX_VALUE); &#125;&#125; 输出12变更的节点为:/super, 变更内容为:456删除的节点为:/super 特别感谢互联网架构师白鹤翔老师，本文大多出自他的视频讲解。笔者主要是记录笔记，以便之后翻阅，正所谓好记性不如烂笔头，烂笔头不如云笔记]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper的ACL]]></title>
    <url>%2F%E6%9E%B6%E6%9E%84%2Fzookeeper7.html</url>
    <content type="text"><![CDATA[什么是ACLACL 叫做Access Control List，ACL（访问控制列表）,例如linux中的文件系统中就有ACL，传统的文件系统中，ACL分为两个维度，一个是属组，一个是权限。 子目录/文件默认继承父目录的ACL。而在Zookeeper中，node的ACL是没有继承关系的，是独立控制的。Zookeeper的ACL，可以从三个维度来理解：一是scheme; 二是user; 三是permission。 为什么zookeeper也要有ACLzookeeper作为一个分布式协调框架，其内部存储的都是一些关乎分布式系统运行时状态的元数据，尤其是设计到分布式锁、Master选举和协调等应用场景。我们需要有效地保障Zookeeper中的数据安全，Zookeeper提供一套完善的ACL权限控制机制来保障数据的安全。ZK提供了三种模式。权限模式、授权对象、权限。 授权模式：scheme world: 它下面只有一个id, 叫anyone, world:anyone代表任何人，zookeeper中对所有人有权限的结点就是属于world:anyone的 auth: 它不需要id, 只要是通过authentication的user都有权限（zookeeper支持通过kerberos来进行authencation, 也支持username/password形式的authentication) digest: 它对应的id为username:BASE64(SHA1(password))，它需要先通过username:password形式的authentication ip: 它对应的id为客户机的IP地址，设置的时候可以设置一个ip段，比如ip:192.168.1.0/16, 表示匹配前16个bit的IP段 super: 在这种scheme情况下，对应的id拥有超级权限，可以做任何事情(cdrwa) 具体代码可以参考bjsxt.zookeeper.auth包下的内容，或者网上查看相关的内容, 网上的确实比较详细，博主抛砖引玉了。 github: https://github.com/zhaikaishun/zookeeper_tutorial]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper分布式锁思路（无代码）]]></title>
    <url>%2F%E6%9E%B6%E6%9E%84%2Fzookeeper6.html</url>
    <content type="text"><![CDATA[什么是分布式锁想必大家肯定很熟悉多线程之间的锁，他们属于同一个jvm，才能实现资源的共享，保证数据一致性，但是，如果在分布式的机器中，我们如何保证数据的一致性呢，这里就需要用到分布式锁的问题。 分布式读写数据库的分布式锁假设有以下场景。 一个WEB应用，部署到多台服务器中，其中通过nginx等一些手段进行负载均衡，每个用户的请求数据，都会负载均衡的由多台服务器处理。 如果多台服务器同时对数据库进行修改的时候，如何保证数据的一致性呢？ 上面的例子我们可以看到，肯定数据库也做了集群模式。例如主从复制，当某个服务器的数据库实例进行修改之后，会update到其余的多台服务器中，保证数据的一致性。但是我们可能会出现这样一种情况。服务器A收到一条数据库请求，需要对user表的某条数据的年龄字段进行+1操作，服务器B也收到一条数据库请求，也需要对这条数据进行年龄+1的操作在，这个时候，假如之前的年龄是10，服务器A加1，变成了11，服务器B在A对数据库写之前在本地也得到了年龄为10，然后服务器B操作后，数据还是11，导致了数据的不一致问题。因为我们想要的却是年龄应该变成12。这时候我们应该有这样的想法，服务器A在操作这个年龄更新的时候，需要加一把锁，让其他机器不能操作，当A操作完后，数据库内部进行数据同步之后，其他的服务器才能进行这个年龄更新的操作。这个当然可以使用zookeeper来实现分布式锁的功能。简单思考一下解决方案 对某个ID的数据进行操作的时候，在zookeeper上，建立一个znode，名称为这个ID 。 服务器需要操作某条数据的时候，先去zookeeper get一下这个ID，看看能不能get到，如果get不到才能进行数据的操作，负责就等待，直到这个能get到为止 上面说的比较浅显，但是大概的思路就是这样，利用一个第三方的工具，来判断某个数据是否正在被操作，从而实现分布式的功能。]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper的watch（原生API）]]></title>
    <url>%2F%E6%9E%B6%E6%9E%84%2Fzookeeper5.html</url>
    <content type="text"><![CDATA[github: https://github.com/zhaikaishun/zookeeper_tutorial Zookeeper的watcher事件zookeeper有watch事件，是一次性触发的，当watch监视的数据发生变化时，通知设置了该watch的client，即watcher。同样，其watcher是监听数据发生了某些变化，那就一定会有对应的事件类型，和状态类型。事件类型（znode节点相关的） EventType.NodeCreated EventType.NodeDataChanged EventType.NodeChildrenChanged EventType.NodeDeleted状态类型：（是跟客户端实例相关的,简单的说就是客户端和服务器端连接状态相关的） KeeperState.Disconnected KeeperState.SyncConnected KeeperState.AuthFailed 认证失败 KeeperState.Expired 过期 watcher和watch简单的说，一个节点上的某个程序监控某个节点，那么这个节点上的这个程序就是一个watcher，而监听的这个事件（动作），就是一个watch。watch事件，是一次性触发的，只能监听一次，第二次对此节点的修改就监听不到了，如果想一直监听，大概有两种方案，一种是在出发事件后执行方法的时候有个watch的参数再设置为true，一种是这个时候再创建一个watch，这种是有点麻烦。看一个例子吧：最好是去github上下载下来自己运行一下。 设置watcher, 我这里每次create的时候，都设置了一下watcher，例如下面代码中 this.zk.exists(path, ifsetTrue); 需要实现implements Watcher接口以及重写实现方法process，我这里监听比较简单。也就不多说了。 反正是要注意，如果只设置一次监听，那么监听完之后，第二次就监听不到了。若需要多次监听，那么最好是再监听一次123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306/** * Zookeeper Wathcher * 本类就是一个Watcher类（实现了org.apache.zookeeper.Watcher类） * @author（alienware） * @since 2015-6-14 */public class ZooKeeperWatcher implements Watcher &#123; /** 定义原子变量 */ AtomicInteger seq = new AtomicInteger(); /** 定义session失效时间 */ private static final int SESSION_TIMEOUT = 10000; /** zookeeper服务器地址 */ private static final String CONNECTION_ADDR = &quot;192.168.1.31:2181&quot;; /** zk父路径设置 */ private static final String PARENT_PATH = &quot;/testWatch&quot;; /** zk子路径设置 */ private static final String CHILDREN_PATH = &quot;/testWatch/children&quot;; /** 进入标识 */ private static final String LOG_PREFIX_OF_MAIN = &quot;【Main】&quot;; /** zk变量 */ private ZooKeeper zk = null; /** 信号量设置，用于等待zookeeper连接建立之后 通知阻塞程序继续向下执行 */ private CountDownLatch connectedSemaphore = new CountDownLatch(1); /** * 创建ZK连接 * @param connectAddr ZK服务器地址列表 * @param sessionTimeout Session超时时间 */ public void createConnection(String connectAddr, int sessionTimeout) &#123; this.releaseConnection(); try &#123; zk = new ZooKeeper(connectAddr, sessionTimeout, this); System.out.println(LOG_PREFIX_OF_MAIN + &quot;开始连接ZK服务器&quot;); connectedSemaphore.await(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; /** * 关闭ZK连接 */ public void releaseConnection() &#123; if (this.zk != null) &#123; try &#123; this.zk.close(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; /** * 创建节点 * @param path 节点路径 * @param data 数据内容 * @return */ public boolean createPath(String path, String data,boolean ifsetTrue) &#123; try &#123; //设置监控(由于zookeeper的监控都是一次性的所以 每次必须设置监控) this.zk.exists(path, ifsetTrue); System.out.println(LOG_PREFIX_OF_MAIN + &quot;节点创建成功, Path: &quot; + this.zk.create( /**路径*/ path, /**数据*/ data.getBytes(), /**所有可见*/ Ids.OPEN_ACL_UNSAFE, /**永久存储*/ CreateMode.PERSISTENT ) + &quot;, content: &quot; + data); &#125; catch (Exception e) &#123; e.printStackTrace(); return false; &#125; return true; &#125; /** * 读取指定节点数据内容 * @param path 节点路径 * @return */ public String readData(String path, boolean needWatch) &#123; try &#123; return new String(this.zk.getData(path, needWatch, null)); &#125; catch (Exception e) &#123; e.printStackTrace(); return &quot;&quot;; &#125; &#125; /** * 更新指定节点数据内容 * @param path 节点路径 * @param data 数据内容 * @return */ public boolean writeData(String path, String data) &#123; try &#123; System.out.println(LOG_PREFIX_OF_MAIN + &quot;更新数据成功，path：&quot; + path + &quot;, stat: &quot; + this.zk.setData(path, data.getBytes(), -1)); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return false; &#125; /** * 删除指定节点 * * @param path * 节点path */ public void deleteNode(String path) &#123; try &#123; this.zk.delete(path, -1); System.out.println(LOG_PREFIX_OF_MAIN + &quot;删除节点成功，path：&quot; + path); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; /** * 判断指定节点是否存在 * @param path 节点路径 */ public Stat exists(String path, boolean needWatch) &#123; try &#123; return this.zk.exists(path, needWatch); &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125; &#125; /** * 获取子节点 * @param path 节点路径 */ private List&lt;String&gt; getChildren(String path, boolean needWatch) &#123; try &#123; return this.zk.getChildren(path, needWatch); &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125; &#125; /** * 删除所有节点 */ public void deleteAllTestPath() &#123; if(this.exists(CHILDREN_PATH, false) != null)&#123; this.deleteNode(CHILDREN_PATH); &#125; if(this.exists(PARENT_PATH, false) != null)&#123; this.deleteNode(PARENT_PATH); &#125; &#125; /** * 收到来自Server的Watcher通知后的处理。 */ @Override public void process(WatchedEvent event) &#123; System.out.println(&quot;进入 process 。。。。。event = &quot; + event); try &#123; Thread.sleep(200); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; if (event == null) &#123; return; &#125; // 连接状态 KeeperState keeperState = event.getState(); // 事件类型 EventType eventType = event.getType(); // 受影响的path String path = event.getPath(); String logPrefix = &quot;【Watcher-&quot; + this.seq.incrementAndGet() + &quot;】&quot;; System.out.println(logPrefix + &quot;收到Watcher通知&quot;); System.out.println(logPrefix + &quot;连接状态:\t&quot; + keeperState.toString()); System.out.println(logPrefix + &quot;事件类型:\t&quot; + eventType.toString()); if (KeeperState.SyncConnected == keeperState) &#123; // 成功连接上ZK服务器 if (EventType.None == eventType) &#123; System.out.println(logPrefix + &quot;成功连接上ZK服务器&quot;); connectedSemaphore.countDown(); &#125; //创建节点 else if (EventType.NodeCreated == eventType) &#123; System.out.println(logPrefix + &quot;节点创建&quot;); try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; this.exists(path, true); &#125; //更新节点 else if (EventType.NodeDataChanged == eventType) &#123; System.out.println(logPrefix + &quot;节点数据更新&quot;); System.out.println(&quot;我看看走不走这里........&quot;); try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(logPrefix + &quot;数据内容: &quot; + this.readData(PARENT_PATH, true)); &#125; //更新子节点 else if (EventType.NodeChildrenChanged == eventType) &#123; System.out.println(logPrefix + &quot;子节点变更&quot;); try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(logPrefix + &quot;子节点列表：&quot; + this.getChildren(PARENT_PATH, true)); &#125; //删除节点 else if (EventType.NodeDeleted == eventType) &#123; System.out.println(logPrefix + &quot;节点 &quot; + path + &quot; 被删除&quot;); &#125; else ; &#125; else if (KeeperState.Disconnected == keeperState) &#123; System.out.println(logPrefix + &quot;与ZK服务器断开连接&quot;); &#125; else if (KeeperState.AuthFailed == keeperState) &#123; System.out.println(logPrefix + &quot;权限检查失败&quot;); &#125; else if (KeeperState.Expired == keeperState) &#123; System.out.println(logPrefix + &quot;会话失效&quot;); &#125; else ; System.out.println(&quot;--------------------------------------------&quot;); &#125; /** * &lt;B&gt;方法名称：&lt;/B&gt;测试zookeeper监控&lt;BR&gt; * &lt;B&gt;概要说明：&lt;/B&gt;主要测试watch功能&lt;BR&gt; * @param args * @throws Exception */ public static void main(String[] args) throws Exception &#123; //建立watcher ZooKeeperWatcher zkWatch = new ZooKeeperWatcher(); //创建连接 zkWatch.createConnection(CONNECTION_ADDR, SESSION_TIMEOUT); //System.out.println(zkWatch.zk.toString()); Thread.sleep(1000); // 清理节点 zkWatch.deleteAllTestPath(); if (zkWatch.createPath(PARENT_PATH, System.currentTimeMillis() + &quot;&quot;,true)) &#123; Thread.sleep(1000); // 读取数据 System.out.println(&quot;---------------------- read parent ----------------------------&quot;); //zkWatch.readData(PARENT_PATH, true); // 读取子节点 System.out.println(&quot;---------------------- read children path ----------------------------&quot;); zkWatch.getChildren(PARENT_PATH, true); // 更新数据 zkWatch.writeData(PARENT_PATH, System.currentTimeMillis() + &quot;&quot;); Thread.sleep(1000); // 创建子节点 zkWatch.createPath(CHILDREN_PATH, System.currentTimeMillis() + &quot;&quot;,true); Thread.sleep(1000); zkWatch.writeData(CHILDREN_PATH, System.currentTimeMillis() + &quot;&quot;); &#125; Thread.sleep(50000); // 清理节点 zkWatch.deleteAllTestPath(); Thread.sleep(1000); zkWatch.releaseConnection(); &#125;&#125; 输出就在这里了，想具体了解的话，自己敲一下，然后覆盖一下代码。一个一个功能的执行，查看功能即可12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879【Main】开始连接ZK服务器进入 process 。。。。。event = WatchedEvent state:SyncConnected type:None path:null【Watcher-1】收到Watcher通知【Watcher-1】连接状态: SyncConnected【Watcher-1】事件类型: None【Watcher-1】成功连接上ZK服务器--------------------------------------------进入 process 。。。。。event = WatchedEvent state:SyncConnected type:NodeCreated path:/testWatch【Main】节点创建成功, Path: /testWatch, content: 1508077399368【Watcher-2】收到Watcher通知【Watcher-2】连接状态: SyncConnected【Watcher-2】事件类型: NodeCreated【Watcher-2】节点创建------------------------------------------------------------------ read parent -------------------------------------------------- read children path ----------------------------进入 process 。。。。。event = WatchedEvent state:SyncConnected type:NodeDataChanged path:/testWatch【Main】更新数据成功，path：/testWatch, stat: 30064771078,30064771079,1508071193606,1508071194762,1,0,0,0,13,0,30064771078【Watcher-3】收到Watcher通知【Watcher-3】连接状态: SyncConnected【Watcher-3】事件类型: NodeDataChanged【Watcher-3】节点数据更新我看看走不走这里........【Watcher-3】数据内容: 1508077400538--------------------------------------------进入 process 。。。。。event = WatchedEvent state:SyncConnected type:NodeCreated path:/testWatch/children【Main】节点创建成功, Path: /testWatch/children, content: 1508077401581【Watcher-4】收到Watcher通知【Watcher-4】连接状态: SyncConnected【Watcher-4】事件类型: NodeCreated【Watcher-4】节点创建--------------------------------------------进入 process 。。。。。event = WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/testWatch【Watcher-5】收到Watcher通知【Watcher-5】连接状态: SyncConnected【Watcher-5】事件类型: NodeChildrenChanged【Watcher-5】子节点变更【Main】更新数据成功，path：/testWatch/children, stat: 30064771080,30064771081,1508071195777,1508071196784,1,0,0,0,13,0,30064771080【Watcher-5】子节点列表：[children]--------------------------------------------进入 process 。。。。。event = WatchedEvent state:SyncConnected type:NodeDataChanged path:/testWatch/children【Watcher-6】收到Watcher通知【Watcher-6】连接状态: SyncConnected【Watcher-6】事件类型: NodeDataChanged【Watcher-6】节点数据更新我看看走不走这里........【Watcher-6】数据内容: 1508077400538--------------------------------------------进入 process 。。。。。event = WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/testWatch【Main】删除节点成功，path：/testWatch/children【Main】删除节点成功，path：/testWatch【Watcher-7】收到Watcher通知【Watcher-7】连接状态: SyncConnected【Watcher-7】事件类型: NodeChildrenChanged【Watcher-7】子节点变更``` ## 实际应用一个场景 我们希望zookeeper对分布式系统的配置文件进行管理，也就是说多个服务器进行watcher,zookeeper节点发送变化，则我们实时更新配置文件。我们要完成多个应用服务器注册watcher，然后实时观察数据的变化，然后反馈给媒体服务器变更的数据信息，观察zookeeper节点下面是一个例子： 代码在bjsxt.zookeeper.cluster中 本例子模拟多台服务器同时监控一个节点。然后另外一个程序进行管理，所监控的这几台机器得到节点变更的通知。 本例中，Client1和Client2相当于两台服务器，共同watch一个节点。Test相当于管理者，用来管理这两个客户端的配置。 代码如下,具体的需要自己下载下来进行调试 Client1```javapublic class Client1 &#123; public static void main(String[] args) throws Exception&#123; ZKWatcher myWatcher = new ZKWatcher(); Thread.sleep(100000000); &#125;&#125;``` Client2 public class Client2 { public static void main(String[] args) throws Exception{ ZKWatcher myWatcher = new ZKWatcher(); Thread.sleep(100000000); } }123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129ZKWatcher```javapublic class ZKWatcher implements Watcher &#123; /** zk变量 */ private ZooKeeper zk = null; /** 父节点path */ static final String PARENT_PATH = &quot;/super&quot;; /** 信号量设置，用于等待zookeeper连接建立之后 通知阻塞程序继续向下执行 */ private CountDownLatch connectedSemaphore = new CountDownLatch(1); private List&lt;String&gt; cowaList = new CopyOnWriteArrayList&lt;String&gt;(); /** zookeeper服务器地址 */ public static final String CONNECTION_ADDR = &quot;192.168.1.31:2181,192.168.1.32:2181,192.168.1.33:2181&quot;; /** 定义session失效时间 */ public static final int SESSION_TIMEOUT = 30000; public ZKWatcher() throws Exception&#123; zk = new ZooKeeper(CONNECTION_ADDR, SESSION_TIMEOUT, this); System.out.println(&quot;开始连接ZK服务器&quot;); connectedSemaphore.await(); &#125; @Override public void process(WatchedEvent event) &#123; // 连接状态 KeeperState keeperState = event.getState(); // 事件类型 EventType eventType = event.getType(); // 受影响的path String path = event.getPath(); System.out.println(&quot;受影响的path : &quot; + path); if (KeeperState.SyncConnected == keeperState) &#123; // 成功连接上ZK服务器 if (EventType.None == eventType) &#123; System.out.println(&quot;成功连接上ZK服务器&quot;); connectedSemaphore.countDown(); try &#123; if(this.zk.exists(PARENT_PATH, false) == null)&#123; this.zk.create(PARENT_PATH, &quot;root&quot;.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); &#125; List&lt;String&gt; paths = this.zk.getChildren(PARENT_PATH, true); for (String p : paths) &#123; System.out.println(p); this.zk.exists(PARENT_PATH + &quot;/&quot; + p, true); &#125; &#125; catch (KeeperException | InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; //创建节点 else if (EventType.NodeCreated == eventType) &#123; System.out.println(&quot;节点创建&quot;); try &#123; this.zk.exists(path, true); &#125; catch (KeeperException | InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; //更新节点 else if (EventType.NodeDataChanged == eventType) &#123; System.out.println(&quot;节点数据更新&quot;); try &#123; //update nodes call function this.zk.exists(path, true); &#125; catch (KeeperException | InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; //更新子节点 else if (EventType.NodeChildrenChanged == eventType) &#123; System.out.println(&quot;子节点 ... 变更&quot;); try &#123; List&lt;String&gt; paths = this.zk.getChildren(path, true); if(paths.size() &gt;= cowaList.size())&#123; paths.removeAll(cowaList); for(String p : paths)&#123; this.zk.exists(path + &quot;/&quot; + p, true); //this.zk.getChildren(path + &quot;/&quot; + p, true); System.out.println(&quot;这个是新增的子节点 : &quot; + path + &quot;/&quot; + p); //add new nodes call function &#125; cowaList.addAll(paths); &#125; else &#123; cowaList = paths; &#125; System.out.println(&quot;cowaList: &quot; + cowaList.toString()); System.out.println(&quot;paths: &quot; + paths.toString()); &#125; catch (KeeperException | InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; //删除节点 else if (EventType.NodeDeleted == eventType) &#123; System.out.println(&quot;节点 &quot; + path + &quot; 被删除&quot;); try &#123; //delete nodes call function this.zk.exists(path, true); &#125; catch (KeeperException | InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; else ; &#125; else if (KeeperState.Disconnected == keeperState) &#123; System.out.println(&quot;与ZK服务器断开连接&quot;); &#125; else if (KeeperState.AuthFailed == keeperState) &#123; System.out.println(&quot;权限检查失败&quot;); &#125; else if (KeeperState.Expired == keeperState) &#123; System.out.println(&quot;会话失效&quot;); &#125; else ; System.out.println(&quot;--------------------------------------------&quot;); &#125; &#125; Test12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public class Test &#123; /** zookeeper地址 */ static final String CONNECT_ADDR = &quot;192.168.1.31:2181,192.168.1.32:2181,192.168.1.33:2181&quot;; /** session超时时间 */ static final int SESSION_OUTTIME = 2000;//ms /** 信号量，阻塞程序执行，用于等待zookeeper连接成功，发送成功信号 */ static final CountDownLatch connectedSemaphore = new CountDownLatch(1); public static void main(String[] args) throws Exception&#123; ZooKeeper zk = new ZooKeeper(CONNECT_ADDR, SESSION_OUTTIME, new Watcher()&#123; @Override public void process(WatchedEvent event) &#123; //获取事件的状态 KeeperState keeperState = event.getState(); EventType eventType = event.getType(); //如果是建立连接 if(KeeperState.SyncConnected == keeperState)&#123; if(EventType.None == eventType)&#123; //如果建立连接成功，则发送信号量，让后续阻塞程序向下执行 connectedSemaphore.countDown(); System.out.println(&quot;zk 建立连接&quot;); &#125; &#125; &#125; &#125;); //进行阻塞 connectedSemaphore.await(); // //创建子节点// zk.create(&quot;/super/c1&quot;, &quot;c1&quot;.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); //创建子节点// zk.create(&quot;/super/c2&quot;, &quot;c2&quot;.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); //创建子节点 String result = zk.create(&quot;/super/c3&quot;, &quot;c3&quot;.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); System.out.println(result); //创建子节点// zk.create(&quot;/super/c4&quot;, &quot;c4&quot;.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); // zk.create(&quot;/super/c4/c44&quot;, &quot;c44&quot;.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); //获取节点信息// byte[] data = zk.getData(&quot;/testRoot&quot;, false, null);// System.out.println(new String(data));// System.out.println(zk.getChildren(&quot;/testRoot&quot;, false)); //修改节点的值// zk.setData(&quot;/super/c1&quot;, &quot;modify c1&quot;.getBytes(), -1);// zk.setData(&quot;/super/c2&quot;, &quot;modify c2&quot;.getBytes(), -1);// byte[] data = zk.getData(&quot;/super/c2&quot;, false, null);// System.out.println(new String(data)); // //判断节点是否存在// System.out.println(zk.exists(&quot;/super/c3&quot;, false));// //删除节点// zk.delete(&quot;/super/c3&quot;, -1); zk.close(); &#125;&#125; 先启动Client1,client1打印123开始连接ZK服务器受影响的path : null成功连接上ZK服务器 再启动Client2,1234开始连接ZK服务器受影响的path : null成功连接上ZK服务器-------------------------------------------- 再启动Test类，此时Client1和Client2打印的内容是123456789开始连接ZK服务器受影响的path : null成功连接上ZK服务器-------------------------------------------- 受影响的path : /super子节点 ... 变更这个是新增的子节点 : /super/c3cowaList: [c3]paths: [c3] Test打印的内容是12zk 建立连接/super/c3 更多测试，需要自己来运行并且思考结果。这里只起抛砖引玉的作用]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[操作zookeeper之原生API的基本操作]]></title>
    <url>%2F%E6%9E%B6%E6%9E%84%2Fzookeeper4.html</url>
    <content type="text"><![CDATA[关键字： java原生API，创建连接，创建节点同步方式，获取节点信息，获取子节点信息，修改节点的值，判断节点是否存在，删除节点，Zookeeper创建删除等节点的异步方式java惭怍zooleeper，一种是原生API，一种是zkclient方式，一种是curator框架操作github： https://github.com/zhaikaishun/zookeeper_tutorial java原生API引入对应版本的jar包，例如我是3.4.5的zookeeper，那么引入zookeeper-3.4.5.jar简要介绍一下原生API的简单使用连接zookeeper1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950ZooKeeper zk = new ZooKeeper(String connectString, int sessionTimeout, Watcher watcher)``` - connectString 连接服务器裂变，用逗号分开，例如192.168.1.31:2181,192.168.1.32:2181,192.168.1.33:2181，当然写一个就可以了 - sessionTimeout 心跳检测时间周期（毫秒） - Watcher watcher，事件处理通知器 - 这个其实有很多个构造方法，具体的，敲一下代码看一下就一清二楚了 比如下面这个例子吧## 创建连接 ```javapublic class ZookeeperBase &#123; /** zookeeper地址 */ static final String CONNECT_ADDR = "192.168.1.31:2181,192.168.1.32:2181,192.168.1.33:2181"; /** session超时时间 */ static final int SESSION_OUTTIME = 2000;//ms /** 信号量，阻塞程序执行，用于等待zookeeper连接成功，发送成功信号 */ static final CountDownLatch connectedSemaphore = new CountDownLatch(1); public static void main(String[] args) throws Exception&#123; ZooKeeper zk = new ZooKeeper(CONNECT_ADDR, SESSION_OUTTIME, new Watcher()&#123; @Override public void process(WatchedEvent event) &#123; //获取事件的状态 KeeperState keeperState = event.getState(); EventType eventType = event.getType(); //如果是建立连接 if(KeeperState.SyncConnected == keeperState)&#123; if(EventType.None == eventType)&#123; //如果建立连接成功，则发送信号量，让后续阻塞程序向下执行 System.out.println("zk 建立连接"); connectedSemaphore.countDown(); &#125; &#125; &#125; &#125;); //进行阻塞 connectedSemaphore.await(); System.out.println("执行了"); &#125;&#125;``` 1. new 了一个Zookeeper的实例，注意内部有一个new Watcher()方法，并且重写了Watcher的process方法 2. WatchedEvent.getState()，返回一个事件的状态KeeperState，keeperState状态有很多种：例如 SyncConnected（异步连接了）AuthFailed（认证失败） ConnectedReadOnly（只读连接）Disconnected（断开连接）等 3. EventType.getType返回一个节点事件，有 None(没有任何操作) ，NodeCreated(节点创建) NodeDeleted(节点删除),NodeDataChanged(节点数据改变),NodeChildrenChanged(子节点改变) 等等的事件 4. 注意这里用了一个CountDownLatch来进行线程之间通线，由于上面的**客户端和服务器端回话的建立是一个异步过程**， 所以程序会往下执行，但是到了connectedSemaphore.await() 进行阻塞，直到connectedSemaphore.countDown();之后，阻塞被唤醒，才往下执行。 当zookeeper集群启动后，运行代码 zk 建立连接执行了12345678910111213141516## 创建节点同步方式 同步方式： - 参数1，节点路径（名称）：、nodeName （不允许递归创建节点，也就是说父节点不存在的情况下，不允许创建子节点） - 参数2，节点内容：要求类型是字节数组（也就是说，不支持序列化方式，如果要实现序列化，可以用java相关序列化框架。如Kryo框架等） - 参数3，节点权限，一般使用Ids.OPEN_ACL_UNSAFE权限即可。（这个参数一般在权限没有太高要求的场景下使用） - 参数4，节点类型：创建节点的类型：CreateMode.*。提供四种节点类型 PERSISTENT（持久节点）, PERSISTENT_SEQUENTIAL（持久顺序节点）, EPHEMERAL（临时节点）, EPHEMERAL_SEQUENTIAL（临时顺序节点） 这几种节点很重要，具体作用和案例，可以参考网上的。例如临时节点一般只在本次session中有效，经常用来做分布式锁 **创建父节点** ```java String result = zk.create(&quot;/testRoot&quot;, &quot;testRoot&quot;.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); System.out.println(result); --------输出-------- /testRoot``` 如果上面的目录存在，再去创建就会报一下错误 KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /testRoot1**创建子节点**： //创建子节点 zk.create(&quot;/testRoot/children&quot;, &quot;children data&quot;.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); 123456789如果子节点的父节点不存在，那么会报 KeeperErrorCode = NoNode for /testRoot1/children 错误 ## 获取节点信息 ```java byte[] data = zk.getData(&quot;/testRoot&quot;, false, null); System.out.println(new String(data)); 获取子节点信息获取相对路径123456789System.out.println(zk.getChildren(&quot;/testRoot&quot;, false));``` 通过相对路径加上前面的路径，然后可以一次获取子节点的信息## 修改节点的值 ```java zk.setData(&quot;/testRoot&quot;, &quot;modify data root&quot;.getBytes(), -1); byte[] data = zk.getData(&quot;/testRoot&quot;, false, null); System.out.println(new String(data)); 判断节点是否存在1zk.exists("/testRoot/children", false) 删除节点1zk.delete("/testRoot/children", -1); Zookeeper创建删除等节点的异步方式在同步参数基础上增加两个参数 参数5，注册一个异步回调函数，要实现AsynCallBack.StringCallBack接口，重写processResult(int rc,String path,Object ctx,String name)方法，当节点创建完毕后执行此方法。rc: 为服务端响应代码0表示调用成功、-4表示端口连接、-110表示指定节点存在、-112表示回话已经过期。path：接口调用时传入API的数据节点的路径参数。ctx：为调用接口传入API的ctx值。name：实际在服务器端创建节点的名称。 参数6，传递给回调函数的参数，一般为上下文（Context）信息举例： 123456789101112131415zk.delete(&quot;/node01&quot;, -1, new AsyncCallback.VoidCallback() &#123; @Override public void processResult(int rc, String path, Object ctx) &#123; try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;rc: &quot;+rc); System.out.println(&quot;path: &quot;+path); System.out.println(&quot;ctx: &quot;+ctx); &#125;&#125;,&quot;a&quot;);System.out.println(&quot;继续执行&quot;);Thread.sleep(5000); 输出1234继续执行rc: 0path: /node01ctx: a 特别感谢互联网架构师白鹤翔老师，本文大多出自他的视频讲解。笔者主要是记录笔记，以便之后翻阅，正所谓好记性不如烂笔头，烂笔头不如云笔记]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建zookeeper集群]]></title>
    <url>%2F%E6%9E%B6%E6%9E%84%2Fzookeeper3.html</url>
    <content type="text"><![CDATA[Zookeeper环境搭建 前期准备：由于Zookeeper需要先安装java 机器： 三台测试机器192.168.1.31192.168.1.32192.168.1.33 上传zookeeper的压缩包 三个节点都解压到usr/local下 1[root@kaishun local]# tar zxvf zookeeper-3.4.5.tar.gz -C /usr/local 三台节点都重命名 zookeeper-3.4.5 为zookeeper 1234[root@kaishun local]# mv zookeeper-3.4.5/ zookeeper``` 4. 修改环境变量 path末尾增加一个zookeeper的bin目录 zookeeperexport ZOOKEEPER_HOME=/usr/local/zookeeperexport PATH=$PATH:$ZOOKEEPER_HOME/bin15. 到zookeeper下修改配置文件 cd /usr/local/zookeeper/conf/mv zoo_sample.cfg zoo.cfg16. 修改conf 1.修改 dataDir=/usr/local/zookeeper/data 后面添加：123server.0=192.168.1.31:2888:3888server.1=192.168.1.32:2888:3888server.2=192.168.1.33:2888:3888 然后去创建/usr/local/zookeeper/data 这个文件夹12cd /usr/local/zookeeper/mkdir data 去data目录下创建myid文件分别每台机器对应server的值 1234vim myid 第一台机器写0，第二台机器写1 ， 第三台机器写2.然后保存退出``` 8. 关闭防火墙 chkconfig iptables off1239. 启动zookeeper 注意了，三台机器都要启动，最好是使用securtCRT批量发送命令 zkServer.sh start 1过一段时间查看是否启动成功 zkServer.sh status 显示下面说明成功：JMX enabled by defaultUsing config: /usr/local/zookeeper/bin/../conf/zoo.cfgMode: follower或者leader jps会有显示QuorumPeerMain12进入zookeeper客户端 zkCli.sh```根据命令进行操作 查找：ls / 创建并赋值 create /kaishun spark 获取 get /kaishun 设值 set/kaishun 6666以上操作可以看到zookeeper集群的数据一致性 递归删除节点 rmr /path 删除指定某个节点 delete /path/child创建节点有两种类型： 短暂(ephemeral) 持久(persistent) 上面命令操作是经常需要使用的，另外，有人做了一个客户端,可以使用界面进行操作。 下载地址链接：http://pan.baidu.com/s/1c2JCLcW 密码：ncfl 点击左上角，输入ip和端口即可连接。然后操作就不多说了， 当然使用eclipse插件也是可以的，使用命令也当然是可以的。 报错解决方案去查看zookeeper.out，什么错误基本都在里面，然后去网上查就好了，如果是首次安装免不了会出错的 zoo.cfg参数详解 tickTime：基本事件单元，以毫秒为单位。这个时间是作为 Zookeeper服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每隔 tickTime时间就会发送一个心跳。 dataDir： 存储内存中数据库快照的位置，顾名思义就是 Zookeeper保存数据的目录，默认情况下， Zookeeper将写数据的日志文件也保存在这个目录里。 clientPort： 这个端口就是客户端连接 Zookeeper 服务器的端口， Zookeeper会监听这个端口，接受客户端的访问请求。 initLimit： 这个配置项是用来配置 Zookeeper接受客户端初始化连接时最长能忍受多少个心跳时间间隔数，当已经超过 10 个心跳的时间（也就是 tickTime）长度后Zookeeper 服务器还没有收到客户端的返回信息，那么表明这个客户端连接失败。总的时间长度就是10*2000=20 秒。 syncLimit： 这个配置项标识 Leader 与 Follower之间发送消息，请求和应答时间长度，最长不能超过多少个 tickTime的时间长度，总的时间长度就是 5*2000=10 秒 server.A = B:C:D :A 表示这个是第几号服务器。B 是这个服务器的 ip 地址。C 表示的是这个服务器与集群中的 Leader服务器交换信息的端口。D 表示的是万一集群中的 Leader 服务器挂了，需要一个端口来重新进行选举，选出一个新的 Leader]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper简介]]></title>
    <url>%2F%E6%9E%B6%E6%9E%84%2Fzookeeper2.html</url>
    <content type="text"><![CDATA[zookeeper简介zookeeper主要是一个分布式服务协调框架，实现同步服务，配置维护和命名服务等分布式应用。是一个高性能的分布式数据一致性解决方案。zookeeper是一个高可用的分布式管理与协调框架，给予ZAB算法（原子消息广播协议）的实现。该框架能很好的保证分布式环境中数据的一致性。也正是由于这样的特性，使得zookeeper成为了解决分布式一致性问题的利器。顺序一致性 从客户端发起的事物请求，最终将严格按照其发起的顺序被应用的zookeeper中去原子性: 所有事物请求的处理结果在整个集群中所有机器上的应用情况是一致的。也就是说，要么整个集群所有机器都成功应用了某一事物，要么没有应用，不会出现部分机器应用了事物，而另一部分机器没有应用的情况单一视图: 无论客户端连接的是哪一个zookeeper服务器，其看到的服务器端数据模型都是一致的。可靠性: 一旦服务器成功地应用了一个事物，并完成对客户端的响应，那么该事物锁引起的服务器端状态将会被一致保留下来。除非有另外一个事物对其更改实时性: 一旦事物被成功应用，那么客户端能立即从服务器上获取变更后的新数据，zookeeper仅仅能保证在一段时间内，客户端最终一定能从服务器读取最新的数据状态。 zookeeper设计目标 目标1：简单的数据结构。zookeeper就是以简单的属性结构来进行相互协调的（也叫树形名字空间） 目标2：可以构建集群。一般zookeeper集群通常由一组机器构成，一般3–5台机器就可以组成一个zookeeper集群了。只要集群中超过半数以上的机器能正常工作，那么整个集群就能够对外提供服务。 目标3：顺序访问。对于每一个客户端的每一个请求，zookeeper都会分配一个全局唯一的递增编号，这个编号反应了所有事物操作的新后顺序，应用程序可以使用zookeeper的这个特性来实现更高层次的同步。 目标4：高性能。由于zookeeper将全量数据存储在内存中，并且直接服务于所有的非事物请求，因此尤其在读操作为主的场景下性能非常突出，在JMater压力测试下（100%度请求场景下），其结果大约在12-13W的QPS。 zookeeper的结构 ZooKeeper目录树中每一个节点对应一个Znode，这个znode是被他所在路径的唯一标识。 Znode可以有子节点目录，并且每个znode可以存储数据，注意EPHEMERAL类型的额目录节点不能有子节点目录 Znode是有版本的，每个Znode中存储的数据可以有很多歌版本，也就是一个访问路径中可以存储多份数据 znode的目录名可以可以自动编号，如App1已经存在，再创建的话，将会自动命名位App2 znode可以被监控：包括这个目录节点中存储的数据的修改，子节点目录的变化，一旦变化可以通知监控的客户端，这个是zookeeper的核心特性，Zookeeper的很多功能都是基于这个特性实现的。 Zookeeper组成ZK server根据其身份特性分为三种： leader,Follower Obderver，其中Follower和Observer又统称为Learner（学习者） Leader：负责客户端的write类型请求 Follow：负责客户端的reader类型请求，参与leader选举等。 Observer：特殊的“Follower”，其可以接受客户端reader请求，但不能选举（扩容系统支撑能力，提高了读取速度。因为他不接受任何同步的写入请求，只负责与leader同步数据）其实，当我们使用java代码操作Zookeeper，那么我们就称我们的这个程序是Observer 典型应用场景zookeeper从设计模式的角度来看，是一个基于观察者模式设计的分布式服务管理框架，他负责存储和管理大家都关心的数据，然后接受观察者的注册，一旦这些数据的状态发生变化，Zookeeper就将负责同志已经在Zookeeper上注册的那些观察者做出相应的反应，从而实现集群中类似Master/Slave的管理模式 管理配置比如分布式应用环境中机器的配置列表，运行时的开关配置，数据库配置信息等，这些配置信息通常具有以下3个特性 数据量比较小 数据内容在运行时动态发生变化。 集群中各个节点共享信息，配置一致 集群管理Zookeeper不仅能帮你维护当前机器中的服务状态，而且能帮你选出一个“总管”来管理集群，这就是Zookeeper的另一个功能，Leader，并且实现集群容错功能。 希望知道当前集群中究竟有多少台机器工作 对集群中每天集群的运行时状态进行数据收集 对集群中每台集群进行上下线操作 发布与订阅Zookeeper时一个典型的分布/订阅模式的分布式管理与协调框架，开发人员可以使用它来进行分布式数据的发布与订阅 数据库切换 分布式日志的收集我们可以做一个日志系统手机集群中所有的日志信息，进行统一管理 分布式锁，队列管理等等zookeeper的特性就是在分布式场景下高可用，但是原生的API实现分布式功能非常困难，团队去实现也太浪费时间，即使实现了也未必稳定，那么可以采用第三方的客户端的完美实现，比如Curator框架，他是Apache的顶级项目。 Zookeeper开源框架应用zookeeper使用场景非常广泛：如Hadoop、Storm、消息中间件、RPC服务框架、数据库增量订阅与消费组件（如Mysql Binlog）、分布式数据库同步系统，淘宝的Otter等。所以，我们很有必要学好zookeeper!]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈分布式服务协调技术 Zookeeper]]></title>
    <url>%2F%E6%9E%B6%E6%9E%84%2Fzookeeper1.html</url>
    <content type="text"><![CDATA[Google的三篇论文影响了很多很多人，也影响了很多很多系统。这三篇论文一直是分布式领域传阅的经典。根据MapReduce，于是我们有了Hadoop；根据GFS，于是我们有了HDFS；根据BigTable，于是我们有了HBase。而在这三篇论文里都提及Google的一个Lock Service —— Chubby，哦，于是我们有了Zookeeper。 随着大数据的火热，Hxx们已经变得耳熟能详，现在作为一个开发人员如果都不知道这几个名词出门都好像不好意思跟人打招呼。但实际上对我们这些非大数据开发人员而言，Zookeeper是比Hxx们可能接触到更多的一个基础服务。但是，无奈的是它一直默默的位于二线，从来没有Hxx们那么耀眼。那么到底什么是Zookeeper呢？Zookeeper可以用来干什么？我们将如何使用Zookeeper？Zookeeper又是怎么实现的？ 什么是Zookeeper在Zookeeper的官网上有这么一句话：ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services。 这大概描述了Zookeeper主要是一个分布式服务协调框架，实现同步服务，配置维护和命名服务等分布式应用。是一个高性能的分布式数据一致性解决方案。 通俗地讲，ZooKeeper是动物园管理员，它是拿来管大象 Hadoop、鲸鱼 HBase、Kafka等的管理员。 Zookeeper和CAP的关系作为一个分布式系统，分区容错性是一个必须要考虑的关键点。一个分布式系统一旦丧失了分区容错性，也就表示放弃了扩展性。因为在分布式系统中，网络故障是经常出现的，一旦出现在这种问题就会导致整个系统不可用是绝对不能容忍的。所以，大部分分布式系统都会在保证分区容错性的前提下在一致性和可用性之间做权衡。 ZooKeeper是个CP（一致性+分区容错性）的，即任何时刻对ZooKeeper的访问请求能得到一致的数据结果，同时系统对网络分割具备容错性；但是它不能保证每次服务请求的可用性。也就是在极端环境下，ZooKeeper可能会丢弃一些请求，消费者程序需要重新请求才能获得结果。 ZooKeeper是分布式协调服务，它的职责是保证数据在其管辖下的所有服务之间保持同步、一致；所以就不难理解为什么ZooKeeper被设计成CP而不是AP特性的了。而且， 作为ZooKeeper的核心实现算法Zab，就是解决了分布式系统下数据如何在多个服务之间保持同步问题的。 Zookeeper节点特性及节点属性分析Zookeeper提供基于类似于文件系统的目录节点树方式的数据存储，但是Zookeeper并不是用来专门存储数据的，它的作用主要是用来维护和监控你存储的数据的状态变化。通过监控这些数据状态的变化，从而可以达到基于数据的集群管理。 数据模型与Linux文件系统不同的是，Linux文件系统有目录和文件的区别，而Zookeeper的数据节点称为ZNode，ZNode是Zookeeper中数据的最小单元，每个ZNode都可以保存数据，同时还可以挂载子节点，因此构成了一个层次化的命名空间，称为树。Zookeeper中 ZNode 的节点创建时候是可以指定类型的，主要有下面几种类型。 PERSISTENT：持久化 ZNode 节点，一旦创建这个 ZNode 点存储的数据不会主动消失，除非是客户端主动的delete。 EPHEMERAL：临时 ZNode 节点，Client连接到Zookeeper Service的时候会建立一个Session，之后用这个Zookeeper连接实例创建该类型的znode，一旦Client关闭了Zookeeper的连接，服务器就会清除Session，然后这个Session建立的 ZNode 节点都会从命名空间消失。总结就是，这个类型的znode的生命周期是和Client建立的连接一样的。 PERSISTENT_SEQUENTIAL：顺序自动编号的 ZNode 节点，这种znoe节点会根据当前已近存在的 ZNode 节点编号自动加 1，而且不会随Session断开而消失。 EPEMERAL_SEQUENTIAL：临时自动编号节点， ZNode 节点编号会自动增加，但是会随Session消失而消失 Watcher数据变更通知Zookeeper使用Watcher机制实现分布式数据的发布/订阅功能。Zookeeper的Watcher机制主要包括客户端线程、客户端WatcherManager、Zookeeper服务器三部分。客户端在向Zookeeper服务器注册的同时，会将Watcher对象存储在客户端的WatcherManager当中。当Zookeeper服务器触发Watcher事件后，会向客户端发送通知，客户端线程从WatcherManager中取出对应的Watcher对象来执行回调逻辑。 ACL保障数据的安全Zookeeper内部存储了分布式系统运行时状态的元数据，这些元数据会直接影响基于Zookeeper进行构造的分布式系统的运行状态，如何保障系统中数据的安全，从而避免因误操作而带来的数据随意变更而导致的数据库异常十分重要，Zookeeper提供了一套完善的ACL权限控制机制来保障数据的安全。 我们可以从三个方面来理解ACL机制：权限模式 Scheme、授权对象 ID、权限 Permission，通常使用”scheme:id:permission”来标识一个有效的ACL信息。 内存数据ookeeper的数据模型是树结构，在内存数据库中，存储了整棵树的内容，包括所有的节点路径、节点数据、ACL信息，Zookeeper会定时将这个数据存储到磁盘上。 DataTree：DataTree是内存数据存储的核心，是一个树结构，代表了内存中一份完整的数据。DataTree不包含任何与网络、客户端连接及请求处理相关的业务逻辑，是一个独立的组件。 DataNode：DataNode是数据存储的最小单元，其内部除了保存了结点的数据内容、ACL列表、节点状态之外，还记录了父节点的引用和子节点列表两个属性，其也提供了对子节点列表进行操作的接口。 ZKDatabase：Zookeeper的内存数据库，管理Zookeeper的所有会话、DataTree存储和事务日志。ZKDatabase会定时向磁盘dump快照数据，同时在Zookeeper启动时，会通过磁盘的事务日志和快照文件恢复成一个完整的内存数据库。 Zookeeper的实现原理分析1. Zookeeper Service网络结构Zookeeper的工作集群可以简单分成两类，一个是Leader，唯一一个，其余的都是follower，如何确定Leader是通过内部选举确定的。 Leader和各个follower是互相通信的，对于Zookeeper系统的数据都是保存在内存里面的，同样也会备份一份在磁盘上。 如果Leader挂了，Zookeeper集群会重新选举，在毫秒级别就会重新选举出一个Leader。 集群中除非有一半以上的Zookeeper节点挂了，Zookeeper Service才不可用。 2. Zookeeper读写数据 写数据，一个客户端进行写数据请求时，如果是follower接收到写请求，就会把请求转发给Leader，Leader通过内部的Zab协议进行原子广播，直到所有Zookeeper节点都成功写了数据后（内存同步以及磁盘更新），这次写请求算是完成，然后Zookeeper Service就会给Client发回响应。 读数据，因为集群中所有的Zookeeper节点都呈现一个同样的命名空间视图（就是结构数据），上面的写请求已经保证了写一次数据必须保证集群所有的Zookeeper节点都是同步命名空间的，所以读的时候可以在任意一台Zookeeper节点上。 3. Zookeeper工作原理Zab协议 Zookeeper的核心是广播，这个机制保证了各个Server之间的同步。实现这个机制的协议叫做Zab协议。 Zab（ZooKeeper Atomic Broadcast）原子消息广播协议作为数据一致性的核心算法，Zab协议是专为Zookeeper设计的支持崩溃恢复原子消息广播算法。 Zab协议核心如下: 所有的事务请求必须一个全局唯一的服务器（Leader）来协调处理，集群其余的服务器称为follower服务器。Leader服务器负责将一个客户端请求转化为事务提议（Proposal），并将该proposal分发给集群所有的follower服务器。之后Leader服务器需要等待所有的follower服务器的反馈，一旦超过了半数的follower服务器进行了正确反馈后，那么Leader服 务器就会再次向所有的follower服务器分发commit消息，要求其将前一个proposal进行提交。 Zab模式 Zab协议包括两种基本的模式： 崩溃恢复 和 消息广播 。 当整个服务框架启动过程中或Leader服务器出现网络中断、崩溃退出与重启等异常情况时，Zab协议就会进入恢复模式 并选举产生新的Leader服务器。 当选举产生了新的Leader服务器，同时集群中已经有过半的机器与该Leader服务器完成了状态同步之后，Zab协议就会退出恢复模式，状态同步是指数据同步，用来保证集群在过半的机器能够和Leader服务器的数据状态保持一致。 当集群中已经有过半的Follower服务器完成了和Leader服务器的状态同步，那么整个服务框架就可以进入 消息广播 模式。 当一台同样遵守Zab协议的服务器启动后加入到集群中，如果此时集群中已经存在一个Leader服务器在负责进行消息广播，那么加入的服务器就会自觉地进入数据恢复模式：找到Leader所在的服务器，并与其进行数据同步，然后一起参与到消息广播流程中去。 Zookeeper只允许唯一的一个Leader服务器来进行事务请求的处理，Leader服务器在接收到客户端的事务请求后，会生成对应的事务提议并发起一轮广播协议，而如果集群中的其他机器收到客户端的事务请求后，那么这些非Leader服务器会首先将这个事务请求转发给Leader服务器。 消息广播Zab协议的消息广播过程使用是一个原子广播协议，类似一个2PC提交过程。具体的： ZooKeeper使用单一主进程Leader用于处理客户端所有事务请求，并采用Zab的原子广播协议，将服务器数据状态变更以事务Proposal的形式广播Follower上，因此能很好的处理客户端的大量并发请求。 另一方面，由于事务间可能存在着依赖关系，Zab协议保证Leader广播的变更序列被顺序的处理，有些状态的变更必须依赖于比它早生成的那些状态变更。 最后，考虑到主进程Leader在任何时候可能崩溃或者异常退出， 因此Zab协议还要Leader进程崩溃的时候可以重新选出Leader并且保证数据的完整性；Follower收到Proposal后，写到磁盘，返回Ack。Leader收到大多数ACK后，广播Commit消息，自己也提交该消息。Follower收到Commit之后，提交该消息。 Zab协议简化了2PC事务提交：去除中断逻辑移除，follower要么ack，要么抛弃Leader。 Leader不需要所有的Follower都响应成功，只要一个多数派Ack即可。 崩溃恢复 上面我们讲了Zab协议在正常情况下的消息广播过程，那么一旦Leader服务器出现崩溃或者与过半的follower服务器失去联系，就进入崩溃恢复模式。 恢复模式需要重新选举出一个新的Leader，让所有的Server都恢复到一个正确的状态。 Zookeeper实践，共享锁，Leader选举分布式锁用于控制分布式系统之间同步访问共享资源的一种方式，可以保证不同系统访问一个或一组资源时的一致性，主要分为排它锁和共享锁。 排它锁又称为写锁或独占锁，若事务T1对数据对象O1加上了排它锁，那么在整个加锁期间，只允许事务T1对O1进行读取和更新操作，其他任何事务都不能再对这个数据对象进行任何类型的操作，直到T1释放了排它锁。 共享锁又称为读锁，若事务T1对数据对象O1加上共享锁，那么当前事务只能对O1进行读取操作，其他事务也只能对这个数据对象加共享锁，直到该数据对象上的所有共享锁都被释放。推荐文章：基于Zk实现分布式锁 Leader选举Leader选举是保证分布式数据一致性的关键所在。当Zookeeper集群中的一台服务器出现以下两种情况之一时，需要进入Leader选举。 服务器初始化启动。 服务器运行期间无法和Leader保持连接。 Zookeeper在3.4.0版本后只保留了TCP版本的 FastLeaderElection 选举算法。当一台机器进入Leader选举时，当前集群可能会处于以下两种状态： 集群中已存在Leader。 集群中不存在Leader。 对于集群中已经存在Leader而言，此种情况一般都是某台机器启动得较晚，在其启动之前，集群已经在正常工作，对这种情况，该机器试图去选举Leader时，会被告知当前服务器的Leader信息，对于该机器而言，仅仅需要和Leader机器建立起连接，并进行状态同步即可。 而在集群中不存在Leader情况下则会相对复杂，其步骤如下： (1) 第一次投票。无论哪种导致进行Leader选举，集群的所有机器都处于试图选举出一个Leader的状态，即LOOKING状态，LOOKING机器会向所有其他机器发送消息，该消息称为投票。投票中包含了SID（服务器的唯一标识）和ZXID（事务ID），(SID, ZXID)形式来标识一次投票信息。假定Zookeeper由5台机器组成，SID分别为1、2、3、4、5，ZXID分别为9、9、9、8、8，并且此时SID为2的机器是Leader机器，某一时刻，1、2所在机器出现故障，因此集群开始进行Leader选举。在第一次投票时，每台机器都会将自己作为投票对象，于是SID为3、4、5的机器投票情况分别为(3, 9)，(4, 8)， (5, 8)。 (2) 变更投票。每台机器发出投票后，也会收到其他机器的投票，每台机器会根据一定规则来处理收到的其他机器的投票，并以此来决定是否需要变更自己的投票，这个规则也是整个Leader选举算法的核心所在，其中术语描述如下 vote_sid：接收到的投票中所推举Leader服务器的SID。 vote_zxid：接收到的投票中所推举Leader服务器的ZXID。 self_sid：当前服务器自己的SID。 self_zxid：当前服务器自己的ZXID。 每次对收到的投票的处理，都是对(vote_sid, vote_zxid)和(self_sid, self_zxid)对比的过程。 规则一：如果vote_zxid大于self_zxid，就认可当前收到的投票，并再次将该投票发送出去。 规则二：如果vote_zxid小于self_zxid，那么坚持自己的投票，不做任何变更。 规则三：如果vote_zxid等于self_zxid，那么就对比两者的SID，如果vote_sid大于self_sid，那么就认可当前收到的投票，并再次将该投票发送出去。 规则四：如果vote_zxid等于self_zxid，并且vote_sid小于self_sid，那么坚持自己的投票，不做任何变更。 结合上面规则，给出下面的集群变更过程。 (3) 确定Leader。经过第二轮投票后，集群中的每台机器都会再次接收到其他机器的投票，然后开始统计投票，如果一台机器收到了超过半数的相同投票，那么这个投票对应的SID机器即为Leader。此时Server3将成为Leader。由上面规则可知，通常那台服务器上的数据越新（ZXID会越大），其成为Leader的可能性越大，也就越能够保证数据的恢复。如果ZXID相同，则SID越大机会越大。 「 浅谈大规模分布式系统中那些技术点」系列文章： 浅谈分布式事务 扩展阅读zookeeper入门基本介绍ZooKeeper解析：分布式系统工程师的瑞士军刀日志系统之基于Zookeeper的分布式协同设计分布式服务框架：Zookeeper为什么不应该使用ZooKeeper做服务发现 ZooKeeper集群的安装、配置、高可用测试浅谈开源大数据平台的演变 来自: http://www.linkedkeeper.com/detail/blog.action?bid=1014]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Container is running beyond virtual memory limits. Current usage- 611.1 MB of 1 GB physical memory u]]></title>
    <url>%2Fhadoop%2Fhadoop%2Fyarn-vmemory-over.html</url>
    <content type="text"><![CDATA[hadoop 在 yarn 上运行时报的虚拟内存错误，或者是物理内存不够错误 异常显示1234Container [pid=100287,containerID=container_1513249052998_0007_01_000009] is running beyond virtual memory limits. Current usage: 611.1 MB of 1 GB physicalmemory used; 4.9 GB of 3 GB virtual memory used. Killing container. 但是后面任务可能还是可以正常的运行的 异常分析611.1MB: 任务所占的物理内存 1GB 是mapreduce.map.memory.mb 设置的 4.9G 是程序占用的虚拟内存： 什么是虚拟内存以及和物理内存的关系 3GB 是mapreduce.map.memory.db 乘以 yarn.nodemanager.vmem-pmem-ratio 得到的 其中yarn.nodemanager.vmem-pmem-ratio 是 虚拟内存和物理内存比例，在yarn-site.xml中设置，默认是2.1, 由于我本地设置的是3， 所以 1*3 = 3GB 很明显，container占用了4.9G的虚拟内存，但是分配给container的却只有3GB。所以kill掉了这个container 上面只是map中产生的报错，当然也有可能在reduce中报错，如果是reduce中，那么就是mapreduce.reduce.memory.db * yarn.nodemanager.vmem-pmem-ratio 解决办法主要是以下四个方面考虑 取消虚拟内存的检查：在yarn-site.xml或者程序中中设置yarn.nodemanager.vmem-check-enabled为false 12345&lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;description&gt;Whether virtual memory limits will be enforced for containers.&lt;/description&gt;&lt;/property&gt; 除了虚拟内存超了，也有可能是物理内存超了，同样也可以设置物理内存的检查为false： yarn.nodemanager.pmem-check-enabled个人认为这种办法并不太好，如果程序有内存泄漏等问题，取消这个检查，可能会导致集群崩溃。 增大mapreduce.map.memory.mb 或者 mapreduce.map.memory.mb个人觉得是一个办法，应该优先考虑这种办法，这种办法不仅仅可以解决虚拟内存，或许大多时候都是物理内存不够了，这个办法正好适用 适当增大yarn.nodemanager.vmem-pmem-ratio的大小，一个物理内存增大多个虚拟内存， 但是这个参数也不能太离谱 如果任务所占用的内存太过离谱，更多考虑的应该是程序是否有内存泄漏，是否存在数据倾斜等，优先程序解决此类问题 参考资料：https://stackoverflow.com/questions/14110428/am-container-is-running-beyond-virtual-memory-limits]]></content>
      <categories>
        <category>大数据</category>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop</tag>
        <tag>yarn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[yarn资源管理最佳实践]]></title>
    <url>%2Fhadoop%2Fhadoop%2Fyarn-resource-best_practice.html</url>
    <content type="text"><![CDATA[原文翻译自 https://mapr.com/blog/best-practices-yarn-resource-management/#.Ve5bLdOqoVU 有改动 – 翻译以及记录的目的是对yarn进行合理的资源配置，以及yarn平台出错后的分析 这篇文章主要是讨论YARN资源管理的最佳实践，YARN的基本理论是将资源管理和任务调度分离，所以设计了一个全局的资源管理器 ResourceManager（RM）和每个应用程序的ApplicationMaster（AM）。 ResourceManager（RM）：有两个主要组件：Scheduler和ApplicationsManager。调度程序负责将资源分配给各种正在运行的应用程序，这些应用程序受到容量，队列等熟悉的约束。==调度程序是纯调度程序，因为它不会监视或跟踪应用程序的状态==。此外，由于应用程序故障或硬件故障，它不提供有关重新启动失败任务的保证。调度器根据应用程序的资源需求执行调度功能; 它是基于资源容器的抽象概念来实现的，它包含了内存，CPU，磁盘，网络等元素。 ApplicationsManager： 负责接受作业提交。协商执行特定于应用程序的ApplicationMaster的第一个容器，并提供失败时重新启动ApplicationMaster容器的服务。每个应用程序的ApplicationMaster负责从调度程序中申请适当的资源容器，跟踪他们的状态并监视进度。 在阅读本文之前，请先阅读官方的YARN文档。 本博客文章涵盖了有关YARN资源管理的以下主题，并为每个主题提供了最佳实践： 监护人如何计算和分配资源给YARN？ YARN的最小和最大分配单位 虚拟/物理内存检查器 Mapper，Reducer和AM的资源请求 瓶颈资源 如何计算和分配资源给YARN？在hadoop集群中，各种资源都是默认分配的，我们可以对其进行修改，具体的各个参数是什么意思和各个参数的默认值，参考mapr的文档 – ps: 个人也准备仔细研究一番 通过在NM节点的yarn-site.xml中设置以下三个配置并重新启动NM, 将可以配置每个节点的资源1234567891011yarn.nodemanager.resource.memory-mbyarn.nodemanager.resource.cpu-vcoresyarn.nodemanager.resource.io-spindles 不知道是什么``` 那么可以在网页上看到每个节点自己所配置的内存，cpu核数![集群参数](https://mapr.com/blog/best-practices-yarn-resource-management/assets/otherpageimages/YarnResourceMang-Fig2.png)上面这几个参数应该适当的设置，如果设置太大可能会有问题，例如如果yarn.nodemanager.resource.memory-mb 设置过大，内存过度分配给YARN，可能会使用巨大的swap，并可能触发内核OOM杀手来终止容器进程 下面的错误是OS OOM的一个标志，可能是内存过量分配了 os::commit_memory(0x0000000000000000, xxxxxxxxx, 0) failed;error=’Cannot allocate memory’ (errno=12)1234567891011121314这时就仔细检查各个节点，降低分配给yarn使用的内存 ## YARN中的最小和最大分配单位 ![yarn最大最小分配](https://mapr.com/blog/best-practices-yarn-resource-management/assets/otherpageimages/YarnResourceMang-Fig3.png)基本上，这意味着RM只能以“yarn.scheduler.minimum-allocation-mb”为增量对容器分配内存，不能超过“yarn.scheduler.maximum-allocation-mb”。它只能以“yarn.scheduler.minimum-allocation-vcores”为增量分配CPU vcore到容器，不能超过“yarn.scheduler.maximum-allocation-vcores”。如果需要更改，请在RM节点的yarn-site.xml中设置以上配置，然后重新启动RM。例如，如果一个作业要求每个映射容器1025 MB的内存（set mapreduce.map.memory.mb = 1025），RM会给它一个2048 MB（2 * yarn.scheduler.minimum-allocation-mb）容器。如果您有一个需要9999 MB地图容器的巨大MR作业，则作业将在AM日志中显示以下错误消息： MAP capability required is more than the supported max container capability in the cluster.Killing the Job. mapResourceRequest: 9999 maxContainerCapability:819212如果YARN作业上的Spark要求容量大于“yarn.scheduler.maximum-allocation-mb”的大容器，则会显示以下错误： Exception in thread “main” java.lang.IllegalArgumentException:Required executor memory (99999+6886 MB) is above the max threshold (8192 MB) of this cluster!1234567891011121314**在以上两种情况下，您可以增加yarn-site.xml中的“yarn.scheduler.maximum-allocation-mb”并重新启动RM**。&lt;font color=#be1a21 size=3 face=&quot;黑体&quot;&gt; 所以在这一步中，需要熟悉每个Mapper和Reducer的资源需求的上下界，并据此设置最小和最大分配单位。&lt;/font&gt;## 虚拟/物理内存检查器 NodeManager可以监视容器的内存使用情况（虚拟的和物理的）。如果它的虚拟内存超过了“yarn.nodemanager.vmem-pmem-ratio”乘以“mapreduce.reduce.memory.mb”或者“mapreduce.map.memory.mb”，并且如果“yarn.nodemanager。 vmem-check-enabled“ 设置的是true; 容器将会被kill掉 同理。如果它的物理内存超过了“mapreduce.reduce.memory.mb”或者“mapreduce.map.memory.mb”，并且“yarn.nodemanager.pmem-check-enabled”设置的为true，容器也将被终止。 下面的参数可以在每个NM节点的yarn-site.xml中设置，以覆盖默认行为。 ![yarn设置](https://mapr.com/blog/best-practices-yarn-resource-management/assets/otherpageimages/YarnResourceMang-Fig4.png) **这是由虚拟内存检查器杀死的容器的示例错误：** Current usage: 347.3 MB of 1 GB physical memory used;2.2 GB of 2.1 GB virtual memory used12**这是物理内存检查器的示例错误：** Current usage: 2.1gb of 2.0gb physical memory used1.1gb of 3.15gb virtual memory used. Killing123456789101112131415161718192021222324252627282930解决办法参考我的一篇文章： http://blog.csdn.net/t1dmzks/article/details/78818874 ## Mapper，Reducer和AM的资源请求 MapReduce v2作业有3种不同的容器类型 - Mapper，Reducer和AM。 Mapper和Reducer可以要求资源 - 内存，CPU和磁盘，而AM只能要求内存和CPU。以下是三种容器类型的资源请求的配置摘要. 默认值来自MapR 4.1的Hadoop 2.5.1，它们可以在客户端节点的mapred-site.xml中被覆盖，也可以在MapReduce java代码，Pig和Hive Cli等应用程序中设置。 **程序设置优先于命令行的配置优先于节点的配置** - **Mapper:** ![mapper配置](https://mapr.com/blog/best-practices-yarn-resource-management/assets/otherpageimages/YarnResourceMang-Fig5.png) - **Reducer:** ![reducer配置](https://mapr.com/blog/best-practices-yarn-resource-management/assets/otherpageimages/YarnResourceMang-Fig6.png) - **AM:** ![AM配置](https://mapr.com/blog/best-practices-yarn-resource-management/assets/otherpageimages/YarnResourceMang-Fig7.png) &lt;font color=#be1a21 size=3 face=&quot;黑体&quot;&gt; 每个容器实际上是一个JVM进程，在java-opts的“-Xmx”之上应该适合分配的内存大小。一个最佳的做法是将其设置为0.8 *（容器内存分配）&lt;/font&gt;。例如，如果请求的mapper容器的mapreduce.map.memory.mb = 4096，我们可以设置mapreduce.map.java.opts = -Xmx3277m。 有很多因素会影响每个容器的内存要求。这些因素包括Mappers / Reducers的数量，文件类型（纯文本文件，Parquet，ORC），数据压缩算法，操作类型（排序，分组，聚合，连接），数据歪斜等。熟悉MapReduce作业的性质，找出Mapper，Reducer和AM的最低要求。任何类型的容器如果不满足最小内存要求，可能会耗尽内存，并被物理/虚拟内存检查器杀死。如果是这样，你需要检查AM日志和失败的容器日志来找出原因。 例如，如果MapReduce作业排序parquet文件，则Mapper需要将整个Parquet行组缓存在内存中。我已经做了测试，证明镶木地板文件的行组大小越大，需要更大的Mapper内存。在这种情况下，确保Mapper内存足够大而不触发OOM。 另外一个例子是AM内存不足。通常情况下AM的1G Java堆大小足够用于许多作业， 但是如果工程中需要写大量的parquet文件， 在此期间commit提交了一个job, AM 将调用ParquetOutputCommitter.commitJob(), 这个方法会将首先读取**所有**输出parquet文件的页脚，并在输出目录中写入名为“_metadata”的元数据文件 在AM日志中，此步骤可能会导致AM的内存不足 Caused by: java.lang.OutOfMemoryError GC overhead limit exceeded at java.lang.StringCoding$StringEncoder.encode(StringCoding.java:300) at java.lang.StringCoding.encode(StringCoding.java:344) at java.lang.String.getBytes(String.java:916) at parquet.org.apache.thrift.protocol.TCompactProtocol.writeString(TCompactProtocol.java:298) at parquet.format.ColumnChunk.write(ColumnChunk.java:512) at parquet.format.RowGroup.write(RowGroup.java:521) at parquet.format.FileMetaData.write(FileMetaData.java:923) at parquet.format.Util.write(Util.java:56) at parquet.format.Util.writeFileMetaData(Util.java:30) at parquet.hadoop.ParquetFileWriter.serializeFooter(ParquetFileWriter.java:322) at parquet.hadoop.ParquetFileWriter.writeMetadataFile(ParquetFileWriter.java:342) at parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:51) … 10 more ```解决方法是增加AM的内存需求，并通过“set parquet.enable.summary-metadata false”来禁用parquet功能。 除了找出每个容器的最小内存需求之外，有时我们需要平衡工作性能和资源容量。例如，进行排序的作业可能需要相对较大的“mapreduce.task.io.sort.mb”以避免或减少溢出文件的数量。如果整个系统有足够的内存容量，我们可以增加“mapreduce.task.io.sort.mb”和容器内存，以获得更好的工作性能。 在这一步中，我们需要确保每种类型的容器都满足适当的资源需求。如果发生OOM，请首先检查AM日志以确定哪个容器以及每个堆栈跟踪的原因是。 瓶颈资源由于有三种类型的资源，来自不同工作的不同容器可能要求不同的资源量。这可能导致其中一个资源成为瓶颈。假设我们有一个容量为1000G RAM，16个核心，16个磁盘的集群，每个Mapper容器需要10G RAM，1个核心，0.5个磁盘，最多可以有16个Mapper并行运行，因为CPU核心成了这里的瓶颈。 因此，（840G RAM，8个磁盘）资源不被任何人使用。如果你遇到这种情况，只需检查RM UI（http：//：8088 /集群/节点）来确定哪个资源是瓶颈。您可以将剩余的资源分配给可以提高性能的作业。例如，您可以分配更多的内存来排序作业溢出到磁盘。 关键点 确保分配给集群的内存大小和cpu核心数要合适 熟悉mapper和reducer的资源需求的上下界。 请注意虚拟和物理内存检查器。 将每个容器的java-opts的-Xmx设置为0.8 *（容器内存分配）。 确保每种类型的容器都符合适当的资源要求。 充分利用瓶颈资源。]]></content>
      <categories>
        <category>大数据</category>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>hadoop</tag>
        <tag>yarn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SecureCRT8.0安装与破解]]></title>
    <url>%2Flinux%2Fxmange-remote-desktop.html</url>
    <content type="text"><![CDATA[最近经常在本地调试的东西，放在服务器上就运行不了，有时候本地的机器远远没有服务器上的机器强大，这时候想的就是在服务器上装上各种环境，例如eclipse，idea或者其他的环境。这时候，就需要自己能直接使用桌面来进行调试了。 想起之前都是使用ubuntu进行开发，centos自带的桌面系统也是可以支持这些idea的呀，可是我们不可能跑到机房去使用远程桌面系统，这时候，Xmanger就派上用场了 本地下载xmangerxmanger 下载地址: 链接：http://pan.baidu.com/s/1dFxrXKl 密码：6ir5 安装按照提示即可 远程连接我本地windows的IP是192.168.1.103 我需要连接服务器centos的桌面，IP地址是192.168.1.31 进入到安装目录运行Xstart.exe 按照如图进行填写，其中注意填写 /usr/bin/gnome-session –display $DISPLAY 点击运行,然后输入密码 出现x11已经打开，完成 如果想关闭图形界面，右击右下角X图标–&gt;关闭 如果这样不行，可能linux是纯净版的，纯净版的个人并没有试过，可以参考网上的内容,我这linux6.5自带的是可以这样使用的。 如果使用root登录可能碰到错误1You are currently trying to run as the root super user. The super user is a specialized account that is not designed to run a normal user session. Various programs will not function properly, and actions performed under this account can cause unrecoverable damage to the operating system. 个人不建议用root用户登录，可以自己创建用户登录，创建办法如下 添加用户12useradd kaishunpasswd kaishun 给kaishun用户赋予root权限切换到root用户 赋予etc/sudoers777权限，然后打开123456## Allow root to run any commands anywhereroot ALL=(ALL) ALLkaishun ALL=(ALL) ALL## Allows people in group wheel to run all commands %wheel ALL=(ALL) ALL 把sudoers的权限改回来成4401chmod 440 /etc/sudoers 后面就可以登陆了]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SecureCRT8.0安装与破解]]></title>
    <url>%2Flinux%2FSecureCRT_8-install.html</url>
    <content type="text"><![CDATA[每次电脑在重装后，想要装几个虚拟机玩一玩，总是要碰到网络配置，SecureCRT等的安装问题，每次都要百度半天，各种破解工具的网站的工具又不敢下载（被各种软件下载的网站病毒搞怕了），这次装好了，做个总结。本教程针对windows版本，若是ubuntu或者其他版本，需要下载对应的工具 安装securtCRT安装包和破解工具下载地址：链接：http://pan.baidu.com/s/1nvQfIMP 密码：5zxk先安装scrt734_x86_V8.0.4_setup.exe 安装好后，关闭。将 securecrt8.0zcj 解压，然后复制里面的securecrt8.0zcj\SecureCRT v8.x 注册机\keygen.exe到安装目录，例如我的安装目录是 C:\Program Files (x86)\VanDyke Software\SecureCRT运行keygen.exe安装下面的图操作 打开SecurtCRT8.0，安装如下所示进行破解]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[fluentd+mongodb构建分布式日志收集系统]]></title>
    <url>%2F%E6%9E%B6%E6%9E%84%2Ffluentd_mongodb_1.html</url>
    <content type="text"><![CDATA[前言：和同事合作安装过一次fluentd+mongodb，网上文档较少，走了好多弯路，幸亏同事比较给力，能成功应用，现将安装笔记记录一下。（这里只安装了一台，分布式也是一样的，使用mongodb集群即可） 一、 mongodb单机安装1.1 下载解压并安装下载mongodb-linux-x86_64-3.4.6.tgz ， 镜像地址https://www.mongodb.org/dl/linux/x86_64 1234#解压tar -xzvf mongodb-linux-x86_64-3.4.6.tgz -C /usr/local/#改名mv mongodb-linux-x86_64-3.4.6 mongodb 1.2 配置环境变量123456vim /etc/profile# 内容export MONGODB_HOME=/usr/local/mongodbexport PATH=$MONGODB_HOME/bin:$PATH# 使立即生效source /etc/profile 二、 fluentd安装2.1 调整安装的Linux环境在安装Fluentd之前，您必须按照以下步骤设置您的环境(不是必要的，我家里电脑没有这样做)。参考官网 2.2 在线或离线安装fluentd两种安装方式，能联网的话，肯定优先使用在线安装了在线安装：1curl -L https://toolbelt.treasuredata.com/sh/install-redhat-td-agent3.sh | sh 然后运行即可, 如果提示报错，那么直接浏览器粘贴https://toolbelt.treasuredata.com/sh/install-redhat-td-agent3.sh, 把里面的内容拷贝到机器上的某个.sh文件， 然后执行， 注意： 如果是root用户，把里面的sudo去掉 离线安装：先下载： td-agent-3.1.1-0.el6.x86_64.rpmcentos6下载地址centos7下载地址下载完后，运行1rpm -ivh td-agent-3.1.1-0.el6.x86_64.rpm 2.3 运行fluentd服务1$ /etc/init.d/td-agent start 查看状态，停止，重启等命令如下1234$ /etc/init.d/td-agent start$ /etc/init.d/td-agent stop$ /etc/init.d/td-agent restart$ /etc/init.d/td-agent status 有问题去 vim /var/log/td-agent/td-agent.log 查看日志信息 刚开始安装的时候，尝试使用官网的那个例子， 测试收集apache的日志 2.4 配置fluentd文件1Vim /etc/td-agent/td-agent.conf 在配置中添加 123456789101112131415161718192021222324252627282930313233&lt;source&gt; @type tail # 从文件的末尾的方式读取数据 path /var/log/httpd/access_log #apache服务器产生的日志文件位置，注意了，这个貌似centos和ubuntu不一致，官网的那个日志目录在centos中不存在的 pos_file /var/log/td-agent/apache2.access_log.pos #一个镜像文件,默认是这个 # apache2：自带的解析器,apache服务器产生的日志，fluentd默认提供了一个解析方法（若不用这个方法，需要自己配置format等配置） &lt;parse&gt; @type apache2 &lt;/parse&gt; tag mongo.apache.access # tag, 接收的tag&lt;/source&gt;&lt;match mongo.**&gt; # plugin type @type mongo # 引入插件为mongo，使用这个必须要安装fluentd-mongo的插件 # mongodb db + collection database apache # mongodb的数据库 collection access # 放在哪个集合中 # mongodb host + port host localhost #数据库host port 27017 # 数据库端口 # interval &lt;buffer&gt; flush_interval 10s #10秒钟刷新读取一次 &lt;/buffer&gt; # make sure to include the time key &lt;inject&gt; time_key time &lt;/inject&gt;&lt;/match&gt; 2.5 重启fluentd，然后查看fluentd的状态12$ /etc/init.d/td-agent restart $ /etc/init.d/td-agent status 上面肯定报错，应该会报fluent-plugin-mongo插件找不到 2.6 fluent-plugin-mongo 插件安装这个插件依赖于ruby2.0以上的版本, 所以先要安装fluent-plugin-mongo下载一个ruby2.0以上的版本编译安装12345tar -xzvf ruby-2.2.9.tar.gzcd /ruby-2.2.9/./configuremakemake install (root) 安装fluent-plugin-mongo 插件在线版：==因为我们用的是rpm安装的，所以一定要使用td-agent-gem来安装，不能直接使用gem！！！== 否则即使安装了插件，可能fluentd还是找不到这个插件。12cd /usr/sbin/ td-agent-gemtd-agent-gem install fluent-plugin-mongo 离线安装: 有的服务器集群不允许连接外网，就只能离线安装了下载 fluent-plugin-mongo-1.0.0.gem 官网–&gt;plugin–&gt;下载fluentd-plugin-mongo-1.0.0.gem123456789cd /usr/sbin/ td-agent-gemtd-agent-gem install fluent-plugin-mongo-1.0.0.gem``` ### 2.7 安装完后重启fluentd ```shell$ /etc/init.d/td-agent restart $ /etc/init.d/td-agent status 2.8 测试使用apache的日志来测试开启一个apache server服务123开启服务的方法 service httpd start``` 使用ab test来测试 ab -n 100 -c 10 http://localhost/12并发10，请求100此，我们用的是centos6.5， 所以此日志会保存在 /var/log/httpd/access_log 中，日志的格式如下 ::1 - - [25/Dec/2017:15:58:03 +0800] “GET / HTTP/1.0” 403 5039 “-“ “ApacheBench/2.3”::1 - - [25/Dec/2017:15:58:03 +0800] “GET / HTTP/1.0” 403 5039 “-“ “ApacheBench/2.3”::1 - - [25/Dec/2017:15:58:03 +0800] “GET / HTTP/1.0” 403 5039 “-“ “ApacheBench/2.3”::1 - - [25/Dec/2017:15:58:03 +0800] “GET / HTTP/1.0” 403 5039 “-“ “ApacheBench/2.3”::1 - - [25/Dec/2017:15:58:03 +0800] “GET / HTTP/1.0” 403 5039 “-“ “ApacheBench/2.3”::1 - - [25/Dec/2017:15:58:03 +0800] “GET / HTTP/1.0” 403 5039 “-“ “ApacheBench/2.3”::1 - - [25/Dec/2017:15:58:03 +0800] “GET / HTTP/1.0” 403 5039 “-“ “ApacheBench/2.3”12**注意**： /var/log/httpd 目录的权限 chmod 777 /var/log/httpd1234567891011如果第一次没改权限有问题了，建议把/var/log/httpd/access_log 文件删掉，重启apache的httpd ,然后ab -test **mongodb查看效果** ```shell&gt; use apache&gt; db[&quot;access&quot;].find()&#123; &quot;_id&quot; : ObjectId(&quot;5a3ce656e138230dc48c58a6&quot;), &quot;host&quot; : &quot;::1&quot;, &quot;user&quot; : null, &quot;method&quot; : &quot;GET&quot;, &quot;path&quot; : &quot;/&quot;, &quot;code&quot; : 403, &quot;size&quot; : 4961, &quot;referer&quot; : null, &quot;agent&quot; : &quot;ApacheBench/2.3&quot;, &quot;time&quot; : ISODate(&quot;2017-12-22T11:02:36Z&quot;) &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5a3ce656e138230dc48c58a7&quot;), &quot;host&quot; : &quot;::1&quot;, &quot;user&quot; : null, &quot;method&quot; : &quot;GET&quot;, &quot;path&quot; : &quot;/&quot;, &quot;code&quot; : 403, &quot;size&quot; : 4961, &quot;referer&quot; : null, &quot;agent&quot; : &quot;ApacheBench/2.3&quot;, &quot;time&quot; : ISODate(&quot;2017-12-22T11:02:36Z&quot;) &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5a3ce656e138230dc48c58a8&quot;), &quot;host&quot; : &quot;::1&quot;, &quot;user&quot; : null, &quot;method&quot; : &quot;GET&quot;, &quot;path&quot; : &quot;/&quot;, &quot;code&quot; : 403, &quot;size&quot; : 4961, &quot;referer&quot; : null, &quot;agent&quot; : &quot;ApacheBench/2.3&quot;, &quot;time&quot; : ISODate(&quot;2017-12-22T11:02:36Z&quot;) &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5a3ce656e138230dc48c58a9&quot;), &quot;host&quot; : &quot;::1&quot;, &quot;user&quot; : null, &quot;method&quot; : &quot;GET&quot;, &quot;path&quot; : &quot;/&quot;, &quot;code&quot; : 403, &quot;size&quot; : 4961, &quot;referer&quot; : null, &quot;agent&quot; : &quot;ApacheBench/2.3&quot;, &quot;time&quot; : ISODate(&quot;2017-12-22T11:02:36Z&quot;) &#125; apache日志收集完成java程序log日志如何写入到fluentd中本人思考有两种方式，一种是往本地写log日志，使用log4j或者其他方法向本地写日志后，fluentd使用in-tail的方式（类似于上面的方式）读取，保存到mongodb中, 这个格式就不再是apache了，需要自己写解析日志的规则 一种是通过forward端口的方式这里介绍第二种方式修改配置文件1vim /etc/td-agent/td-agent.conf 1234567891011121314&lt;source&gt; @type forward port 24234&lt;/source&gt;&lt;match fluentd.test.*&gt; @type mongo host 127.0.0.1 port 27017 database fluentd tag_mapped remove_tag_prefix mongo. collection misc&lt;/match&gt; java代码，为了测试效果，一次发30万条数据这个程序需要引入一系列jar包，什么log4j, slf4j啊，javassist,msgpack,fluentd-logger等jar包，去maven仓库下载即可1234567891011121314public class TestFluentdLog &#123; // fluentd.test 要和上面的配置文件的一致 private static FluentLogger LOG = FluentLogger.getLogger("fluentd.test", "192.168.x.xxx", 24234); public static void main(String[] args) &#123; for (int i = 0; i &lt; 30000; i++) &#123; Map&lt;String, Object&gt; data = new HashMap&lt;String, Object&gt;(); data.put("log1", "aaa"+i); data.put("to", "bbb"+i); LOG.log("follow", data); //... &#125; LOG.close(); &#125; &#125; 进入到mongodb123show dbs 发现出现fluentd, 然后发现这个数据比空的数据库要大，说明有文件了fluentd 0.453GB 查看数据12345678910&gt; use fluentdswitched to db fluentd&gt; db["fluentd.test.follow"].find()&#123; "_id" : ObjectId("5a41e2f08d8c9539b4391bc2"), "from" : "aaa", "to" : "bbb", "time" : ISODate("2017-12-26T05:54:49Z") &#125;&#123; "_id" : ObjectId("5a41e2f08d8c9539b4391bc3"), "from" : "aaa", "to" : "bbb", "time" : ISODate("2017-12-26T05:55:48Z") &#125;&#123; "_id" : ObjectId("5a41e3888d8c953a39227d87"), "from" : "aaa", "to" : "bbb", "time" : ISODate("2017-12-26T05:57:19Z") &#125;&#123; "_id" : ObjectId("5a41e3888d8c953a39227d88"), "from" : "aaa", "to" : "bbb", "time" : ISODate("2017-12-26T05:58:00Z") &#125;&#123; "_id" : ObjectId("5a41e3e48d8c953aa6fe1216"), "from" : "aaa", "to" : "bbb", "time" : ISODate("2017-12-26T05:58:53Z") &#125;&#123; "_id" : ObjectId("5a41e5588d8c953aa6fe1217"), "from" : "aaa0", "to" : "bbb0", "time" : ISODate("2017-12-26T06:05:46Z") &#125;... 三、扩展3.1 fluentd安装过程问题解决有问题都是去vim /var/log/td-agent/td-agent.log 查看，然后看根据问题进行解决，多google，多上github看issues，仔细耐心 3.2 MongoDb web 用户界面mongod –dbpath=/data/db –rest MongoDB 的 Web 界面访问端口比服务的端口多1000。首先得关闭防火墙123456# 查询防火墙状态: [root@localhost ~]# service iptables status # 停止防火墙:[root@localhost ~]# service iptables stop # 永久关闭防火墙:[root@localhost ~]# chkconfig iptables off 如果你的MongoDB运行端口使用默认的27017，你可以在端口号为28017访问web用户界面，即地址为：http://localhost:28017 3.3 mongodb 3.4 集群搭建：分片+副本集参考文章： 亲测可用 https://www.cnblogs.com/ityouknow/p/7344005.html]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>架构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[geohash的特点，局限性和思考]]></title>
    <url>%2F%E7%AE%97%E6%B3%95%2Fgeohash-think_by_myself.html</url>
    <content type="text"><![CDATA[geohash原理原理参考： http://blog.jobbole.com/80633/，其实就是有限二分法而已 使用场景一般用于查找周边,具体可以网上百度 为什么不直接使用经纬度 如果查找周边，使用经纬度，需要用到经度&gt;多少,经度&lt;多少，纬度&gt;多少，纬度&lt;多少,在数据库中就用不到索引 有的人认为如果确定了多少米的范围，那么直接使用经纬度的某个经度，合在一起，也可以作为一维，例如纬经度分别为22.21315,41.59874 作为一个字段，只需要控制纬度多少位，经度多少位即可。但是这也意味着固定死了某个经纬度，geohash的优势还在于，根据不同的位数，可以得到不同的范围 局限性个人认为的局限性有以下几点： 不能做排序，真要做排序，而且还用geohash，那只能查出来，然后对查出来的部分做排序 由于用的是经纬度的有限二分，并且使用的是5个bit位做一个base32编码，那么geohash的精度问题就比较固定。官方的经纬度当然这个精度可以进行改进，只需要控制好经纬度的位数，就可以控制好经纬度的误差，当然就能控制km的误差，参考这篇文章的改进： https://my.oschina.net/shipley/blog/704964当然，基于这篇文章，还可以根据自己的需求进一步进行改进，比如前几位用5位进行base32编码，后几位可以用三位，或者二位进行base32编码，这需要看自己的精度控制。但是整体说来，精度还是如下：现在的每一位，可以更精确了。 求的周边并不是一个圆，也不是一个正方形，大多数情况是一个长方形，具体参考上面的两个表 部分点不准确位于格子边界两侧的两点， 虽然十分接近，但编码会完全不同,可以同时搜索当前格子周围的8个格子，即可解决这个问题 有没有其他的办法栅格法： 其实geohash就类似于一种栅格法：下面介绍自己曾用过的两种栅格的方法方法1： 例如对桂林进行栅格化，那么找出桂林左下角纬经度为原点，lat0,lng0, 对整个北京进行栅格的划分，比如10m作为一个栅格，我们可以计算出每10米的纬度为0.00009°，经度大约为0.0001°，那么按照这个经纬度进行划分栅格即可。如果有一个点，需要计算落在哪个经纬度内，那么x=(lat-lat0)/0.00009,y=(lng-lng0)/(0.0001)方法二：对每一个经纬度1000000，例如32.1256232，114.2135123 乘以1000000为 321256232,1142135123, 对于纬度，我们圆整900,经度，圆整1000，计算出栅格的左下角经纬度例如 (321256232/900)900, (1142135123*1000)/1000,算出的就是左下角的经纬度了，对于一个栅格内的，圆整后，都属于这个左下角的经纬度 代码原理很简单，代码更简单，直接去github上搜索geohash就行了，各种语言的都有]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[资源学习网站，不断更新]]></title>
    <url>%2Fprogramme%2Fresource1.html</url>
    <content type="text"><![CDATA[大数据Laurence的技术博客 http://blog.csdn.net/bluishglc?viewmode=contents过往记忆 https://www.iteblog.com/spark性能调优 http://blog.csdn.net/u012102306/article/details/51322209 一些比较好的文章美团点评技术团队 http://tech.meituan.com/archives gis的一些代码geohash github https://github.com/search?l=Java&amp;q=geohash&amp;type=Repositories&amp;utf8=%E2%9C%93 String工具类 https://github.com/shekhargulati/strman-java 2016 spark 博客 TOP 10https://databricks.com/blog/2016/12/30/top-10-apache-spark-blog-posts-from-2016.html Spark特别好的一篇博客https://databricks.com/blog/page/3 spark 优化spark Kryo序列化最简单使用 http://www.cnblogs.com/shixiangwan/p/6200723.html利用Kryo序列化库是你提升Spark性能要+做的第一件事 http://www.jianshu.com/p/8ccd701490cf Docker 学习资源整理https://zhuanlan.zhihu.com/p/23508637?utm_source=weibo&amp;utm_medium=social 国外特别好的spark学习资源博客： http://data-flair.training/blogs/spark教程总览:http://data-flair.training/blogs/category/spark/bigData教程总览:http://data-flair.training/blogs/category/big-data/hive 教程：http://data-flair.training/blogs/category/hive/ 云栖社区spark 核心技术与实现： https://yq.aliyun.com/topic/69?spm=5176.8279002.620247.3封神的博客 https://yq.aliyun.com/users/1091640433973695?spm=5176.100241.0.0.KpXUK3 spark shuffle 原理深度解析（国外）http://hydronitrogen.com/apache-spark-shuffles-explained-in-depth.html shuffle 金华，原理调优的国内优秀文章http://sharkdtu.com/posts/spark-shuffle.html 优化codegen Tungsten的相关知识https://databricks.com/?s=Tungsten spark国外某大神文章http://blog.ditullio.frhttp://blog.ditullio.fr/2016/01/04/hadoop-basics-total-order-sorting-mapreduce/ 算法爱好者博客http://mp.sohu.com/profile?xpt=YWxnb3JpdGhtZmFuc0Bzb2h1LmNvbQ== 基数估计算法详解http://blog.codinglabs.org/articles/algorithms-for-cardinality-estimation-part-i.htmlbitmap秘密 http://www.cnblogs.com/scott19820130/p/6058677.html 爬虫IP被禁的简单解决办法http://www.cnblogs.com/mooba/p/6484340.html 5Top 50 Hadoop Interview Questions and Answershttp://data-flair.training/blogs/50-hadoop-interview-questions-and-answers/ 50 Frequently Asked Apache Spark Interview Questionshttp://data-flair.training/blogs/50-apache-spark-interview-questions/ 在线工具（各种工具，很丰富很好用）http://tool.lu/ 大数据80篇文章http://lxw1234.com/archives/2016/12/823.htm 中国大数据http://www.thebigdata.cn/]]></content>
      <categories>
        <category>programme</category>
      </categories>
      <tags>
        <tag>资源</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos6编译安装Mysql5.7.18, rpm 安装mysql5.7.18，ubuntu apt安装mysql]]></title>
    <url>%2Flinux%2Fmysql5_17_18-install.html</url>
    <content type="text"><![CDATA[关键字： centos 编译安装mysql5.7.18 rpm安装5.7.18,ubuntu apt -get 安装mysql 一. 编译安装前的准备1.1 卸载原有的Mysql在root用户下操作 找出mysql的相关目录1[root@master hadoop]# find / -name mysql 显示123456789101112131415161718192021222324252627282930313233[root@master hadoop]# find / -name mysql/usr/share/mysql/usr/lib64/mysql``` 通过命令rm -rf将mysql有关目录都删除掉，例如我这里只有上述的两个目录 ```shell[root@master hadoop]# rm -rf /usr/share/mysql[root@master hadoop]# rm -rf /usr/lib64/mysql``` ## **1.2 下载自己想要的版本的mysql** **官网太慢，不建议去下载，建议sohu Mysql的镜像站中下载** http://mirrors.sohu.com/mysql/ 例如我要下载5.7的，那么进入 http://mirrors.sohu.com/mysql/MySQL-5.7/ **Ctrl+F查找到 tar.gz ** ，**找到mysql-boost-5.7.18.tar.gz进行下载** 为什么我要找mysql-boost-5.7.18.tar.gz 而不是 mysql-5.7.18.tar.gz, 因为在Mysql5.7版本更新后有很多变化，比如json等，连安装都有变化，他安装必须要BOOST库，所以我们下载 mysql-boost-5.7.18.tar.gz, 如果是5.6版本，那么就下载 mysql-5.6.35.tar.gz 就可以了 5.5 版本，就下载 mysql-5.5.54.tar.gz 反正自己去镜像库中找就可以了## **1.3 把Mysql的压缩包放在/root下**```shell[root@master Downloads]# mv mysql-boost-5.7.18.tar.gz /root/``` ## **1.4 安装编译源码所需的工具和库** ```shell[root@master Downloads]# yum install gcc gcc-c++ ncurses-devel perl``` ## **1.5 安装cmake编译工具**```shell[root@master Downloads]# yum install cmake -y``` # **二、设置MySQL用户和组** ## **2.1新增mysql用户组**```shell[root@master Downloads]# groupadd mysql 2.2新增mysql用户123456[root@master Downloads]# useradd -r -g mysql mysql``` # **三、新建MySQL所需要的目录**## **3.1新建mysql安装目录** ```shell[root@master Downloads]# mkdir -p /usr/local/mysql 3.2新建mysql数据库数据文件目录1[root@master Downloads]# mkdir -p /data/mysqldb 3.3解压mysql压缩包1234//进入到mysql压缩包的目录下 [root@master Downloads]# cd /root///解压 [root@master ~]# tar -zxvf mysql-boost-5.7.18.tar.gz 3.4进入到mysql目录12345678910111213[root@master ~]# cd mysql-5.7.18``` # **四、编译安装MySQL** ## **4.1 设置编译参数**```shell[root@master mysql-5.7.18]# cmake -DCMAKE_INSTALL_PREFIX=/usr/local/mysql \-DMYSQL_UNIX_ADDR=/usr/local/mysql/mysql.sock -DDEFAULT_CHARSET=utf8 \-DWITH_BOOST=boost -DDEFAULT_COLLATION=utf8_general_ci \-DWITH_INNOBASE_STORAGE_ENGINE=1 -DWITH_ARCHIVE_STORAGE_ENGINE=1 \ -DWITH_BLACKHOLE_STORAGE_ENGINE=1 -DMYSQL_DATADIR=/data/mysqldb \-DMYSQL_TCP_PORT=3306 -DENABLE_DOWNLOADS=1 \``` 当最后有以下显示说明成功， 可能上面有一些 Failed 的报错，但是貌似是正常情况，具体情况有待考究 – Configuring done– Generating done– Build files have been written to: /root/mysql-5.7.18123参数说明： **在mysql-5.7中，多了一个 -DWITH_BOOST=boost** 在其他的版本，不需要 -DWITH_BOOST=boost -DCMAKE_INSTALL_PREFIX=dir_name 设置mysql安装目录-DMYSQL_UNIX_ADDR=file_name 设置监听套接字路径，这必须是一个绝对路径名。默认为/tmp/mysql.sock-DDEFAULT_CHARSET=charset_name 设置服务器的字符集。缺省情况下，MySQL使用latin1的（CP1252西欧）字符集。cmake/character_sets.cmake文件包含允许的字符集名称列表。-DDEFAULT_COLLATION=collation_name 设置服务器的排序规则。-DWITH_INNOBASE_STORAGE_ENGINE=1-DWITH_ARCHIVE_STORAGE_ENGINE=1-DWITH_BLACKHOLE_STORAGE_ENGINE=1-DWITH_PERFSCHEMA_STORAGE_ENGINE=1 存储引擎选项： MyISAM，MERGE，MEMORY，和CSV引擎是默认编译到服务器中，并不需要明确地安装。 静态编译一个存储引擎到服务器，使用-DWITH_engine_STORAGE_ENGINE= 1 可用的存储引擎值有：ARCHIVE, BLACKHOLE, EXAMPLE, FEDERATED, INNOBASE(InnoDB), PARTITION (partitioning support), 和PERFSCHEMA (Performance Schema)-DMYSQL_DATADIR=dir_name 设置mysql数据库文件目录-DMYSQL_TCP_PORT=port_num 设置mysql服务器监听端口，默认为3306-DENABLE_DOWNLOADS=bool12345**注**：重新运行配置，需要删除CMakeCache.txt文件 ## **4.2编译源码** 建议执行完下面的命令，去喝杯茶，休息休息，因为5.7的编译需要好几十分钟（我是看了一集多的权力的游戏才编译完），5.6的可能几分钟就ok了```shell[root@master mysql-5.7.18]# make 4.3 安装1234567[root@master mysql-5.7.18]# make install``` # **五、将mysql目录得所有权交给mysql用户和mysql用户组** ### **5.1修改mysql安装目录**进入到mysql安装目录(上面已经定义了)```shell[root@master mysql-5.7.18]# cd /usr/local/mysql/ 赋予当前目录下所有文件的所有权为mysql, 注意别忘了后面的 . 号1[root@master mysql]# chown -R mysql:mysql . 5.2修改mysql数据库文件目录12[root@master mysql]# cd /data/mysqldb/ [root@master mysqldb]# chown -R mysql:mysql . 六、初始化mysql数据库进入到mysql安装目录1[root@master mysqldb]# cd /usr/local/mysql/ 初始化mysql数据库1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768[root@master mysql]# ./bin/mysql_install_db --user=mysql --datadir=/data/mysqldb/ ``` 注意： 如果是5.7以下的版本，应该这样 // scripts/mysql_install_db --user=mysql --datadir=/data/mysqldb/ 即使是5.7 上述执行后还是报错```shell2017-04-30 21:57:19 [WARNING] mysql_install_db is deprecated. Please consider switching to mysqld --initialize2017-04-30 21:57:48 [WARNING] The bootstrap log isn't empty:2017-04-30 21:57:48 [WARNING] 2017-04-30T13:57:20.320409Z 0 [Warning] --bootstrap is deprecated. Please consider using --initialize instead2017-04-30T13:57:20.329799Z 0 [Warning] Changed limits: max_open_files: 1024 (requested 5000)2017-04-30T13:57:20.329842Z 0 [Warning] Changed limits: table_open_cache: 431 (requested 2000)``` 原因是 之前版本mysql_install_db是在mysql_basedir/script下，5.7放在了mysql_install_db/bin目录下,且已被废弃应该这样```shell[root@master mysql]# bin/mysqld --initialize --user=mysql --datadir=/data/mysqldb/``` 下面会显示```shell2017-04-30T14:28:37.187266Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).2017-04-30T14:28:40.862777Z 0 [Warning] InnoDB: New log files created, LSN=457902017-04-30T14:28:42.292320Z 0 [Warning] InnoDB: Creating foreign key constraint system tables.2017-04-30T14:28:42.589111Z 0 [Warning] No existing UUID has been found, so we assume that this is the first time that this server has been started. Generating a new UUID: 4f97ab23-2db1-11e7-9084-000c293c28f1.2017-04-30T14:28:42.591907Z 0 [Warning] Gtid table is not ready to be used. Table 'mysql.gtid_executed' cannot be opened.2017-04-30T14:28:42.594730Z 1 [Note] A temporary password is generated for root@localhost: OxxO=A0:lB_T``` 由上面的提示可以知道，5.7初始化会自动生成一个初始化密码，我这里的密码是OxxO=A0:lB_T,一定要把密码给记住，以后有用 若是5.5， 5.6版本的，初始化的密码是空 注： 如果需要重新初始化，需要先删除掉数据库data文件夹, 例如我这里```shell[root@master mysqldb]# rm -rf /data/mysqldb``` # **七、复制mysql服务启动配置文件** **5.7和5.6，5.7的不一样，不能像以前一样cp /usr/local/mysql/support-files/my-small.cnf /etc/my.cnf了**。 **需要进入到/etc/my.cnf* 修改datadir=/data/mysqldb socket=/usr/local/mysql/mysql.sock注： /data/mysqldb为前面定义的数据存放目录，/usr/local/mysql/ 是数据安装目录， log-error， pid-file 我暂时没有修改```shell[root@master hadoop]# vim /etc/my.cnf 最终my.cnf如下 [mysqld]# datadir=/var/lib/mysqldatadir=/data/mysqldb#socket=/var/lib/mysql/mysql.socksocket=/usr/local/mysql/mysql.sockuser=mysql# Disabling symbolic-links is recommended to prevent assorted security riskssymbolic-links=0[mysqld_safe]log-error=/var/log/mysqld.logpid-file=/var/run/mysqld/mysqld.pid``` # **八、复制mysql服务启动脚本**```shell[root@master hadoop]# cd /usr/local/mysql/[root@master hadoop]# cp support-files/mysql.server /etc/init.d/mysqld``` # **九、启动mysql服务并加入开机自启动** 启动service mysqld start， 出现OK说明启动成功```shell[root@master mysql]# service mysqld startStarting MySQL [ OK ]``` 加入开机自启动```shell[root@master mysql]# chkconfig --level 35 mysqld on``` # **十、检查mysql服务是否启动** ## **1. 查看端口有没有被占用** [root@master mysql]# netstat -tulnp | grep 3306tcp 0 0 :::3306 :::* LISTEN 2474/mysqld123456789101112131415161718192021222324252627282930313233343536373839## **2. 查看数据库版本** ```shell[root@master mysql]# [root@master etc]# mysql --version有可能出现bash: mysql: command not found``` 这是由于系统默认会查找/usr/bin下的命令，如果这个命令不在这个目录下，当然会找不到命令，我们需要做的就是映射一个链接到/usr/bin目录下，相当于建立一个链接文件, /usr/local/mysql是我们的安装目录 ```shell[root@master mysql]# ln -s /usr/local/mysql/bin/mysql /usr/bin/``` ## **3. 登陆数据库** 账号为-uroot, 密码为前面初始化数据库的时候生成的密码```shell[root@master etc]# mysql -uroot -pOxxO=A0:lB_T``` 登陆成功```shell[root@master etc]# mysql -uroot -pEnter password: ERROR 1045 (28000): Access denied for user &apos;root&apos;@&apos;localhost&apos; (using password: YES)[root@master etc]# mysql -uroot -pOxxO=A0:lB_Tmysql: [Warning] Using a password on the command line interface can be insecure.Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 5Server version: 5.7.18Copyright (c) 2000, 2017, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement.mysql&gt; ``` 查看数据库 mysql&gt; show databases;出现You must reset your password using ALTER USER statement before executing this statement.提示需要重置密码1重置密码，我这里设置成shun mysql&gt; SET PASSWORD = PASSWORD(‘shun’);12345退出重新登陆，成功# **rpm 安装mysql**编译安装实在是太麻烦了，编译按照的好处在于可以自己定制想要的功能，rpm就方便很多## 先下载rpm包，同样的去http://mirrors.sohu.com/mysql/ 去找，也可以去官方网站下载，大概有400兆，然后解压 [root@master hadoop]# wget http://mirrors.sohu.com/mysql/MySQL-5.7/mysql-5.7.18-1.el6.x86_64.rpm-bundle.tar 12## 删除已有的mysql [root@node01 Downloads]# rpm -qa|grep mysqlmysql-libs-5.1.71-1.el6.x86_64[root@node01 Downloads]# rpm -e –nodeps mysql-libs-5.1.71-1.el6.x86_6412## rpm安装，最好按照我这个顺序来 这些全部是上面下载的包解压得到的 [root@node01 Downloads]# rpm -ivh mysql-community-common-5.7.18-1.el6.x86_64.rpm[root@node01 Downloads]# rpm -ivh mysql-community-libs-5.7.18-1.el6.x86_64.rpm[root@node01 Downloads]# rpm -ivh mysql-community-libs-compat-5.7.18-1.el6.x86_64.rpm[root@node01 Downloads]# rpm -ivh mysql-community-client-5.7.18-1.el6.x86_64.rpm[root@node01 Downloads]# rpm -ivh mysql-community-server-5.7.18-1.el6.x86_64.rpm[root@node01 Downloads]# rpm -ivh mysql-community-devel-5.7.18-1.el6.x86_64.rpm[root@node01 Downloads]# rpm -ivh mysql-community-embedded-5.7.18-1.el6.x86_64.rpm[root@node01 Downloads]# rpm -ivh mysql-community-embedded-devel-5.7.18-1.el6.x86_64.rpm[root@node01 Downloads]# rpm -ivh mysql-community-test-5.7.18-1.el6.x86_64.rpm 1安装过程中如果报下面的错误，说明缺少一些包 warning: mysql-community-test-5.7.18-1.el6.x86_64.rpm: Header V3 DSA/SHA1 Signature, key ID 5072e1f5: NOKEYerror: Failed dependencies: perl(JSON) is needed by mysql-community-test-5.7.18-1.el6.x86_64 perl(Time::HiRes) is needed by mysql-community-test-5.7.18-1.el6.x86_64 12**yum 安装依赖包** [root@node01 Downloads]# yum install perl -y[root@node01 Downloads]# yum install perl-JSON.noarchyum install perl-Time-HiRes12## 初始化数据库 [root@node01 Downloads]# mysqld –initialize –user=mysql如果要自定义目录 –datadir=你要设置的目录12## 查看生成的密码 [root@node01 log]# vim /var/log/mysqld.log显示如下，我的用户名是root, 密码是 0a.efyjoekuF2017-05-15T07:30:10.907363Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use –explicit_defaults_for_timestamp server option (see documentation for more details).2017-05-15T08:11:33.040718Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use –explicit_defaults_for_timestamp server option (see documentation for more details).2017-05-15T08:11:34.346242Z 0 [Warning] InnoDB: New log files created, LSN=457902017-05-15T08:11:34.547761Z 0 [Warning] InnoDB: Creating foreign key constraint system tables.2017-05-15T08:11:34.760728Z 0 [Warning] No existing UUID has been found, so we assume that this is the first time that this server has been started. Generating a new UUID: 1c8d4b8e-3946-11e7-a555-000c29ad881f.2017-05-15T08:11:34.836260Z 0 [Warning] Gtid table is not ready to be used. Table ‘mysql.gtid_executed’ cannot be opened.2017-05-15T08:11:34.838073Z 1 [Note] A temporary password is generated for root@localhost: kwjoAoUmF1!e123如果有问题，想重新初始化数据库，需要删除datadir 的目录(没设置就是默认目录)和/var/lib/mysql 目录## 启动数据库 [root@node01 log]# service mysqld startInitializing MySQL database: [ OK ]Installing validate password plugin: [ OK ]Starting mysqld: [ OK ] 123## 查看端口 [root@node01 log]# netstat -tulnp | grep 3306tcp 0 0 :::3306 :::* LISTEN 5255/mysqld12## 登录 [root@node01 mysql]# mysql -uroot -p0a.efyjoekuF 1## 改密码 SET PASSWORD = PASSWORD(‘你的密码’);12345678910111213# **Ubuntu apt 按照mysql5.6, 5.7** 很久没用ubuntu了，貌似ubuntu的还不支持5.7.18，但是apt 有5.7以上的版本了，ubuntu 安装mysql 非常的简单参考 http://blog.csdn.net/t1dmzks/article/details/52079791```shellsudo apt-get install mysql-5.6更新软件源 sudo apt-get update``` 按照提示一路走即可**安装Mysql5.7** sudo apt-get install mysql-server-5.7``` 希望各位都能顺利的安装成mysql由于作者水平有限,本文章错漏缺点在所难免,希望读者批评指正。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos虚拟机网络配置NAT模式和桥接模式总结]]></title>
    <url>%2Flinux%2Flinux-NAT-Bridge.html</url>
    <content type="text"><![CDATA[每次装虚拟机，都需要配置网络IP，或者是装hadoop，zookeeper等各种集群服务器的时候，都需要配置网络，每次都还挺麻烦的，需要网上各种经验，本人也配置过很多次了，但是还是需要去网上找资料，还各种容易出问题，这次重装电脑，就做个笔记吧 NAT模式如果想玩NAT模式：先把该虚拟机设置成NAT模式： 虚拟机设置-&gt;网络适配器-&gt;右边选择NAT模式 修改/etc/sysconfig/network-scripts/ifcfg-eth0的内容为:12345678910111213141516171819202122232425DEVICE=eth0HWADDR=00:0C:29:3D:7C:7ATYPE=EthernetUUID=e204d9f9-c272-4bc2-80e7-6b0687aaad45ONBOOT=yesNM_CONTROLLED=yesBOOTPROTO=dhcpNETMASK=255.255.255.0IPV$_FALLURE_FATAL=yesDNS1=8.8.8.8IPADDR=192.168.221.128PREFIX=24GATEWAY=192.168.221.2``` 上面的参数怎么设置的呢？ BOOTPROTO=dhcp 说明是动态的分配ip了 DNS1=8.8.8.8 是必须这样设置的，这是一台位于美国的DNS服务器地址 GATEWAY=192.168.221.2 这是怎么知道的呢？ VNware-&gt;编辑-&gt;虚拟网络编辑器-&gt;选择NAT模式这个-&gt;选择下面NAT设置就可以看到网关IP了 ![查看网关](http://or49tneld.bkt.clouddn.com/17-10-7/65903862.jpg) IPADDR怎么设置呢？ 上面那个图，还有个DHCP设置，点击可以看到开始IP地址和结束IP地址，在这两个IP中间随便选一个就好了 ![虚拟网络编辑2](http://or49tneld.bkt.clouddn.com/17-10-7/34561050.jpg) ## /etc/resolv.conf的修改 nameserver 8.8.8.81234567service network restart 尝试看看是否成功了呢？ 不成功再继续看其他文章怎么设置吧，可能自己本地也要设置IP共享之类的，具体网上找吧 ## 桥接模式 桥接模式可以设置静态IP了，好处就是另外一台服务器，也可以通过这个ip连接到虚拟机中，在玩分布式的时候，多台电脑之间一起玩的时候还是很有用的。 方法就是将NAT设置成桥接模式。 先在本地cmd看一下自己的ip是多少，然后设置Linux的ifcfg-eth0的内容 vim /etc/sysconfig/network-scripts/ifcfg-eth0 DEVICE=eth0HWADDR=00:0C:29:3D:7C:7ATYPE=EthernetUUID=e204d9f9-c272-4bc2-80e7-6b0687aaad45ONBOOT=yesNM_CONTROLLED=yes #BOOTPROTO=dhcpBOOTPROTO=static NETMASK=255.255.255.0BROADCAST=192.168.1.255IPV4_FALLURE_FATAL=yesDNS1=8.8.8.8IPADDR=192.168.1.31PREFIX=24GATEWAY=192.168.1.112GATEWAY一般都是本地cmd，然后ipconfig查看的 以太网适配器 本地连接: 连接特定的 DNS 后缀 . . . . . . . : 本地链接 IPv6 地址. . . . . . . . : fe80::38a6:2 IPv4 地址 . . . . . . . . . . . . : 192.168.1.10 子网掩码 . . . . . . . . . . . . : 255.255.255. 默认网关. . . . . . . . . . . . . : 192.168.1.1 ```NETMASK=255.255.255.0和DNS1=8.8.8.8必须这样填了BOOTPROTO=static 说明是静态的获取ip地址IPADDR=192.168.1.31 这个是自己设置的，但要注意前三位和网关的前3位一样，并且这个不能和本地的ip重复BROADCAST=192.168.1.255 前3位和网关一样，最后一位设置成255HWADDR 这个就是mac地址了，一般不用改变，这个也可以这样找到：虚拟机设置-&gt;网络适配器-&gt;右下角高级，然后就可以看到MAC地址了 注意： 克隆一个新的虚拟机的时候，ifcfg-eth0的这个HWADDR是要修改的哦，否则可能会有问题。 然后 service network restart 就OK了。ping www.baidu.com 试一试? 若还不行，就再google或者百度吧]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos安装screen ubuntu安装screen 编译安装screen]]></title>
    <url>%2Flinux%2Flinux-screen-install.html</url>
    <content type="text"><![CDATA[yun安装：1yum install screen ubuntu 的 apt-get安装12sudo apt-get updatesudo apt-get install screen 编译安装为什么我需要编译安装，因为我所操作的集群不能连外网 tar.gz 下载地址：https://ftp.gnu.org/gnu/screen/ 解压：略 编译12345cd screen-4.6.2./configure make 安装, 这一步需要root权限12make install install -m 644 etc/etcscreenrc /etc/screenrc 查看是否安装成功123which screen 出现 /bin/screen 则成功 碰到的坑刚开始不敢直接上真实环境操作，所以先使用了一个VPS自己来使用编译安装的方式安装 ./config报错1no acceptable C compiler found in $PATH 网上说是需要安装gcc套件（qqtt: 别问我怎么联网的）1yum -y install gcc 后来还是另外一个错1configure: error: !!! no tgetent - no screen 真是日狗了，这时需要安装ncurses-devel1yum -y install ncurses-devel 可能是VPS上面什么都没有安装吧，后来去集群环境上，上面的依赖都是有的，安装的很顺利。 参考了官方的安装文档：http://www.gnu.org/software/screen/manual/screen.html#Compiling-Screen参考了国外的一篇文章： http://www.linuxfromscratch.org/blfs/view/cvs/general/screen.html强烈推荐看看第二篇文章，源码编译指定自己想要放置的路径，我这里设置的都是默认的路径，没这么复杂。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【多线程高并发】ThreadLocal，高并发下的单例模式]]></title>
    <url>%2F%E6%9E%B6%E6%9E%84%2Fthread7.html</url>
    <content type="text"><![CDATA[2.3 ThreadLocalThreadLocal概念: 线程局部变量，是一种多线程间并发访问变量的解决方案。与其synchronized等加锁方式不同，THreadLocal完全不提供锁，而使用空间换时间的手段，为每个线程提供变量的独立副本，以保障线程安全。在高并发量或者竞争激烈的场景，使用ThreadLocal可以在一定程度上减少锁竞争。当使用ThreadLocal维护变量时，ThreadLocal为每个使用该变量的线程提供独立的变量副本，所以每一个线程都可以独立地改变自己的副本，而不会影响其它线程所对应的副本。案例：每个线程一个独立的变量副本，所以t2线程执行为null123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101public class ConnThreadLocal &#123; public static ThreadLocal&lt;String&gt; th = new ThreadLocal&lt;String&gt;(); public void setTh(String value)&#123; th.set(value); &#125; public void getTh()&#123; System.out.println(Thread.currentThread().getName() + ":" + this.th.get()); &#125; public static void main(String[] args) throws InterruptedException &#123; final ConnThreadLocal ct = new ConnThreadLocal(); Thread t1 = new Thread(new Runnable() &#123; @Override public void run() &#123; ct.setTh("张三"); ct.getTh(); &#125; &#125;, "t1"); Thread t2 = new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; Thread.sleep(1000); ct.getTh(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;, "t2"); t1.start(); t2.start(); &#125; &#125;----------输出------------t1:张三t2:null``` ## 2.4 高并发下的单例模式 **懒汉模式** 需要两层的if判断 ```javapublic class DubbleSingleton &#123; private static DubbleSingleton ds; public static DubbleSingleton getDs()&#123; if(ds == null)&#123; try &#123; //模拟初始化对象的准备时间... Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; synchronized (DubbleSingleton.class) &#123; if(ds == null)&#123; ds = new DubbleSingleton(); &#125; &#125; &#125; return ds; &#125; public static void main(String[] args) &#123; Thread t1 = new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println(DubbleSingleton.getDs().hashCode()); &#125; &#125;,"t1"); Thread t2 = new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println(DubbleSingleton.getDs().hashCode()); &#125; &#125;,"t2"); Thread t3 = new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println(DubbleSingleton.getDs().hashCode()); &#125; &#125;,"t3"); t1.start(); t2.start(); t3.start(); &#125;&#125;------输出的hashCode码是一致的----------219368352193683521936835``` **饿汉模式** public class Singletion { private static class InnerSingletion { private static Singletion single = new Singletion(); } public static Singletion getInstance(){ return InnerSingletion.single; } }```]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【多线程高并发】java锁的高级]]></title>
    <url>%2F%E6%9E%B6%E6%9E%84%2Fthread6.html</url>
    <content type="text"><![CDATA[关键字：Concurrent.util常用类，CountDownLacth，CyclicBarrier，Callable和Future， 重入锁ReentrantLock， 锁的等待、通知，lock锁， 单Condition，多Condition,ReentrantReadWriteLock 读写锁, github 地址： https://github.com/zhaikaishun/concurrent_programming本篇文章代码在Multi_006 中 Concurrent.util常用类CountDownLacth使用:它经常用于监听某些初始化操作，等初始化执行完毕后通知主线程继续工作。举例com.kaishun.height.concurrent019下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class UseCountDownLatch &#123; public static void main(String[] args) &#123; final CountDownLatch countDown = new CountDownLatch(2); Thread t1 = new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; System.out.println(&quot;进入线程t1&quot; + &quot;等待其他线程处理完成...&quot;); countDown.await(); System.out.println(&quot;t1线程继续执行...&quot;); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;,&quot;t1&quot;); Thread t2 = new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; System.out.println(&quot;t2线程进行初始化操作...&quot;); Thread.sleep(3000); System.out.println(&quot;t2线程初始化完毕，通知t1线程继续...&quot;); countDown.countDown(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); Thread t3 = new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; System.out.println(&quot;t3线程进行初始化操作...&quot;); Thread.sleep(4000); System.out.println(&quot;t3线程初始化完毕，通知t1线程继续...&quot;); countDown.countDown(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); t1.start(); t2.start(); t3.start(); &#125;&#125; CyclicBarrier使用:假设有只有的一个场景：每个线程代表一个跑步运动员，当运动员都准备好后，才一起出发，只要有一个人没准备好，大家都等待举例：UseCyclicBarrier1234567891011121314151617181920212223242526272829303132333435363738394041424344public class UseCyclicBarrier &#123; static class Runner implements Runnable &#123; private CyclicBarrier barrier; private String name; public Runner(CyclicBarrier barrier, String name) &#123; this.barrier = barrier; this.name = name; &#125; @Override public void run() &#123; try &#123; Thread.sleep(1000 * (new Random()).nextInt(5)); System.out.println(name + " 准备OK."); barrier.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (BrokenBarrierException e) &#123; e.printStackTrace(); &#125; System.out.println(name + " Go!!"); &#125; &#125; public static void main(String[] args) throws IOException, InterruptedException &#123; CyclicBarrier barrier = new CyclicBarrier(3); // 3 ExecutorService executor = Executors.newFixedThreadPool(3); executor.submit(new Thread(new Runner(barrier, "zhangsan"))); executor.submit(new Thread(new Runner(barrier, "lisi"))); executor.submit(new Thread(new Runner(barrier, "wangwu"))); executor.shutdown(); &#125; &#125; -------输出-------------lisi 准备OK.zhangsan 准备OK.wangwu 准备OK.wangwu Go!!lisi Go!!zhangsan Go!! Callable和Future使用这个例子其实就是我们之前实现的Future模式，jdk给与我们衣蛾实现的封装，使用非常简单， Future模式非常适合在处理耗时很长的业务逻辑时使用，可以有效地减少系统的响应时间，提高系统的吞吐量。示例：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class UseFuture implements Callable&lt;String&gt;&#123; private String para; public UseFuture(String para)&#123; this.para = para; &#125; /** * 这里是真实的业务逻辑，其执行可能很慢 */ @Override public String call() throws Exception &#123; //模拟执行耗时 Thread.sleep(5000); String result = this.para + "处理完成"; return result; &#125; //主控制函数 public static void main(String[] args) throws Exception &#123; String queryStr = "query"; //构造FutureTask，并且传入需要真正进行业务逻辑处理的类,该类一定是实现了Callable接口的类 FutureTask&lt;String&gt; future = new FutureTask&lt;String&gt;(new UseFuture(queryStr)); FutureTask&lt;String&gt; future2 = new FutureTask&lt;String&gt;(new UseFuture(queryStr)); //创建一个固定线程的线程池且线程数为1, ExecutorService executor = Executors.newFixedThreadPool(2); //这里提交任务future,则开启线程执行RealData的call()方法执行 //submit和execute的区别： 第一点是submit可以传入实现Callable接口的实例对象， 第二点是submit方法有返回值 Future f1 = executor.submit(future); //单独启动一个线程去执行的 Future f2 = executor.submit(future2); System.out.println("请求完毕"); try &#123; //这里可以做额外的数据操作，也就是主程序执行其他业务逻辑 System.out.println("处理实际的业务逻辑..."); Thread.sleep(1000); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; //调用获取数据方法,如果call()方法没有执行完成,则依然会进行等待 System.out.println("数据：" + future.get()); System.out.println("数据：" + future2.get()); executor.shutdown(); &#125;&#125;-----------输出-------------------请求完毕处理实际的业务逻辑...数据：query处理完成数据：query处理完成 重入锁ReentrantLock重入锁，在需要进行同步的代码部分加上锁定，但不要忘记最后一定要释放锁定，不然会造成锁永远无法释放，其他线程永远进不来的结果。 【com.kaishun.height.lock020.UseReentrantLock】 使用方法： 实例化一个锁： Lock lock = new ReentrantLock(); 在需要加锁的地方使用lock.lock(); 记住加锁的代码需要加上try catch finally , finally的时候，一定要释放锁 lock.unlock 举例：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public class UseReentrantLock &#123; private Lock lock = new ReentrantLock(); public void method1()&#123; try &#123; lock.lock(); System.out.println("当前线程:" + Thread.currentThread().getName() + "进入method1.."); Thread.sleep(1000); System.out.println("当前线程:" + Thread.currentThread().getName() + "退出method1.."); Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; public void method2()&#123; try &#123; lock.lock(); System.out.println("当前线程:" + Thread.currentThread().getName() + "进入method2.."); Thread.sleep(2000); System.out.println("当前线程:" + Thread.currentThread().getName() + "退出method2.."); Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; public static void main(String[] args) &#123; final UseReentrantLock ur = new UseReentrantLock(); Thread t1 = new Thread(new Runnable() &#123; @Override public void run() &#123; ur.method1(); ur.method2(); &#125; &#125;, "t1"); t1.start(); try &#123; Thread.sleep(10); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; //System.out.println(ur.lock.getQueueLength()); &#125; &#125;-----------输出----------------当前线程:t1进入method1..当前线程:t1退出method1..当前线程:t1进入method2..当前线程:t1退出method2.. 锁的等待、通知还记得我们在使用synchronized的时候，如果需要多线程间进行协作工作则需要Object的wait()和notify方法进行配合工作。那么同样，我们在使用Lock的时候，可以使用一个新的等待、通知的类，他就是Condition, 这个份Cibdutuib一定是针对具体某一吧锁的。也就是只有在有锁的情况下才会产生Condition.使用方法： Condition condition = lock.newCondition(); 等待调用condition.await(); 唤醒调用condition.signal(); 单Condition举例说明：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public class UseCondition &#123; private Lock lock = new ReentrantLock(); private Condition condition = lock.newCondition(); public void method1()&#123; try &#123; lock.lock(); System.out.println("当前线程：" + Thread.currentThread().getName() + "进入等待状态.."); Thread.sleep(3000); System.out.println("当前线程：" + Thread.currentThread().getName() + "释放锁.."); condition.await(); // Object wait System.out.println("当前线程：" + Thread.currentThread().getName() +"继续执行..."); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; public void method2()&#123; try &#123; lock.lock(); System.out.println("当前线程：" + Thread.currentThread().getName() + "进入.."); Thread.sleep(3000); System.out.println("当前线程：" + Thread.currentThread().getName() + "发出唤醒.."); condition.signal(); //Object notify &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; public static void main(String[] args) &#123; final UseCondition uc = new UseCondition(); Thread t1 = new Thread(new Runnable() &#123; @Override public void run() &#123; uc.method1(); &#125; &#125;, "t1"); Thread t2 = new Thread(new Runnable() &#123; @Override public void run() &#123; uc.method2(); &#125; &#125;, "t2"); t1.start(); t2.start(); &#125;&#125;``` t1线程进入method1，然后wait释放锁， t2线程得到锁唤醒了t1 输出结果： 当前线程：t1进入等待状态..当前线程：t1释放锁..当前线程：t2进入..当前线程：t2发出唤醒..当前线程：t1继续执行…123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132### 多Condition我们可以通过一个Lock对象产生多个Condition进行多线程间的交互，非常的灵活。可以使得部分需要唤醒的线程被唤醒，其他线程则继续等待通知。 例如下面这个例子，我们队一个lock，new出了2个Condition 一个是c1一个是c2 . m1和m2方法使用c1.wait。 m3方法使用c2.wait。 m4方法唤醒了c1.signalAll， m5方法唤醒的是c2.signal代码： ```javapublic class UseManyCondition &#123; private ReentrantLock lock = new ReentrantLock(); private Condition c1 = lock.newCondition(); private Condition c2 = lock.newCondition(); public void m1()&#123; try &#123; lock.lock(); System.out.println(&quot;当前线程：&quot; +Thread.currentThread().getName() + &quot;进入方法m1等待..&quot;); c1.await(); System.out.println(&quot;当前线程：&quot; +Thread.currentThread().getName() + &quot;方法m1继续..&quot;); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; public void m2()&#123; try &#123; lock.lock(); System.out.println(&quot;当前线程：&quot; +Thread.currentThread().getName() + &quot;进入方法m2等待..&quot;); c1.await(); System.out.println(&quot;当前线程：&quot; +Thread.currentThread().getName() + &quot;方法m2继续..&quot;); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; public void m3()&#123; try &#123; lock.lock(); System.out.println(&quot;当前线程：&quot; +Thread.currentThread().getName() + &quot;进入方法m3等待..&quot;); c2.await(); System.out.println(&quot;当前线程：&quot; +Thread.currentThread().getName() + &quot;方法m3继续..&quot;); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; public void m4()&#123; try &#123; lock.lock(); System.out.println(&quot;当前线程：&quot; +Thread.currentThread().getName() + &quot;唤醒..&quot;); c1.signalAll(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; public void m5()&#123; try &#123; lock.lock(); System.out.println(&quot;当前线程：&quot; +Thread.currentThread().getName() + &quot;唤醒..&quot;); c2.signal(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; public static void main(String[] args) &#123; final UseManyCondition umc = new UseManyCondition(); Thread t1 = new Thread(new Runnable() &#123; @Override public void run() &#123; umc.m1(); &#125; &#125;,&quot;t1&quot;); Thread t2 = new Thread(new Runnable() &#123; @Override public void run() &#123; umc.m2(); &#125; &#125;,&quot;t2&quot;); Thread t3 = new Thread(new Runnable() &#123; @Override public void run() &#123; umc.m3(); &#125; &#125;,&quot;t3&quot;); Thread t4 = new Thread(new Runnable() &#123; @Override public void run() &#123; umc.m4(); &#125; &#125;,&quot;t4&quot;); Thread t5 = new Thread(new Runnable() &#123; @Override public void run() &#123; umc.m5(); &#125; &#125;,&quot;t5&quot;); t1.start(); // c1 t2.start(); // c1 t3.start(); // c2 try &#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; t4.start(); // c1 try &#123; Thread.sleep(10000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; t5.start(); // c2 &#125;&#125; 输出先输出123456当前线程：t1进入方法m1等待..当前线程：t3进入方法m3等待..当前线程：t2进入方法m2等待..``` 2秒后输出 当前线程：t4唤醒..当前线程：t1方法m1继续..当前线程：t2方法m2继续..110秒后输出 当前线程：t5唤醒..当前线程：t3方法m3继续..123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172## ReentrantReadWriteLock 读写锁 读写锁ReentrantReadWriteLock, 其核心就是实现读写分离的锁，在高并发访问下，尤其是读多写少的情况下，性能要远高于重入锁。 之前学synchronized, ReentrantLock时，我们知道，同一时间内，只能有一个线程进行访问被锁定的代码，而读写锁不同，在读锁，多个线程可以并发的访问，而在写锁的时候，只能一个一个顺序的访问 口诀： 读读共享， 写写互斥， 读写互斥。 举例： ```javapublic class UseReentrantReadWriteLock &#123; private ReentrantReadWriteLock rwLock = new ReentrantReadWriteLock(); private ReadLock readLock = rwLock.readLock(); private WriteLock writeLock = rwLock.writeLock(); public void read()&#123; try &#123; readLock.lock(); System.out.println(&quot;当前线程:&quot; + Thread.currentThread().getName() + &quot;进入...&quot;); Thread.sleep(3000); System.out.println(&quot;当前线程:&quot; + Thread.currentThread().getName() + &quot;退出...&quot;); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; readLock.unlock(); &#125; &#125; public void write()&#123; try &#123; writeLock.lock(); System.out.println(&quot;当前线程:&quot; + Thread.currentThread().getName() + &quot;进入...&quot;); Thread.sleep(3000); System.out.println(&quot;当前线程:&quot; + Thread.currentThread().getName() + &quot;退出...&quot;); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; writeLock.unlock(); &#125; &#125; public static void main(String[] args) &#123; final UseReentrantReadWriteLock urrw = new UseReentrantReadWriteLock(); Thread t1 = new Thread(new Runnable() &#123; @Override public void run() &#123; urrw.read(); &#125; &#125;, &quot;t1&quot;); Thread t2 = new Thread(new Runnable() &#123; @Override public void run() &#123; urrw.read(); &#125; &#125;, &quot;t2&quot;); Thread t3 = new Thread(new Runnable() &#123; @Override public void run() &#123; urrw.write(); &#125; &#125;, &quot;t3&quot;); Thread t4 = new Thread(new Runnable() &#123; @Override public void run() &#123; urrw.write(); &#125; &#125;, &quot;t4&quot;); &#125;&#125; 当运行下面语句时输出,读和读可以并发运行12345678910 t1.start(); t2.start();---------输出----------当前线程:t2进入...当前线程:t1进入...当前线程:t1退出...当前线程:t2退出...``` 当运行下面读和写两个线程时, 读写互斥 t1.start(); // R t3.start(); // W ———-输出———当前线程:t1进入…当前线程:t1退出…当前线程:t3进入…当前线程:t3退出…123456789当运行两个写的时候，写写互斥 ```java t3.start(); t4.start();--------输出---------当前线程:t3进入...当前线程:t3退出...当前线程:t4进入...当前线程:t4退出... 特别感谢互联网架构师白鹤翔老师，本文大多出自他的视频讲解。笔者主要是记录笔记，以便之后翻阅，正所谓好记性不如烂笔头，烂笔头不如云笔记]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【多线程高并发】java线程池]]></title>
    <url>%2F%E6%9E%B6%E6%9E%84%2Fthread5.html</url>
    <content type="text"><![CDATA[关键字：：Executor框架, newFixedThreadPool,newSingleThreadExecutor,newCacheThreadPool,newScheduledThreadPool, ThreadPoolExecutor详解github 地址： https://github.com/zhaikaishun/concurrent_programming本篇文章代码在Multi_005 中 Executor框架为了更好的控制多线程， JDK提供了一套线程框架Executor, 帮助开发人员有效地进行线程控制。他们都在java.util.concurrent包中， 是JDK并发包的核心，其中有一个比较重要的类： Executors，他扮演者线程工厂的角色，我们通过Executors可以创建特定功能的线程池。 Executors创建线程的方法 newFixedThreadPool()方法，该方法返回一个固定数量的线程池，该方法的线程数始终不变，当有一个任务提交时，若线程池中空闲，则立即执行，若没有，则会被暂缓在一个任务队列中等待有空闲的线程去执行。 newSingleThreadExecutor()方法，创建一个线程的线程池，若空闲则执行，若没有空闲线程则暂缓在队列任务中。 newCacheThreadPool()方法，返回一个可以根据实际情况调整线程个数的线程池，不限制最大线程量，若有空闲线程则执行任务，若无任务则不创建线程，并且每一个空闲线程会在60秒后自动回收。 newScheduledThreadPool()方法，该方法返回一个SchededExecutorService对象，但该线程池可以执行线程的数量 ThreadPoolExecutor()其实上面的几种线程池都是通过ThreadPoolExecutor()来实例出来的，如果上面的还不满足我们的要求，我们可以自己创建线程池，ThreadPoolExecutor有几个重载方法：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124 public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue) public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory) public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, RejectedExecutionHandler handler) public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler)``` ![线程池](http://img.blog.csdn.net/20160724192641221) - **corePoolSize** 核心线程数，如果不进行特别的设定，线程池中始终会保持corePoolSize数量的线程数（一创建出来就有这么多个线程） - **maximumPoolSize** 最大允许存在的线程数: 包括核心线程数和非核心线程数,一旦任务数量过多（由等待队列的特性决定），线程池将创建“非核心线程”临时帮助运行任务。你设置的大于corePoolSize参数小于maximumPoolSize参数的部分，就是线程池可以临时创建的“非核心线程”的最大数量。这种情况下如果某个线程没有运行任何任务，在等待keepAliveTime时间后，这个线程将会被销毁，直到线程池的线程数量重新达到corePoolSize。 - **keepAliveTime参数和timeUnit参数**: 配合使用,代表空闲线程的回收时间，如果设置的corePoolSize参数和设置的maximumPoolSize参数一致时，线程池在任何情况下都不会回收空闲线程。keepAliveTime和timeUnit也就失去了意义。 - **workQueue** 等待队列： 当线程数过多时，多余的线程放入到队列等待执行。 - **RejectedExecutionHandler** 拒绝策略,当提交给线程池的某一个新任务无法直接被线程池中“核心线程”直接处理，又无法加入等待队列，也无法创建新的线程执行, 这时候ThreadPoolExecutor线程池会拒绝处理这个任务，触发创建ThreadPoolExecutor线程池时定义的RejectedExecutionHandler接口的实现. 示例： 这个案例简要介绍了如何使用自定义ThreadPoolExecutor **主方法** ```javapublic class UseThreadPoolExecutor1 &#123; public static void main(String[] args) &#123; /** * 在使用有界队列时，若有新的任务需要执行，如果线程池实际线程数小于corePoolSize，则优先创建线程， * 若大于corePoolSize，则会将任务加入队列， * 若队列已满，则在总线程数不大于maximumPoolSize的前提下，创建新的线程， * 若线程数大于maximumPoolSize，则执行拒绝策略。或其他自定义方式。 * */ ThreadPoolExecutor pool = new ThreadPoolExecutor( 1, //coreSize 2, //MaxSize 60, //60 TimeUnit.SECONDS, new ArrayBlockingQueue&lt;Runnable&gt;(3) //指定一种队列 （有界队列） //new LinkedBlockingQueue&lt;Runnable&gt;() , new MyRejected() //, new DiscardOldestPolicy() ); MyTask mt1 = new MyTask(1, "任务1"); MyTask mt2 = new MyTask(2, "任务2"); MyTask mt3 = new MyTask(3, "任务3"); MyTask mt4 = new MyTask(4, "任务4"); MyTask mt5 = new MyTask(5, "任务5"); MyTask mt6 = new MyTask(6, "任务6"); pool.execute(mt1); pool.execute(mt2); pool.execute(mt3); pool.execute(mt4); pool.execute(mt5); pool.execute(mt6); pool.shutdown(); &#125;&#125;``` **MyTask类** ```javapublic class MyTask implements Runnable &#123; private int taskId; private String taskName; public MyTask(int taskId, String taskName)&#123; this.taskId = taskId; this.taskName = taskName; &#125; public int getTaskId() &#123; return taskId; &#125; public void setTaskId(int taskId) &#123; this.taskId = taskId; &#125; public String getTaskName() &#123; return taskName; &#125; public void setTaskName(String taskName) &#123; this.taskName = taskName; &#125; @Override public void run() &#123; try &#123; System.out.println("run taskId =" + this.taskId); Thread.sleep(5*1000); //System.out.println("end taskId =" + this.taskId); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; public String toString()&#123; return Integer.toString(this.taskId); &#125;&#125; 拒绝策略的类MyRejected1234567891011public class MyRejected implements RejectedExecutionHandler&#123; public MyRejected()&#123; &#125; @Override public void rejectedExecution(Runnable r, ThreadPoolExecutor executor) &#123; System.out.println("自定义处理.."); System.out.println("当前被拒绝任务为：" + r.toString()); &#125;&#125; 输出因为2+3 = 5 而总共有6个任务，所以最后一个被拒绝，走了MyRejecte类的rejectedExecution方法1234567自定义处理..run taskId =1当前被拒绝任务为：6run taskId =5run taskId =2run taskId =3run taskId =4 如果没有定义拒绝策略，出现拒绝时会catch RejectedExecutionException 异常 再举个例子深入了解一下coresize和maxsize：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public class UseThreadPoolExecutor2 implements Runnable&#123; private static AtomicInteger count = new AtomicInteger(0); @Override public void run() &#123; try &#123; int temp = count.incrementAndGet(); System.out.println("任务" + temp); Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; public static void main(String[] args) throws Exception&#123; //System.out.println(Runtime.getRuntime().availableProcessors()); BlockingQueue&lt;Runnable&gt; queue = //new LinkedBlockingQueue&lt;Runnable&gt;(); new ArrayBlockingQueue&lt;Runnable&gt;(10); ExecutorService executor = new ThreadPoolExecutor( 5, //core 10, //max 120L, //2fenzhong TimeUnit.SECONDS, queue); for(int i = 0 ; i &lt; 21; i++)&#123; executor.execute(new UseThreadPoolExecutor2()); &#125; Thread.sleep(1000); System.out.println("queue size:" + queue.size()); //10 Thread.sleep(2000); &#125;&#125;``` **输出** 说明是一次执行10个，然后再从队列取出来执行，每次线程池只能维护10个线程```java任务1任务3任务2任务4任务5任务6任务7任务8任务9任务10queue size:10任务11任务12任务13任务14任务15任务16任务17任务20任务19任务18 特别感谢互联网架构师白鹤翔老师，本文大多出自他的讲解。笔者主要是记录笔记，以便之后翻阅，正所谓好记性不如烂笔头，烂笔头不如云笔记]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【多线程高并发】 同步容器和队列]]></title>
    <url>%2F%E6%9E%B6%E6%9E%84%2Fthread4.html</url>
    <content type="text"><![CDATA[关键字: 同步容器，队列，ConcurrentMap, Copy-On-Write容器,并发Queue, ConcurrentLinkedQueue, BlockQueue接口, ArrayBlockingQueue , LinkedBlockingQueue, PriorityBlockingQueue, DelayQueue, SynchronousQueuegithub地址： https://github.com/zhaikaishun/concurrent_programming代码主要在 Multi_003 , Multi_003\Multi_003\src\com\kaishun\base\coll013 1.1 同步容器同步容器都是线程安全的，但是在某些场景下可能需要加锁来保护复合操作，复合类操作如 迭代，跳转，条件运算等，这些复合操作在多线程并发执行的时候。可能会出现意外行为，最经典的便是ConcurrentModificationException，原因是当容器迭代的过程中，被并发的修改了内容。同步类容器： 如古老的Vector，HashTable。这些容器的同步功能其实都是由JDK的Collections.synchronized等工厂方法去实现的，其底层的机制无非就是用传统的synchronized关键字对每个公用的方法都进行同步，使得每次只能有一个线程访问容器的状态。这很明显不符合今天互联网时代高并发的需求，在保证线程安全的同时，也必须要有足够好的性能。 JDK5.0以后提供了多种并发容器来替代同步容器从而改善性能。并发容器是专门针对并发设计的，使用ConcurrentHashMap来代替给予散列的HashTable，而且在ConcurrentHashMap中，添加了一些常见复合操作的支持，以及使用CopyOnWriteArrayList代替Voctor, 并发的CopyOnWriteArraySet, 以及并发的Queue， ConcurrentLinkedQueue和LinkedBlockingQueue, 前者是高性能的队列，后者是以阻塞形式的队列，具体实现Queue还有很多，例如ArrayBlockingQueue, PriorityBlockingQueue, SynchronousQueue等。 1.2 ConcurrentMapConcurrentMap接口有两个重要的实现 ConcurrentHashMap ConcurrentSkipListMap(支持并发排序功能，弥补ConcurrnetHashMap)ConcurrentHashMap主要是利用了Segment(段)的方式，来减小锁的粒度，从而实现提高并发性能的机制, 最大可以分成16段。并且代码中大多共享变量使用Volatile关键字声明，目的是第一时间获取修改的内容，性能非常好如图，传统的HashTable，只有一段，对整个map进行加锁，锁的粒度比较大。而CorrentHashMap, 对这个map的某一个小Segment来进行加锁，在哪一段操作，只锁定哪一个段，其他段不影响，锁的粒度比较小，从而提高并发的性能 具体如何使用，和之前的HashMap几乎是一模一样的，还是随便看个例子吧12345ConcurrentHashMap&lt;String, Object&gt; chm = new ConcurrentHashMap&lt;String, Object&gt;();chm.put("k1", "v1");chm.put("k2", "v2");chm.put("k3", "v3");chm.putIfAbsent("k4", "vvvv"); // 如果key不存在，就加进去 1.3 Copy-On-Write容器Copy-On-Write简称COW， 是一种用于程序设计中的优化策略。Copy容器即写时复制的容器，先将当前容器进行Copy,复制出一个新的容器，然后想信的容器里面添加元素，添加完后，在讲原容器的引用指向新的容器。这样的好处是我们可以对CopyOnWrite容器进行并发的读而不用加锁，因为当前容器不会添加任何元素，所以CopyOnWrite也是一种读写分离的思想，读和写不同的容器，适用于读多写少的场景。JDK里的COW容器有两种：CopyOnWriteArrayList和CopyOnWriteArraySet, COW容器非常有用，可以在非常多的并发容器场景中使用到。使用方法也和原始的ArrayList, set一样12CopyOnWriteArrayList&lt;String&gt; cwal = new CopyOnWriteArrayList&lt;String&gt;();CopyOnWriteArraySet&lt;String&gt; cwas = new CopyOnWriteArraySet&lt;String&gt;(); 2.1 并发Queue在并发队列上JDK提供了两套实现，一个是以ConcurrentLinkedQueue为代表的高性能队列，一个是以BlockingQueue接口为代表的阻塞队列，无论哪种都继承自Queue 2.2 ConcurrentLinkedQueueConcurrentLinkedQueue: 是一个适用于高并发场景下的队列，通过无锁的方式，实现了高并发状态下的高性能，通常ConcurrentLinkedQueue性能好于BlockingQueue,他是一个基于链接节点的无界线程安全队列，该队列的元素遵循先进先出的原则，头是最新加入，尾是最近加入。该队列不允许null元素。 ConcurrentLinkedQueue重要方法：add()和offer都是加入元素的方法(在ConcurrentLinkedQueue中，这两个方法没有任何区别)poll()和peek()都是取头元素节点，区别在于前者会删除元素，后者不会。示例：com.kaishun.base.coll013.UseQueue高性能无阻塞无界队列：ConcurrentLinkedQueue12345678910111213141516171819202122232425262728293031323334353637//高性能无阻塞无界队列：ConcurrentLinkedQueueConcurrentLinkedQueue&lt;String&gt; q = new ConcurrentLinkedQueue&lt;String&gt;();q.offer(&quot;a&quot;);q.offer(&quot;b&quot;);q.offer(&quot;c&quot;);q.offer(&quot;d&quot;);q.add(&quot;e&quot;);System.out.println(q.poll()); //a 从头部取出元素，并从队列里删除System.out.println(q.size()); //4System.out.println(q.peek()); //bSystem.out.println(q.size()); //4----输出-----a4b4``` ## 2.3 BlockQueue接口 有5种queue的实现。 ### ArrayBlockingQueue 基于数组的阻塞队列实现，在ArrayBlockingQueue内部，维护了一个定常数组，一边缓存队列中的数据对象，其内部没有实现读写分离，也就意味着生产和消费不能完全并行，长度是需要定义的，可以指定先进先出或者先进后出。也叫**有界队列**，在很多场合下非常适用。 ```java ArrayBlockingQueue&lt;String&gt; array = new ArrayBlockingQueue&lt;String&gt;(5); array.put(&quot;a&quot;); array.put(&quot;b&quot;); array.add(&quot;c&quot;); array.add(&quot;d&quot;); array.add(&quot;e&quot;);// array.add(&quot;f&quot;); System.out.println(array.offer(&quot;a&quot;, 3, TimeUnit.SECONDS));``` 由于指定的是5个长度，前面已经加了5个了，后面再次添加的时候，3秒内都加不进去，3秒后返回一个false，输出 false123456789101112131415161718192021222324252627282930313233343536373839若超过了还是用add方法，就会抛异常 IllegalStateException: Queue full ，例如 ```java ArrayBlockingQueue&lt;String&gt; array = new ArrayBlockingQueue&lt;String&gt;(5); array.put(&quot;a&quot;); array.put(&quot;b&quot;); array.add(&quot;c&quot;); array.add(&quot;d&quot;); array.add(&quot;e&quot;); array.add(&quot;f&quot;); //这里接回抛异常 System.out.println(array.offer(&quot;a&quot;, 3, TimeUnit.SECONDS));``` ### LinkedBlockingQueue: 基于链表的阻塞队列，同ArrayBlockingQueue类似，其内部也维护者一个数据缓冲队列（该队列是由一个链表构成），LinkBlockingQueue之所以能够搞笑的处理并发数据，是因为其内部实现了读写分离锁，从而实现了生产者和消费者的完全并行运行，他是一个**无界队列**。 ```java //阻塞队列 LinkedBlockingQueue&lt;String&gt; q = new LinkedBlockingQueue&lt;String&gt;(); q.offer(&quot;a&quot;); q.offer(&quot;b&quot;); q.offer(&quot;c&quot;); q.offer(&quot;d&quot;); q.offer(&quot;e&quot;); q.add(&quot;f&quot;); System.out.println(q.size()); for (Iterator iterator = q.iterator(); iterator.hasNext();) &#123; String string = (String) iterator.next(); System.out.println(string); &#125;-----------输出-----------6abcdef drainTo一次取多个元素123456789101112131415161718192021222324 //阻塞队列 LinkedBlockingQueue&lt;String&gt; q = new LinkedBlockingQueue&lt;String&gt;(); q.offer("a"); q.offer("b"); q.offer("c"); q.offer("d"); q.offer("e"); q.add("f"); System.out.println(q.size()); List&lt;String&gt; list = new ArrayList&lt;String&gt;(); //取3个元素，放入到 list 集合中去 System.out.println(q.drainTo(list, 3)); System.out.println(list.size()); for (String string : list) &#123; System.out.println(string); &#125;-----------输出-----------633abc PriorityBlockingQueue:基于优先级的阻塞队列（优先级的判断通过构造函数传入的Compator对象来决定，也就是说传入队列的对象必须实现Comparable接口），在实现PriorityBlockingQueue时，内部控制线程同步的锁采用的是公平锁，他也是一个无界的队列。示例：Task类，实现了Comparable的方法，重写compareTo方法123456789101112131415161718192021222324252627public class Task implements Comparable&lt;Task&gt;&#123; private int id ; private String name; public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; @Override public int compareTo(Task task) &#123; return this.id &gt; task.id ? 1 : (this.id &lt; task.id ? -1 : 0); &#125; public String toString()&#123; return this.id + "," + this.name; &#125; &#125; UsePriorityBlockingQueue类测试是否是有序队列123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class UsePriorityBlockingQueue &#123; public static void main(String[] args) throws Exception&#123; PriorityBlockingQueue&lt;Task&gt; q = new PriorityBlockingQueue&lt;Task&gt;(); Task t1 = new Task(); t1.setId(3); t1.setName(&quot;id为3&quot;); Task t2 = new Task(); t2.setId(4); t2.setName(&quot;id为4&quot;); Task t3 = new Task(); t3.setId(1); t3.setName(&quot;id为1&quot;); //return this.id &gt; task.id ? 1 : 0; q.add(t1); //3 q.add(t2); //4 q.add(t3); //1 // 1 3 4 System.out.println(&quot;容器：&quot; + q); System.out.println(q.take().getId()); System.out.println(&quot;容器：&quot; + q); System.out.println(q.take().getId()); System.out.println(q.take().getId()); &#125;&#125;``` 输出 ```java容器：[1,id为1, 4,id为4, 3,id为3]1容器：[3,id为3, 4,id为4]34``` 说明，这种队列在没有take的时候，还不是排序的，take()时，才利用了排序，比较的方法 ### DelayQueue: 带有延迟时间的Queue, 其中的元素只有当其指定的延迟时间到了，才能够从队列中获取到该元素， **DelayQueue的元素必须实现Delayed 接口**， DelayQueue是一个没有大小限制的队列，应用场景很多，比如对缓存超时的数据进行一处，任务超时处理，空闲连接的关闭等等。 经典的网吧上机案例： Wangmin 实现了Delayed接口 public class Wangmin implements Delayed { private String name; //身份证 private String id; //截止时间 private long endTime; //定义时间工具类 private TimeUnit timeUnit = TimeUnit.SECONDS; public Wangmin(String name,String id,long endTime){ this.name=name; this.id=id; this.endTime = endTime; } public String getName(){ return this.name; } public String getId(){ return this.id; } /** * 用来判断是否到了截止时间 */ @Override public long getDelay(TimeUnit unit) { //return unit.convert(endTime, TimeUnit.MILLISECONDS) - unit.convert(System.currentTimeMillis(), TimeUnit.MILLISECONDS); return endTime - System.currentTimeMillis(); } /** * 相互批较排序用 */ @Override public int compareTo(Delayed delayed) { Wangmin w = (Wangmin)delayed; return this.getDelay(this.timeUnit) - w.getDelay(this.timeUnit) &gt; 0 ? 1:0; } 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051WangBa类 ```javapublic class WangBa implements Runnable &#123; private DelayQueue&lt;Wangmin&gt; queue = new DelayQueue&lt;Wangmin&gt;(); public boolean yinye =true; public void shangji(String name,String id,int money)&#123; Wangmin man = new Wangmin(name, id, 1000 * money + System.currentTimeMillis()); System.out.println(&quot;网名&quot;+man.getName()+&quot; 身份证&quot;+man.getId()+&quot;交钱&quot;+money+&quot;块,开始上机...&quot;); this.queue.add(man); &#125; public void xiaji(Wangmin man)&#123; System.out.println(&quot;网名&quot;+man.getName()+&quot; 身份证&quot;+man.getId()+&quot;时间到下机...&quot;); &#125; @Override public void run() &#123; while(yinye)&#123; try &#123; Wangmin man = queue.take(); xiaji(man); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; public static void main(String args[])&#123; try&#123; System.out.println(&quot;网吧开始营业&quot;); WangBa siyu = new WangBa(); Thread shangwang = new Thread(siyu); shangwang.start(); siyu.shangji(&quot;路人甲&quot;, &quot;123&quot;, 1); siyu.shangji(&quot;路人乙&quot;, &quot;234&quot;, 10); siyu.shangji(&quot;路人丙&quot;, &quot;345&quot;, 5); &#125; catch(Exception e)&#123; e.printStackTrace(); &#125; &#125; &#125; ``` 输出 网吧开始营业网名路人甲 身份证123交钱1块,开始上机…网名路人乙 身份证234交钱10块,开始上机…网名路人丙 身份证345交钱5块,开始上机…网名路人甲 身份证123时间到下机…网名路人丙 身份证345时间到下机…网名路人乙 身份证234时间到下机…1234567891011121314151617181920212223242526### SynchronousQueue: 一种没有缓冲的队列，生产者产生的数据直接会被消费者获取并消费 个人理解为虚拟队列，这个队列不存元素，生产与消费相互扔而已```java final SynchronousQueue&lt;String&gt; q = new SynchronousQueue&lt;String&gt;(); Thread t1 = new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; System.out.println(q.take()); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); t1.start(); Thread t2 = new Thread(new Runnable() &#123; @Override public void run() &#123; q.add(&quot;asdasd&quot;); &#125; &#125;); t2.start();---------输出-------asdasd – 本文总结自前人经验，总结得挺累的，特别感谢互联网架构师白鹤翔老师]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【多线程高并发】多线程的设计模式]]></title>
    <url>%2F%E6%9E%B6%E6%9E%84%2Fthread3.html</url>
    <content type="text"><![CDATA[关键字：：多线程设计模式，Future模式，Master-Worker模式，生产者-消费者模型github 地址： https://github.com/zhaikaishun/concurrent_programming本篇文章代码在Multi_004 中 多线程的设计模式代码在Multi_004当中并行设计模式属于设计优化的一部分，他是对一些常用的多线程结构的总结和抽象，与串行程序相比，并行程序的结构通常更为复杂，因此合理的使用并行模式在多线程开发中更具有意义，在这里主要介绍Future, Master-Worker和生产者-消费者模型。 Future模式Future模式类似于异步请求Future模式Java实现举例(其实JDK自带有实现，这里我先用java来实现)代码在： com.kaishun.height.design014 中 main方法12345678910111213141516171819202122232425262728293031323334353637public class Main &#123; public static void main(String[] args) throws InterruptedException &#123; FutureClient fc = new FutureClient(); Data data = fc.request("请求参数"); System.out.println("请求发送成功!"); System.out.println("做其他的事情..."); String result = data.getRequest(); System.out.println(result); &#125;&#125;``` **FutureClient类** 先返回一个futureData对象，不让主方法阻塞，然后再让这个引用去得到耗时的操作的结果```javapublic class FutureClient &#123; public Data request(final String queryStr)&#123; //1 我想要一个代理对象（Data接口的实现类）先返回给发送请求的客户端，告诉他请求已经接收到，可以做其他的事情 final FutureData futureData = new FutureData(); //2 启动一个新的线程，去加载真实的数据，传递给这个代理对象 new Thread(new Runnable() &#123; @Override public void run() &#123; //3 这个新的线程可以去慢慢的加载真实对象，然后传递给代理对象 RealData realData = new RealData(queryStr); futureData.setRealData(realData); &#125; &#125;).start(); return futureData; &#125; &#125; RealData类，构造方法是一个耗时的类的操作123456789101112131415161718192021public class RealData implements Data&#123; private String result ; public RealData (String queryStr)&#123; System.out.println("根据'" + queryStr + "'进行查询，这是一个很耗时的操作.."); try &#123; Thread.sleep(5000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("操作完毕，获取结果"); result = "'查询结果'"; &#125; @Override public String getRequest() &#123; return result; &#125;&#125; FutureData类 setRealData(RealData realData)和getRequest()加了同步代码块synchronized, 线程之间通过wait/notify进行通信123456789101112131415161718192021222324252627282930313233343536public class FutureData implements Data&#123; private RealData realData ; private boolean isReady = false; public synchronized void setRealData(RealData realData) &#123; //如果已经装载完毕了，就直接返回 if(isReady)&#123; return; &#125; //如果没装载，进行装载真实对象 this.realData = realData; isReady = true; //进行通知 notify(); &#125; @Override public synchronized String getRequest() &#123; //如果没装载好 程序就一直处于阻塞状态 while(!isReady)&#123; try &#123; wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; //装载好直接获取数据即可 return this.realData.getRequest(); &#125;&#125;``` Data接口 public interface Data { String getRequest(); }12**最终输出** 请求发送成功!做其他的事情…根据’请求参数’进行查询，这是一个很耗时的操作..操作完毕，获取结果‘查询结果’12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485## Master-Worker模式Master-Worker模式是常用的并行计算模式。他的核心思想是系统由两类进程协作工作：Master进程和Worker进程.Maseter负责接收和分配任务, Worker负责处理子任务。当各个Worker子进行处理完成后，会将结果返回给Master，由Msster做归纳总结，好处是能将一个大任务分解成若干个小任务，并行执行，从而提高系统的吞吐量![java并发Master-Worker模式](http://or49tneld.bkt.clouddn.com/17-10-4/27593279.jpg) 举例com.kaishun.height.design015 ![Master-Worker模式编写步骤](http://or49tneld.bkt.clouddn.com/17-10-4/85482672.jpg) **Task任务类** ```javapublic class Task &#123; private int id; private int price ; public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id; &#125; public int getPrice() &#123; return price; &#125; public void setPrice(int price) &#123; this.price = price; &#125; &#125;``` **Master类** ```java //1 有一个盛放任务的容器 private ConcurrentLinkedQueue&lt;Task&gt; workQueue = new ConcurrentLinkedQueue&lt;Task&gt;(); //2 需要有一个盛放worker的集合 private HashMap&lt;String, Thread&gt; workers = new HashMap&lt;String, Thread&gt;(); //3 需要有一个盛放每一个worker执行任务的结果集合 private ConcurrentHashMap&lt;String, Object&gt; resultMap = new ConcurrentHashMap&lt;String, Object&gt;(); //4 构造方法 public Master(Worker worker , int workerCount)&#123; worker.setWorkQueue(this.workQueue); worker.setResultMap(this.resultMap); for(int i = 0; i &lt; workerCount; i ++)&#123; this.workers.put(Integer.toString(i), new Thread(worker)); &#125; &#125; //5 需要一个提交任务的方法 public void submit(Task task)&#123; this.workQueue.add(task); &#125; //6 需要有一个执行的方法，启动所有的worker方法去执行任务 public void execute()&#123; for(Map.Entry&lt;String, Thread&gt; me : workers.entrySet())&#123; me.getValue().start(); &#125; &#125; //7 判断是否运行结束的方法 public boolean isComplete() &#123; for(Map.Entry&lt;String, Thread&gt; me : workers.entrySet())&#123; if(me.getValue().getState() != Thread.State.TERMINATED)&#123; return false; &#125; &#125; return true; &#125; //8 计算结果方法 public int getResult() &#123; int priceResult = 0; for(Map.Entry&lt;String, Object&gt; me : resultMap.entrySet())&#123; priceResult += (Integer)me.getValue(); &#125; return priceResult; &#125;&#125; Work类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public class Worker implements Runnable &#123; private ConcurrentLinkedQueue&lt;Task&gt; workQueue; private ConcurrentHashMap&lt;String, Object&gt; resultMap; public void setWorkQueue(ConcurrentLinkedQueue&lt;Task&gt; workQueue) &#123; this.workQueue = workQueue; &#125; public void setResultMap(ConcurrentHashMap&lt;String, Object&gt; resultMap) &#123; this.resultMap = resultMap; &#125; @Override public void run() &#123; while(true)&#123; Task input = this.workQueue.poll(); if(input == null) break; Object output = handle(input); this.resultMap.put(Integer.toString(input.getId()), output); &#125; &#125; private Object handle(Task input) &#123; Object output = null; try &#123; //处理任务的耗时。。 比如说进行操作数据库。。。 Thread.sleep(500); output = input.getPrice(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return output; &#125;&#125;``` **Main方法** ```javapublic class Main &#123; public static void main(String[] args) &#123; Master master = new Master(new Worker(), 20); Random r = new Random(); // 100 个任务 for(int i = 1; i &lt;= 100; i++)&#123; Task t = new Task(); t.setId(i); t.setPrice(r.nextInt(1000)); master.submit(t); &#125; master.execute(); long start = System.currentTimeMillis(); while(true)&#123; if(master.isComplete())&#123; long end = System.currentTimeMillis() - start; int priceResult = master.getResult(); System.out.println("最终结果：" + priceResult + ", 执行时间：" + end); break; &#125; &#125; &#125;&#125; 最终输出结果1最终结果：48098, 执行时间：2500 1.4 生产者-消费者模式生产者和消费者也是一个非常经典的多线程模式，我们在实际中开发应用非常广泛的思想理念。在生产-消费模式中:通常由两类线程，即若干个生产者和若干个消费者的线程。生产者负责提交用户数据，消费者负责具体处理生产者提交的任务，在生产者和消费者之间通过共享内存缓存区进行通信。 示例：现在就来模拟一下上面的图示main方法123456789101112131415161718192021222324252627282930313233343536373839404142public class Main &#123; public static void main(String[] args) throws Exception &#123; //内存缓冲区 BlockingQueue&lt;Data&gt; queue = new LinkedBlockingQueue&lt;Data&gt;(10); //生产者 Provider p1 = new Provider(queue); Provider p2 = new Provider(queue); Provider p3 = new Provider(queue); //消费者 Consumer c1 = new Consumer(queue); Consumer c2 = new Consumer(queue); Consumer c3 = new Consumer(queue); //创建线程池运行,这是一个缓存的线程池，可以创建无穷大的线程，没有任务的时候不创建线程。空闲线程存活时间为60s（默认值） ExecutorService cachePool = Executors.newCachedThreadPool(); cachePool.execute(p1); cachePool.execute(p2); cachePool.execute(p3); cachePool.execute(c1); cachePool.execute(c2); cachePool.execute(c3); try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; p1.stop(); p2.stop(); p3.stop(); try &#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; // cachePool.shutdown(); // cachePool.shutdownNow(); &#125; &#125; Provider1234567891011121314151617181920212223242526272829303132333435363738394041public class Provider implements Runnable&#123; //共享缓存区 private BlockingQueue&lt;Data&gt; queue; //多线程间是否启动变量，有强制从主内存中刷新的功能。即时返回线程的状态 private volatile boolean isRunning = true; //id生成器 private static AtomicInteger count = new AtomicInteger(); //随机对象 private static Random r = new Random(); public Provider(BlockingQueue queue)&#123; this.queue = queue; &#125; @Override public void run() &#123; while(isRunning)&#123; try &#123; //随机休眠0 - 1000 毫秒 表示获取数据(产生数据的耗时) Thread.sleep(r.nextInt(1000)); //获取的数据进行累计... int id = count.incrementAndGet(); //比如通过一个getData方法获取了 Data data = new Data(Integer.toString(id), "数据" + id); System.out.println("当前线程:" + Thread.currentThread().getName() + ", 获取了数据，id为:" + id + ", 进行装载到公共缓冲区中..."); if(!this.queue.offer(data, 2, TimeUnit.SECONDS))&#123; System.out.println("提交缓冲区数据失败...."); //do something... 比如重新提交 &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; public void stop()&#123; this.isRunning = false; &#125; &#125; Consumer12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public class Consumer implements Runnable&#123; private BlockingQueue&lt;Data&gt; queue; public Consumer(BlockingQueue queue)&#123; this.queue = queue; &#125; //随机对象 private static Random r = new Random(); @Override public void run() &#123; while(true)&#123; try &#123; //获取数据 Data data = this.queue.take(); //进行数据处理。休眠0 - 1000毫秒模拟耗时 Thread.sleep(r.nextInt(1000)); System.out.println("当前消费线程：" + Thread.currentThread().getName() + "， 消费成功，消费数据为id: " + data.getId()); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125;``` **Data类**```javapublic final class Data &#123; private String id; private String name; public Data(String id, String name)&#123; this.id = id; this.name = name; &#125; public String getId() &#123; return id; &#125; public void setId(String id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; @Override public String toString()&#123; return "&#123;id: " + id + ", name: " + name + "&#125;"; &#125; &#125;``` **输出** 当前线程:pool-1-thread-2, 获取了数据，id为:1, 进行装载到公共缓冲区中…当前消费线程：pool-1-thread-5， 消费成功，消费数据为id: 1当前线程:pool-1-thread-3, 获取了数据，id为:2, 进行装载到公共缓冲区中…当前线程:pool-1-thread-2, 获取了数据，id为:3, 进行装载到公共缓冲区中…当前消费线程：pool-1-thread-6， 消费成功，消费数据为id: 2当前线程:pool-1-thread-1, 获取了数据，id为:4, 进行装载到公共缓冲区中…当前线程:pool-1-thread-2, 获取了数据，id为:5, 进行装载到公共缓冲区中…当前消费线程：pool-1-thread-5， 消费成功，消费数据为id: 4当前消费线程：pool-1-thread-4， 消费成功，消费数据为id: 3当前线程:pool-1-thread-1, 获取了数据，id为:6, 进行装载到公共缓冲区中…当前线程:pool-1-thread-3, 获取了数据，id为:7, 进行装载到公共缓冲区中…当前消费线程：pool-1-thread-6， 消费成功，消费数据为id: 5当前消费线程：pool-1-thread-4， 消费成功，消费数据为id: 7当前线程:pool-1-thread-2, 获取了数据，id为:8, 进行装载到公共缓冲区中…当前线程:pool-1-thread-2, 获取了数据，id为:9, 进行装载到公共缓冲区中…当前线程:pool-1-thread-1, 获取了数据，id为:10, 进行装载到公共缓冲区中…当前消费线程：pool-1-thread-5， 消费成功，消费数据为id: 6当前线程:pool-1-thread-3, 获取了数据，id为:11, 进行装载到公共缓冲区中…当前线程:pool-1-thread-1, 获取了数据，id为:12, 进行装载到公共缓冲区中…当前消费线程：pool-1-thread-5， 消费成功，消费数据为id: 10当前消费线程：pool-1-thread-6， 消费成功，消费数据为id: 8当前线程:pool-1-thread-1, 获取了数据，id为:13, 进行装载到公共缓冲区中…当前线程:pool-1-thread-3, 获取了数据，id为:14, 进行装载到公共缓冲区中…当前消费线程：pool-1-thread-6， 消费成功，消费数据为id: 12当前线程:pool-1-thread-1, 获取了数据，id为:15, 进行装载到公共缓冲区中…当前消费线程：pool-1-thread-4， 消费成功，消费数据为id: 9当前线程:pool-1-thread-2, 获取了数据，id为:16, 进行装载到公共缓冲区中…当前消费线程：pool-1-thread-6， 消费成功，消费数据为id: 13当前消费线程：pool-1-thread-5， 消费成功，消费数据为id: 11当前线程:pool-1-thread-3, 获取了数据，id为:17, 进行装载到公共缓冲区中…当前消费线程：pool-1-thread-6， 消费成功，消费数据为id: 15当前消费线程：pool-1-thread-4， 消费成功，消费数据为id: 14当前线程:pool-1-thread-2, 获取了数据，id为:18, 进行装载到公共缓冲区中…当前线程:pool-1-thread-1, 获取了数据，id为:19, 进行装载到公共缓冲区中…当前消费线程：pool-1-thread-5， 消费成功，消费数据为id: 16当前消费线程：pool-1-thread-6， 消费成功，消费数据为id: 17当前消费线程：pool-1-thread-5， 消费成功，消费数据为id: 19当前消费线程：pool-1-thread-4， 消费成功，消费数据为id: 18``` 特别感谢互联网架构师白鹤翔老师，本文大多出自他的讲解。笔者主要是记录笔记，正所谓好记性不如烂笔头，烂笔头不如云笔记]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【多线程高并发】线程安全]]></title>
    <url>%2F%E6%9E%B6%E6%9E%84%2Fthread1.html</url>
    <content type="text"><![CDATA[关键字：线程安全，synchronized，多个线程多个锁，对象锁的同步和异步，脏读，synchronized锁重入，synchronized代码块，volatile关键字 github 地址： https://github.com/zhaikaishun/concurrent_programming本篇文章代码在Multi_001 中 线程安全当多个线程访问某一个类（对象或者方法）时，这个累始终都能表现出正确的行为，那么这个类（对象或方法）就是线程安全的 1.1 synchronizedsynchronized可以在任意的对象以及方法上加锁，加锁的这段代码称为”互斥区”。当多个线程访问某一个方法时，会以排队的方式进行处理（这里按照cpu分配的先后顺序而定），一个线程想要执行synchronized修饰的方法里面的代码。首先要尝试获得锁，如果拿到锁，执行synchronized代码体内容，拿不到锁，这个线程就会不断的尝试得到这把锁，知道拿到为止。而且是多个线程同时竞争这把锁 例子 sync001 ：run方法不加锁123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class MyThread extends Thread&#123; private int count = 5 ; //synchronized加锁 public void run()&#123; count--; System.out.println(this.currentThread().getName() + " count = "+ count); &#125; public static void main(String[] args) &#123; /** * 分析：当多个线程访问myThread的run方法时，以排队的方式进行处理（这里排对是按照CPU分配的先后顺序而定的）， * 一个线程想要执行synchronized修饰的方法里的代码： * 1 尝试获得锁 * 2 如果拿到锁，执行synchronized代码体内容；拿不到锁，这个线程就会不断的尝试获得这把锁，直到拿到为止， * 而且是多个线程同时去竞争这把锁。（也就是会有锁竞争的问题） */ MyThread myThread = new MyThread(); Thread t1 = new Thread(myThread,"t1"); Thread t2 = new Thread(myThread,"t2"); Thread t3 = new Thread(myThread,"t3"); Thread t4 = new Thread(myThread,"t4"); Thread t5 = new Thread(myThread,"t5"); t1.start(); t2.start(); t3.start(); t4.start(); t5.start(); &#125;&#125;--- 运行后结果一般不是 4 3 2 1 0, 我的某次运行结果如下-----t1 count = 3t4 count = 2t2 count = 3t3 count = 1t5 count = 0``` 如果给run方法加一把锁，其他代码不变，上面的run方法修改如下```java //synchronized加锁 public synchronized void run()&#123; count--; System.out.println(this.currentThread().getName() + " count = "+ count); &#125;--------无论运行多少次，结果都为----t3 count = 4t2 count = 3t1 count = 2t5 count = 1t4 count = 0 1.2 多个线程多个锁关键字synchronized取得的锁都是对象锁，而不是把一段代码（方法）当做锁，所以代码中哪个线程先执行synchronized关键字的方法，哪个线程就持有该方法所属对象的锁（Lock），两个对象，线程获得的就是两个不同的锁，他们互不影响有一种情况则是相同的锁，即在静态方法上加synchronized 关键字，表示锁定.class类。类一级别的锁例子 sync002：123456789101112131415161718192021222324252627282930313233343536373839404142434445public class MultiThread &#123; private int num = 0; /** static */ public synchronized void printNum(String tag)&#123; try &#123; if(tag.equals("a"))&#123; num = 100; System.out.println("tag a, set num over!"); Thread.sleep(1000); &#125; else &#123; num = 200; System.out.println("tag b, set num over!"); &#125; System.out.println("tag " + tag + ", num = " + num); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; //注意观察run方法输出顺序 public static void main(String[] args) &#123; //俩个不同的对象 final MultiThread m1 = new MultiThread(); final MultiThread m2 = new MultiThread(); Thread t1 = new Thread(new Runnable() &#123; @Override public void run() &#123; m1.printNum("a"); &#125; &#125;); Thread t2 = new Thread(new Runnable() &#123; @Override public void run() &#123; m2.printNum("b"); &#125; &#125;); t1.start(); t2.start(); &#125;&#125;``` 按照我们的希望，由于printNum方法加锁了, 我们希望t1线程执行完后，再执行t2线程, 实际上我们看到的结果如下, t1并没有执行完，就执行了t2线程 tag a, set num over!tag b, set num over!tag b, num = 200tag a, num = 1001234567891011121314151617181920212223原因是虽然在printNum方法中加锁了，但是由于这里是两个对象， m1和m2，这里就对应着两把锁，两把锁各自肯定互不影响.类锁: 在上面的这个例子当中，如果给synchronized方法加上static进行修饰，那么就相当于给这个类加锁。这两把锁就会变成同一把锁了.上面的例子修改代码如下，其他不变```java private static int num = 0; /** static */ public static synchronized void printNum(String tag)&#123; try &#123; if(tag.equals(&quot;a&quot;))&#123; num = 100; System.out.println(&quot;tag a, set num over!&quot;); Thread.sleep(1000); &#125; else &#123; num = 200; System.out.println(&quot;tag b, set num over!&quot;); &#125; System.out.println(&quot;tag &quot; + tag + &quot;, num = &quot; + num); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; 输出为1234tag a, set num over!tag a, num = 100tag b, set num over!tag b, num = 200 1.3 对象锁的同步和异步同步: synchronized同步的概念就是共享，如果不是共享的资源，就没有必要进行同步异步: asynchronized异步的概念就是独立，相互之间不受任何制约，类似于页面Ajax请求，我们还可以继续浏览或操作页面的内容，而这之间没有任何关系同步的目的就是为了线程安全，对于线程安全来说，需要满足两个特性 原子性 可见性 例子sync003 t1线程调用mo.method1()方法，该方法加有锁，这个方法需要等待，也就是同步 t2线程调用mo.method2()方法，该方法没有加锁，这个方法是异步的 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class MyObject &#123; public synchronized void method1()&#123; try &#123; System.out.println(Thread.currentThread().getName()); Thread.sleep(4000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; /** synchronized */ public void method2()&#123; System.out.println(Thread.currentThread().getName()); &#125; public static void main(String[] args) &#123; final MyObject mo = new MyObject(); /** * 分析： * t1线程先持有object对象的Lock锁，t2线程可以以异步的方式调用对象中的非synchronized修饰的方法 * t1线程先持有object对象的Lock锁，t2线程如果在这个时候调用对象中的同步（synchronized）方法则需等待，也就是同步 */ Thread t1 = new Thread(new Runnable() &#123; @Override public void run() &#123; mo.method1(); &#125; &#125;,"t1"); Thread t2 = new Thread(new Runnable() &#123; @Override public void run() &#123; mo.method2(); &#125; &#125;,"t2"); t1.start(); t2.start(); &#125; &#125; 1.4 脏读 对于对象的同步和异步方法，我们在设计自己的程序的时候，一定要考虑问题的整体性，不然就会出现数据不一致的错误，很经典的错误就是脏读(dirtyread )例子： sync004 我们在对一个对象的方法加锁的时候，需要考虑业务的整体性，即为setValue/getValue 方法同时加锁synchronized同步关键字, 保证业务的原子性，不然会出现业务错误 下面的例子中，我希望先set完之后，再get内容，但是我在set的时候让t1线程休眠2秒钟，而主方法只休息一秒，这样，会导致名字设置完了，但是密码还没有设置，但是主线程调用了getValue()方法，产生了密码的脏读 1234567891011121314151617181920212223242526272829303132333435public class DirtyRead &#123; private String username = "kaishun"; private String password = "123"; public synchronized void setValue(String username, String password)&#123; this.username = username; try &#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; this.password = password; System.out.println("setValue最终结果：username = " + username + " , password = " + password); &#125; public void getValue()&#123; System.out.println("getValue方法得到：username = " + this.username + " , password = " + this.password); &#125; public static void main(String[] args) throws Exception&#123; final DirtyRead dr = new DirtyRead(); Thread t1 = new Thread(new Runnable() &#123; @Override public void run() &#123; dr.setValue("z3", "456"); &#125; &#125;); t1.start(); Thread.sleep(1000); dr.getValue(); &#125;&#125;``` 输出 getValue方法得到：username = z3 , password = 123setValue最终结果：username = z3 , password = 45612345678910111213141516171819202122232425262728293031323334353637383940414243若想要达到先set完才能get，只需要在get的时候也加上同步synchronized ## 1.5 synchronized的其他概念 ### synchronized锁重入: 关键字synchronized拥有锁重入的功能，也就是在使用synchronized事，当一个线程得到了一个对象的锁喉，再次请求此对象时是可以再次得到该对象的锁。 示例 SyncException 这个案例是通过一个抛出一个异常，来释放锁，但是，需要注意的是：很多异常释放锁的情况，如果不及时处理，很可能导致程序业务出错。比如你在执行一个队列任务，很多对象都去在等待对一个对象正确执行完毕后再去释放锁，但是第一个对象由于异常的出现，导致业务逻辑没有正常执行完毕，就释放了锁，那么可想而知后续的对象执行的都是错位的逻辑。所以这一点需要引起注意，在编写代码的时候，一定要考虑周全.```javapublic class SyncException &#123; private int i = 0; public synchronized void operation()&#123; while(true)&#123; try &#123; i++; Thread.sleep(100); System.out.println(Thread.currentThread().getName() + &quot; , i = &quot; + i); if(i == 20)&#123; //Integer.parseInt(&quot;a&quot;); throw new RuntimeException(); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; public static void main(String[] args) &#123; final SyncException se = new SyncException(); Thread t1 = new Thread(new Runnable() &#123; @Override public void run() &#123; se.operation(); &#125; &#125;,&quot;t1&quot;); t1.start(); &#125; &#125; Java内置锁synchronized的可重入性当线程请求一个由其它线程持有的对象锁时，该线程会阻塞，而当线程请求由自己持有的对象锁时，如果该锁是重入锁,请求就会成功,否则阻塞.例子 com.kaishun.base.sync005 SyncDubbo1 类 :由于synchronized是重入锁，当调用method1时，在method1的内部，由于是一个线程，会在method1内再次请求一次sd这个对象锁，所以能在method1()方法中执行method2()方法，而不会造成死锁12345678910111213141516171819202122232425262728public class SyncDubbo1 &#123; public synchronized void method1()&#123; System.out.println("method1.."); method2(); &#125; public synchronized void method2()&#123; System.out.println("method2.."); method3(); &#125; public synchronized void method3()&#123; System.out.println("method3.."); &#125; public static void main(String[] args) &#123; final SyncDubbo1 sd = new SyncDubbo1(); Thread t1 = new Thread(new Runnable() &#123; @Override public void run() &#123; sd.method1(); &#125; &#125;); t1.start(); &#125;&#125;``` 输出 method1..method2..method3..1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556再举一个例子 com.kaishun.base.sync005.SyncDubbo2 ： 子类也可以重入父类 ```javapublic class SyncDubbo2 &#123; static class A &#123; public int i = 10; public synchronized void methodA()&#123; try &#123; i--; System.out.println(&quot;A print i = &quot; + i); Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; static class B extends A &#123; public synchronized void methodB()&#123; try &#123; while(i &gt; 0) &#123; i--; System.out.println(&quot;B print i = &quot; + i); Thread.sleep(100); this.methodA(); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; public static void main(String[] args) &#123; Thread t1 = new Thread(new Runnable() &#123; @Override public void run() &#123; B sub = new B(); sub.methodB(); &#125; &#125;); t1.start(); &#125;&#125;------------------输出-----------------B print i = 9A print i = 8B print i = 7A print i = 6B print i = 5A print i = 4B print i = 3A print i = 2B print i = 1A print i = 0 1.6 synchronized代码块synchronized可以使用任意的Object进行加锁，用法比较灵活举例 com.kaishun.base.sync006.ObjectLock：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172/** * 使用synchronized代码块加锁,比较灵活 * @author alienware * */public class ObjectLock &#123; public void method1()&#123; synchronized (this) &#123; //对象锁 try &#123; System.out.println("do method1.."); Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; public void method2()&#123; //类锁 synchronized (ObjectLock.class) &#123; try &#123; System.out.println("do method2.."); Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; private Object lock = new Object(); public void method3()&#123; //任何对象锁 synchronized (lock) &#123; try &#123; System.out.println("do method3.."); Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; public static void main(String[] args) &#123; final ObjectLock objLock = new ObjectLock(); Thread t1 = new Thread(new Runnable() &#123; @Override public void run() &#123; objLock.method1(); &#125; &#125;); Thread t2 = new Thread(new Runnable() &#123; @Override public void run() &#123; objLock.method2(); &#125; &#125;); Thread t3 = new Thread(new Runnable() &#123; @Override public void run() &#123; objLock.method3(); &#125; &#125;); t1.start(); t2.start(); t3.start(); &#125; &#125; 注意1 不要使用String的常量加锁，会出现死循环问题案例 com.kaishun.base.sync006.StringLock123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class StringLock &#123; public void method() &#123; //new String("字符串常量") synchronized ("字符串常量") &#123; try &#123; while(true)&#123; System.out.println("当前线程 : " + Thread.currentThread().getName() + "开始"); Thread.sleep(1000); System.out.println("当前线程 : " + Thread.currentThread().getName() + "结束"); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; public static void main(String[] args) &#123; final StringLock stringLock = new StringLock(); Thread t1 = new Thread(new Runnable() &#123; @Override public void run() &#123; stringLock.method(); &#125; &#125;,"t1"); Thread t2 = new Thread(new Runnable() &#123; @Override public void run() &#123; stringLock.method(); &#125; &#125;,"t2"); t1.start(); t2.start(); &#125;-----将会一直输出------当前线程 : t1开始当前线程 : t1结束当前线程 : t1开始当前线程 : t1结束当前线程 : t1开始当前线程 : t1结束当前线程 : t1开始当前线程 : t1结束当前线程 : t1开始当前线程 : t1结束当前线程 : t1开始当前线程 : t1结束当前线程 : t1开始当前线程 : t1结束...... 注意2 锁对象改变的问题，如果对象本身发生改变的时候，那么持有的锁就不同。如果对象本身不发生改变，那么依然是同步的，即使对象的属性发生了改变。示例1 锁对象的改变问题 com.kaishun.base.sync006.ChangeLock1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package com.kaishun.base.sync006;/** * 锁对象的改变问题 * @author alienware * */public class ChangeLock &#123; private String lock = "lock"; private void method()&#123; synchronized (lock) &#123; try &#123; System.out.println("当前线程 : " + Thread.currentThread().getName() + "开始"); lock = "change lock"; Thread.sleep(2000); System.out.println("当前线程 : " + Thread.currentThread().getName() + "结束"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; public static void main(String[] args) &#123; final ChangeLock changeLock = new ChangeLock(); Thread t1 = new Thread(new Runnable() &#123; @Override public void run() &#123; changeLock.method(); &#125; &#125;,"t1"); Thread t2 = new Thread(new Runnable() &#123; @Override public void run() &#123; changeLock.method(); &#125; &#125;,"t2"); t1.start(); try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; t2.start(); &#125; &#125;------输出------当前线程 : t1开始当前线程 : t2开始当前线程 : t1结束当前线程 : t2结束 示例2 锁对象的属性发生改变 com.kaishun.base.sync006.ModifyLock123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172package com.kaishun.base.sync006;/** * 同一对象属性的修改不会影响锁的情况 * @author alienware * */public class ModifyLock &#123; private String name ; private int age ; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public synchronized void changeAttributte(String name, int age) &#123; try &#123; System.out.println(&quot;当前线程 : &quot; + Thread.currentThread().getName() + &quot; 开始&quot;); this.setName(name); this.setAge(age); System.out.println(&quot;当前线程 : &quot; + Thread.currentThread().getName() + &quot; 修改对象内容为： &quot; + this.getName() + &quot;, &quot; + this.getAge()); Thread.sleep(2000); System.out.println(&quot;当前线程 : &quot; + Thread.currentThread().getName() + &quot; 结束&quot;); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; public static void main(String[] args) &#123; final ModifyLock modifyLock = new ModifyLock(); Thread t1 = new Thread(new Runnable() &#123; @Override public void run() &#123; modifyLock.changeAttributte(&quot;张三&quot;, 20); &#125; &#125;,&quot;t1&quot;); Thread t2 = new Thread(new Runnable() &#123; @Override public void run() &#123; modifyLock.changeAttributte(&quot;李四&quot;, 21); &#125; &#125;,&quot;t2&quot;); t1.start(); try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; t2.start(); &#125; &#125;-----输出------当前线程 : t1 开始当前线程 : t1 修改对象内容为： 张三, 20当前线程 : t1 结束当前线程 : t2 开始当前线程 : t2 修改对象内容为： 李四, 21当前线程 : t2 结束 1.7 volatile关键字volatile概念：volatile关键字的主要作用是使变量在多个线程间可见但是要注意: volatile关键字不具备synchronized关键字的原子性（同步） 在java中，每一个线程都会有一块工作内存区，其中放着所有线程共享的主内存的变量值的拷贝。当线程执行时，他在自己的工作区中操作这些变量，为了存取一个工ixangde变量，一个线程通常先获取锁定并清除他的内存工作区，把这些共享变量从所有线程的共享内存区中正确的装入到他自己所在的工作内存区中，当线程解锁时保证该工作内存区中变量的值写回到共享内存中。volatile的作用就是强制线程到主内存（共享内存）里去读取变量，而不去线程工作内存区里面读取，从而实现了多个线程间的变量可见，也满足线程安全的可见性。 AtomicInteger 保证原子性AtomicInteger是原子性的，但是，一个方法中的多个StomicInteger却不是原子性的示例: 123456789101112131415161718192021222324252627282930313233343536373839public class AtomicUse &#123; private static AtomicInteger count = new AtomicInteger(0); //多个addAndGet在一个方法内是非原子性的，需要加synchronized进行修饰，保证4个addAndGet整体原子性 /**synchronized*/ public int multiAdd()&#123; try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; count.addAndGet(1); count.addAndGet(2); count.addAndGet(3); count.addAndGet(4); //+10 return count.get(); &#125; public static void main(String[] args) &#123; final AtomicUse au = new AtomicUse(); List&lt;Thread&gt; ts = new ArrayList&lt;Thread&gt;(); for (int i = 0; i &lt; 100; i++) &#123; ts.add(new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println(au.multiAdd()); &#125; &#125;)); &#125; for(Thread t : ts)&#123; t.start(); &#125; &#125;&#125;----输出并不是都是10的整数倍----- 上述运行并不都是10的整数倍，因为多个AtomicInteger增加后，就不一定是原子的了，想要是原子性的，需要在 multiAdd() 方法加上synchronized修饰。 – 本文是总结于前人经验，特别感谢互联网架构师教程白鹤翔白老师。]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【多线程高并发】线程之间通信]]></title>
    <url>%2F%E6%9E%B6%E6%9E%84%2Fthread2.html</url>
    <content type="text"><![CDATA[github地址：https://github.com/zhaikaishun/concurrent_programming示例都在Multi_002项目下关键字：线程之间通信，volatile进行线程之间的通信，wait/notify的方法，CountDownLatch实现线程间通信，wait和notify模拟Queue 2.1 线程之间通信示例都在Multi_002项目下线程间通信概念：不介绍了，就是线程之间的通信方法：使用wait/notify方法实现线程间的通信。（注意这两个方法都是object的类的方法） wait和notify必须配合synchronized关键字使用 wait方法释放锁, notify方法不释放锁 使用volatile进行线程之间的通信示例 com.kaishun.base.conn008.ListAdd1:t1线程向list中添加元素，当list的元素到达5个的时候，t2线程打印一句话，并且通过异常释放锁进行退出。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136public class ListAdd1 &#123; private volatile static List list = new ArrayList(); public void add()&#123; list.add("kaishun"); &#125; public int size()&#123; return list.size(); &#125; public static void main(String[] args) &#123; final ListAdd1 list1 = new ListAdd1(); Thread t1 = new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; for(int i = 0; i &lt;10; i++)&#123; list1.add(); System.out.println("当前线程：" + Thread.currentThread().getName() + "添加了一个元素.."); Thread.sleep(500); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;, "t1"); Thread t2 = new Thread(new Runnable() &#123; @Override public void run() &#123; while(true)&#123; if(list1.size() == 5)&#123; System.out.println("当前线程收到通知：" + Thread.currentThread().getName() + " list size = 5 线程停止.."); throw new RuntimeException(); &#125; &#125; &#125; &#125;, "t2"); t1.start(); t2.start(); &#125;&#125;``` 但是这样并不是很好，因为t2这个线程一直在轮询的调用list1.size()来进行判断。于是我们也可以使用wait和notify的方法来实现这个功能**wait/notify的方法** ```java/** * wait notfiy 方法，wait释放锁，notfiy不释放锁 * @author alienware * */public class ListAdd2 &#123; private volatile static List list = new ArrayList(); public void add()&#123; list.add("kaishun"); &#125; public int size()&#123; return list.size(); &#125; public static void main(String[] args) &#123; final ListAdd2 list2 = new ListAdd2(); // 1 实例化出来一个 lock // 当使用wait 和 notify 的时候 ， 一定要配合着synchronized关键字去使用 final Object lock = new Object();// final CountDownLatch countDownLatch = new CountDownLatch(1); Thread t1 = new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; synchronized (lock) &#123; for(int i = 0; i &lt;10; i++)&#123; list2.add(); System.out.println("当前线程：" + Thread.currentThread().getName() + "添加了一个元素.."); Thread.sleep(500); if(list2.size() == 5)&#123; System.out.println("已经发出通知..");// countDownLatch.countDown(); lock.notify(); &#125; &#125; &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;, "t1"); Thread t2 = new Thread(new Runnable() &#123; @Override public void run() &#123; synchronized (lock) &#123; if(list2.size() != 5)&#123; try &#123; System.out.println("t2进入..."); lock.wait();// countDownLatch.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println("当前线程：" + Thread.currentThread().getName() + "收到通知线程停止..");// throw new RuntimeException(); &#125; &#125; &#125;, "t2"); t2.start(); t1.start(); &#125;&#125;------运行结果t2进入...当前线程：t1添加了一个元素..当前线程：t1添加了一个元素..当前线程：t1添加了一个元素..当前线程：t1添加了一个元素..当前线程：t1添加了一个元素..已经发出通知..当前线程：t1添加了一个元素..当前线程：t1添加了一个元素..当前线程：t1添加了一个元素..当前线程：t1添加了一个元素..当前线程：t1添加了一个元素..当前线程：t2收到通知线程停止.. 这个方法可以实现线程之间的通信，但是还不是我们想要的，我们希望只要list的size()到了5, 就立马唤醒t2线程，打印t2线程收到通知。 这时候我们可以使用 CountDownLatch CountDownLatch实现线程间通信123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081public class ListAdd2 &#123; private volatile static List list = new ArrayList(); public void add()&#123; list.add("kaishun"); &#125; public int size()&#123; return list.size(); &#125; public static void main(String[] args) &#123; final ListAdd2 list2 = new ListAdd2(); // 1 实例化出来一个 lock // 当使用wait 和 notify 的时候 ， 一定要配合着synchronized关键字去使用 //final Object lock = new Object(); final CountDownLatch countDownLatch = new CountDownLatch(1); Thread t1 = new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; //synchronized (lock) &#123; for(int i = 0; i &lt;10; i++)&#123; list2.add(); System.out.println("当前线程：" + Thread.currentThread().getName() + "添加了一个元素.."); Thread.sleep(500); if(list2.size() == 5)&#123; System.out.println("已经发出通知.."); countDownLatch.countDown(); //lock.notify(); &#125; &#125; //&#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;, "t1"); Thread t2 = new Thread(new Runnable() &#123; @Override public void run() &#123; //synchronized (lock) &#123; if(list2.size() != 5)&#123; try &#123; //System.out.println("t2进入..."); //lock.wait(); countDownLatch.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println("当前线程：" + Thread.currentThread().getName() + "收到通知线程停止..");// throw new RuntimeException(); //&#125; &#125; &#125;, "t2"); t2.start(); t1.start(); &#125;&#125;-------输出-------------当前线程：t1添加了一个元素..当前线程：t1添加了一个元素..当前线程：t1添加了一个元素..当前线程：t1添加了一个元素..当前线程：t1添加了一个元素..已经发出通知..当前线程：t1添加了一个元素..当前线程：t2收到通知线程停止..当前线程：t1添加了一个元素..当前线程：t1添加了一个元素..当前线程：t1添加了一个元素..当前线程：t1添加了一个元素.. 上面例子可以看出，只要t1到了5，就告诉t2，你可以执行下面的了，但是t2线程还是抢不过t1线程，所以还是稍微比t1慢了一点 2.2 wait和notify模拟QueueBlockingQueue: 是一个支持阻塞的队列，阻塞的放入和得到数据实现这个功能，主要是 在queue队列满了的时候，put方法进入wait，等待take操作之后的notify。 在queue队列为空的时候，take方法进入wait，等待put操作之后的notify，具体查看示例代码如下com.kaishun.base.conn009.MyQueue123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108public class MyQueue &#123; //1 需要一个承装元素的集合 private LinkedList&lt;Object&gt; list = new LinkedList&lt;Object&gt;(); //2 需要一个计数器 private AtomicInteger count = new AtomicInteger(0); //3 需要制定上限和下限 private final int minSize = 0; private final int maxSize ; //4 构造方法 public MyQueue(int size)&#123; this.maxSize = size; &#125; //5 初始化一个对象 用于加锁 private final Object lock = new Object(); //put(anObject): 把anObject加到BlockingQueue里,如果BlockQueue没有空间,则调用此方法的线程被阻断，直到BlockingQueue里面有空间再继续. public void put(Object obj)&#123; synchronized (lock) &#123; while(count.get() == this.maxSize)&#123; try &#123; lock.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; //1 加入元素 list.add(obj); //2 计数器累加 count.incrementAndGet(); //3 通知另外一个线程（唤醒） lock.notify(); System.out.println("新加入的元素为:" + obj); &#125; &#125; //take: 取走BlockingQueue里排在首位的对象,若BlockingQueue为空,阻断进入等待状态直到BlockingQueue有新的数据被加入. public Object take()&#123; Object ret = null; synchronized (lock) &#123; while(count.get() == this.minSize)&#123; try &#123; lock.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; //1 做移除元素操作 ret = list.removeFirst(); //2 计数器递减 count.decrementAndGet(); //3 唤醒另外一个线程 lock.notify(); &#125; return ret; &#125; public int getSize()&#123; return this.count.get(); &#125; public static void main(String[] args) &#123; final MyQueue mq = new MyQueue(5); mq.put("a"); mq.put("b"); mq.put("c"); mq.put("d"); mq.put("e"); System.out.println("当前容器的长度:" + mq.getSize()); Thread t1 = new Thread(new Runnable() &#123; @Override public void run() &#123; mq.put("f"); mq.put("g"); &#125; &#125;,"t1"); t1.start(); Thread t2 = new Thread(new Runnable() &#123; @Override public void run() &#123; Object o1 = mq.take(); System.out.println("移除的元素为:" + o1); Object o2 = mq.take(); System.out.println("移除的元素为:" + o2); &#125; &#125;,"t2"); try &#123; TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; t2.start(); &#125;&#125; 本文乃是笔记，总结前人经验，特别感谢互联网架构师白鹤翔白老师]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty入门三之最佳实践]]></title>
    <url>%2F%E6%9E%B6%E6%9E%84%2Fnetty3.html</url>
    <content type="text"><![CDATA[关键字 最佳实践： 数据通信，心跳检测 代码在 https://github.com/zhaikaishun/NettyTutorial 代码在SocketIO_03下 Netty最佳实践实际场景一：数据通信我们需要考虑两台或者多台机器使用Netty如何进行通信，作者个人大体上把他分为3种 第一种，使用长连接通道不断开的形式进行通信，也就是服务器和客户端的通道一直处于开启状态，如果服务器性能足够好，并且我们的客户端数量比较少的情况下，还是比较推荐这种方式的 第二种，一次性批量提交数据，采用短连接方式。也就是说数据先保存到临时表中，当到达临界值时进行一次性批量提交，这种情况是做不到实时传输，在对实时性不高的应用程序可以推荐使用。 第三种，我们可以使用一种特殊的长连接，在制定某一时间之内，服务器与某台客户端没有任何通信，则断开连接，下次连接则是客户端向服务器发送请求的时候，再次建立连接，但是这种模式我们需要考虑2个因素：第一点： 如何在超时后关闭通道？关闭通道后我们又如何再次建立连接？第二点： 客户端宕机时，我们无需考虑，下次客户端重启之后我们就可以与服务器建立连接，但是当服务器宕机时，我们的客户端如何与服务器进行连接呢？ 第一点解决办法：netty有自带的工具类在超时后关闭通道。关闭后，client再次发送连接请求即可再次建立连接第二点解决办法： 用一个脚本定时的监听Netty是否有宕机，若有宕机，则运行脚本再次启动Server端即可 举个例子： 代码在SocketIO_03 bhz.netty.runtime, 具体的去github上看吧Server端，没什么改变，就是加了一个sc.pipeline().addLast(new ReadTimeoutHandler(5)); 说明如果5秒钟某个Client没有和我通信，就断开和这个Client的连接。Server端1234sc.pipeline().addLast(new ReadTimeoutHandler(5)); ``` 同理Client端也是需要加这个 sc.pipeline().addLast(new ReadTimeoutHandler(5));12如何让Client在断开后重连呢？ 主要是New一个线程，这个线程在断开后开启，然后通过判断是否连接关闭，如果连接关闭，那么就发起一个连接。 public ChannelFuture getChannelFuture(){ if(this.cf == null){ this.connect(); } if(!this.cf.channel().isActive()){ this.connect(); } return this.cf; } 12345678910111213141516171819202122232425262728293031323334353637383940414243444546## 实际场景二：心跳检测我们使用Socket通信一般经常会处理多个服务器之间的心跳检测，我们去维护服务器集群，肯定要有一台或多台服务器主机（Master），然后还应该有N台（Slave），那么我们的主机肯定要时时刻刻知道自己下面的从服务器的各方面情况，然后进行实时监控的功能，这个在分布式架构里面叫做心跳检测或者说心跳监控，最佳处理方案我还是觉得使用一些通信框架进行实现，我们的Netty就可以去做这样一件事。 这里先介绍一个工具jar包sigar，这个可以用来查看一些服务器的信息 使用方法： 网上下载sigar-bin-lib包，我github上有， 打开hyperic-sigar-1.6.4\sigar-bin\lib 将对应的配置文件复制到服务器的jdk的bin目录下， 例如我这里是64位的jdk程序，那么我复制sigar-amd64-winnt.dll 到我的java的bin目录下 linux的我一般复制libsigar-amd64-linux.so到jdk bin目录下， 具体的百度搜索有很多。这里简要介绍如何使用 API例子也在 SocketIO_03的bhz.utils包中心跳例子： 代码在SocketIO_03 bhz.netty.heartBeat **步骤一**： 1. Server端需要检测Client端的认证是否通过（为了确保安全） 2. 如果通过，Client端定时发送自己所在电脑的信息，Server端接收信息 **RequestInfo类： 用来存放机器的信息** ```javapublic class RequestInfo implements Serializable &#123; private String ip ; private HashMap&lt;String, Object&gt; cpuPercMap ; private HashMap&lt;String, Object&gt; memoryMap; //.. other field public String getIp() &#123; return ip; &#125; public void setIp(String ip) &#123; this.ip = ip; &#125; public HashMap&lt;String, Object&gt; getCpuPercMap() &#123; return cpuPercMap; &#125; public void setCpuPercMap(HashMap&lt;String, Object&gt; cpuPercMap) &#123; this.cpuPercMap = cpuPercMap; &#125; public HashMap&lt;String, Object&gt; getMemoryMap() &#123; return memoryMap; &#125; public void setMemoryMap(HashMap&lt;String, Object&gt; memoryMap) &#123; this.memoryMap = memoryMap; &#125; &#125; Server端代码1234567891011121314151617181920212223242526272829public class Server &#123; public static void main(String[] args) throws Exception&#123; EventLoopGroup pGroup = new NioEventLoopGroup(); EventLoopGroup cGroup = new NioEventLoopGroup(); ServerBootstrap b = new ServerBootstrap(); b.group(pGroup, cGroup) .channel(NioServerSocketChannel.class) .option(ChannelOption.SO_BACKLOG, 1024) //设置日志 .handler(new LoggingHandler(LogLevel.INFO)) .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; protected void initChannel(SocketChannel sc) throws Exception &#123; sc.pipeline().addLast(MarshallingCodeCFactory.buildMarshallingDecoder()); sc.pipeline().addLast(MarshallingCodeCFactory.buildMarshallingEncoder()); sc.pipeline().addLast(new ServerHeartBeatHandler()); &#125; &#125;); ChannelFuture cf = b.bind(8765).sync(); cf.channel().closeFuture().sync(); pGroup.shutdownGracefully(); cGroup.shutdownGracefully(); &#125;&#125; ServerHeartBeatHandler处理类12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182public class ServerHeartBeatHandler extends ChannelHandlerAdapter &#123; /** key:ip value:auth */ private static HashMap&lt;String, String&gt; AUTH_IP_MAP = new HashMap&lt;String, String&gt;(); private static final String SUCCESS_KEY = "auth_success_key"; static &#123; AUTH_IP_MAP.put("192.168.1.101", "1234"); &#125; private boolean auth(ChannelHandlerContext ctx, Object msg)&#123; //System.out.println(msg); String [] ret = ((String) msg).split(","); String auth = AUTH_IP_MAP.get(ret[0]); if(auth != null &amp;&amp; auth.equals(ret[1]))&#123; ctx.writeAndFlush(SUCCESS_KEY); return true; &#125; else &#123; ctx.writeAndFlush("auth failure !").addListener(ChannelFutureListener.CLOSE); return false; &#125; &#125; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; if(msg instanceof String)&#123; auth(ctx, msg); &#125; else if (msg instanceof RequestInfo) &#123; RequestInfo info = (RequestInfo) msg; System.out.println("--------------------------------------------"); System.out.println("当前主机ip为: " + info.getIp()); System.out.println("当前主机cpu情况: "); HashMap&lt;String, Object&gt; cpu = info.getCpuPercMap(); System.out.println("总使用率: " + cpu.get("combined")); System.out.println("用户使用率: " + cpu.get("user")); System.out.println("系统使用率: " + cpu.get("sys")); System.out.println("等待率: " + cpu.get("wait")); System.out.println("空闲率: " + cpu.get("idle")); System.out.println("当前主机memory情况: "); HashMap&lt;String, Object&gt; memory = info.getMemoryMap(); System.out.println("内存总量: " + memory.get("total")); System.out.println("当前内存使用量: " + memory.get("used")); System.out.println("当前内存剩余量: " + memory.get("free")); System.out.println("--------------------------------------------"); ctx.writeAndFlush("info received!"); &#125; else &#123; ctx.writeAndFlush("connect failure!").addListener(ChannelFutureListener.CLOSE); &#125; &#125;&#125;``` **Client端代码** ```javapublic class Client &#123; public static void main(String[] args) throws Exception&#123; EventLoopGroup group = new NioEventLoopGroup(); Bootstrap b = new Bootstrap(); b.group(group) .channel(NioSocketChannel.class) .handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel sc) throws Exception &#123; sc.pipeline().addLast(MarshallingCodeCFactory.buildMarshallingDecoder()); sc.pipeline().addLast(MarshallingCodeCFactory.buildMarshallingEncoder()); sc.pipeline().addLast(new ClienHeartBeattHandler()); &#125; &#125;); ChannelFuture cf = b.connect("127.0.0.1", 8765).sync(); cf.channel().closeFuture().sync(); group.shutdownGracefully(); &#125;&#125; ClienHeartBeattHandler处理类12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788public class ClienHeartBeattHandler extends ChannelHandlerAdapter &#123; private ScheduledExecutorService scheduler = Executors.newScheduledThreadPool(1); private ScheduledFuture&lt;?&gt; heartBeat; //主动向服务器发送认证信息 private InetAddress addr ; private static final String SUCCESS_KEY = "auth_success_key"; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; addr = InetAddress.getLocalHost(); String ip = addr.getHostAddress(); String key = "1234"; //证书 String auth = ip + "," + key; ctx.writeAndFlush(auth); &#125; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; try &#123; if(msg instanceof String)&#123; String ret = (String)msg; if(SUCCESS_KEY.equals(ret))&#123; // 握手成功，主动发送心跳消息 this.heartBeat = this.scheduler.scheduleWithFixedDelay(new HeartBeatTask(ctx), 0, 2, TimeUnit.SECONDS); System.out.println(msg); &#125; else &#123; System.out.println(msg); &#125; &#125; &#125; finally &#123; ReferenceCountUtil.release(msg); &#125; &#125; private class HeartBeatTask implements Runnable &#123; private final ChannelHandlerContext ctx; public HeartBeatTask(final ChannelHandlerContext ctx) &#123; this.ctx = ctx; &#125; @Override public void run() &#123; try &#123; System.out.println("ff"); RequestInfo info = new RequestInfo(); //ip info.setIp(addr.getHostAddress()); Sigar sigar = new Sigar(); //cpu prec CpuPerc cpuPerc = sigar.getCpuPerc(); HashMap&lt;String, Object&gt; cpuPercMap = new HashMap&lt;String, Object&gt;(); cpuPercMap.put("combined", cpuPerc.getCombined()); cpuPercMap.put("user", cpuPerc.getUser()); cpuPercMap.put("sys", cpuPerc.getSys()); cpuPercMap.put("wait", cpuPerc.getWait()); cpuPercMap.put("idle", cpuPerc.getIdle()); // memory Mem mem = sigar.getMem(); HashMap&lt;String, Object&gt; memoryMap = new HashMap&lt;String, Object&gt;(); memoryMap.put("total", mem.getTotal() / 1024L); memoryMap.put("used", mem.getUsed() / 1024L); memoryMap.put("free", mem.getFree() / 1024L); info.setCpuPercMap(cpuPercMap); info.setMemoryMap(memoryMap); ctx.writeAndFlush(info); System.out.println("gg"); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; cause.printStackTrace(); if (heartBeat != null) &#123; heartBeat.cancel(true); heartBeat = null; &#125; ctx.fireExceptionCaught(cause); &#125; &#125;&#125; MarshallingCodeCFactory工具类用来解编码的，不需要看懂，能用就行了1234567891011121314151617181920212223242526272829303132333435363738/** * Marshalling工厂 * @author（alienware） * @since 2014-12-16 */public final class MarshallingCodeCFactory &#123; /** * 创建Jboss Marshalling解码器MarshallingDecoder * @return MarshallingDecoder */ public static MarshallingDecoder buildMarshallingDecoder() &#123; //首先通过Marshalling工具类的精通方法获取Marshalling实例对象 参数serial标识创建的是java序列化工厂对象。 final MarshallerFactory marshallerFactory = Marshalling.getProvidedMarshallerFactory("serial"); //创建了MarshallingConfiguration对象，配置了版本号为5 final MarshallingConfiguration configuration = new MarshallingConfiguration(); configuration.setVersion(5); //根据marshallerFactory和configuration创建provider UnmarshallerProvider provider = new DefaultUnmarshallerProvider(marshallerFactory, configuration); //构建Netty的MarshallingDecoder对象，俩个参数分别为provider和单个消息序列化后的最大长度 MarshallingDecoder decoder = new MarshallingDecoder(provider, 1024 * 1024 * 1); return decoder; &#125; /** * 创建Jboss Marshalling编码器MarshallingEncoder * @return MarshallingEncoder */ public static MarshallingEncoder buildMarshallingEncoder() &#123; final MarshallerFactory marshallerFactory = Marshalling.getProvidedMarshallerFactory("serial"); final MarshallingConfiguration configuration = new MarshallingConfiguration(); configuration.setVersion(5); MarshallerProvider provider = new DefaultMarshallerProvider(marshallerFactory, configuration); //构建Netty的MarshallingEncoder对象，MarshallingEncoder用于实现序列化接口的POJO对象序列化为二进制数组 MarshallingEncoder encoder = new MarshallingEncoder(provider); return encoder; &#125;&#125; 注意Sigar的配置文件放在jdk bin目录下，当运行Server端，再启动Client端的时候，如果认证失败，则服务器主动断开与client的连接，如果认证通过，那么Client就每2秒钟发送一次心跳检测给Server端. 特别感谢互联网架构师白鹤翔老师，本文大多出自他的视频讲解。笔者主要是记录笔记，以便之后翻阅，正所谓好记性不如烂笔头，烂笔头不如云笔记]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty入门二 之解编码]]></title>
    <url>%2F%E6%9E%B6%E6%9E%84%2Fnetty2.html</url>
    <content type="text"><![CDATA[关键字：Netty解编码，JBoss Marshalling， 代码在 https://github.com/zhaikaishun/NettyTutorial 在SocketIO_02 kaishun.netty.serial下 Netty解编码技术解编码技术，说白了就是java序列化技术，序列化的目的就两个，第一进行网络传输，第二对象持久化虽然我们可以使用java进行对象序列化，netty去传输，但是java序列化的硬伤太多，比如java序列化没法跨语言、序列化够码流太大、序列化性能太低等等…主流的编码解码框架：JBoss 的 Marshalling包google的Protobuf给予Protobuf的KyroMessagePack框架 JBoss MarshallingJboss Marshalling是一个java对象序列化包，对jdk默认的序列化框架进行了优化，但又保持跟java.io.Serializable接口的兼容，同时增加了一些可调的参数和附加特性，类库 jboss-marshalling-1.3.0、jboss-marshalling-serial-1.3.0Jboss Marshalling与Netty结合后进行序列化对象的代码编写非常简单，我们一起来看一下 首先有个MarshallingCodeCFactory类这个类不需要看懂，就要有一个这个类就好了1234567891011121314151617181920212223242526272829303132333435363738/** * Marshalling工厂 * @author（alienware） * @since 2014-12-16 */public final class MarshallingCodeCFactory &#123; /** * 创建Jboss Marshalling解码器MarshallingDecoder * @return MarshallingDecoder */ public static MarshallingDecoder buildMarshallingDecoder() &#123; //首先通过Marshalling工具类的精通方法获取Marshalling实例对象 参数serial标识创建的是java序列化工厂对象。 final MarshallerFactory marshallerFactory = Marshalling.getProvidedMarshallerFactory("serial"); //创建了MarshallingConfiguration对象，配置了版本号为5 final MarshallingConfiguration configuration = new MarshallingConfiguration(); configuration.setVersion(5); //根据marshallerFactory和configuration创建provider UnmarshallerProvider provider = new DefaultUnmarshallerProvider(marshallerFactory, configuration); //构建Netty的MarshallingDecoder对象，俩个参数分别为provider和单个消息序列化后的最大长度 MarshallingDecoder decoder = new MarshallingDecoder(provider, 1024 * 1024 * 1); return decoder; &#125; /** * 创建Jboss Marshalling编码器MarshallingEncoder * @return MarshallingEncoder */ public static MarshallingEncoder buildMarshallingEncoder() &#123; final MarshallerFactory marshallerFactory = Marshalling.getProvidedMarshallerFactory("serial"); final MarshallingConfiguration configuration = new MarshallingConfiguration(); configuration.setVersion(5); MarshallerProvider provider = new DefaultMarshallerProvider(marshallerFactory, configuration); //构建Netty的MarshallingEncoder对象，MarshallingEncoder用于实现序列化接口的POJO对象序列化为二进制数组 MarshallingEncoder encoder = new MarshallingEncoder(provider); return encoder; &#125;&#125; Server端其他基本不变，就这里1234567 .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; protected void initChannel(SocketChannel sc) throws Exception &#123; sc.pipeline().addLast(MarshallingCodeCFactory.buildMarshallingDecoder()); sc.pipeline().addLast(MarshallingCodeCFactory.buildMarshallingEncoder()); sc.pipeline().addLast(new ServerHandler()); &#125;&#125;); Client端也是这里有变化1234567891011121314151617181920212223 .handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel sc) throws Exception &#123; sc.pipeline().addLast(MarshallingCodeCFactory.buildMarshallingDecoder()); sc.pipeline().addLast(MarshallingCodeCFactory.buildMarshallingEncoder()); sc.pipeline().addLast(new ClientHandler()); &#125; &#125;); for(int i = 0; i &lt; 5; i++ )&#123; Req req = new Req(); req.setId("" + i); req.setName("pro" + i); req.setRequestMessage("数据信息" + i); // String path = System.getProperty("user.dir") + File.separatorChar + "sources" + File.separatorChar + "001.jpg";// File file = new File(path);// FileInputStream in = new FileInputStream(file);// byte[] data = new byte[in.available()];// in.read(data);// in.close();// req.setAttachment(GzipUtils.gzip(data)); cf.channel().writeAndFlush(req); &#125; Req类12345678910111213141516171819202122232425262728293031323334public class Req implements Serializable&#123; private static final long SerialVersionUID = 1L; private String id ; private String name ; private String requestMessage ; private byte[] attachment; public String getId() &#123; return id; &#125; public void setId(String id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getRequestMessage() &#123; return requestMessage; &#125; public void setRequestMessage(String requestMessage) &#123; this.requestMessage = requestMessage; &#125; public byte[] getAttachment() &#123; return attachment; &#125; public void setAttachment(byte[] attachment) &#123; this.attachment = attachment; &#125;&#125; Resp类1234567891011121314151617181920212223242526272829public class Resp implements Serializable&#123; private static final long serialVersionUID = 1L; private String id; private String name; private String responseMessage; public String getId() &#123; return id; &#125; public void setId(String id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getResponseMessage() &#123; return responseMessage; &#125; public void setResponseMessage(String responseMessage) &#123; this.responseMessage = responseMessage; &#125; &#125; ClientHandler123456789@Overridepublic void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; try &#123; Resp resp = (Resp)msg; System.out.println("Client : " + resp.getId() + ", " + resp.getName() + ", " + resp.getResponseMessage()); &#125; finally &#123; ReferenceCountUtil.release(msg); &#125;&#125; ServerHandler1234567891011121314151617 @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; Req req = (Req)msg; System.out.println("Server : " + req.getId() + ", " + req.getName() + ", " + req.getRequestMessage());// byte[] attachment = GzipUtils.ungzip(req.getAttachment());//// String path = System.getProperty("user.dir") + File.separatorChar + "receive" + File.separatorChar + "001.jpg";// FileOutputStream fos = new FileOutputStream(path);// fos.write(attachment);// fos.close(); Resp resp = new Resp(); resp.setId(req.getId()); resp.setName("resp" + req.getId()); resp.setResponseMessage("响应内容" + req.getId()); ctx.writeAndFlush(resp);//.addListener(ChannelFutureListener.CLOSE); &#125; 总结上面的代码：其实只需要在Server和Client端加上这两个，其他方法还是一样的。 sc.pipeline().addLast(MarshallingCodeCFactory.buildMarshallingDecoder()); sc.pipeline().addLast(MarshallingCodeCFactory.buildMarshallingEncoder()); 补充： 如果还想进行大文件（例如图片视频等传输），将上面的注释的代码打开即可，这块代码还使用了Gzip进行压缩，压缩之后再进行传输，这是需要注意的。 特别感谢互联网架构师白鹤翔老师，本文大多出自他的视频讲解。笔者主要是记录笔记，以便之后翻阅，正所谓好记性不如烂笔头，烂笔头不如云笔记]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty入门一]]></title>
    <url>%2F%E6%9E%B6%E6%9E%84%2Fnetty1.html</url>
    <content type="text"><![CDATA[关键字： Netty简介，Netty实现通信的步骤，绑定多个端口，TCP粘包、拆包问题，DellmiterBasedFrameDecoder(自定义分隔符)， FixedLengthFrameDecoder(定长) 代码在 https://github.com/zhaikaishun/NettyTutorial 下的socketIO02 Netty简介Netty是一个高性能、异步事件驱动的NIO框架，它提供了对TCP、UDP和文件传输的支持，作为一个异步NIO框架，Netty的所有IO操作都是异步非阻塞的，通过Future-Listener机制，用户可以方便的主动获取或者通过通知机制获得IO操作结果。作为当前最流行的NIO框架，Netty在互联网领域、大数据分布式计算领域、游戏行业、通信行业等获得了广泛的应用，一些业界著名的开源组件也基于Netty的NIO框架构建。 Netty架构组成http://incdn1.b0.upaiyun.com/2015/04/28c8edde3d61a0411511d3b1866f0636.png代码在SocketIO_02 Netty实现通信的步骤服务器端 创建两个的NIO线程组， 一个专门用于网络事件处理（接受客户端的连接），另一个则进行网络通信读写 创建一个ServerBootstrap对象，配置Netty的一系列参数，例如接受传出数据的缓存大小等等。 创建一个实际处理数据的类Channellnitializer，进行初始化的准备工作，比如设置传出数据的字符集、格式、已经处理数据的接口 绑定端口，执行同步阻塞方法等待服务器端启动即可。 第一个Netty的例子Server端1234567891011121314151617181920212223242526272829303132333435public class Server &#123; public static void main(String[] args) throws Exception &#123; //1 创建线两个程组 //一个是用于处理服务器端接收客户端连接的 //一个是进行网络通信的（网络读写的） EventLoopGroup pGroup = new NioEventLoopGroup(); EventLoopGroup cGroup = new NioEventLoopGroup(); //2 创建辅助工具类，用于服务器通道的一系列配置 ServerBootstrap b = new ServerBootstrap(); b.group(pGroup, cGroup) //绑定俩个线程组 .channel(NioServerSocketChannel.class) //指定NIO的模式 .option(ChannelOption.SO_BACKLOG, 1024) //设置tcp缓冲区 .option(ChannelOption.SO_SNDBUF, 32*1024) //设置发送缓冲大小 .option(ChannelOption.SO_RCVBUF, 32*1024) //这是接收缓冲大小 .option(ChannelOption.SO_KEEPALIVE, true) //保持连接 .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel sc) throws Exception &#123; //3 在这里配置具体数据接收方法的处理 sc.pipeline().addLast(new ServerHandler()); &#125; &#125;); //4 进行绑定 ChannelFuture cf1 = b.bind(8765).sync(); //ChannelFuture cf2 = b.bind(8764).sync(); //5 等待关闭 cf1.channel().closeFuture().sync(); //cf2.channel().closeFuture().sync(); pGroup.shutdownGracefully(); cGroup.shutdownGracefully(); &#125;&#125; ServerHandler12345678910111213141516171819202122232425262728293031323334public class ServerHandler extends ChannelHandlerAdapter &#123; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("server channel active... "); &#125; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; ByteBuf buf = (ByteBuf) msg; byte[] req = new byte[buf.readableBytes()]; buf.readBytes(req); String body = new String(req, "utf-8"); System.out.println("Server :" + body ); String response = "进行返回给客户端的响应：" + body ; ctx.writeAndFlush(Unpooled.copiedBuffer(response.getBytes())); //.addListener(ChannelFutureListener.CLOSE); &#125; @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("读完了"); ctx.flush(); &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable t) throws Exception &#123; ctx.close(); &#125;&#125; Client端1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public class Client &#123; public static void main(String[] args) throws Exception&#123; EventLoopGroup group = new NioEventLoopGroup(); Bootstrap b = new Bootstrap(); b.group(group) .channel(NioSocketChannel.class) .handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel sc) throws Exception &#123; sc.pipeline().addLast(new ClientHandler()); &#125; &#125;); ChannelFuture cf1 = b.connect("127.0.0.1", 8765).sync(); //ChannelFuture cf2 = b.connect("127.0.0.1", 8764).sync(); //发送消息 Thread.sleep(1000); cf1.channel().writeAndFlush(Unpooled.copiedBuffer("777".getBytes())); cf1.channel().writeAndFlush(Unpooled.copiedBuffer("666".getBytes())); //cf2.channel().writeAndFlush(Unpooled.copiedBuffer("888".getBytes())); Thread.sleep(2000); cf1.channel().writeAndFlush(Unpooled.copiedBuffer("888".getBytes())); //cf2.channel().writeAndFlush(Unpooled.copiedBuffer("666".getBytes())); cf1.channel().closeFuture().sync(); //cf2.channel().closeFuture().sync(); group.shutdownGracefully(); &#125;&#125;``` **ClientHandle** ```javapublic class ClientHandler extends ChannelHandlerAdapter&#123; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; &#125; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; try &#123; ByteBuf buf = (ByteBuf) msg; byte[] req = new byte[buf.readableBytes()]; buf.readBytes(req); String body = new String(req, "utf-8"); System.out.println("Client :" + body ); String response = "收到服务器端的返回信息：" + body; &#125; finally &#123; ReferenceCountUtil.release(msg); &#125; &#125; @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception &#123; &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; ctx.close(); &#125;&#125; 先运行Server, 再运行ClientServer端输出1234567server channel active... Server :777666读完了Server :888读完了``` Client端输出 Client :进行返回给客户端的响应：777666Client :进行返回给客户端的响应：88812345例子解读： Server端是不停的监听Client端的消息, Client和Server端的连接是不会停止的，如果想要在Client和Server之间通线后停止Client和Server端的连接，在ServerHandler的channelRead中加上addListener(ChannelFutureListener.CLOSE)，然后在Client端添加cf1.channel().closeFuture().sync(); group.shutdownGracefully(); 就是Client也在异步的监听，如果消息完成，就close掉连接 例如上面那个例子 ServerHandle处 ctx.writeAndFlush(Unpooled.copiedBuffer(response.getBytes())).addListener(ChannelFutureListener.CLOSE)1Client处添加 cf1.channel().closeFuture().sync();group.shutdownGracefully();123456789这样的话，在Client端通信一次之后就断开了，也就是Server只能收到777666的信息，888的信息是接收不到的。 **注意**： 这个异步关闭，一定要在Server端进行关闭，不要在Client端关闭。 如果不想关闭，想保持长连接，那就不加上面那一些代码即可 ## 绑定多个端口 例如上面那个例子，希望再加一个端口 Server端加上 ChannelFuture cf2 = b.bind(8764).sync();cf2.channel().closeFuture().sync();1Client端 ChannelFuture cf2 = b.connect(“127.0.0.1”, 8764).sync();cf2.channel().closeFuture().sync();12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485## TCP粘包、拆包问题 熟悉tcp编程的可能知道，无论是服务端还是客户端，当我们读取或者发送数据的时候，都需要考虑TCP底层的粘包个拆包机制。 tcp是一个“流”协议，所谓流就是没有界限的传输数据，在业务上，一个完整的包可能会被TCP分成多个包进行发送，也可能把多个小包封装成一个大的数据包发送出去，这就是所谓的tcp粘包、拆包问题。 分析TCP粘包拆包问题的产生原因： 1. 应用程序write写入的字节大于套接口缓冲区的大小 2. 进行mss大小的TCP分段 3. 以太网的payload大于MTU进行IP分片粘包拆包问题，根据业界主流协议，有三种主要方案： 1. 消息定长，例如每个报文的大小固定200个字节，如果不够，空位补空格 2. 在包尾部增加特殊字符进行分割，例如加回车等 3. 将消息分为消息头和消息体，在消息头中包含表示消息总长度的字段，然后进行业务逻辑的处理。 ## TCP粘包拆包之分隔符类 DellmiterBasedFrameDecoder(自定义分隔符) 代码在bhz.netty.ende1下 **Server端**```javapublic class Server &#123; public static void main(String[] args) throws Exception&#123; //1 创建2个线程，一个是负责接收客户端的连接。一个是负责进行数据传输的 EventLoopGroup pGroup = new NioEventLoopGroup(); EventLoopGroup cGroup = new NioEventLoopGroup(); //2 创建服务器辅助类 ServerBootstrap b = new ServerBootstrap(); b.group(pGroup, cGroup) .channel(NioServerSocketChannel.class) .option(ChannelOption.SO_BACKLOG, 1024) .option(ChannelOption.SO_SNDBUF, 32*1024) .option(ChannelOption.SO_RCVBUF, 32*1024) .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel sc) throws Exception &#123; //设置特殊分隔符 ByteBuf buf = Unpooled.copiedBuffer(&quot;$_&quot;.getBytes()); sc.pipeline().addLast(new DelimiterBasedFrameDecoder(1024, buf)); //设置字符串形式的解码 sc.pipeline().addLast(new StringDecoder()); sc.pipeline().addLast(new ServerHandler()); &#125; &#125;); //4 绑定连接 ChannelFuture cf = b.bind(8765).sync(); //等待服务器监听端口关闭 cf.channel().closeFuture().sync(); pGroup.shutdownGracefully(); cGroup.shutdownGracefully(); &#125; &#125;``` **ServerHandle端** ```javapublic class ServerHandler extends ChannelHandlerAdapter &#123; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; System.out.println(&quot; server channel active... &quot;); &#125; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; String request = (String)msg; System.out.println(&quot;Server :&quot; + msg); String response = &quot;服务器响应：&quot; + msg + &quot;$_&quot;; ctx.writeAndFlush(Unpooled.copiedBuffer(response.getBytes())); &#125; @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception &#123; &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable t) throws Exception &#123; ctx.close(); &#125;&#125; Client端1234567891011121314151617181920212223242526272829303132public class Client &#123; public static void main(String[] args) throws Exception &#123; EventLoopGroup group = new NioEventLoopGroup(); Bootstrap b = new Bootstrap(); b.group(group) .channel(NioSocketChannel.class) .handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel sc) throws Exception &#123; // ByteBuf buf = Unpooled.copiedBuffer("$_".getBytes()); sc.pipeline().addLast(new DelimiterBasedFrameDecoder(1024, buf)); sc.pipeline().addLast(new StringDecoder()); sc.pipeline().addLast(new ClientHandler()); &#125; &#125;); ChannelFuture cf = b.connect("127.0.0.1", 8765).sync(); cf.channel().writeAndFlush(Unpooled.wrappedBuffer("bbbb$_".getBytes())); cf.channel().writeAndFlush(Unpooled.wrappedBuffer("cccc$_".getBytes())); //等待客户端端口关闭 cf.channel().closeFuture().sync(); group.shutdownGracefully(); &#125;&#125; ClientHandle端123456789101112131415161718192021222324252627public class ClientHandler extends ChannelHandlerAdapter&#123; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("client channel active... "); &#125; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; try &#123; String response = (String)msg; System.out.println("Client: " + response); &#125; finally &#123; ReferenceCountUtil.release(msg); &#125; &#125; @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception &#123; &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; ctx.close(); &#125;&#125; 先启动Server端，再启动Client端Client端输出123client channel active... Client: 服务器响应：bbbbClient: 服务器响应：cccc Server端输出123server channel active... Server :bbbbServer :cccc TCP粘包拆包之FixedLengthFrameDecoder(定长)和上面的代码类似Server端12345678910111213141516171819202122232425262728293031323334353637public class Server &#123; public static void main(String[] args) throws Exception&#123; //1 创建2个线程，一个是负责接收客户端的连接。一个是负责进行数据传输的 EventLoopGroup pGroup = new NioEventLoopGroup(); EventLoopGroup cGroup = new NioEventLoopGroup(); //2 创建服务器辅助类 ServerBootstrap b = new ServerBootstrap(); b.group(pGroup, cGroup) .channel(NioServerSocketChannel.class) .option(ChannelOption.SO_BACKLOG, 1024) .option(ChannelOption.SO_SNDBUF, 32*1024) .option(ChannelOption.SO_RCVBUF, 32*1024) .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel sc) throws Exception &#123; //设置定长字符串接收 sc.pipeline().addLast(new FixedLengthFrameDecoder(5)); //设置字符串形式的解码 sc.pipeline().addLast(new StringDecoder()); sc.pipeline().addLast(new ServerHandler()); &#125; &#125;); //4 绑定连接 ChannelFuture cf = b.bind(8765).sync(); //等待服务器监听端口关闭 cf.channel().closeFuture().sync(); pGroup.shutdownGracefully(); cGroup.shutdownGracefully(); &#125;&#125;``` **ServerHandler** public class ServerHandler extends ChannelHandlerAdapter { @Override public void channelActive(ChannelHandlerContext ctx) throws Exception { System.out.println(&quot; server channel active... &quot;); } @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { String request = (String)msg; System.out.println(&quot;Server :&quot; + msg); String response = request ; ctx.writeAndFlush(Unpooled.copiedBuffer(response.getBytes())); } @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception { } @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable t) throws Exception { } } 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758**Client** ```javapublic class Client &#123; public static void main(String[] args) throws Exception &#123; EventLoopGroup group = new NioEventLoopGroup(); Bootstrap b = new Bootstrap(); b.group(group) .channel(NioSocketChannel.class) .handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel sc) throws Exception &#123; sc.pipeline().addLast(new FixedLengthFrameDecoder(5)); sc.pipeline().addLast(new StringDecoder()); sc.pipeline().addLast(new ClientHandler()); &#125; &#125;); ChannelFuture cf = b.connect(&quot;127.0.0.1&quot;, 8765).sync(); cf.channel().writeAndFlush(Unpooled.wrappedBuffer(&quot;aaaaabbbbb&quot;.getBytes())); cf.channel().writeAndFlush(Unpooled.copiedBuffer(&quot;ccccccc&quot;.getBytes())); //等待客户端端口关闭 cf.channel().closeFuture().sync(); group.shutdownGracefully(); &#125;&#125;``` **ClientHandler** ```javapublic class ClientHandler extends ChannelHandlerAdapter&#123; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; System.out.println(&quot;client channel active... &quot;); &#125; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; String response = (String)msg; System.out.println(&quot;Client: &quot; + response); &#125; @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception &#123; &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; &#125;&#125; 先运行Server端，再运行Client端Client端输出1234client channel active... Client: aaaaaClient: bbbbbClient: ccccc Server端输出 server channel active... Server :aaaaa Server :bbbbb Server :ccccc 可以看到，每次定长都是5个字符， 有两个cc，由于不够五个，导致丢掉了 自定义协议粘包拆包使用消息头和消息体， 暂未记录笔记。 TODO zhaikaishun 特别感谢互联网架构师白鹤翔老师，本文大多出自他的视频讲解。笔者主要是记录笔记，以便之后翻阅，正所谓好记性不如烂笔头，烂笔头不如云笔记]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[传统的socket之BIO到伪异步IO到NIO最后到AIO简介]]></title>
    <url>%2F%E6%9E%B6%E6%9E%84%2FBIO-NIO-AIO.html</url>
    <content type="text"><![CDATA[关键字：NIO， IO,BIO,AIO的简介以及演变原因 本人对nio确实也了解的不深，此文只是简介代码在 https://github.com/zhaikaishun/NettyTutorial 下的socket01 传统的BIO通信：同步阻塞模式看图，侵删 传统的模式是BIO模式，是同步阻塞的，直接看下面这个例子吧。Server端12345678910111213141516 ServerSocket server = null; //bio, 使用一个ServerSocket类进行socket传输 int PROT = 9999; try &#123; server = new ServerSocket(PROT); System.out.println(" server start .. "); //进行阻塞， while (true)&#123; Socket socket = server.accept();//!!直到client建立socket连接的时候，才会走下面的操作 System.out.println("client建立scoket连接后才走这里"); //新建一个线程执行客户端的任务 new Thread(new ServerHandler(socket)).start(); &#125; //下面的不用看了 &#125; catch (Exception e).... ServerHandler处理类,主要用来输出，这里停留5000毫秒123456789in = new BufferedReader(new InputStreamReader(this.socket.getInputStream()));out = new PrintWriter(this.socket.getOutputStream(), true);String body = null;while((body = in.readLine())!=null)&#123; System.out.println(&quot;Server :&quot; + body); Thread.sleep(5000); //给客户端响应 out.println(&quot;这是服务器端回送响的应数据.&quot;);&#125; Client端12345678910111213String ADDRESS = &quot;127.0.0.1&quot;;int PORT = 9999;socket = new Socket(ADDRESS, PORT);in = new BufferedReader(new InputStreamReader(socket.getInputStream()));out = new PrintWriter(socket.getOutputStream(), true);//向服务器端发送数据out.println(&quot;接收到客户端的请求数据...&quot;);System.out.println(&quot;执行完这一句后，下面的in.readLine是阻塞的&quot;);//in.readLine();是阻塞的，必须等待服务器端完成之后才能读的到String response = in.readLine();System.out.println(&quot;Client: &quot; + response); 分析NIO的模式， 先运行Server，Server输出1server start .. 再运行client，client输出1执行完这一句后，下面的in.readLine是阻塞的 Server端输出12client建立scoket连接后才走这里Server :接收到客户端的请求数据... 5秒后再运行client输出:1Client: 这是服务器端回送响的应数据. 总结： Server端的 Socket socket = server.accept(); 会造成阻塞，直到client建立连接的时候才会执行后面的操作Client端的： String response = in.readLine(); 也会造成阻塞，直到Server.accept响应后才能执行后面的操作而且，每个Client都会在Server端建立一个线程，如果client并发较多的时候，Server服务器会承受不住从而导致瘫痪 伪异步IO的模式看图：侵删在没有实现NIO之前的一种模式直接看例子,和上面那个差不错，只不过Server端使用了线程池而已,将客户端的socket封装成一个task任务，这样client并发多的时候，就会通过等待来执行，不会让线程一下子起的太多,下面就只见到记录一下Server端的代码，其他代码和上面的例子一致Server端123456789101112131415int PORT = 9999;ServerSocket server = null;BufferedReader in = null;PrintWriter out = null;try &#123; server = new ServerSocket(PORT); System.out.println(&quot;server start&quot;); Socket socket = null; HandlerExecutorPool executorPool = new HandlerExecutorPool(50, 1000); //其实整体都是一样，只不过这里再加了一个线程池而已 while(true)&#123; socket = server.accept(); executorPool.execute(new ServerHandler(socket)); &#125; &#125; catch (Exception e) 线程池封装类HandlerExecutorPool12345678910111213141516public class HandlerExecutorPool &#123; private ExecutorService executor; public HandlerExecutorPool(int maxPoolSize, int queueSize)&#123; this.executor = new ThreadPoolExecutor( Runtime.getRuntime().availableProcessors(), maxPoolSize, 120L, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;Runnable&gt;(queueSize)); &#125; public void execute(Runnable task)&#123; this.executor.execute(task); &#125;&#125; NIO和IO的关系与区别：阻塞概念：应用程序在获取网络数据的时候，如果网络传输很慢，那么程序就一直等着，直到传输完毕为止非阻塞：应用程序直接可以获取已经准备好的数据，无需等待 NIO： 同步非阻塞 同步：应用程序直接参与io读写，程序会直接阻塞到某个方法上异步： 所有的io都交给操作系统，当操作系统完成了io的操作的时候，会给我们应用程序发通知 同步异步说的是Server服务端的执行方式阻塞说的是技术，接受数据的方式，状态（IO,NIO） NIO几大概念bufferchannelselecter 就是一个插座 buffer使用一下flip方法复位，位置才回到0，但是后面空的会清零（比较好），不建议使用posistion的方法进行处理具体的buffer操作案例，这里也不太想记了（nio真的直接用起来非常少）, 代码都在kaishun.nio.test.TestBuffer 类下 AIO异步非阻塞： 我这里也没有细看，暂时也不是很明白，TODO zhaikaishun 2017-06-10[有时间吧]]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java本地调用cmd，shell命令，远程调用Linux执行命令方法总结]]></title>
    <url>%2Fprogramme%2Fjava-rmi.html</url>
    <content type="text"><![CDATA[有时候经常会碰到需要远程调用Linux或者本地调用Linux或者本地调用cmd的一些命令，最近小结了一下这几种用法 本地调用cmd命令1234567891011121314151617181920212223@Testpublic void testCmd()throws Exception&#123; String cmd="cmd /c date"; //命令的前面必须要有cmd /c execCmd(cmd);&#125;public static void execCmd(String cmd)&#123; try&#123; Runtime rt = Runtime.getRuntime(); //执行命令, 最后一个参数，可以使用new File("path")指定运行的命令的位置 Process proc = rt.exec(cmd,null,null); InputStream stderr = proc.getInputStream(); InputStreamReader isr = new InputStreamReader(stderr,"GBK"); BufferedReader br = new BufferedReader(isr); String line=""; while ((line = br.readLine()) != null) &#123; // 打印出命令执行的结果 System.out.println(line); &#125; &#125;catch (Exception e)&#123; e.printStackTrace(); &#125;&#125; 本地调用Linux命令123456789101112131415161718192021222324@Testpublic void testCmd()throws Exception&#123; String cmd=&quot;/bin/sh -c date&quot;; //命令的前面必须要有/bin/sh -c execCmd(cmd); &#125;public static void execCmd(String cmd)&#123; try&#123; Runtime rt = Runtime.getRuntime(); //执行命令, 最后一个参数，可以使用new File(&quot;path&quot;)指定运行的命令的位置 Process proc = rt.exec(cmd,null,null); InputStream stderr = proc.getInputStream(); InputStreamReader isr = new InputStreamReader(stderr,&quot;GBK&quot;); BufferedReader br = new BufferedReader(isr); String line=&quot;&quot;; while ((line = br.readLine()) != null) &#123; // 打印出命令执行的结果 System.out.println(line); &#125; &#125;catch (Exception e)&#123; e.printStackTrace(); &#125;&#125; 远程调用Linux执行命令jar包下载1http://central.maven.org/maven2/com/jcraft/jsch/0.1.54/jsch-0.1.54.jar 或者maven引用12345&lt;dependency&gt; &lt;groupId&gt;com.jcraft&lt;/groupId&gt; &lt;artifactId&gt;jsch&lt;/artifactId&gt; &lt;version&gt;0.1.54&lt;/version&gt;&lt;/dependency&gt; 参考代码如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263import java.io.BufferedReader;import java.io.IOException;import java.io.InputStream;import java.io.InputStreamReader;import com.jcraft.jsch.ChannelExec;import com.jcraft.jsch.JSch;import com.jcraft.jsch.JSchException;import com.jcraft.jsch.Session;public class SSHHelper &#123; /** * 远程 执行命令并返回结果调用过程 是同步的（执行完才会返回） * @param host 主机名 * @param user 用户名 * @param psw 密码 * @param port 端口 * @param command 命令 * @return */ public static String exec(String host,String user,String psw,int port,String command)&#123; StringBuffer sb= new StringBuffer(); Session session =null; ChannelExec openChannel =null; try &#123; JSch jsch=new JSch(); session = jsch.getSession(user, host, port); java.util.Properties config = new java.util.Properties(); config.put("StrictHostKeyChecking", "no");//跳过公钥的询问 session.setConfig(config); session.setPassword(psw); session.connect(5000);//设置连接的超时时间 openChannel = (ChannelExec) session.openChannel("exec"); openChannel.setCommand(command); //执行命令 int exitStatus = openChannel.getExitStatus(); //退出状态为-1，直到通道关闭 System.out.println(exitStatus); // 下面是得到输出的内容 openChannel.connect(); InputStream in = openChannel.getInputStream(); BufferedReader reader = new BufferedReader(new InputStreamReader(in)); String buf = null; while ((buf = reader.readLine()) != null) &#123; sb.append(buf+"\n"); &#125; &#125; catch (JSchException | IOException e) &#123; sb.append(e.getMessage()+"\n"); &#125;finally&#123; if(openChannel!=null&amp;&amp;!openChannel.isClosed())&#123; openChannel.disconnect(); &#125; if(session!=null&amp;&amp;session.isConnected())&#123; session.disconnect(); &#125; &#125; return sb.toString(); &#125; public static void main(String args[])&#123; String exec = exec("192.168.1.xx", "user", "name", 22, "ls"); System.out.println(exec); &#125;&#125;]]></content>
      <categories>
        <category>programme</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java本地常用IO操作]]></title>
    <url>%2Fprogramme%2Fxml-io-util.html</url>
    <content type="text"><![CDATA[BufferWriter 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778package cn.mastercom.filterSpark;import java.io.*;import java.util.regex.Matcher;import java.util.regex.Pattern;/** * Created by Administrator on 2017/3/18. */public class GenerateConfXml &#123; public static void main(String[] args) &#123; FileInputStream fis = null; InputStreamReader isr = null; BufferedReader br = null ;//ÓÃÓÚ°ü×°InputStreamReader,Ìá¸ß´¦ÀíÐÔÄÜ¡£ÒòÎªBufferedReaderÓÐ»º³åµÄ£¬¶øInputStreamReaderÃ»ÓÐ¡£ String str =&quot;&quot;; String str1=&quot;&quot;; try &#123; fis = new FileInputStream(&quot;conf_test/TB_MODEL_SIGNAL_SAMPLE.sql&quot;); isr = new InputStreamReader(fis); br = new BufferedReader(isr);// ´Ó×Ö·ûÊäÈëÁ÷ÖÐ¶ÁÈ¡ÎÄ¼þÖÐµÄÄÚÈÝ,·â×°ÁËÒ»¸önew InputStreamReaderµÄ¶ÔÏó while ((str = br.readLine())!=null)&#123; str1+=str.trim()+&quot;qqqq&quot;+&quot;\n&quot;; &#125; System.out.println(str1); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; try &#123; br.close(); isr.close(); fis.close(); // ¹Ø±ÕµÄÊ±ºò×îºÃ°´ÕÕÏÈºóË³Ðò¹Ø±Õ×îºó¿ªµÄÏÈ¹Ø±ÕËùÒÔÏÈ¹Øs,ÔÙ¹Øn,×îºó¹Øm &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; File file = new File(&quot;conf_test/TB_MODEL_SIGNAL_SAMPLE.xml&quot;); if(!file.exists())&#123; try &#123; file.createNewFile(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; FileWriter fw = null; try &#123; //ÕâÀïÈç¹ûÊÇ FileWriter fw = new FileWriter(file.getAbsoluteFile(),true);, ÄÇÃ´ÊÇÔÚºóÃæÌí¼Ó fw = new FileWriter(file.getAbsoluteFile()); BufferedWriter bw = new BufferedWriter(fw); bw.write(&quot;&lt;?xml version=\&quot;1.0\&quot; encoding=\&quot;UTF-8\&quot;?&gt; \n&quot;); bw.write(&quot;&lt;table&gt; \n&quot;); bw.write(xml); bw.write(&quot;&lt;/table&gt;&quot;); bw.close(); &#125;catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; private static String attrTransform(String fieldOrAttr) &#123; if(&quot;[varchar]&quot;.equals(fieldOrAttr))&#123; fieldOrAttr = &quot;string&quot;; &#125;else if(&quot;[bigint]&quot;.equals(fieldOrAttr))&#123; fieldOrAttr = &quot;long&quot;; &#125;else if(&quot;[smallint]&quot;.equals(fieldOrAttr)||&quot;[tinyint]&quot;.equals(fieldOrAttr))&#123; fieldOrAttr = &quot;int&quot;; &#125; return fieldOrAttr; &#125;&#125; ,FileOutStrem+StringBuffer123456789101112131415161718192021222324252627282930public class TestFileIo &#123; public static void main(String[] args) &#123; File file=new File("G:\\sqldata2\\result\\out1358\\test.txt"); if(!file.exists()) try &#123; file.createNewFile(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; FileOutputStream out= null; try &#123; out = new FileOutputStream(file,true); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; StringBuffer sb=new StringBuffer(); sb.append("ÕâÊÇµÚ"+1+"ÐÐ:Ç°Ãæ½éÉÜµÄ¸÷ÖÖ·½·¨¶¼²»¹ØÓÃ,ÎªÊ²Ã´×ÜÊÇÆæ¹ÖµÄÎÊÌâ "+"\n"); try &#123; out.write(sb.toString().getBytes("utf-8")); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; try &#123; out.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; 1234567891011121314151617181920212223242526272829/** * */ public static void readFileByLines(String fileName) &#123; File file = new File(fileName); BufferedReader reader = null; try &#123; System.out.println(&quot;ÒÔÐÐÎªµ¥Î»¶ÁÈ¡ÎÄ¼þÄÚÈÝ£¬Ò»´Î¶ÁÒ»ÕûÐÐ£º&quot;); reader = new BufferedReader(new FileReader(file)); String tempString = null; int line = 1; // Ò»´Î¶ÁÈëÒ»ÐÐ£¬Ö±µ½¶ÁÈënullÎªÎÄ¼þ½áÊø while ((tempString = reader.readLine()) != null) &#123; // ÏÔÊ¾ÐÐºÅ System.out.println(&quot;line &quot; + line + &quot;: &quot; + tempString); line++; &#125; reader.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; if (reader != null) &#123; try &#123; reader.close(); &#125; catch (IOException e1) &#123; &#125; &#125; &#125; &#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243/** * */ public static void readFileByBytes(String fileName) &#123; File file = new File(fileName); InputStream in = null; try &#123; System.out.println(&quot;ÒÔ×Ö½ÚÎªµ¥Î»¶ÁÈ¡ÎÄ¼þÄÚÈÝ£¬Ò»´Î¶ÁÒ»¸ö×Ö½Ú£º&quot;); // Ò»´Î¶ÁÒ»¸ö×Ö½Ú in = new FileInputStream(file); int tempbyte; while ((tempbyte = in.read()) != -1) &#123; System.out.write(tempbyte); &#125; in.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); return; &#125; try &#123; System.out.println(&quot;ÒÔ×Ö½ÚÎªµ¥Î»¶ÁÈ¡ÎÄ¼þÄÚÈÝ£¬Ò»´Î¶Á¶à¸ö×Ö½Ú£º&quot;); // Ò»´Î¶Á¶à¸ö×Ö½Ú byte[] tempbytes = new byte[100]; int byteread = 0; in = new FileInputStream(fileName); ReadFromFile.showAvailableBytes(in); // ¶ÁÈë¶à¸ö×Ö½Úµ½×Ö½ÚÊý×éÖÐ£¬bytereadÎªÒ»´Î¶ÁÈëµÄ×Ö½ÚÊý while ((byteread = in.read(tempbytes)) != -1) &#123; System.out.write(tempbytes, 0, byteread); &#125; &#125; catch (Exception e1) &#123; e1.printStackTrace(); &#125; finally &#123; if (in != null) &#123; try &#123; in.close(); &#125; catch (IOException e1) &#123; &#125; &#125; &#125; &#125; °´×Ö·û¶ÁÈ¡ÎÄ¼þÄÚÈÝ /** * ÒÔ×Ö·ûÎªµ¥Î»¶ÁÈ¡ÎÄ¼þ£¬³£ÓÃÓÚ¶ÁÎÄ±¾£¬Êý×ÖµÈÀàÐÍµÄÎÄ¼þ */ public static void readFileByChars(String fileName) { File file = new File(fileName); Reader reader = null; try { System.out.println(&quot;ÒÔ×Ö·ûÎªµ¥Î»¶ÁÈ¡ÎÄ¼þÄÚÈÝ£¬Ò»´Î¶ÁÒ»¸ö×Ö½Ú£º&quot;); // Ò»´Î¶ÁÒ»¸ö×Ö·û reader = new InputStreamReader(new FileInputStream(file)); int tempchar; while ((tempchar = reader.read()) != -1) { // ¶ÔÓÚwindowsÏÂ£¬\r\nÕâÁ½¸ö×Ö·ûÔÚÒ»ÆðÊ±£¬±íÊ¾Ò»¸ö»»ÐÐ¡£ // µ«Èç¹ûÕâÁ½¸ö×Ö·û·Ö¿ªÏÔÊ¾Ê±£¬»á»»Á½´ÎÐÐ¡£ // Òò´Ë£¬ÆÁ±Îµô\r£¬»òÕßÆÁ±Î\n¡£·ñÔò£¬½«»á¶à³öºÜ¶à¿ÕÐÐ¡£ if (((char) tempchar) != &apos;\r&apos;) { System.out.print((char) tempchar); } } reader.close(); } catch (Exception e) { e.printStackTrace(); } try { System.out.println(&quot;ÒÔ×Ö·ûÎªµ¥Î»¶ÁÈ¡ÎÄ¼þÄÚÈÝ£¬Ò»´Î¶Á¶à¸ö×Ö½Ú£º&quot;); // Ò»´Î¶Á¶à¸ö×Ö·û char[] tempchars = new char[30]; int charread = 0; reader = new InputStreamReader(new FileInputStream(fileName)); // ¶ÁÈë¶à¸ö×Ö·ûµ½×Ö·ûÊý×éÖÐ£¬charreadÎªÒ»´Î¶ÁÈ¡×Ö·ûÊý while ((charread = reader.read(tempchars)) != -1) { // Í¬ÑùÆÁ±Îµô\r²»ÏÔÊ¾ if ((charread == tempchars.length) &amp;&amp; (tempchars[tempchars.length - 1] != &apos;\r&apos;)) { System.out.print(tempchars); } else { for (int i = 0; i &lt; charread; i++) { if (tempchars[i] == &apos;\r&apos;) { continue; } else { System.out.print(tempchars[i]); } } } } } catch (Exception e1) { e1.printStackTrace(); } finally { if (reader != null) { try { reader.close(); } catch (IOException e1) { } } } } 123456java¶ÁÐ´Ñ¹ËõÎÄ¼þ.gzÎÄ¼þµÄ¶ÁÐ´¶ÁÈ¡ ÆäÊµ¾ÍÊÇ¶àÁËÒ»²½£¬¾ÍÊÇ°ÑÓÃInputStreamReader ¶Ô FileInputStream½øÐÐÁËÒ»²ã°ü×° public static void main(String[] args) throws Exception { FileInputStream fis =new FileInputStream(“path”); GZIPInputStream gzipInputStream = new GZIPInputStream(fis); InputStreamReader in = new InputStreamReader(gzipInputStream); BufferedReader br = new BufferedReader(in); String str=””; while ((str=br.readLine())!=null){ String[] split = str.split(“\t”); System.out.println(split[0]); } //¹Ø±ÕÁ÷£¬²Î¿¼Ç°ÃæµÄ£¬ÕâÀï¾Í²»Ð´ÁË}12Ð´ Ïà±ÈÓÚÃ»ÓÐÑ¹ËõµÄ£¬¾Í¶àÁËÒ»²½ GZIPOutputStream ¶Ô FileOutputStreamµÄ°ü×° public static void saveToGZFiles(String text, File file) { if(!file.exists()){ try { file.createNewFile(); } catch (IOException e) { e.printStackTrace(); } } try { FileOutputStream out= null; out = new FileOutputStream(file,true); GZIPOutputStream gzipOut = new GZIPOutputStream(out); StringBuffer sb=new StringBuffer(); sb.append(text); gzipOut.write(sb.toString().getBytes(&quot;utf-8&quot;)); // out.close(); gzipOut.close(); }catch (Exception e){ e.printStackTrace(); } } 1¶ÁÈ¡ÎÄ¼þ¼ÐÏÂµÄËùÓÐÎÄ¼þ String path=”E:\datatest\input\TB_CQTSIGNAL_SAMPLE_01_170221”;File files = new File(path);if(files.isFile()){ //ËµÃ÷ÊÇÎÄ¼þ System.out.println(“ÎÄ¼þ”); System.out.println(“path=” + files.getPath()); System.out.println(“absolutepath=” + files.getAbsolutePath()); System.out.println(“name=” + files.getName());}if(files.isDirectory()){ //ËµÃ÷ÊÇÎÄ¼þ¼Ð String[] filelist = files.list(); for (int j = 0; j &lt; filelist.length; j++){ //µ÷ÓÃÉÏÃæµÄ¶ÁÈ¡·½·¨ fis = new FileInputStream(path+”\“+filelist[i]); …. }}123¸´ÖÆÎÄ¼þÒÔÎÄ¼þÁ÷µÄ·½Ê½¸´ÖÆÎÄ¼þ public void copyFile(String src,String dest) throws IOException…{ FileInputStream in=new FileInputStream(src); File file=new File(dest); if(!file.exists()) file.createNewFile(); FileOutputStream out=new FileOutputStream(file); int c; byte buffer[]=new byte[1024]; while((c=in.read(buffer))!=-1)…{ for(int i=0;i]]></content>
      <categories>
        <category>programme</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>io</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jar读取资源配置文件，jar包内包外，以及包内读取目录的方法]]></title>
    <url>%2Fprogramme%2Fjava-read-jarfile.html</url>
    <content type="text"><![CDATA[java程序打成jar包后，经常碰到一些资源文件找不到等问题，最近总结了一下之前用到的几种获取路径、资源文件的方法 测试代码代码如下，并且打成jar包123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package cn.zks.pathtest;import java.io.BufferedReader;import java.io.File;import java.io.FileReader;import java.io.IOException;import java.io.InputStream;import java.io.InputStreamReader;public class TestPath &#123; public static void main(String[] args) &#123; TestPath testPath = new TestPath(); testPath.testPath(); &#125; public void testPath()&#123; try &#123; String userPath = System.getProperty("user.dir"); String path = TestPath.class.getProtectionDomain().getCodeSource().getLocation().getPath(); String pathGetClass =this.getClass().getProtectionDomain().getCodeSource().getLocation().getPath(); String classLoaderPath = getClass().getClassLoader().toString(); System.out.println("userPath: "+userPath); System.out.println("path: "+path); System.out.println("pathGetClass: "+pathGetClass); System.out.println("classLoaderPath: "+classLoaderPath); //看看getClassLoader是读取jar包里面还是读取jar包外面 InputStream inputStream = getClass().getClassLoader().getResourceAsStream("text.txt"); BufferedReader br = new BufferedReader(new InputStreamReader(inputStream)); String line=""; while((line=br.readLine())!=null)&#123; System.out.println("getClassLoader: "+line); &#125; //看看pathGetClass读取的是jar包里面，还是jar包外面 File file = new File(pathGetClass+"text.txt"); BufferedReader reader = new BufferedReader(new FileReader(file)); String tempString =""; while ((tempString = reader.readLine()) != null)&#123; System.out.println("pathGetClass: "+tempString); &#125; &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125; 测试1第一次，jar包里面不放text.txt, 只在jar包外面放text.txt,外面的text的内容是:it is jar out ，在jar包当前目录运行jar，运行后1234567G:\testpath&gt;java -jar testpath.jaruserPath: G:\testpathpath: /G:/testpath/testpath.jarpathGetClass: /G:/testpath/testpath.jarclassLoaderPath: java.net.URLClassLoader@14ae5a5getClassLoader: it is jar outpathGetClass: it is jar out userPath是当前执行命令的目录， 测试2第二次：jar包里面放text.txt,里面的text.txt内容是:it is in jar ，外面也放了text.txt, 内容是it is jar out：, 在jar包当前目录运行jar，运行后1234567G:\testpath&gt;java -jar testpath.jaruserPath: G:\testpathpath: /G:/testpath/testpath.jarpathGetClass: /G:/testpath/testpath.jarclassLoaderPath: java.net.URLClassLoader@14ae5a5getClassLoader: it is in jarpathGetClass: it is jar out 测试3第三次：在Administrator目录运行指定目录的jar包,Administrator目录下没有text.txt1234567891011121314151617181920212223242526272829303132333435363738394041C:\Users\Administrator&gt;java -jar G:\testpath\testpath.jaruserPath: C:\Users\Administratorpath: /G:/testpath/testpath.jarpathGetClass: /G:/testpath/testpath.jarclassLoaderPath: java.net.URLClassLoader@14ae5a5getClassLoader: it is in jarjava.io.FileNotFoundException: .\text.txt (系统找不到指定的文件。) at java.io.FileInputStream.open0(Native Method) at java.io.FileInputStream.open(Unknown Source) at java.io.FileInputStream.&lt;init&gt;(Unknown Source) at java.io.FileReader.&lt;init&gt;(Unknown Source) at cn.zks.pathtest.TestPath.testPath(TestPath.java:44) at cn.zks.pathtest.TestPath.main(TestPath.java:17) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) at java.lang.reflect.Method.invoke(Unknown Source) at org.eclipse.jdt.internal.jarinjarloader.JarRsrcLoader.main(JarRsrcLoader.java:58)``` ## 总结： 1. getClass().getClassLoader().getResourceAsStream(path), 先是读取jar包内部有没有这个path，如果内部没有时，再读取jar包当前目录下的path, 值得注意的是这个需要有一个实例化类，才能getClass，否则是编译不过的2. this.getClass().getProtectionDomain().getCodeSource().getLocation().getPath(); 得到的是程序运行所在路径3. System.getProperty(&quot;user.dir&quot;); 和 this.getClass().getProtectionDomain().getCodeSource().getLocation().getPath() 是不一样的， System.getProperty(&quot;user.dir&quot;)得到的是用户执行的时候的那个路径，不一定是jar包所在路径 总之：我还是尽量选择第一个,getClass().getClassLoader().getResourceAsStream(path)，但是要小心第一种的配置，是优先读取jar包内部的路径，jar包内部读取不到时才读取当前jar包位置的路径对于System.getProperty()更多用法，参考：http://blog.csdn.net/kongqz/article/details/3987198 ## 读取jar包内某个目录所有文件的方法读取jar包内某个目录的所有文件，还是比较麻烦的，getClassLoader().getResourceAsStream也只能得到某一个文件，并不能得到文件夹，网上的大多数方法都不太可取，最后，自己想了一个取巧的办法来获取，方法如下 1. 得到jar包所在路径 2. JarFile打开这这个jar包3. JarFile.entries();4. 就是过滤到自己的目录5. 使用getClassLoader().getResourceAsStream来分别获取每一个文件代码示例: jar包内有个conf文件夹，里面有很多个文件，我想要输出里面的内容 String path = TestPath.class.getProtectionDomain().getCodeSource().getLocation().getPath(); System.out.println(&quot;path: &quot;+path); JarFile localJarFile = new JarFile(new File(path)); Enumeration&lt;JarEntry&gt; entries = localJarFile.entries(); while (entries.hasMoreElements()) { JarEntry jarEntry = entries.nextElement(); System.out.println(jarEntry.getName()); String innerPath = jarEntry.getName(); if(innerPath.startsWith(&quot;conf&quot;)){ InputStream inputStream = TestPath.class.getClassLoader().getResourceAsStream(innerPath); BufferedReader br = new BufferedReader(new InputStreamReader(inputStream)); String line=&quot;&quot;; while((line=br.readLine())!=null){ System.out.println(innerPath+&quot;内容为:&quot;+line); } } } 1234**17年9月6号额外补充** ## System.getProperty方法 public class TestSystemProperty { public static void main(String[] args) { System.out.println(&quot;java版本号：&quot; + System.getProperty(&quot;java.version&quot;)); // java版本号 System.out.println(&quot;Java提供商名称：&quot; + System.getProperty(&quot;java.vendor&quot;)); // Java提供商名称 System.out.println(&quot;Java提供商网站：&quot; + System.getProperty(&quot;java.vendor.url&quot;)); // Java提供商网站 System.out.println(&quot;jre目录：&quot; + System.getProperty(&quot;java.home&quot;)); // Java，哦，应该是jre目录 System.out.println(&quot;Java虚拟机规范版本号：&quot; + System.getProperty(&quot;java.vm.specification.version&quot;)); // Java虚拟机规范版本号 System.out.println(&quot;Java虚拟机规范提供商：&quot; + System.getProperty(&quot;java.vm.specification.vendor&quot;)); // Java虚拟机规范提供商 System.out.println(&quot;Java虚拟机规范名称：&quot; + System.getProperty(&quot;java.vm.specification.name&quot;)); // Java虚拟机规范名称 System.out.println(&quot;Java虚拟机版本号：&quot; + System.getProperty(&quot;java.vm.version&quot;)); // Java虚拟机版本号 System.out.println(&quot;Java虚拟机提供商：&quot; + System.getProperty(&quot;java.vm.vendor&quot;)); // Java虚拟机提供商 System.out.println(&quot;Java虚拟机名称：&quot; + System.getProperty(&quot;java.vm.name&quot;)); // Java虚拟机名称 System.out.println(&quot;Java规范版本号：&quot; + System.getProperty(&quot;java.specification.version&quot;)); // Java规范版本号 System.out.println(&quot;Java规范提供商：&quot; + System.getProperty(&quot;java.specification.vendor&quot;)); // Java规范提供商 System.out.println(&quot;Java规范名称：&quot; + System.getProperty(&quot;java.specification.name&quot;)); // Java规范名称 System.out.println(&quot;Java类版本号：&quot; + System.getProperty(&quot;java.class.version&quot;)); // Java类版本号 System.out.println(&quot;Java类路径：&quot; + System.getProperty(&quot;java.class.path&quot;)); // Java类路径 System.out.println(&quot;Java lib路径：&quot; + System.getProperty(&quot;java.library.path&quot;)); // Java lib路径 System.out.println(&quot;Java输入输出临时路径：&quot; + System.getProperty(&quot;java.io.tmpdir&quot;)); // Java输入输出临时路径 System.out.println(&quot;Java编译器：&quot; + System.getProperty(&quot;java.compiler&quot;)); // Java编译器 System.out.println(&quot;Java执行路径：&quot; + System.getProperty(&quot;java.ext.dirs&quot;)); // Java执行路径 System.out.println(&quot;操作系统名称：&quot; + System.getProperty(&quot;os.name&quot;)); // 操作系统名称 System.out.println(&quot;操作系统的架构：&quot; + System.getProperty(&quot;os.arch&quot;)); // 操作系统的架构 System.out.println(&quot;操作系统版本号：&quot; + System.getProperty(&quot;os.version&quot;)); // 操作系统版本号 System.out.println(&quot;文件分隔符：&quot; + System.getProperty(&quot;file.separator&quot;)); // 文件分隔符 System.out.println(&quot;路径分隔符：&quot; + System.getProperty(&quot;path.separator&quot;)); // 路径分隔符 System.out.println(&quot;直线分隔符：&quot; + System.getProperty(&quot;line.separator&quot;)); // 直线分隔符 System.out.println(&quot;操作系统用户名：&quot; + System.getProperty(&quot;user.name&quot;)); // 用户名 System.out.println(&quot;操作系统用户的主目录：&quot; + System.getProperty(&quot;user.home&quot;)); // 用户的主目录 System.out.println(&quot;当前程序所在目录：&quot; + System.getProperty(&quot;user.dir&quot;)); // 当前程序所在目录 } }```]]></content>
      <categories>
        <category>programme</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive 1.2.2安装 教程]]></title>
    <url>%2Fhadoop%2Fhive-1.2.2-install.html</url>
    <content type="text"><![CDATA[关键字：hive 1.2.2 安装，hive配置 安装前提： 安装好了hadoop，hadoop的集群搭建：http://blog.csdn.net/t1dmzks/article/details/68958562 安装好了mysql： mysql的编译安装（yum安装） http://blog.csdn.net/t1dmzks/article/details/71374740 下载并且配置环境http://mirrors.hust.edu.cn/apache/hive/ 进入hive-1.2.2/ 下载 apache-hive-1.2.2-bin.tar.gz 1234567891011[root@master Downloads]# cp apache-hive-1.2.2-bin.tar.gz /usr/local/[root@master Downloads]# cd /usr/local[root@master local]# tar zxvf apache-hive-1.2.2-bin.tar.gz[root@master hadoop]# vim /etc/profile# set hive environmentexport HIVE_HOME=/usr/local/apache-hive-1.2.2-binexport PATH=$PATH:$HIVE_HOME/bin mysql 的配置以及权限mysql安装： http://blog.csdn.net/t1dmzks/article/details/71374740mysql配置hive表12345678910mysql&gt; CREATE DATABASE hive; # 创建hive表mysql&gt; grant all on hive.* to hive@&apos;%&apos; identified by &apos;shun&apos;; #给hive的权限，shun是我数据库的密码Query OK, 0 rows affected, 1 warning (0.00 sec)mysql&gt; grant all on hive.* to hive@localhost identified by &apos;shun&apos;;Query OK, 0 rows affected, 1 warning (0.00 sec)mysql&gt; flush privileges;Query OK, 0 rows affected (0.00 sec) 配置hive先复制配置文件1234567[root@master apache-hive-1.2.2-bin]# cd $HIVE_HOME/conf/[root@master conf]# cp hive-default.xml.template hive-site.xml[root@master conf]# cp hive-default.xml.template hive-site.xml[root@master conf]# cp hive-env.sh.template hive-env.sh[root@master conf]# cp hive-exec-log4j.properties.template hive-exec-log4j.properties[root@master conf]# cp hive-log4j.properties.template hive-log4j.properties 创建一些目录 hdfs上的目录下面的配置需要用到这些目录123[hadoop@node01 ~]$ hadoop fs -mkdir -p /hive/warehouse[hadoop@master ~]$ hadoop fs -mkdir -p /hive/logs[hadoop@master ~]$ hadoop fs -mkdir -p /hive/tmp 创建本地的目录下面的配置需要用到这些目录 1234[root@master bin]# mkdir -p /hive/logs[root@master bin]# mkdir -p /hive/exec[root@master bin]# mkdir -p /hive/downloadedsource[root@master /]# chown hadoop:hadoop /hive/ -R 配置hive-site.xmlhive-ite.xml中的配置很多，我们需要修改的是下面的这些，下面的description就能看出来配置的作用12345678910111213141516171819202122232425262728293031323334353637383940414243444546&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;description&gt;Username to use against metastore database&lt;/description&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;shun&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://master:3306/hive?createDatabaseIfNotExist=true&amp;amp;useSSL=false&lt;/value&gt; &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.uris&lt;/name&gt; &lt;value/&gt; //这里我是默认的，没变 &lt;description&gt;Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/hive/warehouse&lt;/value&gt; //到时候需要在hdfs上建立想要的目录 &lt;description&gt;location of default database for the warehouse，Hive在HDFS上的根目录&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt; &lt;value&gt;/hive/exec&lt;/value&gt; &lt;description&gt;Local scratch space for Hive jobs&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.downloaded.resources.dir&lt;/name&gt; &lt;value&gt;/hive/downloadedsource&lt;/value&gt; &lt;description&gt;Temporary local directory for added resources in the remote file system.&lt;/description&gt; &lt;/property&gt; 配置log4j.property修改hive.log.dir为1hive.log.dir=/hive/logs 下载mysql驱动包12345678[root@master conf]# cd $HIVE_HOME/lib[root@master lib]# wget http://central.maven.org/maven2/mysql/mysql-connector-java/5.1.38/mysql-connector-java-5.1.38.jar``` # 启动 进入到 bin 目录下，用hadoop用户启动先启动 ./hive –service metastore &amp;1再另外开一个终端启动hive [hadoop@master bin]$ hive1显示 [hadoop@master bin]$ hive Logging initialized using configuration in file:/usr/local/apache-hive-1.2.2-bin/conf/hive-log4j.propertieshive&gt;```若有问题就尽量按照提示，可以参考google，baidu 等]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解析XML文件的几种方式]]></title>
    <url>%2Fprogramme%2Fxml-parse1.html</url>
    <content type="text"><![CDATA[前言虽然更喜欢用json,但是xml更灵活，在配置文件的时候更方便 要用SAXRead需要的jar包dom4j-1.6.1.jar, jaxen-1.1-beta-10.jar XML文件格式如下1234567891011121314151617181920212223242526272829303132&lt;?xml version="1.0" encoding="UTF-8"?&gt; &lt;!-- tablename保存数据表的名称 fieldname保存字段名称， type对应的是字段的属性，都必须得有 --&gt;&lt;tables&gt; &lt;table tablename="people"&gt; &lt;fieldname fieldname="name" type="string"/&gt; &lt;fieldname fieldname="age" type="int"/&gt; &lt;splitsign rule="," /&gt; &lt;/table&gt; &lt;table tablename="book"&gt; &lt;fieldname fieldname="name" type="string"/&gt; &lt;fieldname fieldname="price" type="int"/&gt; &lt;splitsign rule="." /&gt; &lt;/table&gt; &lt;table tablename="cat"&gt; &lt;fieldname fieldname="color" type="string"/&gt; &lt;fieldname fieldname="category" type="string"/&gt; &lt;splitsign rule=" " /&gt; &lt;/table&gt; &lt;table tablename="people1"&gt; &lt;fieldname fieldname="name" type="string"/&gt; &lt;fieldname fieldname="age" type="int"/&gt; &lt;fieldname fieldname="sex" type="string"/&gt; &lt;splitsign rule="," /&gt; &lt;/table&gt; &lt;/tables&gt; 读取所有的表名称12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576private String configPath="conf/config_table.xml"; /** * * @return an arrayList tableList * @throws DocumentException */ public ArrayList getTableName() throws DocumentException&#123; ArrayList&lt;String&gt; tableNameArray = new ArrayList&lt;String&gt;(); SAXReader reader = new SAXReader(); Document document = reader.read(new File(configPath)); String xpathtable = "//tables//table"; List&lt;String&gt; list = document.selectNodes(xpathtable); Iterator iter = list.iterator(); while(iter.hasNext())&#123; Element element = (Element) iter.next(); tableNameArray.add(element.attribute(0).getValue()); &#125; return tableNameArray; &#125; ``` ## 根据表名称，获取所有的字段以及属性```java /** * @param table name * @return an String[] String[0] is field, String[1] is sttribute * @throws DocumentException */ public String[] getFieldAndAttr(String tablename) throws DocumentException&#123; // Initialize table field String fieldString = ""; // Initialize table attribute String attrString=""; // Initialize the split sign String splitsignString = ""; String[] tableFieldAndAttr=null; SAXReader reader = new SAXReader(); Document document = reader.read(new File(configPath)); String xpath = "//tables//table[@tablename='"+tablename+"']//fieldname"; List&lt;String&gt; list = document.selectNodes(xpath); Iterator iter = list.iterator(); while(iter.hasNext())&#123; Element element = (Element) iter.next(); // connect the field fieldString = fieldString+element.attribute(0).getValue()+ " "; // connect the attribute attrString = attrString+element.attribute(1).getValue()+ " "; &#125; // remove the last character fieldString = fieldString.substring(0,fieldString.length()-1); attrString = attrString.substring(0,attrString.length()-1); String xpathSplit = "//tables//table[@tablename='"+tablename+"']//splitsign"; List&lt;String&gt; signlist = document.selectNodes(xpathSplit); Iterator splititer = signlist.iterator(); while(splititer.hasNext())&#123; Element splitElement = (Element) splititer.next(); splitsignString = splitsignString+splitElement.attribute(0).getValue(); &#125; tableFieldAndAttr = new String[]&#123;fieldString,attrString,splitsignString&#125;; return tableFieldAndAttr; &#125; ``` ## 另外介绍一下XMLConfiguration, 将所有的字段以及属性放入到map中, 使用XMLConfiguration需要引入的jar包commons-configuration-1.0.jar, package teststring; import org.apache.commons.configuration.ConfigurationException;import org.apache.commons.configuration.XMLConfiguration; import java.util.HashMap;import java.util.Iterator;import java.util.Map; /** Created by Administrator on 2017/3/21.*/public class TestConf { public static void main(String[] args) { Map&lt;String,Object&gt; dataMap=new HashMap&lt;String,Object&gt;(); try { XMLConfiguration config = new XMLConfiguration(&quot;conf/datawatch_config.xml&quot;); config.setEncoding(&quot;UTF-8&quot;); config.setDelimiterParsingDisabled(true); Iterator&lt;String&gt; iter = config.getKeys(); while (iter.hasNext()) { String key = iter.next().toString(); dataMap.put(key, config.getProperty(key)); } } catch (ConfigurationException e) { e.printStackTrace(); } }} 12345678910111213## XMLconfig的XPATH的使用, 使用XPATH解析， xpath是我比较喜欢的一种解析方式```xml&lt;comm&gt; &lt;data&gt; &lt;name&gt;zks&lt;/name&gt; &lt;age&gt;24&lt;/age&gt; &lt;/data&gt; &lt;data&gt; &lt;name&gt;skz&lt;/name&gt; &lt;age&gt;23&lt;/age&gt; &lt;/data&gt;&lt;/comm&gt; 需要引入的jar包 commons-jxpath-1.3.jar,commons-configuration-1.0.jar其中 commos-jaxpath-1.3下载地址1234567 XMLConfiguration config = new XMLConfiguration(&quot;conf/datawatch_config.xml&quot;); //注意下面的两句话，必须这样才能使用xpath去解析XPathExpressionEngine xPathExpressionEngine = new XPathExpressionEngine();config.setExpressionEngine(xPathExpressionEngine);System.out.println(config.getString(&quot;data[1]/name&quot;));System.out.println(config.getString(&quot;data[2]/name&quot;));System.out.println(config.getString(&quot;data[name=&apos;skz&apos;]/age&quot;)); 将会输出zksskz23 更多XPATH解析方法参考 菜鸟教程 补充xml如下 &lt;comm&gt; &lt;data&gt; &lt;name&gt;zks&lt;/name&gt; &lt;age&gt;24&lt;/age&gt; &lt;sex&gt;man&lt;/sex&gt; &lt;/data&gt; &lt;data&gt; &lt;name&gt;skz&lt;/name&gt; &lt;age&gt;23&lt;/age&gt; &lt;/data&gt; &lt;/comm&gt; 如果我想得到第一个data下的所有的， 然后按照上面的，存入map中，怎么做呢， 我们可以得到data下的这个config，可以使用SubnodeConfiguration subnodeConfiguration = config.configurationAt(key),代码如下 SubnodeConfiguration subnodeConfiguration = config.configurationAt("data[1]"); // 下面就可以用了 Iterator itea1 = subnodeConfiguration.getKeys(); while (itea1.hasNext()){ System.out.println("iter.next().toString(): "+itea1.next().toString()); System.out.println("conf.getProperty"+subnodeConfiguration.getProperty(key)); } 参考自stackoverflow 和 http://commons.apache.org/proper/commons-configuration/userguide/howto_xml.html]]></content>
      <categories>
        <category>programme</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>xml解析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[动态规划之基本概念]]></title>
    <url>%2F%E7%AE%97%E6%B3%95%2Fdynamic-programming-1.html</url>
    <content type="text"><![CDATA[怀念曾经 曾经大学学过的数学当中，最喜欢的就是高等数学和运筹学，不仅仅和自己兴趣有关，也和老师的敬业、以及负责性有关。运筹学，本就以实际例子为例讲解，模型设计极为巧妙，优化方法非常有趣，并且叫我们运筹学的老师年轻漂亮，教学效果还很好。所以虽然这么久了。我依然还记得很多里面的内容。由于最近工作碰到几个算法，属于道路优化相关，设计到了运筹学的一些知识，于是又花时间将将运筹学给看了一遍，突然发现运筹学博大精深，不仅仅在计算机中，在控制论，经济学，以及整个人生都存在，多年后再看运筹学，已经不是以前的只为考试了，慢慢体会到运筹学中动态规划的意义了，本文大多属于书上的内容，这里算是做做笔记把。 是一种方法而不是算法动态规划的方法，动态规划作为一种数学方法，是解决某类问题的一种途径，是一种思维，而非特定的某种算法，不能像快排，线性规划一样，有具体的规则，本篇文章主要介绍动态规划的相关理念和方法。## 引出问题-线路网络问题在这个方法之前，先引出一个问题,下图是一个线路网络，两点之间的连线代表两点间的距离，试求一条从A到G的道路，使总距离为最短## 最原始的解决方法：穷举法即把从A到G的所有路线都列出来，然后相互之间找出最短的路线。这样，从A到G，一共有2 3 2 2 2 1 = 48 条不同的路线, 这样需要进行比较47次（最后一次不需要比较），在计算距离的时候，需要计算多少次加法呢，总共需要计算6 + 12 + 24 +48 +48 = 138 次加法运算， 这个数是怎么算的呢， 如果稍微敏感点的都应该知道了，如果你知道或者相信这就是138次加法运算，就跳过下面几个阶段的描述A到B阶段： 有两种情况，A-&gt;B1或者A-&gt;B2, 这一次计算距离不需要使用加法。B到C时阶段: B1到C有3情况种，这时候都要和A到B1的数值5相加， B2到C也有三种情况，也要和A到B2的数值3相加，所以, B到C，需要计算23=6次加法。 也可以这样理解，A到B总共有两条路线，B发出有三条路线，那么B到C 有2 3 =6 个加法。C到D阶段：因为从A到C总共有2 3=6 条路线，每条路线发出2条到D1， 那么总共有 6 2=12条方法。*D到E，E到F，F到G就不累赘了这里先记住这道题的这个结论，这道题使用穷举法，需要把这48条路都给比较一次,总共需要比较47次，并且计算出这48条路每条路的距离，需要计算138次加法运算 概念原理原理-&gt;例子我学习大多数知识（什么算法，大数据，hadoop，spark，微积分，机器学习等等等等）都有个习惯，一般都是先看原理，再看解决问题的例子，最后再倒过来看原理。 第一次看原理的时候，可能是看不懂的，这时候没关系，直接看案例，最后搞懂案例，再回过来思考原理，然后站在一个较高的的位置再看例子。我还是以根据这种方法 我们先来看一下原理 动态规划的基本概念1. 阶段把所给问题的过程, 恰当地分为若干个相互联的阶段, 以便能按一定的次序去求解。描述阶段的变量称为阶段变量 , 常用 k 表示,如上面那一个题目，可分为6个阶段来求解 , k分别等于 1、2、3、4、5、6。 2. 状态状态表示每个阶段开始所处的自然状况或客观条件(可能太深奥无法理解，可以暂时不去思考这句话),通常一个阶段有若干个状态, 第一阶段有一个状态就是点A,第二阶段有两个状态, 即点集合{B1, B2} , 一般第 k 阶段的状态就是第 k 阶段所有始点的集合描述过程状态的变量称为状态变量。 它可用一个数、一组数或一向量(多维情形)来描述。 常用 Sk 表示第 k 阶段的状态变量如在上个例子第三阶段有四个状态 , 则状态变量 Sk 可取四个值, 即 C1、C2、C3、C4 。点集合{C1, C2, C3, C4 }就称 为第三阶段的 可达状态集合， 也可以叫做某阶段的状态集合有时候为了方便，直接记做 S3 = {1 ,2 ,3 ,4}。 第 k 阶段的可达状态集合就记为Sk 这里所说的状态应具有下面的性质 : 如果某阶段状态给定后 , 则在这阶段以后过程的发展不受这阶段以前各段状态的影响，这个性质称为无后效性还是上面那个例子，如果说确定了从A到G要经过D2， 那么1 2 3 阶段无论是怎么样的，456阶段都没有影响。 3. 决策决策表示当过程处于某一阶段的某个状态时 , 可以作出不同的决定( 或选择) , 从而确定下一阶段的状态 , 这种决定称为决策常用$u_k(s_k)$表示第k阶段当状态处于 sk 时的决策变量。 它是状态变量的函数, 某一个状态，其决策时有限的，我们某个限制下的所有决策放在集合中，称这个集合为允许决策集合$u_k(s_k) \varepsilon D_k(Sk)$4. 策略决策排好顺序，比如第一阶段的一个决策，第二阶段的一个决策，第三阶段的一个决策，一个一个的排好顺序，就叫做策略，所以,策略是一个按顺序排列的决策组成的集合,由过程的第 k 阶段开始到终止状态为止的过程 , 称为问题的后部子过程，记做$p{k,n}(s_k) ={u_k(sk), u{k + 1}(s_{k+1}) ,… u_n(sn)}$当 k = 1 时 , 此决策函数序列称为全过程的一个策略, 简称策略,记为$p{1,n}(s_k) ={u_1(s_1), u_2(s_2) ,… u_n(s_n)} $在实际问题中 , 可供选择的策略有一定的范围 , 此范围称为允许策略集, 用P表示。 从允许策略集合中找出达到最优效果的策略称为最优策略 5. 状态转移方程这一点非常重要，一定要理解： 所谓的状态转移方程，就是从一个状态到另外一个状态的一种转移规律，可以这样理解，一个状态，经过一定的变化，到达下一个状态，这个变化的函数，为状态转移函数，这个转移的规律，就是状态转移方程 $s_{k+1} = T_k (s_k,u_k)$ 上述方程可以理解为， sk，这个sk阶段的决策变量uk已经确定，那么下一个状态s(k+1)也已经确定. 6. 指标函数和最优值函数就是每段过程中，我们想要的最优化的结果，例如上面那个问题，我们想要的结果是A到G的总路线最短，最优函数就是各个阶段指标之和 线路网络例子再再次把问题列一下： 求A到G的最短路线 先说一个重要特性，这是解决这一问题最重要的一个特性 ==如果由起点A经过P点和H点而到达终点G是一条最短路线 , 则由点P出发经过H点到达终点G的这条子路线,对于从点P出发到达终点的所有可能选择的不同路线来说, 必定也是最短路线==例如 例如, 在最短路线问题中, 若找到了 A→ B1 → C2 → D1 → E2 → F2 → G 是由A到G的最短路线 , 则 E2 → F2 → G 应该是 由E2 出发到 G 点的所有可能选择的不同路线中的 最短路线。这个仔细想想就能想通了，如果要证明，就要用反证法了，例如E2到G有另外一个路线，更加快，那么原来的路线到E2，再经过这条路线才能是最短的，然而这样与原来的路线是最短路线矛盾。 根据最短路线这一特性, 寻找最短路线的方法, 就是从最后一段开始, 用由后向前逐步 递推的方法, 求出各点到G点的最短路线,最后求得由A点到G点的最短路线，所以,动态规划的方法是从终点逐段向始点方向寻找最短路线的一种方法。 下面是解题思路 动态规划的基本方程 动态规划和穷举法对比的优点 减少了计算量，还是以这道题为例子，穷举法（计算机中一般采用递归）要比较47次，加法计算138次，而动态规划方法，只需要进行3+3+4+4+1=15次比较，每次比较有两次加法，实际只有28次运算。如果阶段更多，这种减少的效果会更明显。 丰富了计算结果。在逆序排序中，我们得到了所有点出发到G点的距离，这在很多实际分析中将十分有用 如果建立动态规划模型 将问题的过程分成恰当的阶段，正确选择状态变量SK。 正确选择状态变量 sk , 使它既能描述过程的演变 , 又要满足无后效性。 确定决策变量 uk 及每阶段的允许决策集合 D k ( sk )。 正确写出状态转移方程。 正确写出指标函数 V k, n 的关系 未来，将继续介绍动态规划的另外几个例子，来自运筹学中的排序问题，背包问题，货郎担问题，算法导论中的钢条分割，最长公共子序列，以及图论中的一些问题等1234参考资料：运筹学第三版(胡运权) 清华大学出版社马尔科夫决策规划-1981-董泽清算法导论第三版 12]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>动态规划</tag>
        <tag>数学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我为什么写博客]]></title>
    <url>%2F%E9%9A%8F%E7%AC%94%2Fmylife2.html</url>
    <content type="text"><![CDATA[引入点由于前几天天行vpn被封，都不知道如何访问google了，突然感觉没有google的世界是多么的恐怖，很是怀恋之前的vps，突然想到，曾经的大学舍友很喜欢捣鼓这个，就问了问还有没有代理的方法，哇，不问不知道，一问吓一跳，一年多不见，今天才知道肥羊竟然是这方面的高手，然后完美解决了我这里的问题：肥羊的博客: https://91vps.club/ – 怕是被封了 另外的他的个人博客 http://feiyang.li/ 反正我觉得我关注他就能无限使用vps了 ^_^ 把我带入博客世界的两个人：shilei： 曾经带我的师傅，csdn前600名（不知道怎么坚持下来的），博客地址：http://blog.csdn.net/hanshileiai shilei为人极其低调，专业技术很强，曾经给予了我很多的帮助。有的人是刚开始很好，接触久了就变得很一般了，而shilei却相反，和他接触的时间越久，不仅仅是技术，更重要的是，更欣赏他的为人。 正是我们有不懂的都问他，他有时候直接给我们他的一篇文章，慢慢的，我开始有了写博客的想法 qiuyan: csdn博客： http://blog.csdn.net/q1059081877q 相比于 shilei，qiuyan给人一种平易近人的感觉，所以，我已有不懂的，更多的是找他帮我解决，qiuyan比我早毕业三年，作为一个女生，性格很开朗，也喜欢写博客，有自己独立的博客，也在csdn上写。由于平时聊得比较多。还有周围的一些小伙伴都有自己的博客，此时我写博客的想法也更加的强烈了。 最终决定写博客的原因我从小就不愿意背东西，也不愿意记很多没意思的东西，更愿意去理解。需要记忆的，我一直都觉得没有任何兴趣，而且我只要记在外部存储就行了。想要的时候查看既可以了。所以，我很早的时候就有了记笔记的情况，从最开始的用纸质的笔记本，到后来发现纸质的笔记本完全满足不了自己的需求，慢慢的转向了用一些云笔记记录。其中，有道云是我用的比较久的一款笔记。 由于平时爱好比较多，什么都懂一点，就是不精通，我笔记里面记录了很多的技术文章，2016年3月的时候，大概有了40多个目录，400多篇文章，现在目录到了70多个，文章数超过了700多篇，有自己的爱好，也有自己的感悟，也有一些学习的笔记，可是，笔记只要一多起来，就感觉很乱了， 乱了总要想办法给自己整理整理，思考一下，还是写博客比较方便，于是就开始将部分有道云笔记中记录的很乱的，或者常用的给写成博客。 所以，我写博客，最终的目的就是微课记笔记，偶尔也为分享 为什么选择HEXO随便选的，因为那时候朋友用的要么是github，要么是个人博客，要么是csdn。 github我只用来托管代码，加上那时候确实也没那么熟悉，就没使用。个人博客我是不会去做的，因为我觉得浪费自己的时间。目前我用额外时间所搭建了3个网站，已经够累了。csdn博客专栏已经不错了，其实最开始选择的是csdn, 但是想想也想自己随便写点什么，反正最终就用成了HEXO]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer前10题]]></title>
    <url>%2F%E7%AE%97%E6%B3%95%2Foffer-harvester.html</url>
    <content type="text"><![CDATA[二维数组中的查找问题:在一个二维数组中，每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394 /*思路一： * 把每一行看成有序递增的数组， 利用二分查找， 通过遍历每一行得到答案， 时间复杂度是nlogn */ public boolean find(int [][] array,int target) &#123; for(int i=0;i&lt;array.length;i++)&#123; int low=0; int high=array[i].length-1; while(low&lt;=high)&#123; int mid=(low+high)/2; if(target&gt;array[i][mid])&#123; low=mid+1; &#125;else if(target&lt;array[i][mid])&#123; high=mid-1; &#125;else&#123; return true; &#125; &#125; &#125; return false; &#125; /* * 思路二: * 利用二维数组由上到下，由左到右递增的规律， 那么选取右上角或者左下角的元素a[row][col]与target进行比较， 当target小于元素a[row][col]时，那么target必定在元素a所在行的左边, 即col--； 当target大于元素a[row][col]时，那么target必定在元素a所在列的下边, 即row++； */ public boolean find2(int [][] array,int target)&#123; int rows=0; int cols=array[0].length-1; while(rows&lt;=array.length-1&amp;&amp;cols&gt;=0)&#123; if(target==array[rows][cols])&#123; return true; &#125;else if(target&lt;array[rows][cols])&#123; cols--; &#125;else&#123; rows++; &#125; &#125; return false; &#125;``` ## 替换空格请实现一个函数，将一个字符串中的空格替换成“%20”。例如，当字符串为We Are Happy.则经过替换之后的字符串为We%20Are%20Happy。```java//方法1public String replaceSpace(StringBuffer str) &#123; return str.toString().replace(&quot; &quot;, &quot;%20&quot;); &#125;//方法2 肯定是不能用replace函数了 /* * 思路：从后往前替换； （1）统计str中空格的个数 （2）定义两个指针变量：原始str的结束索引indexOld 和 替换后str的结束索引indexNew （3）从后向前循环替换，直到indexOld与indexNew碰头为止 代码如下（java）： */public String replaceSpace2(StringBuffer str) &#123; //遍历可变字符序列，来统计blank的个数 int count =0; for (int i = 0; i &lt; str.length(); i++) &#123; if(str.charAt(i) == &apos; &apos;) count++; &#125; //定义两个指针变量：原始字符串的结束索引indexOld，替换后str的结束索引indexNew int indexOld = str.length()-1; int indexNew = indexOld + 2*count; //扩大StringBuffer的长度 str.setLength(indexNew+1); //循环 while(indexOld &gt;=0 &amp;&amp; indexNew &gt; indexOld)&#123; if(str.charAt(indexOld) == &apos; &apos;)&#123; str.setCharAt(indexNew--, &apos;0&apos;); str.setCharAt(indexNew--, &apos;2&apos;); str.setCharAt(indexNew--, &apos;%&apos;); //注意：替换完后indexOld需要减1 indexOld--; &#125;else str.setCharAt(indexNew--, str.charAt(indexOld--)); &#125; return str.toString(); &#125; 输入一个链表，从尾到头打印链表每个节点的值。12345678910public class Solution &#123; ArrayList&lt;Integer&gt; arrayList=new ArrayList&lt;Integer&gt;(); public ArrayList&lt;Integer&gt; printListFromTailToHead(ListNode listNode) &#123; if(listNode!=null)&#123; this.printListFromTailToHead(listNode.next); arrayList.add(listNode.val); &#125; return arrayList; &#125;&#125; 重建二叉树输入某二叉树的前序遍历和中序遍历的结果，请重建出该二叉树。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。例如输入前序遍历序列{1,2,4,7,3,5,6,8}和中序遍历序列{4,7,2,1,5,3,8,6}，则重建二叉树并返回。参考文章 旋转数组中的最小数字把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。输入一个非递减排序的数组的一个旋转，输出旋转数组的最小元素。例如数组{3,4,5,1,2}为{1,2,3,4,5}的一个旋转，该数组的最小值为1。NOTE：给出的所有元素都大于0，若数组大小为0，请返回0。123456789101112131415161718 public int Fibonacci(int n) &#123; if(n==0)&#123; return 0; &#125;else if(n==1)&#123; return 1; &#125; //不能用迭代，会内存溢出// return Fibonacci(n-1)+Fibonacci(n-2); int recode = 0; //记录上n-2的值 int result=1; //记录n-1的值 int trans=0; //为了中转而已 for(int i=2;i&lt;=n;i++)&#123; trans=result; result= recode+result; recode=trans; &#125; return result; &#125; 跳台阶一只青蛙一次可以跳上1级台阶，也可以跳上2级。求该青蛙跳上一个n级的台阶总共有多少种跳法。1234567891011121314151617181920212223242526/* * 对于本题,前提只有 一次 1阶或者2阶的跳法。 a.如果两种跳法，1阶或者2阶，那么假定第一次跳的是一阶，那么剩下的是n-1个台阶，跳法是f(n-1); b.假定第一次跳的是2阶，那么剩下的是n-2个台阶，跳法是f(n-2) c.由a\b假设可以得出总跳法为: f(n) = f(n-1) + f(n-2) d.然后通过实际的情况可以得出：只有一阶的时候 f(1) = 1 ,只有两阶的时候可以有 f(2) = 2 e.可以发现最终得出的是一个斐波那契数列： */ public int JumpFloor(int target) &#123; if(target==1)&#123; return 1; &#125;else if(target==2)&#123; return 2; &#125; //用迭代或者下面的方法// return JumpFloor(n-1)+JumpFloor(n-2); int recode = 1; //记录上n-2的值 int result=2; //记录n-1的值 int trans=0; //为了中转而已 for(int i=3;i&lt;=target;i++)&#123; trans=result; result= recode+result; recode=trans; &#125; return result; &#125; 变态跳台阶一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。求该青蛙跳上一个n级的台阶总共有多少种跳法。 分析:f(1) = 1f(2) = f(2-1) + f(2-2)f(3) = f(3-1) + f(3-2) + f(3-3)…f(n) = f(n-1) + f(n-2) + f(n-3) + … + f(n-(n-1)) + f(n-n) 所以f(n)= f(n) = f(0) + f(1) + f(2) + f(3) + … + f(n-2) + f(n-1) = f(n-1) + f(n-1)=2*f(n-1)所以 fn=除了0之后，其他是等比数列1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public int JumpFloorII(int target) &#123; if(target==0)&#123; return 1; &#125; return 1&lt;&lt;(target-1); &#125;``` ## 矩形覆盖我们可以用2\*1的小矩形横着或者竖着去覆盖更大的矩形。请问用n个2\*1的小矩形无重叠地覆盖一个2\*n的大矩形，总共有多少种方法？```java//这里就直接用递归了，其实最好别用递归，万一n很大会内存溢出public class Solution &#123; public int RectCover(int target) &#123; if (target &lt; 1) &#123; return 0; &#125; else if (target == 1 || target == 2) &#123; return target; &#125; else &#123; return RectCover(target-1) + RectCover(target-2); &#125; &#125;&#125;``` ## 二进制中1的个数输入一个整数，输出该数二进制表示中1的个数。其中负数用补码表示。 分析: 如果一个整数不为0，那么这个整数至少有一位是1。如果我们把这个整数减1，那么原来处在整数最右边的1就会变为0，原来在1后面的所有的0都会变成1(如果最右边的1后面还有0的话)。其余所有位将不会受到影响。 负数也是一样的，因为题目中说了用补码表示```javapublic class Solution &#123; public int NumberOf1(int n) &#123; int count = 0; while(n!= 0)&#123; count++; n = n &amp; (n - 1); &#125; return count; &#125;&#125;``` ## 数值的整数次方给定一个double类型的浮点数base和int类型的整数exponent。求base的exponent次方。 分析：这个题目有多种解法，有递归，累乘，都比较容易，这里介绍累乘的方法:```javapublic static double power(double base, int exponent) &#123; double result = 1.0; for (int i = 0; i &lt; Math.abs(exponent); i++) &#123; result *= base; &#125; if (exponent &gt;= 0) return result; else return 1 / result; &#125; 调整数组顺序使奇数位于偶数前面输入一个整数数组，实现一个函数来调整该数组中数字的顺序，使得所有的奇数位于数组的前半部分，所有的偶数位于位于数组的后半部分，并保证奇数和奇数，偶数和偶数之间的相对位置不变。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>剑指向offer</tag>
        <tag>笔试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论加班]]></title>
    <url>%2Fhadoop%2Fspark%2Fspark-sql-dataType-transform.html</url>
    <content type="text"><![CDATA[it行业加班是普遍现象，千万不要和程序猿抱怨工作累！曾经的一个在国企朋友向我抱怨，内容是这样的：“8点30过一分钟打卡就算迟到，5点30打卡就算早退，觉得太不公平了，工作太累了”，据我所知，该同学工作很轻松，每天基本就是盖盖章，谢谢报告等，每天能8点半上班，5点30还没到，公司都人都准备好东西排队打卡回家了。！！！！！有没有搞错，what’s the fuck，你向谁抱怨不好，竟然向程序猿抱怨工作累！！！ 还一次，上个月高中几个在深圳的同学聚会，一个做证券，一个做药物研发，一个做财务管理的，就我一个程序猿。我们四个人自然而然的讨论到了工作。证券的同说：刚毕业还是很累的，每天8点30晨会，下午还要做到3点半才能下班，但是刚毕业为了积攒客户，基本都是5点下班的。药物研发：你这算累吗，我们工作更无聊，更累，每天要走好几十分钟，下班回家都6点半了财务管理：我也累呀，经常这里那里的培训。有时候一次就是半天，也累得很。我 下班后我基本是这样的 画面切换到曾经的宿舍微信群：舍友A – 游戏公司研发工程师，：（时间周六凌晨三点）真刺激，跟着大神把数据进行了转码，终于成功了。舍友B –硬件研发工程师 ：每天回家都是11点了，洗个脚都没有时间！我：我已经躺下了，哈哈 以上全是我最近碰到的真人真事，身为一个程序猿，千万别和其他行业的比较哪个累，最终发现，最累的其实还是程序猿。 是否是你自身的问题导致你加班为什么自己经常加班，首先得问自己，是否在上班过程中打小差了，是否是自己能力不足，在原本合理的预估时间内没有做完，如果真的是这样，加班那也是没办法的事情。一般这种情况，通过自己加班来解决，一般还是会比较开心的。 加班是否有意义如果整个公司的氛围都是加班加班加班，下班后都要迟一两个小时再走，谁谁谁走的晚谁谁谁就比较认真负责，走的早就是态度不好。如果碰到了这样的公司，先看加班氛围有多严重，是否大多数白天不怎么干活，就靠加班来干活，如果真的是这样。那加班的意义也就不存在了，我所认为加班的意义，就是在白天很努力的工作下，还是完成不了，需要加班来完成，这样才是加班的意义，然而很多公司的氛围，导致了员工白天不努力工作，只有靠加班才能完成任务。 是否是由于压榨造成加班部分公司真的压榨，想尽可能的少招人，多做事来减少自己的成本，巴不得一个人干2个人的活，如果你碰到了一个这样的公司，业务很忙，员工每天很努力，然而还是要加很久的班，公司却不继续招人。碰到这样的公司，最好在试用期之前就看清楚，该辞就应该辞了。因为一旦有一天你只能干一个人的活的时候，他们就会觉得你在偷懒了。 有没有对你的生活造成较大影响适当的加班属于正常现象，如果频繁的加班，对生活造成了巨大的影响，那就要好好考虑一下了。毕竟生活才是最重要的 你真的愿意主动加班吗什么？ 加班也快乐？ 是的，适当的加班，完成尚未完成的任务，自己会有一种成就感。下面几种情况，我会主动加班： 自己预估（计划）能完成的没有完成。 和别人合作的时候，需要双方都完成的时候，为了不影响其他人的时候。 确实是自己的问题，一些基础的问题，或者一些很不应该的bug问题。 但是，大多数情况下，加班是不快乐的。其中有几点我最反感的，最不愿意加班的 产品或者上级乱改需求导致加班。 共同负责的问题只有我一个人加班。 周末打电话询问工作的事情。 加班成氛围，有事没事都得迟几个小时走。 打扰自己的学习计划。]]></content>
      <categories>
        <category>大数据</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark常用RDD算子 汇总（java和scala版本）]]></title>
    <url>%2Fhadoop%2Fspark%2Fspark-rdd-tutorial.html</url>
    <content type="text"><![CDATA[github： https://github.com/zhaikaishun/spark_tutorialspark RDD的算子挺多，有时候如何灵活的使用，该如何用一下子想不起来，这一段时间将spark的算子如何使用的例子给记录了下来，下面是spark RDD 的一些常用算子的使用这些算子包括有java的，也有scala的语言，由于精力有限，暂时没有python的，以后有空再加上吧spark RDD算子（一） parallelize，makeRDD，textFile spark RDD算子（二） filter,map ,flatMap spark RDD算子（三） distinct，union，intersection，subtract，cartesian spark RDD算子（四）之创建键值对RDD mapToPair flatMapToPair spark RDD算子（五）之键值对聚合操作 combineByKey spark RDD算子（六）之键值对聚合操作reduceByKey，foldByKey，排序操作sortByKey spark RDD算子（七）之键值对分组操作 groupByKey，cogroup spark RDD算子（八）之键值对关联操作 subtractByKey, join, rightOuterJoin, leftOuterJoin spark RDD算子（九）之基本的Action操作 first, take, collect, count, countByValue, reduce, aggregate, fold,top spark RDD算子（十）之PairRDD的Action操作countByKey, collectAsMap spark RDD算子（十一）之RDD Action 保存操作saveAsTextFile,saveAsSequenceFile,saveAsObjectFile,saveAsHadoopFile 等 spark RDD算子（十二）之RDD 分区操作上mapPartitions, mapPartitionsWithIndex spark RDD算子（十三）之RDD 分区 HashPartitioner，RangePartitioner，自定义分区]]></content>
      <categories>
        <category>大数据</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark-2.2.0源码阅读环境的搭建]]></title>
    <url>%2Fhadoop%2Fspark%2Fspark-sourcecode-install.html</url>
    <content type="text"><![CDATA[下载源码去官网 下载 spark-2.2.0.tgz 解压后，用idea打开pom maven需要下载很多包，并且加载，需要等待一下，最好建议改成阿里云的依赖仓库，这样比较快。不改也可以 找到example包中的SparkPi类，setMaster后运行这个类运行的时候肯定会报错，很多情况下就是找不到类。spark源码放在idea中还是比较坑的啊，真是麻烦，但是一般也就下面两种情况，安装下面的方法就可以解决了 报错SparkFlumeProtocol 找不到在intellij ieda里面： 打开View -&gt; Tool Windows -&gt; Maven Projects 右击Spark Project External Flume Sink 点击Generate Sources and Update Folders随后，Intellij IDEA会自动下载Flume Sink相关的包 然后重新build -&gt; Make Project，一切ok!! This should generate source code from sparkflume.avdl.Generate Sources and Update Folders do can resolve type SparkFlumeProtocol not found issue.来源： http://apache-spark-developers-list.1001551.n3.nabble.com/A-Spark-Compilation-Question-td8402.html 报错 SparkSqlParser.包找不到 IntelliJ 下载并且安装 antlr4插件 打开View -&gt; Tool Windows -&gt; Maven Projects 右击park Project Catalyst 点击Generate sources and update folders 运行java的sparkpi的时候报这个错1Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/apache/spark/SparkConfat org.apache.spark.examples.JavaSparkPi.main(JavaSparkPi.java:41) 实在找不到问题，编译不报错，运行的时候就报错，肯定是少了某个包，估计是少了scala的包，然后去运行scala的LocalPi这个scala类，哇，这个都运行不了,报scala错误，搞了好久，难道在玩我。后来自己写个scala的类，也报错，OK ，那确定是scala没引入。最后解决办法就是给加上scala的包了:file-&gt;Project Structure-&gt;对这个example的项目加上scala的lib，然后再次运行~, OK 没问题，环境也就搭建好了]]></content>
      <categories>
        <category>大数据</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Sql 编程式结构DataType转换 代码类小结]]></title>
    <url>%2Fhadoop%2Fspark%2Fspark-sql-dataType-transform.html</url>
    <content type="text"><![CDATA[基本类型数据转DataType12345678910111213141516171819202122232425262728293031323334353637383940public DataType attrToDataType(String attrstr)&#123; DataType returnDataType = DataTypes.StringType; switch(attrstr.toLowerCase())&#123; case "short" : returnDataType = DataTypes.StringType; break; case "char" : returnDataType = DataTypes.StringType; break; case "int" : returnDataType = DataTypes.IntegerType; break; case "integer" : returnDataType = DataTypes.IntegerType; break; case "float" : returnDataType = DataTypes.FloatType; break; case "double" : returnDataType = DataTypes.DoubleType; break; case "string" : returnDataType = DataTypes.StringType; break; case "varchar": returnDataType = DataTypes.StringType; break; case "long": returnDataType = DataTypes.LongType; break; case "boolean": returnDataType = DataTypes.BooleanType; break; case "date": returnDataType = DataTypes.DateType; break; &#125; return returnDataType; &#125; 基本类型转封装类1234567891011121314151617181920212223242526272829303132333435363738/** * @param fields field name * @param attrstr field attribute * @return an object -- String,Integer,Long and etc */public Object arrtToObject(String fields,String attrstr)&#123; Object fieldObject = fields; switch(attrstr.toLowerCase())&#123; case "short" : fieldObject = Short.parseShort(fields); break; case "char" : fieldObject = fields; break; case "int" : fieldObject = Integer.parseInt(fields); break; case "integer" : fieldObject = Integer.parseInt(fields); break; case "float" : fieldObject = Float.parseFloat(fields); break; case "double" : fieldObject = Double.parseDouble(fields); break; case "long": fieldObject = Long.parseLong(fields); break; case "boolean": fieldObject = Boolean.parseBoolean(fields); break; case "date": fieldObject = Date.parse(fields); break; &#125; return fieldObject;&#125; 当字段列表和属性列表都生成后，用for循环去createStructField，字段和属性要对应上1234 for(int i=0;i&lt;schemaList.length;i++)&#123; fields.add(DataTypes.createStructField(schemaList[i], dataTypesList[i], true)); &#125;StructType schema = DataTypes.createStructType(fields); 生成rowRDD的时候，字段和属性也要对应上1234567891011121314JavaRDD&lt;Row&gt; rowRDD = people.map( new Function&lt;String, Row&gt;() &#123; public Row call(String record) throws Exception &#123; String[] fields = record.split(FieldSplitSign); //TODO maybe cause the memory leak Object[] objeck = new Object[fields.length];// System.out.println("field length:" + fields.length); for(int i=0;i&lt;fields.length;i++)&#123;// System.out.println("fields[i].trim()"+fields[i].trim()); objeck[i] = dataTypUtils.arrtToObject(fields[i].trim(),fieldAttyList[i].trim()); &#125; return RowFactory.create(objeck); &#125; &#125;);]]></content>
      <categories>
        <category>大数据</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Shuffle FetchFailedException异常]]></title>
    <url>%2Fhadoop%2Fspark%2Fspark-fetchfailedException.html</url>
    <content type="text"><![CDATA[翻译http://stackoverflow.com/questions/34941410/fetchfailedexception-or-metadatafetchfailedexception-when-processing-big-data-se 123456org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0org.apache.spark.shuffle.FetchFailedException: Failed to connect to ip-xxxxxxxxorg.apache.spark.shuffle.FetchFailedException: Error in opening FileSegmentManagedBuffer&#123;file=/mnt/yarn/nm/usercache/xxxx/appcache/application_1450751731124_8446/blockmgr-8a7b17b8-f4c3-45e7-aea8-8b0a7481be55/08/shuffle_0_224_0.data, offset=12329181, length=2104094&#125; This error is almost guaranteed to be caused by memory issues on your executors. I can think of a couple of ways to address these types of problems. 1) You could try to run with more partitions (do a repartition on your dataframe). Memory issues typically arise when one or more partitions contain more data than will fit in memory. 2) I’m noticing that you have not explicitly set spark.yarn.executor.memoryOverhead, so it will default to max(386, 0.10* executorMemory) which in your case will be 400MB. That sounds low to me. I would try to increase it to say 1GB (note that if you increase memoryOverhead to 1GB, you need to lower –executor-memory to 3GB) 3) Look in the log files on the failing nodes. You want to look for the text “Killing container”. If you see the text “running beyond physical memory limits”, increasing memoryOverhead will - in my experience - solve the problem. 大概的翻译：这个错误几乎肯定会被记忆的问题在你的遗嘱执行人造成的。我能想到几种方法来解决这些类型的问题。 1）你可以尝试更多的分区上运行。存储器问题通常出现在一个或多个分区包含比将适合存储器更多的数据。 2）我注意到，你还没有明确设置spark.yarn.executor.memoryOverhead，所以它会默认max(386, 0.10* executorMemory)而你的情况将是400MB。这可能太低了。我会尽量增加到1GB（注意，如果你增加memoryOverhead为1GB，你需要降低–executor-memory至3GB） 3）看在日志文件中失败的节点上。你想查找文本“杀容器”。如果你看到文本“运行超出物理内存限制”，增加memoryOverhead 这是我解决的经验 额外补充一些： 如果存在数据倾斜，那么可以从这方面来解决，使用更好的分区方式，例如修改分区的数量，方式，或者自定义分区的方式]]></content>
      <categories>
        <category>大数据</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark碰到的问题]]></title>
    <url>%2Fhadoop%2Fspark%2Fspark-fqa.html</url>
    <content type="text"><![CDATA[java.io.FileNotFoundException: File does not existjava.io.FileNotFoundException: File does not exist: hdfs://master:9000/user/hmaster/.sparkStaging/application_1498791665418_0041/spark-assembly-1.4.0-hadoop2.6.0.jar at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1309) at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1301) at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81) at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1301) at org.apache.hadoop.yarn.util.FSDownload.copy(FSDownload.java:253) at org.apache.hadoop.yarn.util.FSDownload.access$000(FSDownload.java:63) at org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:361) at org.apache.hadoop.yarn.util.FSDownload$2.run(FSDownload.java:359) at java.lang.Thread.run(Thread.java:745) ... Failing this attempt. Failing the application. ApplicationMaster host: N/A ApplicationMaster RPC port: -1 queue: root.hmaster start time: 1499311838058 final status: FAILED tracking URL: http://master:8088/cluster/app/application_1498791665418_0041 user: hmaster Exception in thread &quot;main&quot; org.apache.spark.SparkException: Application application_1498791665418_0041 finished with failed status at org.apache.spark.deploy.yarn.Client.run(Client.scala:841) at org.apache.spark.deploy.yarn.Client$.main(Client.scala:867) at org.apache.spark.deploy.yarn.Client.main(Client.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:664) ... 原因： 目前主要碰到这两种 代码中本地测试setMaster(“local”)，集群中运行应该去掉这个 编译打包用的jdk版本和集群上配置不一样 spark yarn 模式下 一直 运行不报错spark yarn模式下，打开节点很少spark standalone 模式下，不报错，停在某个statge额某个任务个人目前觉得standalone模式在一些复杂的计算中不太使用，之前测试一份数据，计算比较复杂，standalone为了追求速度，所有节点火力全开，导致cpu阻塞 不断增加]]></content>
      <categories>
        <category>大数据</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 定时任务Crontab]]></title>
    <url>%2FLinux%2Flinux-crontab.html</url>
    <content type="text"><![CDATA[Crontab 介绍Crontab是Linux系统中在固定时间执行某一个程序的工具，类似于Windows系统中的任务计划程序，在一些自动化的程序或者运维中，起到很大的作用，在很多数据处理方面，例如定时备份数据库，定时指定某些任务等等，用linux的Crontab往往比较方便，下面介绍如何安装与使用crontab crontab安装检查是否已经安装，若已经安装那么不需要重复安装先查看crontab有没有安装，有的话就不需要安装了,==root用户下==12345678[root@master crontest]# service crond status 若没有出现status说明已经安装``` 若没有安装，则按以下步骤进行## 安装```shellyum -y install vixie-cronyum -y install crontabs 说明：vixie-cron 软件包是 cron 的主程序；crontabs 软件包是用来安装、卸装、或列举用来驱动 cron 守护进程的表格的程序。 配置123456service crond start //启动服务service crond stop //关闭服务service crond restart //重启服务service crond reload //重新载入配置service crond status //查看crontab服务状态chkconfig --level 345 crond on //在CentOS系统中加入开机自动启动: 添加定时器任务1crontab -e 如果是root用户，可以直接编写下面的1/etc/crontab cron文件语法:下面是打开/etc/crontab显示的内容12345678910111213141516SHELL=/bin/bashPATH=/sbin:/bin:/usr/sbin:/usr/binMAILTO=rootHOME=/# For details see man 4 crontabs# Example of job definition:# .---------------- minute (0 - 59)# | .------------- hour (0 - 23)# | | .---------- day of month (1 - 31)# | | | .------- month (1 - 12) OR jan,feb,mar,apr ...# | | | | .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat# | | | | |# * * * * * user-name command to be executed# */1 * * * * hadoop /home/hadoop/crontest/testcronmkdir.sh 分 时 日 月 周 用户 命令注意最后一个是周，不是年！！！ ,/etc/crontab 需要指定用户, crontable -e就没有 0-59 0-23 1-31 1-12 0-6 command (取值范围,0表示周日一般一行对应一个任务) 下面有一些特殊符号需要记住，通过这些特殊符号，可以很灵活的设置定时任务1234“*”代表取值范围内的数字,“/”代表”每”,“-”代表从某个数字到某个数字,“,”分开几个离散的数字 如果把最后的命令换做成脚本，例如我上面的那个，记得必须得是.sh脚本，那就需要在最前面引入1#!/bin/bash]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>crontab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 常见的查找相关笔记]]></title>
    <url>%2FLinux%2Flinux-find-xargs.html</url>
    <content type="text"><![CDATA[查找文件名 find -name filename 按照文件名查找1234find / -name httpd.conf #在根目录下查找文件httpd.conf，表示在整个硬盘查找find /etc -name httpd.conf #在/etc目录下文件httpd.conffind /etc -name &apos;*srm*&apos; #使用通配符*(0或者任意多个)。表示在/etc目录下查找文件名中含有字符串‘srm’的文件find . -name &apos;srm*&apos; #表示当前目录下查找文件名开头是字符串‘srm’的文件 按照文件特征查找 123456789find / -amin -10 # 查找在系统中最后10分钟访问的文件(access time)find / -atime -2 # 查找在系统中最后48小时访问的文件find / -empty # 查找在系统中为空的文件或者文件夹find / -group cat # 查找在系统中属于 group为cat的文件find / -mmin -5 # 查找在系统中最后5分钟里修改过的文件(modify time)find / -mtime -1 #查找在系统中最后24小时里修改过的文件find / -user fred #查找在系统中属于fred这个用户的文件find / -size +10000c #查找出大于10000000字节的文件(c:字节，w:双字，k:KB，M:MB，G:GB)find / -size -1000k #查找出小于1000KB的文件 使用混合查找方式查找文件参数有： ！，-and(-a)，-or(-o)。 123find /tmp -size +10000c -and -mtime +2 #在/tmp目录下查找大于10000字节并在最后2分钟内修改的文件find / -user fred -or -user george #在/目录下查找用户是fred或者george的文件文件find /tmp ! -user panda #在/tmp目录中查找所有不属于panda用户的文件 查找包含字符串的文件 grep “string” ./ “string”为待查找串 ， ./ 表示当前目录下所有文件二、grep命令 基本格式：find expression 1.主要参数 [options]主要参数： －c：只输出匹配行的计数。 －i：不区分大小写 －h：查询多文件时不显示文件名。 －l：查询多文件时只输出包含匹配字符的文件名。 －n：显示匹配行及行号。 －s：不显示不存在或无匹配文本的错误信息。 －v：显示不包含匹配文本的所有行。 pattern正则表达式主要参数： \： 忽略正则表达式中特殊字符的原有含义。 ^：匹配正则表达式的开始行。 $: 匹配正则表达式的结束行。 \&lt;：从匹配正则表达 式的行开始。 >：到匹配正则表达式的行结束。 [ ]：单个字符，如[A]即A符合要求 。 [ - ]：范围，如[A-Z]，即A、B、C一直到Z都符合要求 。 .：所有的单个字符。 * ：有字符，长度可以为0。 2.实例 (1)grep ‘test’ d* #显示所有以d开头的文件中包含 test的行 (2)grep ‘test’ aa bb cc #显示在aa，bb，cc文件中包含test的行 (3)grep ‘[a-z]{5}’ aa #显示所有包含每行字符串至少有5个连续小写字符的字符串的行 (4)grep magic /usr/src #显示/usr/src目录下的文件(不含子目录)包含magic的行 (5)grep -r magic /usr/src #显示/usr/src目录下的文件(包含子目录)包含magic的行 (6)grep -w pattern files ：只匹配整个单词，而不是字符串的一部分(如匹配’magic’，而不是’magical’)，]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[异常Exception in thread -AWT-EventQueue-0- java.lang.NullPointerException]]></title>
    <url>%2Fprogramme%2Fjava-awt-eventqueue-exception.html</url>
    <content type="text"><![CDATA[Exception in thread “AWT-EventQueue-0” java.lang.NullPointerException更新UI等操作时异常个人理解原因：类似于Android中的更新ui一样，ui线程相当于主线程，再对此进行直接更新，两个线程事物会冲突报这个异常，在Android中使用的是handle的方法，同理,类似于，在java 界面编程上，我们把其他的线程放在一个队列当中，我们称这个队列为消息队列，可以使用如下方法，把更新UI的操作，放在下面的run方法中去。 这样，就实现了消息队列的方式。 最后由主线程有序的处理这些消息事件12345SwingUtilities.invokeLater(new Runnable()&#123; public void run() &#123; //放入方法 &#125; &#125;); 例子如下12345SwingUtilities.invokeLater(new Runnable()&#123; public void run() &#123; tree.updateUI(); &#125; &#125;); 12345SwingUtilities.invokeLater(new Runnable() &#123; public void run() &#123; SwingUtilities.updateComponentTreeUI(jf); &#125; &#125;);]]></content>
      <categories>
        <category>programme</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>Exception</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDBC批量插入与更新工具类]]></title>
    <url>%2Fprogramme%2Fjdbc-util.html</url>
    <content type="text"><![CDATA[批量添加1234567891011121314151617181920212223242526272829303132 public void UpdateFileData(String sSQL, ArrayList&lt;String[]&gt; objParams) &#123; GetConn(); int iResult = 0; try &#123;// Statement ps = _CONN.createStatement(); PreparedStatement preparedStatement = _CONN.prepareStatement(sSQL); for (int i = 0; i &lt;objParams.size() ; i++) &#123; preparedStatement.setString(1,objParams.get(i)[0]); preparedStatement.setString(2,objParams.get(i)[1]); preparedStatement.setString(3,objParams.get(i)[2]); preparedStatement.setLong(4,Integer.parseInt(objParams.get(i)[3])); preparedStatement.setString(5,objParams.get(i)[4]); preparedStatement.setString(6,objParams.get(i)[5]); preparedStatement.addBatch(); &#125; preparedStatement.executeBatch(); &#125; catch (Exception ex) &#123; System.out.println(ex.getMessage()); &#125; finally &#123; CloseConn(); &#125; &#125;``` ## 参考网上的例子 import java.sql.Connection;import java.sql.Date;import java.sql.PreparedStatement;import java.sql.Statement; import org.junit.Test; import xuezaipiao1.JDBC_Tools;/** 向Oracle 的 temp 数据表中添加 10万 条记录 测试如何插入，用时最短*/ public class JDBCTest { /** * * 1.使用 Statement . * 测试用时:35535 */ @Test public void testBbatchStatement() { Connection conn = null; Statement statement = null; String sql = null; try { conn = JDBC_Tools.getConnection(); JDBC_Tools.beginTx(conn); long beginTime = System.currentTimeMillis(); statement = conn.createStatement(); for(int i = 0;i&lt;100000;i++){ sql = &quot;INSERT INTO temp values(&quot;+(i+1) +&quot;,&apos;name_&quot;+(i+1)+&quot;&apos;,&apos;13-6月 -15&apos;)&quot;; statement.executeUpdate(sql); } long endTime = System.currentTimeMillis(); System.out.println(&quot;Time : &quot;+(endTime - beginTime)); JDBC_Tools.commit(conn); } catch (Exception e) { e.printStackTrace(); JDBC_Tools.rollback(conn); }finally{ JDBC_Tools.relaseSource(conn, statement); } } /** * 使用PreparedStatement * 测试用时:9717 */ @Test public void testBbatchPreparedStatement() { Connection conn = null; PreparedStatement ps = null; String sql = null; try { conn = JDBC_Tools.getConnection(); JDBC_Tools.beginTx(conn); long beginTime = System.currentTimeMillis(); sql = &quot;INSERT INTO temp values(?,?,?)&quot;; ps = conn.prepareStatement(sql); Date date = new Date(new java.util.Date().getTime()); for(int i = 0;i&lt;100000;i++){ ps.setInt(1, i+1); ps.setString(2, &quot;name_&quot;+i); ps.setDate(3, date); ps.executeUpdate();//9717 } long endTime = System.currentTimeMillis(); System.out.println(&quot;Time : &quot;+(endTime - beginTime)); JDBC_Tools.commit(conn); } catch (Exception e) { e.printStackTrace(); JDBC_Tools.rollback(conn); }finally{ JDBC_Tools.relaseSource(conn, ps); } } /** * 测试用时 : 658 */ @Test public void testBbatch() { Connection conn = null; PreparedStatement ps = null; String sql = null; try { conn = JDBC_Tools.getConnection(); JDBC_Tools.beginTx(conn); long beginTime = System.currentTimeMillis(); sql = &quot;INSERT INTO temp values(?,?,?)&quot;; ps = conn.prepareStatement(sql); Date date = new Date(new java.util.Date().getTime()); for(int i = 0;i&lt;100000;i++){ ps.setInt(1, i+1); ps.setString(2, &quot;name_&quot;+i); ps.setDate(3, date); //积攒SQL ps.addBatch(); //当积攒到一定程度,就执行一次,并且清空记录 if((i+1) % 300==0){ ps.executeBatch(); ps.clearBatch(); } } //总条数不是批量值整数倍,则还需要在执行一次 if(100000 % 300 != 0){ ps.executeBatch(); ps.clearBatch(); } long endTime = System.currentTimeMillis(); System.out.println(&quot;Time : &quot;+(endTime - beginTime)); JDBC_Tools.commit(conn); } catch (Exception e) { e.printStackTrace(); JDBC_Tools.rollback(conn); }finally{ JDBC_Tools.relaseSource(conn, ps); } } }123## JDBC批量更新 public void insertInTransaction(String[] sqls) throws SQLException{ try ( Connection conn = DriverManager.getConnection(url, user, pass)) { //关闭自动提交，即开启事务 conn.setAutoCommit(false); try(Statement stmt = conn.createStatement()) { for (String sql : sqls) { stmt.addBatch(sql); } stmt.executeBatch(); //提交事务 conn.commit(); } catch (SQLException e) { conn.rollback(); System.out.println(“Exception:”); System.out.println(e.getMessage()); } finally { conn.setAutoCommit(true); } } }```]]></content>
      <categories>
        <category>programme</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>JDBC</tag>
        <tag>工具类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala高阶函数学习]]></title>
    <url>%2Fprogramme%2Fscala-higher-order-functions.html</url>
    <content type="text"><![CDATA[参考文章http://www.cnblogs.com/wzm-xu/p/4063814.htmlhttp://www.cnblogs.com/wzm-xu/p/4064389.html scala高阶函数上高阶函数是函数式编程里面一个非常重要的特色，所谓的高阶函数，就是以其它函数作为参数的函数。 下面以一个小例子演示Scala的高阶函数特性，非常有意思，也非常强大。 首先看这么一个程序：code1： 12345678910111213141516171819202122232425object higherorderfuntion&#123; def sum1(a:Int,b:Int):Int= if(a&gt;b) 0 else a+sum1(a+1,b) def sum2(a:Int,b:Int):Int= if(a&gt;b) 0 else cube(a)+sum2(a+1,b) def sum3(a:Int,b:Int):Int= if(a&gt;b) 0 else fac(a)+sum3(a+1,b) def cube(a:Int):Int=a*a*a def fac(a:Int):Int= if (a==0) 1 else a*fac(a-1) def main(args:Array[String])=&#123; println(sum1(1,3)) println(sum2(1,3)) println(sum3(1,3)) &#125;&#125; 上面这个例子“没有”用到高阶函数，sum1是计算a+(a+1)+(a+2)+…+(b), sum2是计算a\^3+(a+1)\^3+(a+2)\^3+…+b\^3, sum3是计算a!+(a+1)!+(a+2)!+…+b!。分析sum1,sum2,sum3的代码，很容易发现这三个函数有着相似的“Pattern”， 抛开函数名不论，这三个函数唯一的区别在于，在 else语句中对a的处理：a, cube(a) , fac(a). 那么来看，在函数式 编程里面，是如何非常精彩的利用这个“Pattern”来使得代码更加精简的： code2：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748object higherorderfuntion&#123; def sum(f:Int=&gt;Int,a:Int,b:Int):Int= if(a&gt;b) 0 else f(a)+sum(f,a+1,b) def sumInt(a:Int,b:Int):Int = sum(id,a,b) def sumCube(a:Int,b:Int):Int = sum(cube,a,b) def sumFac(a:Int,b:Int):Int = sum(fac,a,b) def id(a:Int) = a def cube(a:Int):Int=a*a*a def fac(a:Int):Int= if (a==0) 1 else a*fac(a-1) def main(args:Array[String])=&#123; println(sumInt(1,3)) println(sumCube(1,3)) println(sumFac(1,3)) &#125;&#125;``` 重头戏来了，我们来看sum函数的实现：```scaladef sum(f:Int=&gt;Int,a:Int,b:Int):Int= if(a&gt;b) 0 else f(a)+sum(f,a+1,b)``` sum函数接收三个参数，第二个和第三个参数分别是： a:Int 和 b:Int, 这和第一个例子中是一样的。比较令人费解的是sum函数的第一个参数：f:Int=&gt;Int 这个是什么意思呢？意思是sum函数接收一个名字叫做 “f” 的函数作为参数，而 Int=&gt;Int 是对f的说明：=&gt;左边的Int是说：函数f接收一个Int类型的参数，=&gt;右边的Int是说：函数f的返回值是Int类型的。好了，那么既然sum函数接收函数f作为一个参数，那么sum就可以利用f了，事实也是这样的，看sum函数的else语句就知道了：**f(a)**然后再看sumInt、sumCube和sumFac三个函数的定义：```scaladef sumInt(a:Int,b:Int):Int = sum(id,a,b)def sumCube(a:Int,b:Int):Int = sum(cube,a,b)def sumFac(a:Int,b:Int):Int = sum(fac,a,b) 以sumCube函数为例吧，它在定义的时候，把cube函数作为参数，传递给sum函数， 而我们看cube函数的定义：1def cube(a:Int):Int=a*a*a 发现，cube函数接收一个Int作为参数，并且返回一个Int，也就是说cube函数是符合sum函数的第一个 参数: f:Int=&gt;Int 的 是不是很精彩呢？ 事实上上述代码还可以进一步简化，因为我们观察到id函数和cube函数的功能非常简单，不需要单独作为 一个函数出现，进一步简化后的代码如下：code312345678910111213141516171819object higherorderfuntion&#123; def sum(f:Int=&gt;Int,a:Int,b:Int):Int= if(a&gt;b) 0 else f(a)+sum(f,a+1,b) def sumInt(a:Int,b:Int):Int = sum(x=&gt;x,a,b) def sumCube(a:Int,b:Int):Int = sum(x=&gt;x*x*x,a,b) def sumFac(a:Int,b:Int):Int = sum(fac,a,b) def fac(a:Int):Int= if (a==0) 1 else a*fac(a-1) def main(args:Array[String])=&#123; println(sumInt(1,3)) println(sumCube(1,3)) println(sumFac(1,3)) &#125;&#125; code3和code2相比，去掉了id函数和cube这两个函数，并且sumInt和sumCube函数的声明也发生了 一点变化：12def sumInt(a:Int,b:Int):Int = sum(x=&gt;x,a,b)def sumCube(a:Int,b:Int):Int = sum(x=&gt;x*x*x,a,b) 像： x=&gt;x 和 x=&gt;xxx 这样的东西是什么呢？在函数式编程里面，这被叫做“function literal”,又称“匿名函数”， 说白了，这也是函数的一种表达方式，只是这个函数没有名字罢了。 code3其实也还有点小毛病，那就是sum函数和fac函数都不是“尾递归”，所以呢，把它们改成尾递归如下： code4：12345678910111213141516171819202122232425262728293031323334353637383940object higherorderfuntion&#123; def sum(f:Int=&gt;Int,a:Int,b:Int):Int=&#123; def loop(a:Int,acc:Int):Int= if(a&gt;b) acc else loop(a+1,f(a)+acc) loop(a,0) &#125; def sumInt(a:Int,b:Int):Int = sum(x=&gt;x,a,b) def sumCube(a:Int,b:Int):Int = sum(x=&gt;x*x*x,a,b) def sumFac(a:Int,b:Int):Int = sum(fac,a,b) def fac(a:Int):Int= &#123; def loop(a:Int,acc:Int):Int= if(a==0) acc else loop(a-1,a*acc) loop(a,1) &#125; def main(args:Array[String])=&#123; println(sumInt(1,4)) println(sumCube(1,4)) println(sumFac(1,4)) &#125;&#125;``` # scala高阶函数下在scala高阶函数上我们演示了如何把一个函数作为参数传递给另外一个函数。在本文里面，我们来演示函数式编程另外一个重要的特性：返回一个函数。首先来看这么一段代码：**code piece 1：**```scaladef sum(f:Int=&gt;Int):(Int,Int)=&gt;Int=&#123; def sumF(a:Int,b:Int):Int= if(a&gt;b) 0 else f(a)+sumF(a+1,b) sumF &#125; 一点点来看，f:Int=&gt;Int 是sum函数接收的参数，该参数是一个函数。 “:” 号后面的 (Int,Int) =&gt; Int 是sum函数的返回值，又(Int,Int) =&gt; Int是一个函数的类型：接收两个Int型的数， 返回一个Int的数。也就是说调用sum函数，其返回的值是一个函数。 这一点对于已经习惯C、C++、Java等编程语言的 程序员来说有一点难以理解。 继续看例子吧，如果执行下面的一行代码会发生什么呢？1sum(x=&gt;x*x)(1,4) 结果会返回30。为什么是30呢？ 30= 1^2+ 2^2 + 3^2 + 4^2. 首先，sum(x=&gt;xx) 是一个函数，并且sum(x=&gt;xx)的类型是(Int,Int)=&gt;Int，正因为sum(x=&gt;x*x)是一个函数， 所以它才可以继续接收参数(1,4). 好吧，我可能没有把这个事儿说清楚，实在是太抽象了。但是，读者应该有了一个基本的印象，那就是： 在函数式编程里面，函数f可以接收函数g作为参数，也可以返回函数h。 到这里还没完。。。。。。 Scala里面可以继续对code piece1 进行简化，如下： code piece2：1234567891011121314151617181920212223object higherorderfuntion&#123; def sum(f:Int=&gt;Int)(a:Int,b:Int):Int=&#123; if(a&gt;b) 0 else f(a)+sum(f)(a+1,b) &#125; def fac(a:Int):Int= &#123; def loop(a:Int,acc:Int):Int= if(a==0) acc else loop(a-1,a*acc) loop(a,1) &#125; def main(args:Array[String])=&#123; println(sum(x=&gt;x)(1,4)) println(sum(x=&gt;x*x*x)(1,4)) println(sum(fac)(1,4)) &#125;&#125;``` 这里我们需要特别注意，这里sum函数的参数列表变成了两个：(f:Int=&gt;Int)和(a:Int,b:Int)在scala里面，函数可以有多个参数列表。比如下面的函数定义： def f(args_1)(args_2)…(args_n)=E12345E代表一个函数体，且n&gt;1那么上述定义等价于： ```scaladef f(args_1)(args_2)...(args_n-1)=&#123;def g(args_n)=E;g&#125; 还是以code piece2中的sum函数为例：123def sum(f:Int=&gt;Int)(a:Int,b:Int):Int=&#123; if(a&gt;b) 0 else f(a)+sum(f)(a+1,b) &#125; 等价于：12345def sum(f:Int=&gt;Int):(Int,Int)=&gt;Int=&#123; def g(a:Int,b:Int):Int = if(a&gt;b) 0 else f(a)+sum(f)(a+1,b) g &#125; 这个定义和code piece1中的sum函数的定义是一致的。 扩展： 什么是尾递归参考https://www.zhihu.com/question/20761771/answer/19996299这个虽然是python的，但是也能让我们理解什么是尾递归尾递归和一般的递归不同在对内存的占用，普通递归创建stack累积而后计算收缩，尾递归只会占用恒量的内存（和迭代一样）。SICP中描述了一个内存占用曲线，用以上答案中的Python代码为例（普通递归）：12345def recsum(x): if x == 1: return x else: return x + recsum(x - 1) 当调用recsum(5)，Python调试器中发生如下状况：12345678910recsum(5)5 + recsum(4)5 + (4 + recsum(3))5 + (4 + (3 + recsum(2)))5 + (4 + (3 + (2 + recsum(1))))5 + (4 + (3 + (2 + 1)))5 + (4 + (3 + 3))5 + (4 + 6)5 + 1015 这个曲线就代表内存占用大小的峰值，从左到右，达到顶峰，再从右到左收缩。而我们通常不希望这样的事情发生，所以使用迭代，只占据常量stack space(更新这个栈！而非扩展他)。（一个替代方案：迭代）12for i in range(6): sum += i 因为Python，Java，Pascal等等无法在语言中实现尾递归优化(Tail Call Optimization, TCO)，所以采用了for, while, goto等特殊结构代替recursive的表述。Scheme则不需要这样曲折地表达，一旦写成尾递归形式，就可以进行尾递归优化。———————Python中可以写（尾递归）： 12345def tailrecsum(x, running_total=0): if x == 0: return running_total else: return tailrecsum(x - 1, running_total + x) 理论上类似上面：1234567tailrecsum(5, 0)tailrecsum(4, 5)tailrecsum(3, 9)tailrecsum(2, 12)tailrecsum(1, 14)tailrecsum(0, 15)15 观察到，tailrecsum(x, y)中形式变量y的实际变量值是不断更新的，对比普通递归就很清楚，后者每个recsum()调用中y值不变，仅在层级上加深。所以，尾递归是把变化的参数传递给递归函数的变量了。怎么写尾递归？形式上只要最后一个return语句是单纯函数就可以。如：123return tailrec(x+1);``` 而 return tailrec(x+1) + x;1234567则不可以，因为无法更新tailrec()函数内的实际变量，只是新建一个栈。但Python不能尾递归优化（Java不行，C可以，我不知道为什么），这里是用它做个例子。====================================如何优化尾递归：在编译器处理过程中生成中间代码（通常是三地址代码），用编译器优化。另外一个的回答尾递归就是操作的最后一步是调用自身的递归。这是尾递归： function f(x) { if (x === 1) return 1; return f(x-1);}123（这个程序没什么意义，仅作为理解辅助之用）。这不是尾递归： function f(x) { if (x === 1) return 1; return 1 + f(x-1);}123456后者不是尾递归，是因为该函数的最后一步操作是用1加上f(x-1)的返回结果，因此，最后一步操作不是调用自身。注意，后面这段代码的递归也发生在函数末尾，但它不是尾递归。尾递归的判断标准是函数运行最后一步是否调用自身，而不是是否在函数的最后一行调用自身。因此阶乘n!的递归求法，尽管看起来递归发生在函数末尾，其实也不是尾递归：```pythonfunction factorial(n) &#123; if (n === 1) return 1; return n * factorial(n-1); // 最后一步不是调用自身，因此不是尾递归&#125; 使用尾递归可以带来一个好处：因为进入最后一步后不再需要参考外层函数（caller）的信息，因此没必要保存外层函数的stack，递归需要用的stack只有目前这层函数的，因此避免了栈溢出风险。一些语言提供了尾递归优化，当识别出使用了尾递归时，会相应地只保留当前层函数的stack，节省内存。所以，在没有尾递归优化的语言，如java, python中，鼓励用迭代iteration来改写尾递归；在有尾递归优化的语言如Erlang中，鼓励用尾递归来改写其他形式的递归。谢谢 @jamesr 的指正~关于如何改写，参考：尾调用优化 - 阮一峰的网络日志知乎尾递归参考: https://www.zhihu.com/question/20761771]]></content>
      <categories>
        <category>programme</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala集合]]></title>
    <url>%2Fprogramme%2Fscala-collect.html</url>
    <content type="text"><![CDATA[本文参考至scala编程，菜鸟教程，然后将自己的判断以及重要方法的提取，解释，合并 字符串在 Scala 中，字符串的类型实际上是 Java String，它本身没有 String 类。在 Scala 中，String 是一个不可变的对象，所以该对象不可被修改。这就意味着你如果修改字符串就会产生一个新的字符串对象。 String 对象是不可变的，如果你需要创建一个可以修改的字符串，可以使用 String Builder 类，如下实例:12345678 object Test &#123; def main(args: Array[String]) &#123; val buf = new StringBuilder; buf += 'a' buf ++= "bcdef" println( "buf is : " + buf.toString ); &#125;&#125; 同java一样，scala的字符串用过length()方法得到长度，String 类中使用string1.concat(string2); 方法来连接两个字符串也可以直接用+号连接字符串java.lang.String的所有方法，在scala中也可以使用, 这里不仔细介绍 数组声明数组12345var z:Array[String] = new Array[String](3)或var z = new Array[String](3) 赋值1z(0) = "Runoob"; z(1) = "Baidu"; z(4/2) = "Google" 也可以这样定义一个数组1var z = Array("Runoob", "Baidu", "Google") 处理数组123456789101112131415161718192021222324252627282930313233object Test &#123; def main(args: Array[String]) &#123; var myList = Array(1.9, 2.9, 3.4, 3.5) // 输出所有数组元素 for ( x &lt;- myList ) &#123; println( x ) &#125; // 计算数组所有元素的总和 var total = 0.0; for ( i &lt;- 0 to (myList.length - 1)) &#123; total += myList(i); &#125; println("总和为 " + total); // 查找数组中的最大元素 var max = myList(0); for ( i &lt;- 1 to (myList.length - 1) ) &#123; if (myList(i) &gt; max) max = myList(i); &#125; println("最大值为 " + max); &#125;&#125;-------输出-------1.92.93.43.5总和为 11.7最大值为 3.5 多维数组12345678910111213141516171819def main(args: Array[String]) &#123; var myMatrix = ofDim[Int](3,3) // 创建矩阵 for (i &lt;- 0 to 2) &#123; for ( j &lt;- 0 to 2) &#123; myMatrix(i)(j) = j; &#125; &#125; // 打印二维阵列 for (i &lt;- 0 to 2) &#123; for ( j &lt;- 0 to 2) &#123; print(&quot; &quot; + myMatrix(i)(j)); &#125; println(); &#125; &#125; 合并数组使用concat() 方法来合并两个数组123456789101112131415161718192021def main(args: Array[String]) &#123; var myList1 = Array(1, 2, 3, 4,) var myList2 = Array(5, 6, 7, 8) var myList3 = concat( myList1, myList2) // 输出所有数组元素 for ( x &lt;- myList3 ) &#123; println( x ) &#125; &#125; ----------输出-----------12345678 创建区间数组以下实例中，我们使用了 range() 方法来生成一个区间范围内的数组。range() 方法最后一个参数为步长，默认为 1：12345678910111213141516def main(args: Array[String]) &#123; var myList1 = range(10, 20, 2) var myList2 = range(10,20) // 输出所有数组元素 for ( x &lt;- myList1 ) &#123; print( " " + x ) &#125; println() for ( x &lt;- myList2 ) &#123; print( " " + x ) &#125; &#125; -----------------输出---------------10 12 14 16 1810 11 12 13 14 15 16 17 18 19 Scala 集合ListScala 列表类似于数组，它们所有元素的类型都相同，但是它们也有所不同：列表是不可变的，值一旦被定义了就不能改变，其次列表 具有递归的结构（也就是链接表结构）而数组不是。。列表的元素类型 T 可以写成 List[T]。例如，以下列出了多种类型的列表： 列表构造方式构造列表方式一12345678910111213141516// 字符串列表val site: List[String] = List("Runoob", "Google", "Baidu")// 整型列表val nums: List[Int] = List(1, 2, 3, 4)// 空列表val empty: List[Nothing] = List()// 二维列表val dim: List[List[Int]] = List( List(1, 0, 0), List(0, 1, 0), List(0, 0, 1) ) 构造列表方式二构造列表的两个基本单位是 Nil 和 ::(发音为cons) Nil代表空列表，中缀符号:: 表示列表从前端扩展。也就是说x::xs代表了一个元素为x，后面紧贴着xs的列表，因此以上实例我们可以写成1234567891011121314// 字符串列表val site = "Runoob" :: ("Google" :: ("Baidu" :: Nil))// 整型列表val nums = 1 :: (2 :: (3 :: (4 :: Nil)))// 空列表val empty = Nil// 二维列表val dim = (1 :: (0 :: (0 :: Nil))) :: (0 :: (1 :: (0 :: Nil))) :: (0 :: (0 :: (1 :: Nil))) :: Nil 方式二的简化由于以::结尾，::遵循右结合的规则，A::B::C 等价于A::(B::C), 因此，前面定义用到的括号可以去掉1val names = 1::2::3::4::Nil 与前面的names定义一致 列表的基本操作 Scala列表有三个基本操作： head 返回列表第一个元素 tail 返回一个列表，包含除了第一元素之外的其他元素 isEmpty 在列表为空时返回true对于Scala列表的任何操作都可以使用这三个基本操作来表达。实例如下:12345678910111213141516object Test &#123; def main(args: Array[String]) &#123; val site = "Runoob" :: ("Google" :: ("Baidu" :: Nil)) val nums = Nil println( "第一网站是 : " + site.head ) println( "最后一个网站是 : " + site.tail ) println( "查看列表 site 是否为空 : " + site.isEmpty ) println( "查看 nums 是否为空 : " + nums.isEmpty ) &#125;&#125;-----------输出--------第一网站是 : Runoob最后一个网站是 : List(Google, Baidu)查看列表 site 是否为空 : false查看 nums 是否为空 : true List类的一阶方法连接列表你可以使用 ::: 运算符或 List.:::() 方法或 List.concat() 方法来连接两个或多个列表。实例如下:1234567891011121314151617181920object Test &#123; def main(args: Array[String]) &#123; val site1 = "Runoob" :: ("Google" :: ("Baidu" :: Nil)) val site2 = "Facebook" :: ("Taobao" :: Nil) // 使用 ::: 运算符 var fruit = site1 ::: site2 println( "site1 ::: site2 : " + fruit ) // 使用 Set.:::() 方法 fruit = site1.:::(site2) println( "site1.:::(site2) : " + fruit ) // 使用 concat 方法 fruit = List.concat(site1, site2) println( "List.concat(site1, site2) : " + fruit ) &#125;&#125; List.fill()我们可以使用 List.fill() 方法来创建一个指定重复数量的元素列表：123456789101112object Test &#123; def main(args: Array[String]) &#123; val site = List.fill(3)("Runoob") // 重复 Runoob 3次 println( "site : " + site ) val num = List.fill(10)(2) // 重复元素 2, 10 次 println( "num : " + num ) &#125;&#125;---------输出--------site : List(Runoob, Runoob, Runoob)num : List(2, 2, 2, 2, 2, 2, 2, 2, 2, 2) List.tabulate()List.tabulate() 方法是通过给定的函数来创建列表。方法的第一个参数为元素的数量，可以是二维的，第二个参数为指定的函数，我们通过指定的函数计算结果并返回值插入到列表中，起始值为 0，实例如下：1234567891011121314object Test &#123; def main(args: Array[String]) &#123; // 通过给定的函数创建 5 个元素 val squares = List.tabulate(6)(n =&gt; n * n) println( "一维 : " + squares ) // 创建二维列表 val mul = List.tabulate( 4,5 )( _ * _ ) println( "多维 : " + mul ) &#125;&#125;----------------输出---------一维 : List(0, 1, 4, 9, 16, 25)多维 : List(List(0, 0, 0, 0, 0), List(0, 1, 2, 3, 4), List(0, 2, 4, 6, 8), List(0, 3, 6, 9, 12)) 列表反转List.reverse 用于将列表的顺序反转，实例如下：12345678910111213object Test &#123; def main(args: Array[String]) &#123; val site = "Runoob" :: ("Google" :: ("Baidu" :: Nil)) println( "site 反转前 : " + site ) println( "site 反转前 : " + site.reverse ) &#125;&#125;------------输出---------------$ vim Test.scala $ scala Test.scala site 反转前 : List(Runoob, Google, Baidu)site 反转前 : List(Baidu, Google, Runoob) 前缀与后缀 drop take splitAt1234567891011121314def main(args: Array[String]) &#123; var ls = "google"::"baidu"::"tenxun"::"alibaba"::"apples"::Nil var takeTest =ls.take(2) var dropTest = ls.drop(2) var splitTest = ls.splitAt(3) println("takeTest: "+takeTest) println("dropTest: "+dropTest) println("splitTest: "+splitTest) &#125; --------------输出-------------takeTest: List(google, baidu)dropTest: List(tenxun, alibaba, apples)splitTest: (List(google, baidu, tenxun),List(alibaba, apples)) 元素选择 apply方法和indices方法apply方法是获取第几个值，list.apply(2)和list(2)是一样的indices是获取所有的列表的Range,暂时不知道有多大用处123456789101112131415161718192021def main(args: Array[String]) &#123; var ls = "google"::"baidu"::"tenxun"::"alibaba"::"apples"::"stackoverflow"::Nil var applyTest = ls.apply(2) var indicesTest = ls.indices println("applyTest: "+applyTest) println("applyTest2: "+ls(2)) println("indicesTest: "+indicesTest) for(i&lt;-indicesTest)&#123; println(ls(i)) &#125; &#125;-------------输出--------applyTest: tenxunapplyTest2: tenxunindicesTest: Range(0, 1, 2, 3, 4, 5)googlebaidutenxunalibabaapplesstackoverflow toString和mkStringtoString和java的一样mkString mkString(start: String,sep: String,end: String): String第一个参数是以什么开始，第二个参数是以什么分割，第三个参数是以什么结尾，函数返回一个String12345678def main(args: Array[String]) &#123; var ls = "google" :: "baidu" :: "tenxun" :: "alibaba" :: "apples" :: "stackoverflow" :: Nil println("toString: "+ls.toString()) println("mkString: "+ls.mkString("&#123;",";","&#125;")) &#125;---------输出----------toString: List(google, baidu, tenxun, alibaba, apples, stackoverflow)mkString: &#123;google;baidu;tenxun;alibaba;apples;stackoverflow&#123; 转换列表 toArray element iterator想要在数组Array和列表list之间转换，可以使用List的toArray和Array的toList例子12345678scala&gt; var ls = &quot;google&quot; :: &quot;baidu&quot; :: &quot;tenxun&quot; :: &quot;alibaba&quot; :: &quot;apples&quot; :: &quot;stackoverflow&quot; :: Nills: List[String] = List(google, baidu, tenxun, alibaba, apples, stackoverflow)scala&gt; var arrays = ls.toArrayarrays: Array[String] = Array(google, baidu, tenxun, alibaba, apples, stackoverflow)scala&gt; var lists = arrays.toListlists: List[String] = List(google, baidu, tenxun, alibaba, apples, stackoverflow) element在很旧的版本有，现在已经过时不用了。如果要用枚举器访问列表元素，可以使用iterator123456789def main(args: Array[String]) &#123; var ls = "google" :: "baidu" :: "tenxun" :: "alibaba" :: "apples" :: "stackoverflow" :: Nil val it =ls.iterator while(it.hasNext)&#123; print(it.next()+",") &#125; &#125;--------------输出------------google, baidu, tenxun, alibaba, apples, stackoverflow, list类的高阶用法在java中，若要提取出满足特点条件的元素，或者检查所有元素是否满足某种性质，或者用某种方式转变列表的所有元素，这样的需求一般都需要使用for或者while循环的固定表达式，在scala中，可以通过使用List的一些高阶方法（函数）来更为简介的实现 列表间映射：map、flatMap和foreach xs map f 操作返回把函数f应用在xs的每个列表元素之后由此组成的新列表。如：12scala&gt; List(1,2,3).map(_ +1)res1: List[Int] = List(2, 3, 4) 12345scala&gt; val words = List("zks","zhaikaishun","kaishun","kai","xiaozhai")words: List[String] = List(zks, zhaikaishun, kaishun, kai, xiaozhai)scala&gt; words.map(_.length)res2: List[Int] = List(3, 11, 7, 3, 8) flatMap操作符与map类似，不过它的右操作元是能够返回元素列表的函数。它对列表的每个元素调用该方法，然后连接所有方法的结果并返回。map与flatMap的差异举例说明如下：12345scala&gt; words.map(_.toList)res3: List[List[Char]] = List(List(z, k, s), List(z, h, a, i, k, a, i, s, h, u, n), List(k, a, i, s, h, u, n), List(k, a, i), List(x, i, a, o, z, h, a, i))scala&gt; words.flatMap(_.toList)res4: List[Char] = List(z, k, s, z, h, a, i, k, a, i, s, h, u, n, k, a, i, s, h, u, n, k, a, i, x, i, a, o, z, h, a, i) map与flatMap的差异和协作可以用下面的例子体会12scala&gt; List.range(1, 5).flatMap(i =&gt; List.range(1, i).map(j =&gt; (i, j)))res9: List[(Int, Int)] = List((2,1), (3,1), (3,2), (4,1), (4,2), (4,3)) 解释 : List.range(1,5)生成了List(1,2,3,4),注意没有5.flatMap对内部的每一个元素进行操作, 后面有个.map是对内部List.range(1, i)的每一个元素进行操作，最后flatMap返回的还是一个List上述例子也可以用for循环+yield来完成12scala&gt; for (i &lt;- List.range(1, 5); j &lt;- List.range(1, i)) yield (i,j)res10: List[(Int, Int)] = List((2,1), (3,1), (3,2), (4,1), (4,2), (4,3)) foreach是第三种与映射类似的操作。它的右操作元是过程（返回Unit的函数）。它只是对每个列表元素都调用一遍过程。操作的结果仍然是Unit，不会产生结果列表。例如：1234567def main(args: Array[String]) &#123; var sum =0 List(1, 2, 3, 4, 5) foreach (sum += _) println(sum) &#125;-----输出----15 列表过滤：filter、partition、find、takeWhile、dropWhile和span1.xs filter p操作产生xs中符合p（x）为true的所有元素组成的列表。如：12345scala&gt; List (1, 2, 3, 4, 5) filter (_ % 2 == 0)res10: List[Int] = List(2, 4)scala&gt; words filter (_.length == 3)res11: List[String] = List(the, fox) 2.partition方法与filter类似，不过返回的是列表对。其中一个包含所有论断为真的元素，另一个包含所有论断为假的元素。xs partition p 等价于 (xs filter p, xs filter (!p())) 举例如下：12scala&gt; List(1, 2, 3, 4, 5) partition (_ % 2 ==0)res12: (List[Int], List[Int]) = (List(2, 4),List(1, 3, 5)) 3.find方法同样与filter方法类似，不过返回的是第一个满足给定论断的元素，而并不是全部。xs find p 操作以列表xs和论断p为操作元。返回可选值。如果xs中存在元素x使得p（x）为真，Some（x）将返回。否则，若p对所有元素都不成立，None将返回。举例如下：12345scala&gt; List(1, 2, 3, 4, 5) find (_ % 2 == 0)res13: Option[Int] = Some(2)scala&gt; List(1, 2, 3, 4, 5) find (_ &lt;= 0)res15: Option[Int] = None xs takeWhile p操作返回列表xs中最长的能够满足p的前缀。例如：12scala&gt; List(1, 2, 3, -4, 5) takeWhile (_ &gt; 0)res16: List[Int] = List(1, 2, 3) 5.xs dropWhile p操作移除最长能够满足p的前缀。举例如下：12345scala&gt; val words = List("the", "quick", "brown", "fox")words: List[String] = List(the, quick, brown, fox)scala&gt; words dropWhile (_ startsWith "t")res11: List[String] = List(quick, brown, fox) 6.span方法把takeWhile和dropWhile组合成一个操作。它返回一对列表，定义与下列等式一致：xs span p 等价于 （xs takeWhile p， xs dropWhile p）12scala&gt; List(1, 2, 3, -4, 5) span (_ &gt;0)res18: (List[Int], List[Int]) = (List(1, 2, 3),List(-4, 5)) 列表的论断：forall和exists xs forall p 如果列表的所有元素满足p则返回true xs exists p 如果列表中有一个值满足p就返回true12345678 def hasZeroRow(m: List[List[Int]]) = m.exists(row =&gt; row forall (_ == 0)) def main(args: Array[String]) &#123; val m= List(List(3,0,0), List(0,3,0), List(0,0,3)) var flag :Boolean= hasZeroRow(m) println(flag) &#125;----------输出--------false 折叠操作如果我们把集合看成是一张纸条，每一小段代表一个元素，那么reduceLeft就将这张纸条从左向右”折叠”，最前面的两个元素会首先“重叠”在一起，这时会使用传给reduceLeft的参数函数进行计算，返回的结果就像是已经折叠在一起的两段纸条，它们已经是一个叠加的状态了，所以它，也就是上次重叠的结果会继续做为一个单一的值和下一个元素继续“叠加”，直到折叠到集合的最后一个元素 reduceLeft 12scala&gt; List.range(1, 5).reduceLeft(_+_)res12: Int = 10 reduceRight和reduceLeft相似，但是是从右向左折叠,注意:==它的操作方向是从右到左，但是参数的顺序却并不是，而是依然第一参数是左边的元素，第二参数是右边的元素== 1234scala&gt; List.range(1, 4) reduceRight(_ - _)res15: Int = 2// 2-3 = -1// 1-(-1)=2 foldLeft类似于reduceLeft, 不过开始折叠的第一个元素不是集合中的第一个元素，而是传入的一个元素，有点类似于先将这个参数放入的集合中的首位，然后在reduceLeft 12scala&gt; List.range(1, 5).foldLeft(1)(_+_)res1: Int = 11 foldRight类似于foldLeft，不过是从右向左，不再举例 12scala&gt; List.range(1, 4).foldRight(3)(_-_)res1: Int = -1 scala 集合SetSet和List基本相似，只是所有的元素都是唯一的默认的Set是不可变的,默认引用的是 scala.collection.immutable.Set12345val set = Set(1,2,3)println(set.getClass.getName) // println(set.exists(_ % 2 == 0)) //trueprintln(set.drop(1)) //Set(2,3) 如果需要使用可变集合需要引入 scala.collection.mutable.Set：1234567891011121314import scala.collection.mutable.Set // 可以在任何地方引入 可变集合val mutableSet = Set(1,2,3)println(mutableSet.getClass.getName) // scala.collection.mutable.HashSetmutableSet.add(4)mutableSet.remove(1)mutableSet += 5mutableSet -= 2println(mutableSet) // Set(5, 3, 4)val another = mutableSet.toSetprintln(another.getClass.getName) // scala.collection.immutable.Set 连接集合1var site = site1 ++ site2 查找集合中最大与最小元素Set.minSet.max 交集Set.intersect1set1.intersect(set2) mapap(映射)是一种可迭代的键值对（key/value）结构。所有的值都可以通过键来获取。Map 中的键都是唯一的。Map 也叫哈希表（Hash tables）。Map 有两种类型，可变与不可变，区别在于可变对象可以修改它，而不可变对象不可以。默认情况下 Scala 使用不可变 Map。如果你需要使用可变集合，你需要显式的引入 import scala.collection.mutable.Map 类在 Scala 中 你可以同时使用可变与不可变 Map，不可变的直接使用 Map，可变的使用 mutable.Map。以下实例演示了不可变 Map 的应用：不可变map123456789101112131415scala&gt; val colors = Map("red" -&gt; "#FF0000", "azure" -&gt; "#F0FFFF")colors: scala.collection.immutable.Map[String,String] = Map(red -&gt; #FF0000, azure -&gt; #F0FFFF)``` ## **Map的赋值** 如果需要添加 key-value 对，可以使用 + 号，如下所示：```scala// 空哈希表，键为字符串，值为整型var A:Map[Char,Int] = Map()A += ('I' -&gt; 1)A += ('J' -&gt; 5)A += ('K' -&gt; 10)A += ('L' -&gt; 100)println（A）-------输出--------Map(I -&gt; 1, J -&gt; 5, K -&gt; 10, L -&gt; 100) Map的基本操作 keys 返回 Map 所有的键(key) values 返回 Map 所有的值(value) isEmpty 在 Map 为空时返回true例子12345678910111213141516171819object Test &#123; def main(args: Array[String]) &#123; val colors = Map(&quot;red&quot; -&gt; &quot;#FF0000&quot;, &quot;azure&quot; -&gt; &quot;#F0FFFF&quot;, &quot;peru&quot; -&gt; &quot;#CD853F&quot;) val nums: Map[Int, Int] = Map() println( &quot;colors 中的键为 : &quot; + colors.keys ) println( &quot;colors 中的值为 : &quot; + colors.values ) println( &quot;检测 colors 是否为空 : &quot; + colors.isEmpty ) println( &quot;检测 nums 是否为空 : &quot; + nums.isEmpty ) &#125;&#125;---------输出---------colors 中的键为 : Set(red, azure, peru)colors 中的值为 : MapLike(#FF0000, #F0FFFF, #CD853F)检测 colors 是否为空 : false检测 nums 是否为空 : true Map 合并++ 运算符或 Map.++() 方法来连接两个 Map, Map 合并时会移除重复的 key。1234567891011121314151617181920212223242526272829303132333435363738394041val colors1 = Map("red" -&gt; "#FF0000", "azure" -&gt; "#F0FFFF", "peru" -&gt; "#CD853F") val colors2 = Map("blue" -&gt; "#0033FF", "yellow" -&gt; "#FFFF00", "red" -&gt; "#FF0001") // ++ 作为运算符 var colors = colors1 ++ colors2 println( "colors1 ++ colors2 : " + colors ) // ++ 作为方法 colors = colors1.++(colors2) println( "colors1.++(colors2)) : " + colors )--------输出--------------------colors1 ++ colors2 : Map(blue -&gt; #0033FF, azure -&gt; #F0FFFF, peru -&gt; #CD853F, yellow -&gt; #FFFF00, red -&gt; #FF0001)colors1.++(colors2)) : Map(blue -&gt; #0033FF, azure -&gt; #F0FFFF, peru -&gt; #CD853F, yellow -&gt; #FFFF00, red -&gt; #FF0001)``` ## **输出map和key**```scaladef main(args: Array[String]): Unit = &#123; val sites = Map("runoob" -&gt; "http://www.runoob.com", "baidu" -&gt; "http://www.baidu.com", "taobao" -&gt; "http://www.taobao.com") sites.keys.foreach&#123; i=&gt;print("key: "+i) println("value: "+sites(i)) &#125; &#125;----------输出-----key: runoobvalue: http://www.runoob.comkey: baiduvalue: http://www.baidu.comkey: taobaovalue: http://www.taobao.com``` ## **Map.contains查看是否存在指定的key**## **元组**元组可以把固定数量的条目组合在一起以便于整体的传送，不像数组或者列表，元祖可以保存不同类型的对象 元组的值是通过将单个的值包含在圆括号中构成的。例如```scalaval t = (1, 3.14, "Fred") 以上实例在元组中定义了三个元素，对应的类型分别为[Int, Double, java.lang.String]。此外我们也可以使用以下方式来定义：12345678910111213141516171819val t = new Tuple3(1, 3.14, "Fred")``` ### **定义与取值**元组的实际类型取决于它的元素的类型，比如 (99, "runoob") 是 Tuple2[Int, String]。 ('u', 'r', "the", 1, 4, "me") 为 Tuple6[Char, Char, String, Int, Int, String]。 目前 Scala 支持的元组最大长度为 22。对于更大长度你可以使用集合，或者扩展元组。访问元组的元素可以通过数字索引，如下一个元组： 我们可以使用 t.\_1 访问第一个元素， t.\_2 访问第二个元素，如下所示： ```scala def main(args: Array[String]) &#123; val t = (4,3,2,1) val sum = t._1 + t._2 + t._3 + t._4 var secondTuple=t._2 println("第二个元素为: "+secondTuple) println( "元素之和为: " + sum ) &#125;----输出----------第二个元素为: 3元素之和为: 10 迭代元组你可以使用 Tuple.productIterator() 方法来迭代输出元组的所有元素：1234567891011121314151617181920212223242526272829303132333435363738394041 def main(args: Array[String]) &#123; val t = (4,3,2,1) t.productIterator.foreach(i=&gt;println("value: "+i)) &#125;--------输出--------------value: 4value: 3value: 2value: 1``` ### **元组转为字符串**Tuple.toString() ### **元素交换**Tuple.swap 方法来交换元组的元素 ```scala def main(args: Array[String]) &#123; val t = new Tuple2("www.google.com", "http://blog.csdn.net/t1dmzks") println("交换后的元组: " + t ) &#125;-------输出-----------交换后的元组: (www.google.com,www.runoob.com)``` # **Scala Option**TODO# **Scala Iterator**Scala Iterator（迭代器）不是一个集合，它是一种用于访问集合的方法。迭代器 it 的两个基本操作是 next 和 hasNext。调用 it.next() 会返回迭代器的下一个元素，并且更新迭代器的状态。调用 it.hasNext() 用于检测集合中是否还有元素。让迭代器 it 逐个返回所有元素最简单的方法是使用 while 循环：```scalaobject Test &#123; def main(args: Array[String]) &#123; val it = Iterator("Baidu", "Google", "Runoob", "Taobao") while (it.hasNext)&#123; println(it.next()) &#125; &#125;&#125; 查找最大与最小元素 it.min 和 it.max 方法 获取迭代器的长度it.size 或 it.length]]></content>
      <categories>
        <category>programme</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala编程基础]]></title>
    <url>%2Fprogramme%2Fscala-basic.html</url>
    <content type="text"><![CDATA[多行字符串的表示方法多行字符串用三个双引号来表示分隔符，格式为：””” … “””。实例如下：12345val foo = """菜鸟教程www.runoob.comwww.w3cschool.ccwww.runnoob.com以上三个地址都能访问""" 变量变量声明12345678910var VariableName : DataType [= Initial Value]或val VariableName : DataType [= Initial Value]变量声明不一定需要初始值，以下也是正确的：var myVar :Int;val myVal :String;``` 例如 var myVar : String = “Foo”var myVar : String = “Too” var myVar :Int;val myVal :String;1234### 变量类型引用在 Scala 中声明变量和常量不一定要指明数据类型，在没有指明数据类型的情况下，其数据类型是通过变量或常量的初始值推断出来的。所以，如果在没有指明数据类型的情况下声明变量或常量必须要给出其初始值，否则将会报错。 var myVar = 10;val myVal = “Hello, Scala!”;12345## 访问修饰符Scala 访问修饰符基本和Java的一样，分别有：private，protected，public。如果没有指定访问修饰符符，默认情况下，Scala对象的访问级别都是 public。**Scala 中的 private 限定符，比 Java 更严格，在嵌套类情况下，外层类甚至不能访问被嵌套类的私有成员。** class Outer{ class Inner{ private def f(){println(“f”)} class InnerMost{ f() // 正确 } } (new Inner).f() //错误}12### 作用域保护Scala中，访问修饰符可以通过使用限定词强调。格式为: private[x]或protected[x]12这里的x指代某个所属的包、类或单例对象。如果写成private[x],读作&quot;这个成员除了对[…]中的类或[…]中的包中的类及它们的伴生对像可见外，对其它所有类都是private。这种技巧在横跨了若干包的大型项目中非常有用，它允许你定义一些在你项目的若干子包中可见但对于项目外部的客户却始终不可见的东西。 package bobsrocckets{ package navigation{ private[bobsrockets] class Navigator{ protected[navigation] def useStarChart(){} class LegOfJourney{ private[Navigator] val distance = 100 } private[this] var speed = 200 } } package launch{ import navigation._ object Vehicle{ private[launch] val guide = new Navigator } }}123456上述例子中，类Navigator被标记为private[bobsrockets]就是说这个类对包含在bobsrockets包里的所有的类和对象可见。比如说，从Vehicle对象里对Navigator的访问是被允许的，因为对象Vehicle包含在包launch中，而launch包在bobsrockets中，相反，所有在包bobsrockets之外的代码都不能访问类Navigator。## 运算符运算符和java的基本类似### 位运算符 A = 0011 1100 B = 0000 1101 ——-位运算———- A&amp;B = 0000 1100 A|B = 0011 1101 A^B = 0011 0001 ~A = 1100 001112 object Test { def main(args: Array[String]) { var a = 60; / 60 = 0011 1100 / var b = 13; / 13 = 0000 1101 / var c = 0; c = a &amp; b; /* 12 = 0000 1100 */ println(&quot;a &amp; b = &quot; + c ); c = a | b; /* 61 = 0011 1101 */ println(&quot;a | b = &quot; + c ); c = a ^ b; /* 49 = 0011 0001 */ println(&quot;a ^ b = &quot; + c ); c = ~a; /* -61 = 1100 0011 */ println(&quot;~a = &quot; + c ); c = a &lt;&lt; 2; /* 240 = 1111 0000 */ println(&quot;a &lt;&lt; 2 = &quot; + c ); c = a &gt;&gt; 2; /* 215 = 1111 */ println(&quot;a &gt;&gt; 2 = &quot; + c ); c = a &gt;&gt;&gt; 2; /* 215 = 0000 1111 */ println(&quot;a &gt;&gt;&gt; 2 = &quot; + c ); }}1执行以上代码结果为 $ scalac Test.scala$ scala Testa &amp; b = 12a | b = 61a ^ b = 49~a = -61a &lt;&lt; 2 = 240a &gt;&gt; 2 = 15a &gt;&gt;&gt; 2 = 15 12345678910## 循环### for循环Scala 语言中 for 循环的语法：```scalafor( var x &lt;- Range )&#123; statement(s);&#125; 以上语法中，Range 可以是一个数字区间表示 i to j ，或者 i until j。左箭头 &lt;- 用于为变量 x 赋值。 实例 123456789object Test &#123; def main(args: Array[String]) &#123; var a = 0; // for 循环 for( a &lt;- 1 to 10)&#123; println( "Value of a: " + a ); &#125; &#125;&#125; 输出12345678910value of a: 1value of a: 2value of a: 3value of a: 4value of a: 5value of a: 6value of a: 7value of a: 8value of a: 9value of a: 10 以下是使用了 i until j123456789object Test &#123; def main(args: Array[String]) &#123; var a = 0; // for 循环 for( a &lt;- 1 until 10)&#123; println( "Value of a: " + a ); &#125; &#125;&#125; 输出123456789value of a: 1value of a: 2value of a: 3value of a: 4value of a: 5value of a: 6value of a: 7value of a: 8value of a: 9 在 for 循环 中你可以使用分号 (;) 来设置多个区间，它将迭代给定区间所有的可能值。以下实例演示了两个区间的循环实例：1234567891011object Test &#123; def main(args: Array[String]) &#123; var a = 0; var b = 0; // for 循环 for( a &lt;- 1 to 3; b &lt;- 1 to 3)&#123; println( &quot;Value of a: &quot; + a ); println( &quot;Value of b: &quot; + b ); &#125; &#125;&#125; for 循环集合123456789101112131415161718object Test &#123; def main(args: Array[String]) &#123; var a = 0; val numList = List(1,2,3,4,5,6); // for 循环 for( a &lt;- numList )&#123; println( &quot;Value of a: &quot; + a ); &#125; &#125;&#125;------------------------输出value of a: 1value of a: 2value of a: 3value of a: 4value of a: 5value of a: 6 for 循环过滤Scala 可以使用一个或多个 if 语句来过滤一些元素。以下是在 for 循环中使用过滤器的语法。 你可以使用分号(;)来为表达式添加一个或多个的过滤条件。12345678910111213141516171819object Test &#123; def main(args: Array[String]) &#123; var a = 0; val numList = List(1,2,3,4,5,6,7,8,9,10); // for 循环 for( a &lt;- numList if a != 3; if a &lt; 8 )&#123; println( &quot;Value of a: &quot; + a ); &#125; &#125;&#125;------------------输出-----------value of a: 1value of a: 2value of a: 4value of a: 5value of a: 6value of a: 7 for 使用 yield你可以将 for 循环的返回值作为一个变量存储。语法格式如下：123var retVal = for&#123; var x &lt;- List if condition1; if condition2...&#125;yield x 实例以下实例演示了 for 循环中使用 yield：1234567891011121314151617181920212223object Test &#123; def main(args: Array[String]) &#123; var a = 0; val numList = List(1,2,3,4,5,6,7,8,9,10); // for 循环 var retVal = for&#123; a &lt;- numList if a != 3; if a &lt; 8 &#125;yield a // 输出返回值 for( a &lt;- retVal)&#123; println( &quot;Value of a: &quot; + a ); &#125; &#125;&#125;--------------------输出--------------value of a: 1value of a: 2value of a: 4value of a: 5value of a: 6value of a: 7 Scala 函数Scala 有函数和方法，二者在语义上的区别很小。Scala 方法是类的一部分，而函数是一个对象可以赋值给一个变量。换句话来说在类中定义的函数即是方法。我们可以在任何地方定义函数，甚至可以在函数内定义函数（内嵌函数）。更重要的一点是 Scala 函数名可以由以下特殊字符：+, ++, ~, &amp;,-, – , \, /, : 等。 函数声明123456def functionName ([参数列表]) : [return type]``` ### 函数定义方法定义由一个def 关键字开始，紧接着是可选的参数列表，一个冒号&quot;：&quot; 和方法的返回类型，一个等于号&quot;=&quot;，最后是方法的主体。Scala 函数定义格式如下： def functionName ([参数列表]) : [return type] = { function body return [expr]}12以上代码中 return type 可以是任意合法的 Scala 数据类型。参数列表中的参数可以使用逗号分隔。以下函数的功能是将两个传入的参数相加并求和： object add{ def addInt( a:Int, b:Int ) : Int = { var sum:Int = 0 sum = a + b return sum }}1**如果函数没有返回值，可以返回为 Unit，这个类似于 Java 的 void**, 实例如下： object Hello{ def printMe( ) : Unit = { println(“Hello, Scala!”) }}1### 函数调用 functionName( 参数列表 )1如果函数使用了实例的对象来调用，我们可以使用类似java的格式 (使用 . 号)： [instance.]functionName( 参数列表 )1以下实例演示了定义与调用函数的实例: object Test { def main(args: Array[String]) { println( “Returned Value : “ + addInt(5,7) ); } def addInt( a:Int, b:Int ) : Int = { var sum:Int = 0 sum = a + b return sum }}1234567891011121314151617181920212223**++Scala也是一种函数式语言++，所以函数是 Scala 语言的核心。以下一些函数概念有助于我们更好的理解 Scala 编程：**### 函数传名调用传值调用（call-by-value）：先计算参数表达式的值，再应用到函数内部； 传名调用（call-by-name）：将未计算的参数表达式直接应用到函数内部看例子 ```scalapackage com.doggie object Add &#123; def addByName(a: Int, b: =&gt; Int) = a + b def addByValue(a: Int, b: Int) = a + b &#125; ------------------------addByName(2, 2 + 2) -&gt;2 + (2 + 2) -&gt;2 + 4 -&gt;6 addByValue(2, 2 + 2) -&gt;addByValue(2, 4) -&gt;2 + 4 -&gt;6 addByName是传名调用，addByValue是传值调用。语法上可以看出，使用传名调用时，在参数名称和参数类型中间有一个=&gt;符号。例子： 酒鬼喝酒123456789101112131415161718192021222324252627282930313233343536373839404142434445package first.example/** * Created by Administrator on 2017/3/28. */object CallByName &#123; //最开始拥有的软妹币 var money = 10 //每天喝掉一个软妹币 def drink(): Unit = &#123; money -= 1 &#125; //数钱时要算上被喝掉的软妹币 def count(): Int = &#123; drink() return money &#125; //每天都数钱 def printByName(x: =&gt; Int): Unit = &#123; for(i &lt;- 0 until 5) println("每天算一算，酒鬼还剩" + x + "块钱！") &#125; //第一天数一下记墙上，以后每天看墙上的余额 def printByValue(x: Int): Unit = &#123; for(i &lt;- 0 until 5) println("只算第一天，酒鬼还剩" + x + "块钱！") &#125; def main(args: Array[String]) = &#123; printByName(count()) printByValue(count()) &#125;&#125;------------------输出-----------------每天算一算，酒鬼还剩9块钱！每天算一算，酒鬼还剩8块钱！每天算一算，酒鬼还剩7块钱！每天算一算，酒鬼还剩6块钱！每天算一算，酒鬼还剩5块钱！只算第一天，酒鬼还剩4块钱！只算第一天，酒鬼还剩4块钱！只算第一天，酒鬼还剩4块钱！只算第一天，酒鬼还剩4块钱！只算第一天，酒鬼还剩4块钱！ Scala 指定函数参数名一般情况下函数调用参数，就按照函数定义时的参数顺序一个个传递。但是我们也可以通过指定函数参数名，并且不需要按照顺序向函数传递参数，实例如下123456789101112object Test &#123; def main(args: Array[String]) &#123; printInt(b=5, a=7); &#125; def printInt( a:Int, b:Int ) = &#123; println("Value of a : " + a ); println("Value of b : " + b ); &#125;&#125;-------------输出--------------------Value of a : 7Value of b : 5 Scala 函数 - 可变参数Scala 允许你指明函数的最后一个参数可以是重复的，即我们不需要指定函数参数的个数，可以向函数传入可变长度参数列表。Scala 通过在参数的类型之后放一个星号来设置可变参数(可重复的参数)。例如：12345678910111213141516object Test &#123; def main(args: Array[String]) &#123; printStrings("Runoob", "Scala", "Python"); &#125; def printStrings( args:String* ) = &#123; var i : Int = 0; for( arg &lt;- args )&#123; println("Arg value[" + i + "] = " + arg ); i = i + 1; &#125; &#125;&#125;-------------输出-------------Arg value[0] = RunoobArg value[1] = ScalaArg value[2] = Python Scala 递归函数例如计算阶乘123456789101112131415161718192021222324object Test &#123; def main(args: Array[String]) &#123; for (i &lt;- 1 to 10) println(i + " 的阶乘为: = " + factorial(i) ) &#125; def factorial(n: BigInt): BigInt = &#123; if (n &lt;= 1) 1 else n * factorial(n - 1) &#125;&#125;----------------输出-------------------1 的阶乘为: = 12 的阶乘为: = 23 的阶乘为: = 64 的阶乘为: = 245 的阶乘为: = 1206 的阶乘为: = 7207 的阶乘为: = 50408 的阶乘为: = 403209 的阶乘为: = 36288010 的阶乘为: = 3628800 Scala 函数 - 默认参数值Scala 可以为函数参数指定默认参数值，使用了默认参数，你在调用函数的过程中可以不需要传递参数，这时函数就会调用它的默认参数值，如果传递了参数，则传递值会取代默认值。实例如下：123456789101112131415object Test &#123; def main(args: Array[String]) &#123; println( "返回值 : " + addInt() ); &#125; def addInt( a:Int=5, b:Int=7 ) : Int = &#123; var sum:Int = 0 sum = a + b return sum &#125;&#125;-----------------------------输出-----------------------$ scalac Test.scala$ scala Test返回值 : 12 scala函数嵌套TODO Scala 偏应用函数Scala 偏应用函数是一种表达式，你不需要提供函数需要的所有参数，只需要提供部分，或不提供所需参数。如下实例，我们打印日志信息： 实例中，log() 方法接收两个参数：date 和 message。我们在程序执行时调用了三次，参数 date 值都相同，message 不同。我们可以使用偏应用函数优化以上方法，绑定第一个 date 参数，第二个参数使用下划线(_)替换缺失的参数列表，并把这个新的函数值的索引的赋给变量。实例修改如下：1234567891011121314def main(args: Array[String]) &#123; val date = new Date val logWithDateBound = log(date, _ : String) logWithDateBound(&quot;message1&quot; ) Thread.sleep(1000) logWithDateBound(&quot;message2&quot; ) Thread.sleep(1000) logWithDateBound(&quot;message3&quot; )&#125;def log(date: Date, message: String) = &#123; println(date + &quot;----&quot; + message)&#125; Scala匿名函数Scala 中定义匿名函数的语法很简单，箭头左边是参数列表，右边是函数体。使用匿名函数后，我们的代码变得更简洁了。下面的表达式就定义了一个接受一个Int类型输入参数的匿名函数: ,这里可能不好理解，其实可以先暂时放下，先看我另外一篇高阶函数的文章，再过来看匿名函数12345678var inc = (x:Int) =&gt; x+1``` 上述定义的匿名函数，其实是下面这种写法的简写：```scaladef add2 = new Function1[Int,Int]&#123; def apply(x:Int):Int = x+1; &#125; ----------更多参考http://www.runoob.com/scala/anonymous-functions.html---------- Scala 高阶函数下一篇文章]]></content>
      <categories>
        <category>programme</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java曲线拟合commons-math3-3.6.1函数]]></title>
    <url>%2Fprogramme%2Fjava-commons-math.html</url>
    <content type="text"><![CDATA[需要的jar 包 commons-math3-3.6.1.jarjar包下载地址 API参考地址 示例代码如下 package cn.mingtong.testdistance; import org.apache.commons.math3.fitting.PolynomialCurveFitter; import org.apache.commons.math3.fitting.WeightedObservedPoint; import java.awt.*; import java.util.ArrayList; import java.util.Collection; import static java.lang.Math.toRadians; /** * Created by Administrator on 2017/3/24. */ public class TestInstance { public static void main(String[] args) { double[] doubles = trainPolyFit(3, 10000); //下面是测试 System.out.println("a1: "+doubles[0]); System.out.println("a2: "+doubles[1]); System.out.println("a3: "+doubles[2]); System.out.println("a4: "+doubles[3]); System.out.println("double的个数"+doubles.length); double hehe = Math.cos(toRadians(40)); double fo = doubles[3]*40*40*40+doubles[2]*40*40+doubles[1]*40+doubles[0]; System.out.println("hehe: "+hehe); System.out.println("fo: "+fo); double sub = hehe-fo; System.out.println(sub*300000); } /** * * @param degree 代表你用几阶去拟合 * @param Length 把10 --60 分成多少个点去拟合，越大应该越精确 * @return */ public static double[] trainPolyFit(int degree, int Length){ PolynomialCurveFitter polynomialCurveFitter = PolynomialCurveFitter.create(degree); double minLat = 10.0; //中国最低纬度 double maxLat = 60.0; //中国最高纬度 double interv = (maxLat - minLat) / (double)Length; ArrayList weightedObservedPoints = new ArrayList(); for(int i = 0; i &lt; Length; i++) { WeightedObservedPoint weightedObservedPoint = new WeightedObservedPoint(1, minLat + (double)i*interv, Math.cos(toRadians(minLat + (double)i*interv))); weightedObservedPoints.add(weightedObservedPoint); } return polynomialCurveFitter.fit(weightedObservedPoints); } public static double distanceSimplifyMore(double lat1, double lng1, double lat2, double lng2, double[] a) { //1) 计算三个参数 double dx = lng1 - lng2; // 经度差值 double dy = lat1 - lat2; // 纬度差值 double b = (lat1 + lat2) / 2.0; // 平均纬度 //2) 计算东西方向距离和南北方向距离(单位：米)，东西距离采用三阶多项式 double Lx = (a[3] * b*b*b + a[2]* b*b +a[1] * b + a[0] ) * toRadians(dx) * 6367000.0; // 东西距离 double Ly = 6367000.0 * toRadians(dy); // 南北距离 //3) 用平面的矩形对角距离公式计算总距离 return Math.sqrt(Lx * Lx + Ly * Ly); } }]]></content>
      <categories>
        <category>programme</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>曲线拟合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[虚心竹有低头叶 傲骨梅无仰面花]]></title>
    <url>%2F%E9%9A%8F%E7%AC%94%2Fmylife1.html</url>
    <content type="text"><![CDATA[虚心竹有低头叶；傲骨梅无仰面花.——郑燮 不讲大道理，只说工作事今天，2017年6月21日，曾经花了一两个月的时间，设计并且实现了一个数据处理的程序，做是做完了，可是一当用于生产，遍出现各种问题。主要问题有以下几点 问题的所在1. 程序处理的性能性能不堪入目。某当个大市的前期数据处理就需要两天。2. 代码各种bug总是有些深藏中的bug未被发现，在使用时才发现，从而需要急急忙忙的修改，并且数据又要从新生成3. 代码的质量代码质量很低，代码挺乱，需要重构的方法很多4. 真的很难测试程序写出来，是否满足需求，需要大量的测试，确实是比较难 曾做过的1. 针对程序处理的性能读写问题：：发现在写的地方很花时间，有大量的小文件需要写，想进办法依然无法解决spark处理：甚至想过使用spark完成，也做了一份spark的程序，使用集群处理，但是数据需要放在hdfs上读写，非常麻烦2. 针对各种bug平时一个人如何也发现不了问题，需要在使用过程中发现问题，解决问题3. 代码质量想过需要提高代码的可读性，想过重构，但是比较随便，只提取了部分方法。代码依然臃肿4. 测试问题使用小数据测试，但是由于数据复杂性，对于某些数据可能成功，对于其他数据，若格式稍有变化，不保证一定可用 这两天的碰到的人或事c哥： 大神，我的项目负责人，现在带我的师傅。很多事都只有他才能处理，c哥做事极其负责，对于这个程序， 给予了非常多的建议与思路，可以说，这整个程序基本都是按照他的思路完成的，刚开始觉得，再难的问题，只要想通了，也就清晰了。sf兄：高级工程师，低调大神，业务熟悉，善于优化， 协助我优化代码，sf听我思路，一下就能找到问题的所在，一看代码，立刻动手帮我把代码翻了个底儿朝天，刚开始挺不开心的，我的代码思路怎么的好，也不能从头到尾给我修改吧，逐渐的，我体会到了大神的用意，我之前的代码实在是太差了，一些最基本的性能的优化我都忽略，代码也不规范，比如map中，key能用int就不用string，经常需要拼接的字符串，千万不能随意使用string，尝试使用StringBuffer或者List，代码方法分散，做到真正的面相对象，主方法主思路，代码量不能多，多提炼方法出去。保证代码的清晰整洁，sf给我修改并且讲解大半天，真的是受益匪浅。y总： 10多年的大神，总工之一，依然坚守在研发岗位，y总甚至提出了另外的一种处理方法，最主要的原因是我们对业务的不熟悉，y总使用的完全不一样的方法，目前觉得能极大的提高效率。shihong：同事，朋友，差不多同时进公司，安排的任务基本是一起完成，合作非常愉快，这个程序是我们合作完成的，遇到不清晰的，我们便相互帮助，1+1&gt;2 从这些人或事的感悟 虚心竹有低头叶 傲骨梅无仰面花， 向优秀的人请教，多他人的的优缺点。 最基础的知识还不够牢固。需要一步一步，脚踏实地的积累。 思维这种东西，需要使劲的去想，实在想不通，换一个方向使劲去想。 写程序之前，其实内心已经想好了需要如何写了，这时候，不要忙，先想想有没有什么问题，有没有更好的方法，真的全部想通后再开始码。做事也一样。 写程序，特别是做数据，先要知道数据源，先想好要如何测试，先想好可能的测试结果。 代码太乱，需要多看优秀代码，不断向前辈同事学习，不断思考自己写的代码，必须让自己的代码给其他人也能看懂。 多多分享，分享其实也是学习与复习最有效的办法之一 多感谢家人与同事，朋友。与同事愉快的合作，感谢他们提出的意见，不能太坚持自己的想法，先接受他人的意见，再看看和自己的想法有哪些不同，使用更好的，如果觉得自己的想法更好，可以提出来一起讨论。]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark RDD算子（十三）之RDD 分区 HashPartitioner，RangePartitioner，自定义分区]]></title>
    <url>%2Fhadoop%2Fspark%2Fspark-rdd-13.html</url>
    <content type="text"><![CDATA[关键字: spark分区方式，java HashPartitioner分区，scala HashPartitioner分区， java RangePartitioner 分区，scala RangePartitioner分区， java 自定义分区，scala自定义分区 默认分区和HashPartitioner分区默认的分区就是HashPartition分区,默认分区不再介绍，下面介绍HashPartition的使用 通过上一章 mapPartitionsWithIndex的例子，我们可以构建一个方法，用来查看RDD的分区123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111 def mapPartIndexFunc(i1:Int,iter: Iterator[(Int,Int)]):Iterator[(Int,(Int,Int))]=&#123; var res = List[(Int,(Int,Int))]() while(iter.hasNext)&#123; var next = iter.next() res=res.::(i1,next) &#125; res.iterator &#125; def printRDDPart(rdd:RDD[(Int,Int)]): Unit =&#123; var mapPartIndexRDDs = rdd.mapPartitionsWithIndex(mapPartIndexFunc) mapPartIndexRDDs.foreach(println( _)) &#125;``` **HashPartitioner分区 scala** 使用pairRdd.partitionBy(new spark.HashPartitioner(n)), 可以分为n个区```scala var pairRdd = sc.parallelize(List((1,1), (1,2), (2,3), (2,4), (3,5), (3,6),(4,7), (4,8),(5,9), (5,10))) //未分区的输出 printRDDPart(pairRdd) println("=========================") val partitioned = pairRdd.partitionBy(new spark.HashPartitioner(3)) //分区后的输出 printRDDPart(partitioned)-----------输出------------(0,(5,10))(0,(5,9))(0,(4,8))(0,(4,7))(0,(3,6))(0,(3,5))(0,(2,4))(0,(2,3))(0,(1,2))(0,(1,1))=========================(0,(3,6))(0,(3,5))(1,(4,8))(1,(4,7))(1,(1,2))(1,(1,1))(2,(5,10))(2,(5,9))(2,(2,4))(2,(2,3))``` **HashPartitioner是如何分区的**： 国内很多说法都是有问题的，参考国外的一个说法 Uses Java’s Object.hashCodemethod to determine the partition as partition = key.hashCode() % numPartitions. 翻译过来就是使用java对象的hashCode来决定是哪个分区，对于piarRDD, 分区就是key.hashCode() % numPartitions, 3%3=0，所以 (3,6) 这个元素在0 分区， 4%3=1，所以元素(4,8) 在1 分区。 下面参考一张图 ![spark分区](https://cdn.edureka.co/blog/wp-content/uploads/2016/03/11.png) # **RangePartitioner** 我理解成范围分区器使用一个范围，将范围内的键分配给相应的分区。这种方法适用于键中有自然排序，键不为负。本文主要介绍如何使用，原理以后再仔细研究,以下代码片段显示了RangePartitioner的用法**RangePartitioner 分区 scala **```scala var pairRdd = sc.parallelize(List((1,1), (5,10), (5,9), (2,4), (3,5), (3,6),(4,7), (4,8),(2,3), (1,2))) printRDDPart(pairRdd) println("=========================") val partitioned = pairRdd.partitionBy(new RangePartitioner(3,pairRdd)) printRDDPart(partitioned) &#125;-------------------输出------------------(0,(1,2))(0,(2,3))(0,(4,8))(0,(4,7))(0,(3,6))(0,(3,5))(0,(2,4))(0,(5,9))(0,(5,10))(0,(1,1))=========================(0,(1,2))(0,(2,3))(0,(2,4))(0,(1,1))(1,(4,8))(1,(4,7))(1,(3,6))(1,(3,5))(2,(5,9))(2,(5,10))``` 上面的RDD生成的时候是乱的，但是我们让他分成三个范围，按照范围，key值为1,2的划分到第一个分区，key值为3，4的划分到第二个分区，key值为5的划分到第三个分区 # **自定义分区** 要实现自定义的分区器，你需要继承 org.apache.spark.Partitioner 类并实现下面三个方法 - **numPartitions: Int**：返回创建出来的分区数。 - **getPartition(key: Any): Int**：返回给定键的分区编号（ 0 到 numPartitions-1）。下面我自定义一个分区，让key大于等于4的落在第一个分区，key&gt;=2并且key&lt;4的落在第二个分区，其余的落在第一个分区。 **scala版本**自定义分区器```scalaclass CustomPartitioner(numParts: Int) extends Partitioner&#123; override def numPartitions: Int = numParts override def getPartition(key: Any): Int = &#123; if(key.toString.toInt&gt;=4)&#123; 0 &#125;else if(key.toString.toInt&gt;=2&amp;&amp;key.toString.toInt&lt;4)&#123; 1 &#125;else&#123; 2 &#125; &#125;&#125; 分区, 然后调用前面我们写的printRDDPart方法把各个分区中的RDD打印出来1234567891011121314 var pairRdd = sc.parallelize(List((1,1), (5,10), (5,9), (2,4), (3,5), (3,6),(4,7), (4,8),(2,3), (1,2))) val partitionedRdd = pairRdd.partitionBy(new CustomPartitioner(3)) printRDDPart(partitionedRdd)----------输出-----------------(0,(4,8))(0,(4,7))(0,(5,9))(0,(5,10))(1,(2,3))(1,(3,6))(1,(3,5))(1,(2,4))(2,(1,2))(2,(1,1)) ==java 分区的用法==同样，写个方法，该方法能打印RDD下的每个分区下的各个元素 打印每个分区下的各个元素的printPartRDD函数12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849 private static void printPartRDD(JavaPairRDD&lt;Integer, Integer&gt; pairRDD) &#123; JavaRDD&lt;Tuple2&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt;&gt; mapPartitionIndexRDD = pairRDD.mapPartitionsWithIndex(new Function2&lt;Integer, Iterator&lt;Tuple2&lt;Integer, Integer&gt;&gt;, Iterator&lt;Tuple2&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt;&gt;&gt;() &#123; @Override public Iterator&lt;Tuple2&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt;&gt; call(Integer partIndex, Iterator&lt;Tuple2&lt;Integer, Integer&gt;&gt; tuple2Iterator) &#123; ArrayList&lt;Tuple2&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt;&gt; tuple2s = new ArrayList&lt;&gt;(); while (tuple2Iterator.hasNext()) &#123; Tuple2&lt;Integer, Integer&gt; next = tuple2Iterator.next(); tuple2s.add(new Tuple2&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt;(partIndex, next)); &#125; return tuple2s.iterator(); &#125; &#125;, false); mapPartitionIndexRDD.foreach(new VoidFunction&lt;Tuple2&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt;&gt;() &#123; @Override public void call(Tuple2&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt; integerTuple2Tuple2) throws Exception &#123; System.out.println(integerTuple2Tuple2); &#125; &#125;); &#125;``` **java HashPartitioner 分区** ```java JavaRDD&lt;Tuple2&lt;Integer, Integer&gt;&gt; tupRdd = sc.parallelize(Arrays.asList(new Tuple2&lt;Integer, Integer&gt;(1, 1), new Tuple2&lt;Integer, Integer&gt;(1, 2) , new Tuple2&lt;Integer, Integer&gt;(2, 3), new Tuple2&lt;Integer, Integer&gt;(2, 4) , new Tuple2&lt;Integer, Integer&gt;(3, 5), new Tuple2&lt;Integer, Integer&gt;(3, 6) , new Tuple2&lt;Integer, Integer&gt;(4, 7), new Tuple2&lt;Integer, Integer&gt;(4, 8) , new Tuple2&lt;Integer, Integer&gt;(5, 9), new Tuple2&lt;Integer, Integer&gt;(5, 10) ), 3); JavaPairRDD&lt;Integer, Integer&gt; pairRDD = JavaPairRDD.fromJavaRDD(tupRdd); JavaPairRDD&lt;Integer, Integer&gt; partitioned = pairRDD.partitionBy(new HashPartitioner(3)); System.out.println("============HashPartitioner=================="); printPartRDD(partitioned);--------------打印--------------------============HashPartitioner==================(0,(3,5))(0,(3,6))(1,(1,2))(1,(4,8))(1,(4,7))(1,(1,1))(2,(5,10))(2,(2,4))(2,(2,3))(2,(5,9)) java 自定义分区自定义分区器 ，key大于4的落在第一个分区，[2,4)之间的落在第二个分区，其余的落在第三个分区1234567891011121314151617181920212223public class JavaCustomPart extends Partitioner &#123; int i = 1; public JavaCustomPart(int i)&#123; this.i=i; &#125; public JavaCustomPart()&#123;&#125; @Override public int numPartitions() &#123; return i; &#125; @Override public int getPartition(Object key) &#123; int keyCode = Integer.parseInt(key.toString()); if(keyCode&gt;=4)&#123; return 0; &#125;else if(keyCode&gt;=2&amp;&amp;keyCode&lt;4)&#123; return 1; &#125;else &#123; return 2; &#125; &#125;&#125; 分区并打印 System.out.println("============CustomPartition=================="); JavaPairRDD&lt;Integer, Integer&gt; customPart = pairRDD.partitionBy(new JavaCustomPart(3)); printPartRDD(customPart); --------------打印--------------- ============CustomPartition================== (0,(5,10)) (0,(4,8)) (0,(4,7)) (0,(5,9)) (1,(2,4)) (1,(3,5)) (1,(3,6)) (1,(2,3)) (2,(1,2)) (2,(1,1)) java RangePartitionerTODO]]></content>
      <categories>
        <category>大数据</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark RDD算子（十二）之RDD 分区操作上mapPartitions, mapPartitionsWithIndex保存操作saveAsTextFile,saveAsSequenceFile,saveAsObjectFile,saveAsHadoopFile 等]]></title>
    <url>%2Fhadoop%2Fspark%2Fspark-rdd-12.html</url>
    <content type="text"><![CDATA[mapPartitionsmapPartition可以倒过来理解，先partition，再把每个partition进行map函数，适用场景如果在映射的过程中需要频繁创建额外的对象，使用mapPartitions要比map高效的过。 比如，将RDD中的所有数据通过JDBC连接写入数据库，如果使用map函数，可能要为每一个元素都创建一个connection，这样开销很大，如果使用mapPartitions，那么只需要针对每一个分区建立一个connection。下面的例子，把每一个元素平方java 每一个元素平方123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263 JavaRDD&lt;Integer&gt; rdd = sc.parallelize( Arrays.asList(1,2,3,4,5,6,7,8,9,10)); JavaRDD&lt;Integer&gt; mapPartitionRDD = rdd.mapPartitions(new FlatMapFunction&lt;Iterator&lt;Integer&gt;, Integer&gt;() &#123; @Override public Iterable&lt;Integer&gt; call(Iterator&lt;Integer&gt; it) throws Exception &#123; ArrayList&lt;Integer&gt; results = new ArrayList&lt;&gt;(); while (it.hasNext()) &#123; int i = it.next(); results.add(i*i); &#125; return results; &#125; &#125;); mapPartitionRDD.foreach(new VoidFunction&lt;Integer&gt;() &#123; @Override public void call(Integer integer) throws Exception &#123; System.out.println(integer); &#125; &#125;);----------输出-------------149162536496481100``` **把每一个数字i变成一个map(i,i*i)的形式****java，把每一个元素变成map(i,i*i)**```java JavaRDD&lt;Tuple2&lt;Integer, Integer&gt;&gt; tuple2JavaRDD = rdd.mapPartitions(new FlatMapFunction&lt;Iterator&lt;Integer&gt;, Tuple2&lt;Integer, Integer&gt;&gt;() &#123; @Override public Iterable&lt;Tuple2&lt;Integer, Integer&gt;&gt; call(Iterator&lt;Integer&gt; it) throws Exception &#123; ArrayList&lt;Tuple2&lt;Integer, Integer&gt;&gt; tuple2s = new ArrayList&lt;&gt;(); while (it.hasNext()) &#123; Integer next = it.next(); tuple2s.add(new Tuple2&lt;Integer, Integer&gt;(next, next * next)); &#125; return tuple2s; &#125; &#125;); tuple2JavaRDD.foreach(new VoidFunction&lt;Tuple2&lt;Integer, Integer&gt;&gt;() &#123; @Override public void call(Tuple2&lt;Integer, Integer&gt; tp2) throws Exception &#123; System.out.println(tp2); &#125; &#125;);---------输出---------------(1,1)(2,4)(3,9)(4,16)(5,25)(6,36)(7,49)(8,64)(9,81)(10,100) scala 把每一个元素变成map(i,i*i)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110 val rdd = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10),3) def mapPartFunc(iter: Iterator[Int]):Iterator[(Int,Int)]=&#123; var res = List[(Int,Int)]() while (iter.hasNext)&#123; val cur = iter.next res=res.::(cur,cur*cur) &#125; res.iterator &#125; val mapPartRDD = rdd.mapPartitions(mapPartFunc) mapPartRDD.foreach(maps=&gt;println(maps))----------输出-----------(3,9)(2,4)(1,1)(6,36)(5,25)(4,16)(10,100)(9,81)(8,64)(7,49)``` **mapPartitions操作键值对 把(i,j) 变成(i,j*j)****scala版本**```scala var rdd = sc.parallelize(List((1,1), (1,2), (1,3), (2,1), (2,2), (2,3))) def mapPartFunc(iter: Iterator[(Int,Int)]):Iterator[(Int,Int)]=&#123; var res = List[(Int,Int)]() while (iter.hasNext)&#123; val cur = iter.next res=res.::(cur._1,cur._2*cur._2) &#125; res.iterator &#125; val mapPartionsRDD = rdd.mapPartitions(mapPartFunc) mapPartionsRDD.foreach(println( _))``` **java版本**```java JavaRDD&lt;Tuple2&lt;Integer, Integer&gt;&gt; rdd = sc.parallelize(Arrays.asList(new Tuple2&lt;Integer, Integer&gt;(1, 1), new Tuple2&lt;Integer, Integer&gt;(1, 2) , new Tuple2&lt;Integer, Integer&gt;(1, 3), new Tuple2&lt;Integer, Integer&gt;(2, 1) , new Tuple2&lt;Integer, Integer&gt;(2, 2), new Tuple2&lt;Integer, Integer&gt;(2, 3)), 3); JavaPairRDD&lt;Integer, Integer&gt; pairRDD = JavaPairRDD.fromJavaRDD(rdd); JavaRDD&lt;Tuple2&lt;Integer, Integer&gt;&gt; tuple2JavaRDD = pairRDD.mapPartitions(new FlatMapFunction&lt;Iterator&lt;Tuple2&lt;Integer, Integer&gt;&gt;, Tuple2&lt;Integer, Integer&gt;&gt;() &#123; @Override public Iterable&lt;Tuple2&lt;Integer, Integer&gt;&gt; call(Iterator&lt;Tuple2&lt;Integer, Integer&gt;&gt; tp2It) throws Exception &#123; ArrayList&lt;Tuple2&lt;Integer, Integer&gt;&gt; tuple2s = new ArrayList&lt;&gt;(); while (tp2It.hasNext())&#123; Tuple2&lt;Integer, Integer&gt; next = tp2It.next(); tuple2s.add(new Tuple2&lt;Integer, Integer&gt;(next._1,next._2*next._2)); &#125; return tuple2s; &#125; &#125;); tuple2JavaRDD.foreach(new VoidFunction&lt;Tuple2&lt;Integer, Integer&gt;&gt;() &#123; @Override public void call(Tuple2&lt;Integer, Integer&gt; tp2) throws Exception &#123; System.out.println(tp2); &#125; &#125;);-----------------输出---------------(1,1)(1,4)(1,9)(2,1)(2,4)(2,9)``` # **mapPartitionsWithIndex** 与mapPartitionWithIndex类似，也是按照分区进行的map操作，不过mapPartitionsWithIndex传入的参数多了一个分区的值，下面举个例子,为统计各个分区中的元素 (稍加修改可以做统计各个分区的数量)**java**```javaJavaRDD&lt;Integer&gt; rdd = sc.parallelize(Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10), 3); JavaRDD&lt;Tuple2&lt;Integer, Integer&gt;&gt; tuple2JavaRDD = rdd.mapPartitionsWithIndex(new Function2&lt;Integer, Iterator&lt;Integer&gt;, Iterator&lt;Tuple2&lt;Integer, Integer&gt;&gt;&gt;() &#123; @Override public Iterator&lt;Tuple2&lt;Integer, Integer&gt;&gt; call(Integer partIndex, Iterator&lt;Integer&gt; it) throws Exception &#123; ArrayList&lt;Tuple2&lt;Integer, Integer&gt;&gt; tuple2s = new ArrayList&lt;&gt;(); while (it.hasNext()) &#123; int next = it.next(); tuple2s.add(new Tuple2&lt;&gt;(partIndex, next)); &#125; return tuple2s.iterator(); &#125; &#125;, false); tuple2JavaRDD.foreach(new VoidFunction&lt;Tuple2&lt;Integer, Integer&gt;&gt;() &#123; @Override public void call(Tuple2&lt;Integer, Integer&gt; tp2) throws Exception &#123; System.out.println(tp2); &#125; &#125;);-------输出-------------(0,1)(0,2)(0,3)(1,4)(1,5)(1,6)(2,7)(2,8)(2,9)(2,10)``` **scala** val rdd = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10),3) def mapPartIndexFunc(i1:Int,iter: Iterator[Int]):Iterator[(Int,Int)]={ var res = List[(Int,Int)]() while(iter.hasNext){ var next = iter.next() res=res.::(i1,next) } res.iterator } var mapPartIndexRDDs = rdd.mapPartitionsWithIndex(mapPartIndexFunc) mapPartIndexRDDs.foreach(println( _)) ————输出——-(0,3)(0,2)(0,1)(1,6)(1,5)(1,4)(2,10)(2,9)(2,8)(2,7)12345678910111213141516171819202122232425262728**mapPartitionsWithIndex 统计键值对中的各个分区的元素** **scala版本**```scalavar rdd = sc.parallelize(List((1,1), (1,2), (2,3), (2,4), (3,5), (3,6),(4,7), (4,8),(5,9), (5,10)),3) def mapPartIndexFunc(i1:Int,iter: Iterator[(Int,Int)]):Iterator[(Int,(Int,Int))]=&#123; var res = List[(Int,(Int,Int))]() while(iter.hasNext)&#123; var next = iter.next() res=res.::(i1,next) &#125; res.iterator &#125; val mapPartIndexRDD = rdd.mapPartitionsWithIndex(mapPartIndexFunc) mapPartIndexRDD.foreach(println( _))-----------输出---------(0,(1,1))(0,(1,2))(0,(2,3))(1,(2,4))(1,(3,5))(1,(3,6))(2,(4,7))(2,(4,8))(2,(5,9))(2,(5,10)) java版本1234567891011121314151617181920212223242526272829303132333435363738 JavaRDD&lt;Tuple2&lt;Integer, Integer&gt;&gt; rdd = sc.parallelize(Arrays.asList(new Tuple2&lt;Integer, Integer&gt;(1, 1), new Tuple2&lt;Integer, Integer&gt;(1, 2) , new Tuple2&lt;Integer, Integer&gt;(2, 3), new Tuple2&lt;Integer, Integer&gt;(2, 4) , new Tuple2&lt;Integer, Integer&gt;(3, 5), new Tuple2&lt;Integer, Integer&gt;(3, 6) , new Tuple2&lt;Integer, Integer&gt;(4, 7), new Tuple2&lt;Integer, Integer&gt;(4, 8) , new Tuple2&lt;Integer, Integer&gt;(5, 9), new Tuple2&lt;Integer, Integer&gt;(5, 10) ), 3); JavaPairRDD&lt;Integer, Integer&gt; pairRDD = JavaPairRDD.fromJavaRDD(rdd); JavaRDD&lt;Tuple2&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt;&gt; mapPartitionIndexRDD = pairRDD.mapPartitionsWithIndex(new Function2&lt;Integer, Iterator&lt;Tuple2&lt;Integer, Integer&gt;&gt;, Iterator&lt;Tuple2&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt;&gt;&gt;() &#123; @Override public Iterator&lt;Tuple2&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt;&gt; call(Integer partIndex, Iterator&lt;Tuple2&lt;Integer, Integer&gt;&gt; tuple2Iterator) &#123; ArrayList&lt;Tuple2&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt;&gt; tuple2s = new ArrayList&lt;&gt;(); while (tuple2Iterator.hasNext()) &#123; Tuple2&lt;Integer, Integer&gt; next = tuple2Iterator.next(); tuple2s.add(new Tuple2&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt;(partIndex, next)); &#125; return tuple2s.iterator(); &#125; &#125;, false); mapPartitionIndexRDD.foreach(new VoidFunction&lt;Tuple2&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt;&gt;() &#123; @Override public void call(Tuple2&lt;Integer, Tuple2&lt;Integer, Integer&gt;&gt; integerTuple2Tuple2) throws Exception &#123; System.out.println(integerTuple2Tuple2); &#125; &#125;);--------------输出---------(0,(1,1))(0,(1,2))(0,(2,3))(1,(2,4))(1,(3,5))(1,(3,6))(2,(4,7))(2,(4,8))(2,(5,9))(2,(5,10)) mapPartitionsWithIndex 中 第二个参数，true还是false这篇文章有些探讨,http://stackoverflow.com/questions/38048904/how-to-use-function-mappartitionswithindex-in-spark/38049239，个人还未理解 TODO 补充： 打印各个分区的操作，可以使用 glom 的方法12345678910111213141516171819202122JavaRDD&lt;Tuple2&lt;Integer, Integer&gt;&gt; rdd1 = sc.parallelize(Arrays.asList(new Tuple2&lt;Integer, Integer&gt;(1, 1), new Tuple2&lt;Integer, Integer&gt;(1, 2) , new Tuple2&lt;Integer, Integer&gt;(2, 3), new Tuple2&lt;Integer, Integer&gt;(2, 4) , new Tuple2&lt;Integer, Integer&gt;(3, 5), new Tuple2&lt;Integer, Integer&gt;(3, 6) , new Tuple2&lt;Integer, Integer&gt;(4, 7), new Tuple2&lt;Integer, Integer&gt;(4, 8) , new Tuple2&lt;Integer, Integer&gt;(5, 9), new Tuple2&lt;Integer, Integer&gt;(5, 10) ), 3); JavaPairRDD&lt;Integer, Integer&gt; pairRDD = JavaPairRDD.fromJavaRDD(rdd1); /*补充：打印各个分区的操作，可以使用 glom 的方法*/ System.out.println("打印各个分区的操作，可以使用 glom 的方法"); JavaRDD&lt;List&lt;Tuple2&lt;Integer, Integer&gt;&gt;&gt; glom = pairRDD.glom(); glom.foreach(new VoidFunction&lt;List&lt;Tuple2&lt;Integer, Integer&gt;&gt;&gt;() &#123; @Override public void call(List&lt;Tuple2&lt;Integer, Integer&gt;&gt; tuple2s) throws Exception &#123; System.out.println(tuple2s); &#125; &#125;);//************************* 输出 打印各个分区的操作，可以使用 glom 的方法[(1,1), (1,2), (2,3)][(2,4), (3,5), (3,6)][(4,7), (4,8), (5,9), (5,10)]]]></content>
      <categories>
        <category>大数据</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark RDD算子（十一）之RDD Action 保存操作saveAsTextFile,saveAsSequenceFile,saveAsObjectFile,saveAsHadoopFile 等]]></title>
    <url>%2Fhadoop%2Fspark%2Fspark-rdd-11.html</url>
    <content type="text"><![CDATA[关键字:Spark算子、Spark函数、Spark RDD行动Action、Spark RDD存储操作、saveAsTextFile、saveAsSequenceFile、saveAsObjectFile,saveAsHadoopFile、saveAsHadoopDataset,saveAsNewAPIHadoopFile、saveAsNewAPIHadoopDataset saveAsTextFiledef saveAsTextFile(path: String): Unit def saveAsTextFile(path: String, codec: Class[_ &lt;: CompressionCodec]): Unit saveAsTextFile用于将RDD以文本文件的格式存储到文件系统中。 codec参数可以指定压缩的类名。12345678var rdd1 = sc.makeRDD(1 to 10,2)scala&gt; rdd1.saveAsTextFile("hdfs://cdh5/tmp/lxw1234.com/") //保存到HDFShadoop fs -ls /tmp/lxw1234.comFound 2 items-rw-r--r-- 2 lxw1234 supergroup 0 2015-07-10 09:15 /tmp/lxw1234.com/_SUCCESS-rw-r--r-- 2 lxw1234 supergroup 21 2015-07-10 09:15 /tmp/lxw1234.com/part-00000 hadoop fs -cat /tmp/lxw1234.com/part-00000 注意：如果使用rdd1.saveAsTextFile(“file:///tmp/lxw1234.com”)将文件保存到本地文件系统，那么只会保存在Executor所在机器的本地目录。指定压缩格式保存1234567rdd1.saveAsTextFile("hdfs://cdh5/tmp/lxw1234.com/",classOf[com.hadoop.compression.lzo.LzopCodec]) hadoop fs -ls /tmp/lxw1234.com-rw-r--r-- 2 lxw1234 supergroup 0 2015-07-10 09:20 /tmp/lxw1234.com/_SUCCESS-rw-r--r-- 2 lxw1234 supergroup 71 2015-07-10 09:20 /tmp/lxw1234.com/part-00000.lzo hadoop fs -text /tmp/lxw1234.com/part-00000.lzo saveAsSequenceFilesaveAsSequenceFile用于将RDD以SequenceFile的文件格式保存到HDFS上。 用法同saveAsTextFile。 saveAsObjectFiledef saveAsObjectFile(path: String): Unit saveAsObjectFile用于将RDD中的元素序列化成对象，存储到文件中。 对于HDFS，默认采用SequenceFile保存。12345var rdd1 = sc.makeRDD(1 to 10,2)rdd1.saveAsObjectFile("hdfs://cdh5/tmp/lxw1234.com/") hadoop fs -cat /tmp/lxw1234.com/part-00000SEQ !org.apache.hadoop.io.NullWritable"org.apache.hadoop.io.BytesWritableT saveAsHadoopFiledef saveAsHadoopFile(path: String, keyClass: Class[], valueClass: Class[], outputFormatClass: Class[ &lt;: OutputFormat[, ]], codec: Class[ &lt;: CompressionCodec]): Unit def saveAsHadoopFile(path: String, keyClass: Class[], valueClass: Class[], outputFormatClass: Class[ &lt;: OutputFormat[, ]], conf: JobConf = …, codec: Option[Class[ &lt;: CompressionCodec]] = None): Unit saveAsHadoopFile是将RDD存储在HDFS上的文件中，支持老版本Hadoop API。 可以指定outputKeyClass、outputValueClass以及压缩格式。 每个分区输出一个文件。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169var rdd1 = sc.makeRDD(Array(("A",2),("A",1),("B",6),("B",3),("B",7))) import org.apache.hadoop.mapred.TextOutputFormatimport org.apache.hadoop.io.Textimport org.apache.hadoop.io.IntWritable rdd1.saveAsHadoopFile("/tmp/lxw1234.com/",classOf[Text],classOf[IntWritable],classOf[TextOutputFormat[Text,IntWritable]]) rdd1.saveAsHadoopFile("/tmp/lxw1234.com/",classOf[Text],classOf[IntWritable],classOf[TextOutputFormat[Text,IntWritable]], classOf[com.hadoop.compression.lzo.LzopCodec])``` # **saveAsHadoopDataset** def saveAsHadoopDataset(conf: JobConf): UnitsaveAsHadoopDataset用于将RDD保存到除了HDFS的其他存储中，比如HBase。在JobConf中，通常需要关注或者设置五个参数：文件的保存路径、key值的class类型、value值的class类型、RDD的输出格式(OutputFormat)、以及压缩相关的参数。 **##使用saveAsHadoopDataset将RDD保存到HDFS中** ```scalaimport org.apache.spark.SparkConfimport org.apache.spark.SparkContextimport SparkContext._import org.apache.hadoop.mapred.TextOutputFormatimport org.apache.hadoop.io.Textimport org.apache.hadoop.io.IntWritableimport org.apache.hadoop.mapred.JobConf var rdd1 = sc.makeRDD(Array(("A",2),("A",1),("B",6),("B",3),("B",7)))var jobConf = new JobConf()jobConf.setOutputFormat(classOf[TextOutputFormat[Text,IntWritable]])jobConf.setOutputKeyClass(classOf[Text])jobConf.setOutputValueClass(classOf[IntWritable])jobConf.set("mapred.output.dir","/tmp/lxw1234/")rdd1.saveAsHadoopDataset(jobConf) 结果：hadoop fs -cat /tmp/lxw1234/part-00000A 2A 1hadoop fs -cat /tmp/lxw1234/part-00001B 6B 3B 7``` **##保存数据到HBASE** HBase建表：create ‘lxw1234′,&#123;NAME =&gt; ‘f1′,VERSIONS =&gt; 1&#125;,&#123;NAME =&gt; ‘f2′,VERSIONS =&gt; 1&#125;,&#123;NAME =&gt; ‘f3′,VERSIONS =&gt; 1&#125; ```scalaimport org.apache.spark.SparkConfimport org.apache.spark.SparkContextimport SparkContext._import org.apache.hadoop.mapred.TextOutputFormatimport org.apache.hadoop.io.Textimport org.apache.hadoop.io.IntWritableimport org.apache.hadoop.mapred.JobConfimport org.apache.hadoop.hbase.HBaseConfigurationimport org.apache.hadoop.hbase.mapred.TableOutputFormatimport org.apache.hadoop.hbase.client.Putimport org.apache.hadoop.hbase.util.Bytesimport org.apache.hadoop.hbase.io.ImmutableBytesWritable var conf = HBaseConfiguration.create() var jobConf = new JobConf(conf) jobConf.set("hbase.zookeeper.quorum","zkNode1,zkNode2,zkNode3") jobConf.set("zookeeper.znode.parent","/hbase") jobConf.set(TableOutputFormat.OUTPUT_TABLE,"lxw1234") jobConf.setOutputFormat(classOf[TableOutputFormat]) var rdd1 = sc.makeRDD(Array(("A",2),("B",6),("C",7))) rdd1.map(x =&gt; &#123; var put = new Put(Bytes.toBytes(x._1)) put.add(Bytes.toBytes("f1"), Bytes.toBytes("c1"), Bytes.toBytes(x._2)) (new ImmutableBytesWritable,put) &#125; ).saveAsHadoopDataset(jobConf) ##结果：hbase(main):005:0&gt; scan 'lxw1234'ROW COLUMN+CELL A column=f1:c1, timestamp=1436504941187, value=\x00\x00\x00\x02 B column=f1:c1, timestamp=1436504941187, value=\x00\x00\x00\x06 C column=f1:c1, timestamp=1436504941187, value=\x00\x00\x00\x07 3 row(s) in 0.0550 seconds``` 注意：保存到HBase，运行时候需要在SPARK_CLASSPATH中加入HBase相关的jar包。可参考：http://lxw1234.com/archives/2015/07/332.htm # **saveAsNewAPIHadoopFile** def saveAsNewAPIHadoopFile[F &lt;: OutputFormat[K, V]](path: String)(implicit fm: ClassTag[F]): Unitdef saveAsNewAPIHadoopFile(path: String, keyClass: Class[_], valueClass: Class[_], outputFormatClass: Class[_ &lt;: OutputFormat[_, _]], conf: Configuration = self.context.hadoopConfiguration): Unit saveAsNewAPIHadoopFile用于将RDD数据保存到HDFS上，使用新版本Hadoop API。用法基本同saveAsHadoopFile。```scalaimport org.apache.spark.SparkConfimport org.apache.spark.SparkContextimport SparkContext._import org.apache.hadoop.mapreduce.lib.output.TextOutputFormatimport org.apache.hadoop.io.Textimport org.apache.hadoop.io.IntWritable var rdd1 = sc.makeRDD(Array(("A",2),("A",1),("B",6),("B",3),("B",7)))rdd1.saveAsNewAPIHadoopFile("/tmp/lxw1234/",classOf[Text],classOf[IntWritable],classOf[TextOutputFormat[Text,IntWritable]])``` # **saveAsNewAPIHadoopDataset**def saveAsNewAPIHadoopDataset(conf: Configuration): Unit作用同saveAsHadoopDataset,只不过采用新版本Hadoop API。以写入HBase为例： HBase建表：create ‘lxw1234′,&#123;NAME =&gt; ‘f1′,VERSIONS =&gt; 1&#125;,&#123;NAME =&gt; ‘f2′,VERSIONS =&gt; 1&#125;,&#123;NAME =&gt; ‘f3′,VERSIONS =&gt; 1&#125; 完整的Spark应用程序： ```scalapackage com.lxw1234.test import org.apache.spark.SparkConfimport org.apache.spark.SparkContextimport SparkContext._import org.apache.hadoop.hbase.HBaseConfigurationimport org.apache.hadoop.mapreduce.Jobimport org.apache.hadoop.hbase.mapreduce.TableOutputFormatimport org.apache.hadoop.hbase.io.ImmutableBytesWritableimport org.apache.hadoop.hbase.client.Resultimport org.apache.hadoop.hbase.util.Bytesimport org.apache.hadoop.hbase.client.Put object Test &#123; def main(args : Array[String]) &#123; val sparkConf = new SparkConf().setMaster("spark://lxw1234.com:7077").setAppName("lxw1234.com") val sc = new SparkContext(sparkConf); var rdd1 = sc.makeRDD(Array(("A",2),("B",6),("C",7))) sc.hadoopConfiguration.set("hbase.zookeeper.quorum ","zkNode1,zkNode2,zkNode3") sc.hadoopConfiguration.set("zookeeper.znode.parent","/hbase") sc.hadoopConfiguration.set(TableOutputFormat.OUTPUT_TABLE,"lxw1234") var job = new Job(sc.hadoopConfiguration) job.setOutputKeyClass(classOf[ImmutableBytesWritable]) job.setOutputValueClass(classOf[Result]) job.setOutputFormatClass(classOf[TableOutputFormat[ImmutableBytesWritable]]) rdd1.map( x =&gt; &#123; var put = new Put(Bytes.toBytes(x._1)) put.add(Bytes.toBytes("f1"), Bytes.toBytes("c1"), Bytes.toBytes(x._2)) (new ImmutableBytesWritable,put) &#125; ).saveAsNewAPIHadoopDataset(job.getConfiguration) sc.stop() &#125;&#125; 注意：保存到HBase，运行时候需要在SPARK_CLASSPATH中加入HBase相关的jar包。 可参考：http://lxw1234.com/archives/2015/07/332.htm 感谢原作者的总结本文转自: lxw的大数据田地http://lxw1234.com/archives/2015/07/402.htmhttp://lxw1234.com/archives/2015/07/404.htmhttp://lxw1234.com/archives/2015/07/406.htm]]></content>
      <categories>
        <category>大数据</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark RDD算子（十）之PairRDD的Action操作countByKey, collectAsMap]]></title>
    <url>%2Fhadoop%2Fspark%2Fspark-rdd-10.html</url>
    <content type="text"><![CDATA[countByKeydef countByKey(): Map[K, Long]以RDD{(1, 2),(2,4),(2,5), (3, 4),(3,5), (3, 6)}为例 rdd.countByKey会返回{(1,1),(2,2),(3,3)}scala例子1234scala&gt; val rdd = sc.parallelize(Array((1, 2),(2,4),(2,5), (3, 4),(3,5), (3, 6)))scala&gt; val countbyKeyRDD = rdd.countByKey()countbyKeyRDD: scala.collection.Map[Int,Long] = Map(1 -&gt; 1, 2 -&gt; 2, 3 -&gt; 3) java例子1234567891011121314151617181920 JavaRDD&lt;Tuple2&lt;Integer, Integer&gt;&gt; tupleRDD = sc.parallelize(Arrays.asList(new Tuple2&lt;&gt;(1, 2), new Tuple2&lt;&gt;(2, 4), new Tuple2&lt;&gt;(2, 5), new Tuple2&lt;&gt;(3, 4), new Tuple2&lt;&gt;(3, 5), new Tuple2&lt;&gt;(3, 6))); JavaPairRDD&lt;Integer, Integer&gt; mapRDD = JavaPairRDD.fromJavaRDD(tupleRDD); //countByKey Map&lt;Integer, Object&gt; countByKeyRDD = mapRDD.countByKey(); for (Integer i:countByKeyRDD.keySet()) &#123; System.out.println("("+i+", "+countByKeyRDD.get(i)+")"); &#125;/*输出 (1, 1)(3, 3)(2, 2)*/ collectAsMap将pair类型(键值对类型)的RDD转换成map, 还是上面的例子 scala例子1234scala&gt; val rdd = sc.parallelize(Array((1, 2),(2,4),(2,5), (3, 4),(3,5), (3, 6)))scala&gt; rdd.collectAsMap()res1: scala.collection.Map[Int,Int] = Map(2 -&gt; 5, 1 -&gt; 2, 3 -&gt; 6) java例子12345678910JavaRDD&lt;Tuple2&lt;Integer, Integer&gt;&gt; tupleRDD = sc.parallelize(Arrays.asList(new Tuple2&lt;&gt;(1, 2), new Tuple2&lt;&gt;(2, 4), new Tuple2&lt;&gt;(2, 5), new Tuple2&lt;&gt;(3, 4), new Tuple2&lt;&gt;(3, 5), new Tuple2&lt;&gt;(3, 6)));JavaPairRDD&lt;Integer, Integer&gt; mapRDD = JavaPairRDD.fromJavaRDD(tupleRDD);Map&lt;Integer, Integer&gt; collectMap = mapRDD.collectAsMap();]]></content>
      <categories>
        <category>大数据</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark RDD算子（九）之基本的Action操作 first, take, collect, count, countByValue, reduce, aggregate, fold,top]]></title>
    <url>%2Fhadoop%2Fspark%2Fspark-rdd-9.html</url>
    <content type="text"><![CDATA[first返回第一个元素scala1234scala&gt; val rdd = sc.parallelize(List(1,2,3,3))scala&gt; rdd.first()res1: Int = 1 java12JavaRDD&lt;Integer&gt; rdd = sc.parallelize(Arrays.asList(1, 2, 3, 3));Integer first = rdd.first(); takerdd.take(n)返回第n个元素scala1234scala&gt; val rdd = sc.parallelize(List(1,2,3,3))scala&gt; rdd.take(2)res3: Array[Int] = Array(1, 2) java12JavaRDD&lt;Integer&gt; rdd = sc.parallelize(Arrays.asList(1, 2, 3, 3));List&lt;Integer&gt; take = rdd.take(2); collectrdd.collect() 返回 RDD 中的所有元素scala1234scala&gt; val rdd = sc.parallelize(List(1,2,3,3))scala&gt; rdd.collect()res4: Array[Int] = Array(1, 2, 3, 3) java12JavaRDD&lt;Integer&gt; rdd = sc.parallelize(Arrays.asList(1, 2, 3, 3));List&lt;Integer&gt; collect = rdd.collect(); countrdd.count() 返回 RDD 中的元素个数scala1234scala&gt; val rdd = sc.parallelize(List(1,2,3,3))scala&gt; rdd.count()res5: Long = 4 java12JavaRDD&lt;Integer&gt; rdd = sc.parallelize(Arrays.asList(1, 2, 3, 3));long count = rdd.count(); countByValue各元素在 RDD 中出现的次数 返回{(key1,次数),(key2,次数),…(keyn,次数)}scala1234scala&gt; val rdd = sc.parallelize(List(1,2,3,3))scala&gt; rdd.countByValue()res6: scala.collection.Map[Int,Long] = Map(1 -&gt; 1, 2 -&gt; 1, 3 -&gt; 2) java12JavaRDD&lt;Integer&gt; rdd = sc.parallelize(Arrays.asList(1, 2, 3, 3));Map&lt;Integer, Long&gt; integerLongMap = rdd.countByValue(); reducerdd.reduce(func)并行整合RDD中所有数据， 类似于是scala中集合的reducescala1234scala&gt; val rdd = sc.parallelize(List(1,2,3,3))scala&gt; rdd.reduce((x,y)=&gt;x+y)res7: Int = 9 java123456Integer reduce = rdd.reduce(new Function2&lt;Integer, Integer, Integer&gt;() &#123; @Override public Integer call(Integer integer, Integer integer2) throws Exception &#123; return integer + integer2; &#125;&#125;); aggregate和 reduce() 相 似， 但 是 通 常返回不同类型的函数 一般不用这个函数 scala12scala&gt; val rdd = sc.parallelize(List(1,2,3,3))TODO java12 foldrdd.fold(num)(func) 一般不用这个函数和 reduce() 一 样， 但是提供了初始值num,每个元素计算时，先要合这个初始值进行折叠, 注意，这里会按照每个分区进行fold，然后分区之间还会再次进行fold提供初始值scala12345// 解释 TODO scala&gt; val rdd = sc.parallelize(List(1,2,3,3)，2)scala&gt; rdd.fold(1)((x,y)=&gt;x+y)res8: Int = 12 java12345678910 JavaRDD&lt;Integer&gt; rdd = sc.parallelize(Arrays.asList(1, 2, 3, 3),2); Integer fold = rdd.fold(1, new Function2&lt;Integer, Integer, Integer&gt;() &#123; @Override public Integer call(Integer integer, Integer integer2) throws Exception &#123; return integer + integer2; &#125; &#125;); System.out.println(fold);-------输出-----12 toprdd.top(n)按照降序的或者指定的排序规则，返回前n个元素scala1234scala&gt; val rdd = sc.parallelize(List(1,2,3,3))scala&gt; rdd.top(2)res9: Array[Int] = Array(3, 3) java12JavaRDD&lt;Integer&gt; rdd = sc.parallelize(Arrays.asList(1, 2, 3, 3),2);List&lt;Integer&gt; top = rdd.top(2); takeOrderedrdd.take(n)对RDD元素进行升序排序,取出前n个元素并返回，也可以自定义比较器（这里不介绍），类似于top的相反的方法scala1234scala&gt; val rdd = sc.parallelize(List(1,2,3,3))scala&gt; rdd.takeOrdered(2)res10: Array[Int] = Array(1, 2) java12JavaRDD&lt;Integer&gt; rdd = sc.parallelize(Arrays.asList(1, 2, 3, 3),2);List&lt;Integer&gt; integers = rdd.takeOrdered(2); foreach对 RDD 中的每个元素使用给定的函数scala1234 val rdd = sc.parallelize(List(1,2,3,3)) rdd.foreach(print(_))-----输出-----------1233 java123456rdd.foreach(new VoidFunction&lt;Integer&gt;() &#123; @Override public void call(Integer integer) throws Exception &#123; System.out.println(integer); &#125;&#125;);]]></content>
      <categories>
        <category>大数据</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark RDD算子（八）之键值对关联操作 subtractByKey, join, rightOuterJoin, leftOuterJoin]]></title>
    <url>%2Fhadoop%2Fspark%2Fspark-rdd-8.html</url>
    <content type="text"><![CDATA[先从spark-learning中的一张图大致了解其功能 subtractByKey函数定义12345def subtractByKey[W](other: RDD[(K, W)])(implicit arg0: ClassTag[W]): RDD[(K, V)]def subtractByKey[W](other: RDD[(K, W)], numPartitions: Int)(implicit arg0: ClassTag[W]): RDD[(K, V)]def subtractByKey[W](other: RDD[(K, W)], p: Partitioner)(implicit arg0: ClassTag[W]): RDD[(K, V)] 类似于subtrac，删掉 RDD 中键与 other RDD 中的键相同的元素 join函数定义12345def join[W](other: RDD[(K, W)]): RDD[(K, (V, W))]def join[W](other: RDD[(K, W)], numPartitions: Int): RDD[(K, (V, W))]def join[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (V, W))] RDD1.join(RDD2)可以把RDD1,RDD2中的相同的key给连接起来，类似于sql中的join操作 leftOuterJoin1234567891011121314151617181920212223242526272829def leftOuterJoin[W](other: RDD[(K, W)]): RDD[(K, (V, Option[W]))]def leftOuterJoin[W](other: RDD[(K, W)], numPartitions: Int): RDD[(K, (V, Option[W]))]def leftOuterJoin[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (V, Option[W]))]``` 直接看图即可 对两个 RDD 进行连接操作，类似于sql中的左外连接# **rightOuterJoin**对两个 RDD 进行连接操作，类似于sql中的右外连接，存在的话，value用的Some, 不存在用的None,具体的看上面的图和下面的代码即可 # **代码示例****scala语言**```scala scala&gt; val rdd = sc.makeRDD(Array((1,2),(3,4),(3,6))) scala&gt; val other = sc.makeRDD(Array((3,9))) scala&gt; rdd.subtractByKey(other).collect() res0: Array[(Int, Int)] = Array((1,2)) scala&gt; rdd.join(other).collect() res1: Array[(Int, (Int, Int))] = Array((3,(4,9)), (3,(6,9))) scala&gt; rdd.leftOuterJoin(other).collect() res2: Array[(Int, (Int, Option[Int]))] = Array((1,(2,None)), (3,(4,Some(9))), (3,(6,Some(9)))) scala&gt; rdd.rightOuterJoin(other).collect() res3: Array[(Int, (Option[Int], Int))] = Array((3,(Some(4),9)), (3,(Some(6),9))) java语言12345678910111213141516171819 JavaRDD&lt;Tuple2&lt;Integer,Integer&gt;&gt; rddPre = sc.parallelize(Arrays.asList(new Tuple2(1,2) , new Tuple2(3,4) , new Tuple2(3,6))); JavaRDD&lt;Tuple2&lt;Integer,Integer&gt;&gt; otherPre = sc.parallelize(Arrays.asList(new Tuple2(3,10)));//JavaRDD转换成JavaPairRDD JavaPairRDD&lt;Integer, Integer&gt; rdd = JavaPairRDD.fromJavaRDD(rddPre); JavaPairRDD&lt;Integer, Integer&gt; other = JavaPairRDD.fromJavaRDD(otherPre); //subtractByKey JavaPairRDD&lt;Integer, Integer&gt; subRDD = rdd.subtractByKey(other); //join JavaPairRDD&lt;Integer, Tuple2&lt;Float, Integer&gt;&gt; joinRDD = rdd.join(other); //leftOuterJoin JavaPairRDD&lt;Integer, Tuple2&lt;Integer, Optional&lt;Integer&gt;&gt;&gt; integerTuple2JavaPairRDD = rdd.leftOuterJoin(other); //rightOutJoin JavaPairRDD&lt;Integer, Tuple2&lt;Optional&lt;Integer&gt;, Integer&gt;&gt; rightOutJoin = rdd.rightOuterJoin(other);]]></content>
      <categories>
        <category>大数据</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark RDD算子（七）之键值对分组操作 groupByKey，cogroup]]></title>
    <url>%2Fhadoop%2Fspark%2Fspark-rdd-7.html</url>
    <content type="text"><![CDATA[groupByKey12345def groupByKey(): RDD[(K, Iterable[V])]def groupByKey(numPartitions: Int): RDD[(K, Iterable[V])]def groupByKey(partitioner: Partitioner): RDD[(K, Iterable[V])] groupByKey会将RDD[key,value] 按照相同的key进行分组，形成RDD[key,Iterable[value]]的形式， 有点类似于sql中的groupby，例如类似于mysql中的group_concat例如这个例子， 我们对学生的成绩进行分组scala版本1234567 val scoreDetail = sc.parallelize(List(("xiaoming",75),("xiaoming",90),("lihua",95),("lihua",100),("xiaofeng",85))) scoreDetail.groupByKey().collect().foreach(println(_)); /*输出(lihua,CompactBuffer(95, 100))(xiaoming,CompactBuffer(75, 90))(xiaofeng,CompactBuffer(85)) */ java版本12345678910JavaRDD&lt;Tuple2&lt;String,Float&gt;&gt; scoreDetails = sc.parallelize(Arrays.asList(new Tuple2("xiaoming", 75) , new Tuple2("xiaoming", 90) , new Tuple2("lihua", 95) , new Tuple2("lihua", 188)));//将JavaRDD&lt;Tuple2&lt;String,Float&gt;&gt; 类型转换为 JavaPairRDD&lt;String, Float&gt;JavaPairRDD&lt;String, Float&gt; scoreMapRDD = JavaPairRDD.fromJavaRDD(scoreDetails);Map&lt;String, Iterable&lt;Float&gt;&gt; resultMap = scoreMapRDD.groupByKey().collectAsMap();for (String key:resultMap.keySet()) &#123; System.out.println("("+key+", "+resultMap.get(key)+")");&#125; cogroupgroupByKey是对单个 RDD 的数据进行分组，还可以使用一个叫作 cogroup() 的函数对多个共享同一个键的 RDD 进行分组例如RDD1.cogroup(RDD2) 会将RDD1和RDD2按照相同的key进行分组，得到(key,RDD[key,Iterable[value1],Iterable[value2]])的形式cogroup也可以多个进行分组例如RDD1.cogroup(RDD2,RDD3,…RDDN), 可以得到(key,Iterable[value1],Iterable[value2],Iterable[value3],…,Iterable[valueN])案例,scoreDetail存放的是学生的优秀学科的分数，scoreDetai2存放的是刚刚及格的分数，scoreDetai3存放的是没有及格的科目的分数，我们要对每一个学生的优秀学科，刚及格和不及格的分数给分组统计出来scala版本123456789101112131415161718192021222324252627282930313233343536scala&gt; val scoreDetail = sc.parallelize(List(("xiaoming",95),("xiaoming",90),("lihua",95),("lihua",98),("xiaofeng",97)))scala&gt; val scoreDetai2 = sc.parallelize(List(("xiaoming",65),("lihua",63),("lihua",62),("xiaofeng",67)))scala&gt; val scoreDetai3 = sc.parallelize(List(("xiaoming",25),("xiaoming",15),("lihua",35),("lihua",28),("xiaofeng",36)))scala&gt; scoreDetail.cogroup(scoreDetai2,scoreDetai3)//输出res1: Array[(String, (Iterable[Int], Iterable[Int], Iterable[Int]))] = Array((xiaoming,(CompactBuffer(95, 90),CompactBuffer(65),CompactBuffer(25, 15))), (lihua,(CompactBuffer(95, 98),CompactBuffer(63, 62),CompactBuffer(35, 28))), (xiaofeng,(CompactBuffer(97),CompactBuffer(67),CompactBuffer(36))))``` **java版本** ```java JavaRDD&lt;Tuple2&lt;String,Float&gt;&gt; scoreDetails1 = sc.parallelize(Arrays.asList(new Tuple2("xiaoming", 75) , new Tuple2("xiaoming", 90) , new Tuple2("lihua", 95) , new Tuple2("lihua", 96))); JavaRDD&lt;Tuple2&lt;String,Float&gt;&gt; scoreDetails2 = sc.parallelize(Arrays.asList(new Tuple2("xiaoming", 75) , new Tuple2("lihua", 60) , new Tuple2("lihua", 62))); JavaRDD&lt;Tuple2&lt;String,Float&gt;&gt; scoreDetails3 = sc.parallelize(Arrays.asList(new Tuple2("xiaoming", 75) , new Tuple2("xiaoming", 45) , new Tuple2("lihua", 24) , new Tuple2("lihua", 57))); JavaPairRDD&lt;String, Float&gt; scoreMapRDD1 = JavaPairRDD.fromJavaRDD(scoreDetails1); JavaPairRDD&lt;String, Float&gt; scoreMapRDD2 = JavaPairRDD.fromJavaRDD(scoreDetails2); JavaPairRDD&lt;String, Float&gt; scoreMapRDD3 = JavaPairRDD.fromJavaRDD(scoreDetails2); JavaPairRDD&lt;String, Tuple3&lt;Iterable&lt;Float&gt;, Iterable&lt;Float&gt;, Iterable&lt;Float&gt;&gt;&gt; cogroupRDD = (JavaPairRDD&lt;String, Tuple3&lt;Iterable&lt;Float&gt;, Iterable&lt;Float&gt;, Iterable&lt;Float&gt;&gt;&gt;) scoreMapRDD1.cogroup(scoreMapRDD2, scoreMapRDD3); Map&lt;String, Tuple3&lt;Iterable&lt;Float&gt;, Iterable&lt;Float&gt;, Iterable&lt;Float&gt;&gt;&gt; tuple3 = cogroupRDD.collectAsMap(); for (String key:tuple3.keySet()) &#123; System.out.println("("+key+", "+tuple3.get(key)+")"); &#125; -----输出----------(lihua, ([95, 96],[60, 62],[60, 62]))(xiaoming, ([75, 90],[75],[75]))]]></content>
      <categories>
        <category>大数据</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark RDD算子（六）之键值对聚合操作reduceByKey，foldByKey，排序操作sortByKey]]></title>
    <url>%2Fhadoop%2Fspark%2Fspark-rdd-6.html</url>
    <content type="text"><![CDATA[reduceByKey12345def reduceByKey(func: (V, V) =&gt; V): RDD[(K, V)]def reduceByKey(func: (V, V) =&gt; V, numPartitions: Int): RDD[(K, V)]def reduceByKey(partitioner: Partitioner, func: (V, V) =&gt; V): RDD[(K, V)] 接收一个函数，按照相同的key进行reduce操作，类似于scala的reduce的操作例如RDD {(1, 2), (3, 4), (3, 6)}进行reducescala版本123456 var mapRDD = sc.parallelize(List((1,2),(3,4),(3,6))) var reduceRDD = mapRDD.reduceByKey((x,y)=&gt;x+y) reduceRDD.foreach(x=&gt;println(x))------输出---------(1,2)(3,10) 再举例单词计数F:\sparktest\sample.txt中的内容如下1234aa bb cc aa aa aa dd dd ee ee ee ee ff aa bb zksee kksee zz zks scala版本 1234567891011121314 val lines = sc.textFile("F:\\sparktest\\sample.txt") val wordsRDD = lines.flatMap(x=&gt;x.split("\\s+")).map(x=&gt;(x,1)) val wordCountRDD = wordsRDD.reduceByKey((x,y)=&gt;x+y) wordCountRDD.foreach(x=&gt;println(x))---------输出-----------(ee,6)(aa,5)(dd,2)(zz,1)(zks,2)(kks,1)(ff,1)(bb,2)(cc,1) java版本1234567891011121314151617181920212223242526272829303132333435 JavaRDD&lt;String&gt; lines = sc.textFile("F:\\sparktest\\sample.txt"); JavaPairRDD&lt;String, Integer&gt; wordPairRDD = lines.flatMapToPair(new PairFlatMapFunction&lt;String, String, Integer&gt;() &#123; @Override public Iterable&lt;Tuple2&lt;String, Integer&gt;&gt; call(String s) throws Exception &#123; ArrayList&lt;Tuple2&lt;String, Integer&gt;&gt; tpLists = new ArrayList&lt;Tuple2&lt;String, Integer&gt;&gt;(); String[] split = s.split("\\s+"); for (int i = 0; i &lt;split.length ; i++) &#123; Tuple2 tp = new Tuple2&lt;String,Integer&gt;(split[i], 1); tpLists.add(tp); &#125; return tpLists; &#125; &#125;); JavaPairRDD&lt;String, Integer&gt; wordCountRDD = wordPairRDD.reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() &#123; @Override public Integer call(Integer i1, Integer i2) throws Exception &#123; return i1 + i2; &#125; &#125;); Map&lt;String, Integer&gt; collectAsMap = wordCountRDD.collectAsMap(); for (String key:collectAsMap.keySet()) &#123; System.out.println("("+key+","+collectAsMap.get(key)+")"); &#125;----------输出-------------------------------(kks,1)(ee,6)(bb,2)(zz,1)(ff,1)(cc,1)(zks,2)(dd,2)(aa,5) foldByKey12345def foldByKey(zeroValue: V)(func: (V, V) =&gt; V): RDD[(K, V)]def foldByKey(zeroValue: V, numPartitions: Int)(func: (V, V) =&gt; V): RDD[(K, V)]def foldByKey(zeroValue: V, partitioner: Partitioner)(func: (V, V) =&gt; V): RDD[(K, V)] 该函数用于RDD[K,V]根据K将V做折叠、合并处理，其中的参数zeroValue表示先根据映射函数将zeroValue应用于V,进行初始化V,再将映射函数应用于初始化后的V.foldByKey可以参考我之前的scala的fold的介绍与reduce不同的是 foldByKey开始折叠的第一个元素不是集合中的第一个元素，而是传入的一个元素参考LXW的博客 scala的例子直接看例子123456scala&gt; var rdd1 = sc.makeRDD(Array(("A",0),("A",2),("B",1),("B",2),("C",1)))scala&gt; rdd1.foldByKey(0)(_+_).collectres75: Array[(String, Int)] = Array((A,2), (B,3), (C,1)) //将rdd1中每个key对应的V进行累加，注意zeroValue=0,需要先初始化V,映射函数为+操//作，比如("A",0), ("A",2)，先将zeroValue应用于每个V,得到：("A",0+0), ("A",2+0)，即：//("A",0), ("A",2)，再将映射函数应用于初始化后的V，最后得到(A,0+2),即(A,2) 再看：1234scala&gt; rdd1.foldByKey(2)(_+_).collectres76: Array[(String, Int)] = Array((A,6), (B,7), (C,3))//先将zeroValue=2应用于每个V,得到：("A",0+2), ("A",2+2)，即：("A",2), ("A",4)，再将映射函//数应用于初始化后的V，最后得到：(A,2+4)，即：(A,6) 再看乘法操作：123456789scala&gt; rdd1.foldByKey(0)(_*_).collectres77: Array[(String, Int)] = Array((A,0), (B,0), (C,0))//先将zeroValue=0应用于每个V,注意，这次映射函数为乘法，得到：("A",0*0), ("A",2*0)，//即：("A",0), ("A",0)，再将映射函//数应用于初始化后的V，最后得到：(A,0*0)，即：(A,0)//其他K也一样，最终都得到了V=0 scala&gt; rdd1.foldByKey(1)(_*_).collectres78: Array[(String, Int)] = Array((A,0), (B,2), (C,1))//映射函数为乘法时，需要将zeroValue设为1，才能得到我们想要的结果。 SortByKey1def sortByKey(ascending : scala.Boolean = &#123; /* compiled code */ &#125;, numPartitions : scala.Int = &#123; /* compiled code */ &#125;) : org.apache.spark.rdd.RDD[scala.Tuple2[K, V]] = &#123; /* compiled code */ &#125; SortByKey用于对pairRDD按照key进行排序，第一个参数可以设置true或者false，默认是truescala例子 scala&gt; val rdd = sc.parallelize(Array((3, 4),(1, 2),(4,4),(2,5), (6,5), (5, 6))) // sortByKey不是Action操作，只能算是转换操作 scala&gt; rdd.sortByKey() res9: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[28] at sortByKey at &lt;console&gt;:24 //看看sortByKey后是什么类型 scala&gt; rdd.sortByKey().collect() res10: Array[(Int, Int)] = Array((1,2), (2,5), (3,4), (4,4), (5,6), (6,5)) //降序排序 scala&gt; rdd.sortByKey(false).collect() res12: Array[(Int, Int)] = Array((6,5), (5,6), (4,4), (3,4), (2,5), (1,2)) java例子也是一样的，这里就不写了 参考文章: Spark快速大数据分析,LXW的大数据田地,spark官网]]></content>
      <categories>
        <category>大数据</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark RDD算子（五）之键值对聚合操作 combineByKey]]></title>
    <url>%2Fhadoop%2Fspark%2Fspark-rdd-5.html</url>
    <content type="text"><![CDATA[combineByKey聚合数据一般在集中式数据比较方便，如果涉及到分布式的数据集，该如何去实现呢。这里介绍一下combineByKey, 这个是各种聚集操作的鼻祖，应该要好好了解一下,参考scala API 简要介绍123def combineByKey[C](createCombiner: (V) =&gt; C, mergeValue: (C, V) =&gt; C, mergeCombiners: (C, C) =&gt; C): RD createCombiner: combineByKey() 会遍历分区中的所有元素，因此每个元素的键要么还没有遇到过，要么就和之前的某个元素的键相同。如果这是一个新的元素， combineByKey() 会使用一个叫作 createCombiner() 的函数来创建那个键对应的累加器的初始值 mergeValue: 如果这是一个在处理当前分区之前已经遇到的键， 它会使用 mergeValue() 方法将该键的累加器对应的当前值与这个新的值进行合并 mergeCombiners: 由于每个分区都是独立处理的， 因此对于同一个键可以有多个累加器。如果有两个或者更多的分区都有对应同一个键的累加器， 就需要使用用户提供的 mergeCombiners() 方法将各个分区的结果进行合并。 计算学生平均成绩例子这里举一个计算学生平均成绩的例子,例子参考至https://www.edureka.co/blog/apache-spark-combinebykey-explained, github源码 我对此进行了解析创建一个学生成绩说明的类1case class ScoreDetail(studentName: String, subject: String, score: Float) 下面是一些测试数据，加载测试数据集合 key = Students name and value = ScoreDetail instance123456789val scores = List( ScoreDetail("xiaoming", "Math", 98), ScoreDetail("xiaoming", "English", 88), ScoreDetail("wangwu", "Math", 75), ScoreDetail("wangwu", "English", 78), ScoreDetail("lihua", "Math", 90), ScoreDetail("lihua", "English", 80), ScoreDetail("zhangsan", "Math", 91), ScoreDetail("zhangsan", "English", 80)) 将集合转换成二元组， 也可以理解成转换成一个map, 利用了for 和 yield的组合1val scoresWithKey = for &#123; i &lt;- scores &#125; yield (i.studentName, i) 创建RDD, 并且指定三个分区1val scoresWithKeyRDD = sc.parallelize(scoresWithKey).partitionBy(new HashPartitioner(3)).cache 输出打印一下各个分区的长度和各个分区的一些数据123456789101112131415161718192021222324252627 println("&gt;&gt;&gt;&gt; Elements in each partition") scoresWithKeyRDD.foreachPartition(partition =&gt; println(partition.length)) // explore each partition... println("&gt;&gt;&gt;&gt; Exploring partitions' data...") scoresWithKeyRDD.foreachPartition( partition =&gt; partition.foreach( item =&gt; println(item._2)))/*会输出 &gt;&gt;&gt;&gt; Elements in each partition620&gt;&gt;&gt;&gt; Exploring partitions' data...ScoreDetail(xiaoming,Math,98.0)ScoreDetail(xiaoming,English,88.0)ScoreDetail(lihua,Math,90.0)ScoreDetail(lihua,English,80.0)ScoreDetail(zhangsan,Math,91.0)ScoreDetail(zhangsan,English,80.0)ScoreDetail(wangwu,Math,75.0)ScoreDetail(wangwu,English,78.0)*/ 聚合求平均值让后打印12345678910111213141516171819202122232425262728293031323334353637 val avgScoresRDD = scoresWithKeyRDD.combineByKey( (x: ScoreDetail) =&gt; (x.score, 1) /*createCombiner*/, (acc: (Float, Int), x: ScoreDetail) =&gt; (acc._1 + x.score, acc._2 + 1) /*mergeValue*/, (acc1: (Float, Int), acc2: (Float, Int)) =&gt; (acc1._1 + acc2._1, acc1._2 + acc2._2) /*mergeCombiners*/ // calculate the average ).map( &#123; case(key, value) =&gt; (key, value._1/value._2) &#125;) avgScoresRDD.collect.foreach(println)/*输出:(zhangsan,85.5)(lihua,85.0)(xiaoming,93.0)(wangwu,76.5)*/``` **解释一下scoresWithKeyRDD.combineByKey** **createCombiner:** (x: ScoreDetail) =&gt; (x.score, 1) 这是第一次遇到zhangsan，创建一个函数，把map中的value转成另外一个类型 ，这里是把(zhangsan,(ScoreDetail类))**转换成**(zhangsan,(91,1)) **mergeValue:** (acc: (Float, Int), x: ScoreDetail) =&gt; (acc._1 + x.score, acc._2 + 1) 再次碰到张三， 就把这两个合并, 这里是将(zhangsan,(91,1)) 这种类型 和 (zhangsan,(ScoreDetail类))这种类型合并，合并成了(zhangsan,(171,2)) **mergeCombiners** (acc1: (Float, Int), acc2: (Float, Int)) 这个是将多个分区中的zhangsan的数据进行合并， 我们这里zhansan在同一个分区，这个地方就没有用上 ## java版本的介绍**ScoreDetail类**```javapublic class ScoreDetail implements Serializable&#123; //case class ScoreDetail(studentName: String, subject: String, score: Float) public String studentName; public String subject; public float score; public ScoreDetail(String studentName, String subject, float score) &#123; this.studentName = studentName; this.subject = subject; this.score = score; &#125;&#125; CombineByKey的测试类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class CombineTest &#123; public static void main(String[] args) &#123; SparkConf sparkConf = new SparkConf().setAppName("JavaWordCount").setMaster("local"); JavaSparkContext sc = new JavaSparkContext(sparkConf); ArrayList&lt;ScoreDetail&gt; scoreDetails = new ArrayList&lt;&gt;(); scoreDetails.add(new ScoreDetail("xiaoming", "Math", 98)); scoreDetails.add(new ScoreDetail("xiaoming", "English", 88)); scoreDetails.add(new ScoreDetail("wangwu", "Math", 75)); scoreDetails.add(new ScoreDetail("wangwu", "Englist", 78)); scoreDetails.add(new ScoreDetail("lihua", "Math", 90)); scoreDetails.add(new ScoreDetail("lihua", "English", 80)); scoreDetails.add(new ScoreDetail("zhangsan", "Math", 91)); scoreDetails.add(new ScoreDetail("zhangsan", "English", 80)); JavaRDD&lt;ScoreDetail&gt; scoreDetailsRDD = sc.parallelize(scoreDetails); JavaPairRDD&lt;String, ScoreDetail&gt; pairRDD = scoreDetailsRDD.mapToPair(new PairFunction&lt;ScoreDetail, String, ScoreDetail&gt;() &#123; @Override public Tuple2&lt;String, ScoreDetail&gt; call(ScoreDetail scoreDetail) throws Exception &#123; return new Tuple2&lt;&gt;(scoreDetail.studentName, scoreDetail); &#125; &#125;);// new Function&lt;ScoreDetail, Float,Integer&gt;(); Function&lt;ScoreDetail, Tuple2&lt;Float, Integer&gt;&gt; createCombine = new Function&lt;ScoreDetail, Tuple2&lt;Float, Integer&gt;&gt;() &#123; @Override public Tuple2&lt;Float, Integer&gt; call(ScoreDetail scoreDetail) throws Exception &#123; return new Tuple2&lt;&gt;(scoreDetail.score, 1); &#125; &#125;; // Function2传入两个值，返回一个值 Function2&lt;Tuple2&lt;Float, Integer&gt;, ScoreDetail, Tuple2&lt;Float, Integer&gt;&gt; mergeValue = new Function2&lt;Tuple2&lt;Float, Integer&gt;, ScoreDetail, Tuple2&lt;Float, Integer&gt;&gt;() &#123; @Override public Tuple2&lt;Float, Integer&gt; call(Tuple2&lt;Float, Integer&gt; tp, ScoreDetail scoreDetail) throws Exception &#123; return new Tuple2&lt;&gt;(tp._1 + scoreDetail.score, tp._2 + 1); &#125; &#125;; Function2&lt;Tuple2&lt;Float, Integer&gt;, Tuple2&lt;Float, Integer&gt;, Tuple2&lt;Float, Integer&gt;&gt; mergeCombiners = new Function2&lt;Tuple2&lt;Float, Integer&gt;, Tuple2&lt;Float, Integer&gt;, Tuple2&lt;Float, Integer&gt;&gt;() &#123; @Override public Tuple2&lt;Float, Integer&gt; call(Tuple2&lt;Float, Integer&gt; tp1, Tuple2&lt;Float, Integer&gt; tp2) throws Exception &#123; return new Tuple2&lt;&gt;(tp1._1 + tp2._1, tp1._2 + tp2._2); &#125; &#125;; JavaPairRDD&lt;String, Tuple2&lt;Float,Integer&gt;&gt; combineByRDD = pairRDD.combineByKey(createCombine,mergeValue,mergeCombiners); //打印平均数 Map&lt;String, Tuple2&lt;Float, Integer&gt;&gt; stringTuple2Map = combineByRDD.collectAsMap(); for ( String et:stringTuple2Map.keySet()) &#123; System.out.println(et+" "+stringTuple2Map.get(et)._1/stringTuple2Map.get(et)._2); &#125; &#125;&#125; 注意有个坑的地方 createCombine方法必须是这样的123456Function&lt;ScoreDetail, Tuple2&lt;Float, Integer&gt;&gt; createCombine = new Function&lt;ScoreDetail, Tuple2&lt;Float, Integer&gt;&gt;() &#123; @Override public Tuple2&lt;Float, Integer&gt; call(ScoreDetail scoreDetail) throws Exception &#123; return new Tuple2&lt;&gt;(scoreDetail.score, 1); &#125;&#125;; 而不能是这样的, 即使最后的RDD都类似123456PairFunction&lt;ScoreDetail, Float, Integer&gt; createCombine = new PairFunction&lt;ScoreDetail, Float, Integer&gt;() &#123; @Override public Tuple2&lt;Float, Integer&gt; call(ScoreDetail scoreDetail) throws Exception &#123; return new Tuple2&lt;&gt;(scoreDetail.score, 1); &#125;&#125;; 再推荐几篇比较好的文章 LXW的大数据田地: combineByKey 推荐图书快速大数据分析中combineByKey的讲解]]></content>
      <categories>
        <category>大数据</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark RDD算子（四）之创建键值对RDD mapToPair flatMapToPair]]></title>
    <url>%2Fhadoop%2Fspark%2Fspark-rdd-4.html</url>
    <content type="text"><![CDATA[mapToPair举例，在F:\sparktest\sample.txt 文件的内容如下1234aa bb cc aa aa aa dd dd ee ee ee ee ff aa bb zksee kksee zz zks 将每一行的第一个单词作为键，1 作为value创建pairRDDscala版本scala是没有mapToPair函数的，scala版本只需要map就可以了123456scala&gt; val lines = sc.textFile("F:\\sparktest\\sample.txt")scala&gt; val pairs = lines.map(x =&gt; (x.split("\\s+")(0), 1))scala&gt; pairs.collectres0: Array[(String, Int)] = Array((aa,1), (ff,1), (ee,1), (ee,1)) java版本12345678JavaRDD&lt;String&gt; lines = sc.textFile("F:\\sparktest\\sample.txt");//输入的是一个string的字符串，输出的是一个(String, Integer) 的mapJavaPairRDD&lt;String, Integer&gt; pairRDD = lines.mapToPair(new PairFunction&lt;String, String, Integer&gt;() &#123; @Override public Tuple2&lt;String, Integer&gt; call(String s) throws Exception &#123; return new Tuple2&lt;String, Integer&gt;(s.split("\\s+")[0], 1); &#125;&#125;); flatMapToPair类似于xxx连接 mapToPair是一对一，一个元素返回一个元素，而flatMapToPair可以一个元素返回多个，相当于先flatMap,在mapToPair例子: 将每一个单词都分成键为scala版本123456val lines = sc.textFile("F:\\sparktest\\sample.txt")val flatRDD = lines.flatMap(x =&gt; (x.split("\\s+")))val pairs = flatRDD.map(x=&gt;(x,1))scala&gt; pairs.collectres1: Array[(String, Int)] = Array((aa,1), (bb,1), (cc,1), (aa,1), (aa,1), (aa,1), (dd,1), (dd,1), (ee,1), (ee,1), (ee,1), (ee,1), (ff,1), (aa,1), (bb,1), (zks,1), (ee,1), (kks,1), (ee,1), (zz,1), (zks,1)) java版本 spark2.0以下12345678910111213141516171819202122232425262728JavaPairRDD&lt;String, Integer&gt; wordPairRDD = lines.flatMapToPair(new PairFlatMapFunction&lt;String, String, Integer&gt;() &#123; @Override public Iterable&lt;Tuple2&lt;String, Integer&gt;&gt; call(String s) throws Exception &#123; ArrayList&lt;Tuple2&lt;String, Integer&gt;&gt; tpLists = new ArrayList&lt;Tuple2&lt;String, Integer&gt;&gt;(); String[] split = s.split("\\s+"); for (int i = 0; i &lt;split.length ; i++) &#123; Tuple2 tp = new Tuple2&lt;String,Integer&gt;(split[i], 1); tpLists.add(tp); &#125; return tpLists; &#125; &#125;);``` **java版本 spark2.0以上**主要是iterator和iteratable的一些区别```java JavaPairRDD&lt;String, Integer&gt; wordPairRDD = lines.flatMapToPair(new PairFlatMapFunction&lt;String, String, Integer&gt;() &#123; @Override public Iterator&lt;Tuple2&lt;String, Integer&gt;&gt; call(String s) throws Exception &#123; ArrayList&lt;Tuple2&lt;String, Integer&gt;&gt; tpLists = new ArrayList&lt;Tuple2&lt;String, Integer&gt;&gt;(); String[] split = s.split("\\s+"); for (int i = 0; i &lt;split.length ; i++) &#123; Tuple2 tp = new Tuple2&lt;String,Integer&gt;(split[i], 1); tpLists.add(tp); &#125; return tpLists.iterator(); &#125; &#125;);]]></content>
      <categories>
        <category>大数据</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark RDD算子（三） distinct，union，intersection，subtract，cartesian]]></title>
    <url>%2Fhadoop%2Fspark%2Fspark-rdd-3.html</url>
    <content type="text"><![CDATA[spark伪集合尽管 RDD 本身不是严格意义上的集合，但它也支持许多数学上的集合操作，比如合并和相交操作, 下图展示了这四种操作 distinctdistinct用于去重， 我们生成的RDD可能有重复的元素，使用distinct方法可以去掉重复的元素, 不过此方法涉及到混洗，操作开销很大scala版本123456789101112131415161718192021222324 scala&gt; var RDD1 = sc.parallelize(List("aa","aa","bb","cc","dd")) scala&gt; RDD1.collect res3: Array[String] = Array(aa, aa, bb, cc, dd) scala&gt; var distinctRDD = RDD1.distinct scala&gt; distinctRDD.collect res5: Array[String] = Array(aa, dd, bb, cc)``` **java版本**```java JavaRDD&lt;String&gt; RDD1 = sc.parallelize(Arrays.asList("aa", "aa", "bb", "cc", "dd")); JavaRDD&lt;String&gt; distinctRDD = RDD1.distinct(); List&lt;String&gt; collect = distinctRDD.collect(); for (String str:collect) &#123; System.out.print(str+", "); &#125;---------输出----------aa, dd, bb, cc,``` # **union** 两个RDD进行合并**scala版本** scala&gt; var RDD1 = sc.parallelize(List(&quot;aa&quot;,&quot;aa&quot;,&quot;bb&quot;,&quot;cc&quot;,&quot;dd&quot;)) scala&gt; var RDD2 = sc.parallelize(List(&quot;aa&quot;,&quot;dd&quot;,&quot;ff&quot;)) scala&gt; RDD1.collect res6: Array[String] = Array(aa, aa, bb, cc, dd) scala&gt; RDD2.collect res7: Array[String] = Array(aa, dd, ff) scala&gt; RDD1.union(RDD2).collect res8: Array[String] = Array(aa, aa, bb, cc, dd, aa, dd, ff) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455**java版本**```java JavaRDD&lt;String&gt; RDD1 = sc.parallelize(Arrays.asList(&quot;aa&quot;, &quot;aa&quot;, &quot;bb&quot;, &quot;cc&quot;, &quot;dd&quot;)); JavaRDD&lt;String&gt; RDD2 = sc.parallelize(Arrays.asList(&quot;aa&quot;,&quot;dd&quot;,&quot;ff&quot;)); JavaRDD&lt;String&gt; unionRDD = RDD1.union(RDD2); List&lt;String&gt; collect = unionRDD.collect(); for (String str:collect) &#123; System.out.print(str+&quot;, &quot;); &#125;-----------输出---------aa, aa, bb, cc, dd, aa, dd, ff,``` # **intersection**RDD1.intersection(RDD2) 返回两个RDD的交集，并且去重 intersection 需要混洗数据，比较浪费性能 **scala版本** ```scala scala&gt; var RDD1 = sc.parallelize(List(&quot;aa&quot;,&quot;aa&quot;,&quot;bb&quot;,&quot;cc&quot;,&quot;dd&quot;)) scala&gt; var RDD2 = sc.parallelize(List(&quot;aa&quot;,&quot;dd&quot;,&quot;ff&quot;)) scala&gt; RDD1.collect res6: Array[String] = Array(aa, aa, bb, cc, dd) scala&gt; RDD2.collect res7: Array[String] = Array(aa, dd, ff) scala&gt; var insertsectionRDD = RDD1.intersection(RDD2) scala&gt; insertsectionRDD.collect res9: Array[String] = Array(aa, dd)``` **java版本** ```java JavaRDD&lt;String&gt; RDD1 = sc.parallelize(Arrays.asList(&quot;aa&quot;, &quot;aa&quot;, &quot;bb&quot;, &quot;cc&quot;, &quot;dd&quot;)); JavaRDD&lt;String&gt; RDD2 = sc.parallelize(Arrays.asList(&quot;aa&quot;,&quot;dd&quot;,&quot;ff&quot;)); JavaRDD&lt;String&gt; intersectionRDD = RDD1.intersection(RDD2); List&lt;String&gt; collect = intersectionRDD.collect(); for (String str:collect) &#123; System.out.print(str+&quot; &quot;); &#125;-------------输出-----------aa dd``` # **subtract**RDD1.subtract(RDD2),返回在RDD1中出现，但是不在RDD2中出现的元素，不去重 **scala版本**```scala JavaRDD&lt;String&gt; RDD1 = sc.parallelize(Arrays.asList(&quot;aa&quot;, &quot;aa&quot;,&quot;bb&quot;, &quot;cc&quot;, &quot;dd&quot;)); JavaRDD&lt;String&gt; RDD2 = sc.parallelize(Arrays.asList(&quot;aa&quot;,&quot;dd&quot;,&quot;ff&quot;)); scala&gt; var substractRDD =RDD1.subtract(RDD2) scala&gt; substractRDD.collect res10: Array[String] = Array(bb, cc) java版本 JavaRDD&lt;String&gt; RDD1 = sc.parallelize(Arrays.asList("aa", "aa", "bb","cc", "dd")); JavaRDD&lt;String&gt; RDD2 = sc.parallelize(Arrays.asList("aa","dd","ff")); JavaRDD&lt;String&gt; subtractRDD = RDD1.subtract(RDD2); List&lt;String&gt; collect = subtractRDD.collect(); for (String str:collect) { System.out.print(str+" "); } ------------输出----------------- bb cc cartesianRDD1.cartesian(RDD2) 返回RDD1和RDD2的笛卡儿积，这个开销非常大 scala版本 scala&gt; var RDD1 = sc.parallelize(List("1","2","3")) scala&gt; var RDD2 = sc.parallelize(List("a","b","c")) scala&gt; var cartesianRDD = RDD1.cartesian(RDD2) scala&gt; cartesianRDD.collect res11: Array[(String, String)] = Array((1,a), (1,b), (1,c), (2,a), (2,b), (2,c), (3,a), (3,b), (3,c)) java版本 JavaRDD&lt;String&gt; RDD1 = sc.parallelize(Arrays.asList("1", "2", "3")); JavaRDD&lt;String&gt; RDD2 = sc.parallelize(Arrays.asList("a","b","c")); JavaPairRDD&lt;String, String&gt; cartesian = RDD1.cartesian(RDD2); List&lt;Tuple2&lt;String, String&gt;&gt; collect1 = cartesian.collect(); for (Tuple2&lt;String, String&gt; tp:collect1) { System.out.println("("+tp._1+" "+tp._2+")"); } ------------输出----------------- (1 a) (1 b) (1 c) (2 a) (2 b) (2 c) (3 a) (3 b) (3 c)]]></content>
      <categories>
        <category>大数据</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark RDD算子（二） filter,map ,flatMap]]></title>
    <url>%2Fhadoop%2Fspark%2Fspark-rdd-2.html</url>
    <content type="text"><![CDATA[先来一张spark快速大数据中的图片进行快速入门，后面有更详细的例子 filter举例，在F:\sparktest\sample.txt 文件的内容如下123456789101112131415161718192021222324252627282930313233343536373839aa bb cc aa aa aa dd dd ee ee ee ee ff aa bb zksee kksee zz zks``` 我要将包含zks的行的内容给找出来**scala版本**```scala val lines = sc.textFile(&quot;F:\\sparktest\\sample.txt&quot;).filter(line=&gt;line.contains(&quot;zks&quot;)) //打印内容 lines.collect().foreach(println(_));-------------输出------------------ff aa bb zksee zz zks``` **java版本**```java JavaRDD&lt;String&gt; lines = sc.textFile(&quot;F:\\sparktest\\sample.txt&quot;); JavaRDD&lt;String&gt; zksRDD = lines.filter(new Function&lt;String, Boolean&gt;() &#123; @Override public Boolean call(String s) throws Exception &#123; return s.contains(&quot;zks&quot;); &#125; &#125;); //打印内容 List&lt;String&gt; zksCollect = zksRDD.collect(); for (String str:zksCollect) &#123; System.out.println(str); &#125;----------------输出-------------------ff aa bb zksee zz zks``` ## **map** map() 接收一个函数，把这个函数用于 RDD 中的每个元素，将函数的返回结果作为结果RDD编程 ｜ 31RDD 中对应元素的值 map是一对一的关系举例，在F:\sparktest\sample.txt 文件的内容如下 aa bb cc aa aa aa dd dd ee ee ee eeff aa bb zksee kksee zz zks12345678910111213141516171819202122232425262728把每一行变成一个数组**scala版本**```scala//读取数据scala&gt; val lines = sc.textFile(&quot;F:\\sparktest\\sample.txt&quot;)//用map，对于每一行数据，按照空格分割成一个一个数组，然后返回的是一对一的关系scala&gt; var mapRDD = lines.map(line =&gt; line.split(&quot;\\s+&quot;))---------------输出-----------res0: Array[Array[String]] = Array(Array(aa, bb, cc, aa, aa, aa, dd, dd, ee, ee, ee, ee), Array(ff, aa, bb, zks), Array(ee, kks), Array(ee, zz, zks))//读取第一个元素scala&gt; mapRDD.first---输出----res1: Array[String] = Array(aa, bb, cc, aa, aa, aa, dd, dd, ee, ee, ee, ee)``` **java版本**```java JavaRDD&lt;Iterable&lt;String&gt;&gt; mapRDD = lines.map(new Function&lt;String, Iterable&lt;String&gt;&gt;() &#123; @Override public Iterable&lt;String&gt; call(String s) throws Exception &#123; String[] split = s.split(&quot;\\s+&quot;); return Arrays.asList(split); &#125; &#125;); //读取第一个元素 System.out.println(mapRDD.first()); ---------------输出------------- [aa, bb, cc, aa, aa, aa, dd, dd, ee, ee, ee, ee] flatMap有时候，我们希望对某个元素生成多个元素，实现该功能的操作叫作 flatMap()faltMap的函数应用于每一个元素，对于每一个元素返回的是多个元素组成的迭代器(想要了解更多，请参考scala的flatMap和map用法)例如我们将数据切分为单词scala版本12345 scala&gt; val lines = sc.textFile("F:\\sparktest\\sample.txt") scala&gt; val flatMapRDD = lines.flatMap(line=&gt;line.split("\\s")) scala&gt; flatMapRDD.first() ---输出----res0: String = aa java版本，spark2.0以下123456789101112 JavaRDD&lt;String&gt; lines = sc.textFile("F:\\sparktest\\sample.txt"); JavaRDD&lt;String&gt; flatMapRDD = lines.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123; @Override public Iterable&lt;String&gt; call(String s) throws Exception &#123; String[] split = s.split("\\s+"); return Arrays.asList(split); &#125; &#125;); //输出第一个 System.out.println(flatMapRDD.first());------------输出----------aa java版本，spark2.0以上spark2.0以上，对flatMap的方法有所修改，就是flatMap中的Iterator和Iteratable的小区别1234567JavaRDD&lt;String&gt; flatMapRDD = lines.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123; @Override public Iterator&lt;String&gt; call(String s) throws Exception &#123; String[] split = s.split("\\s+"); return Arrays.asList(split).iterator(); &#125;&#125;);]]></content>
      <categories>
        <category>大数据</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark RDD算子（一） parallelize，makeRDD，textFile]]></title>
    <url>%2Fhadoop%2Fspark%2Fspark-rdd-1.html</url>
    <content type="text"><![CDATA[parallelize调用SparkContext 的 parallelize()，将一个存在的集合，变成一个RDD，这种方式试用于学习spark和做一些spark的测试scala版本def parallelizeT(implicit arg0: ClassTag[T]): RDD[T] 第一个参数一是一个 Seq集合 第二个参数是分区数 返回的是RDD[T]1234567891011121314151617181920scala&gt; sc.parallelize(List("shenzhen", "is a beautiful city"))res1: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[1] at parallelize at &lt;console&gt;:22``` &lt;!-- more --&gt;**java版本** def parallelize[T](list : java.util.List[T], numSlices : scala.Int) : org.apache.spark.api.java.JavaRDD[T] = &#123; /* compiled code */ &#125; - 第一个参数是一个List集合- 第二个参数是一个分区，可以默认- 返回的是一个JavaRDD[T]java版本只能接收List的集合```javaJavaRDD&lt;String&gt; javaStringRDD = sc.parallelize(Arrays.asList("shenzhen", "is a beautiful city"));``` ## makeRDD只有scala版本的才有makeRDD def makeRDD[T](seq : scala.Seq[T], numSlices : scala.Int = &#123; /* compiled code */ &#125;) 跟parallelize类似```scalasc.makeRDD(List("shenzhen", "is a beautiful city")) textFile调用SparkContext.textFile()方法，从外部存储中读取数据来创建 RDD例如在我本地F:\dataexample\wordcount\input下有个sample.txt文件，文件随便写了点内容，我需要将里面的内容读取出来创建RDDscala版本1var lines = sc.textFile("F:\\dataexample\\wordcount\\input") java版本12345 JavaRDD&lt;String&gt; lines = sc.textFile("F:\\dataexample\\wordcount\\input");``` 注: textFile支持分区，支持模式匹配，例如把F:\\dataexample\\wordcount\\目录下inp开头的给转换成RDD```scalavar lines = sc.textFile("F:\\dataexample\\wordcount\\inp*") 多个路径可以使用逗号分隔，例如1var lines = sc.textFile("dir1,dir2",3)]]></content>
      <categories>
        <category>大数据</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark lost task 异常笔记]]></title>
    <url>%2Fhadoop%2Fspark%2Fspark-exception.html</url>
    <content type="text"><![CDATA[Lost task java.lang.NullPointerExceptionException in thread “main” org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.NullPointerException我之前的代码类似于1234567891011121314151617public Iterable&lt;Tuple2&lt;String, String&gt;&gt; call(String s) throws Exception&#123; Tuple2&lt;String, String&gt; tp=null; try&#123; if(XXX)&#123; tp=new Tuple2&lt;&gt;(xxx, xxx); &#125; if(xxx)&#123; tp=new Tuple2&lt;&gt;(xxx, xxx); &#125; return Arrays.asList(tp); &#125; catch(Exception e)&#123; e.printStackTrace(); return new ArrayList&lt;Tuple2&lt;String, String&gt;&gt;(); &#125;&#125; 发现没有进入catch，那应该没问题啊，最后才发现，原来是tp对于两个if都不满足，return Arrays.asList(tp);中，tp为null，导致程序出错,更加要注意的是，对于容错，如果某条数据不要，千万不能返回null,可以返回类似于new ArrayList]]></content>
      <categories>
        <category>大数据</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark 从1.x 转到2.x，编写程序的的一些区别]]></title>
    <url>%2Fhadoop%2Fspark%2Fspark-v1-v2.html</url>
    <content type="text"><![CDATA[spark 2.x 版本相对于1.x版本，有挺多地方的修改，一是类似于flatMapRDD 中 iteator iteatable之类的区别2是类似于dataset的一些问题 下面是2.x版本的iteatable和iteartor之类的区别，只举例了两个，其实只要和iteartor有关的都有了修改 flatMap1234567JavaRDD&lt;String&gt; flatMapRDD = lines.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123; @Override public Iterator&lt;String&gt; call(String s) throws Exception &#123; String[] split = s.split("\\s+"); return Arrays.asList(split).iterator(); &#125;&#125;); flatMapToPair javaJavaPairRDD&lt;String, Integer&gt; wordPairRDD = lines.flatMapToPair(new PairFlatMapFunction&lt;String, String, Integer&gt;() { @Override public Iterator&lt;Tuple2&lt;String, Integer&gt;&gt; call(String s) throws Exception { ArrayList&lt;Tuple2&lt;String, Integer&gt;&gt; tpLists = new ArrayList&lt;Tuple2&lt;String, Integer&gt;&gt;(); String[] split = s.split("\\s+"); for (int i = 0; i &lt;split.length ; i++) { Tuple2 tp = new Tuple2&lt;String,Integer&gt;(split[i], 1); tpLists.add(tp); } return tpLists.iterator(); } }); spark中初始化driver的区别spark2.0中，可以使用session来创建一个sparkContext作为一个新的入口，具体参考例子就可以了 jar包的区别spark2.x版本中不再有spark-assembly-xxx jar包，jar包全都在.jars 中 scala的版本spark2.x版本的，对scala的版本最低要求是2.11 下面是sql中的区别2.x 版本的 sparkSql中1.x 版本的 DataFrame与Dataset 统一化了，只剩下DataSet了，具体的也可以直接参看官方给的spark sql 的例子即可具体 todo]]></content>
      <categories>
        <category>大数据</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windows搭建spark运行环境(windows scala,hadoop,spark安装,idea使用配置等)]]></title>
    <url>%2Fhadoop%2Fspark%2Fspark-windows-install.html</url>
    <content type="text"><![CDATA[关键字: spark windows安装，spark运行环境，idea普通模式构建spark程序，idea maven构建spark程序，idea运行wordcount 安装scala 下载地址 http://www.scala-lang.org/download/ 下载scala-xxx.msi ，然后直接安装好就行了（注意，默认的位置最好别有空格，对于windows10我直接安装没有出现问题，但是对于windows7却出现 此时不应有XXX，这种情况就是安装目录有空格），一般他会默认帮我们配置好环境，当在命令行窗口输出scala -version能出现版本，和输入scala能出现scala的编辑的时候，说明成功了 安装spark直接去官网，下载spark-1.4.0-bin-hadoop2.6.tgz，下载地址，解压后建议改名为spark，然后配置环境变量，把解压后的bin目录，写在path里面，然后尝试运行spark-shell, 一般可以成功运行，但是spark还依赖于hadoop。所以还需要安装hadoop 安装hadoop直接去官网下载，我下的是2.6，我之前测试2.7版本的其实也兼容2.6版本的，为了保险起见，下载和spark对应版本的hadoop，下载地址http://mirrors.hust.edu.cn/apache/hadoop/common/下载好后解压，然后把bin目录写在path环境里面，这时候，为了防止运行程序的时候出现nullpoint异常，我们需要去github https://github.com/steveloughran/winutils 找到对应的hadoop版本，然后进入bin目录下，下载hadoop.dll和winutils.exe, 然后复制到所安装hadoop目录下 安装idea进入官网 http://www.jetbrains.com/idea/download/#section=windows 下载右边的Community版本的idea, 然后默认一直安装，安装好后，如果是运行scala项目，需要安装scala插件，在File-&gt;Settings&gt;Pligins下，点击Install JeBrains plugins, 找到scala，注意要看其版本和自己的idea要对应（一般都是对应的），然后点击install, 需要等待一段时间，然后安装好重启idea就行了 运行wordcountFile-&gt;New-&gt;Project-&gt;scala-&gt;IDEA 选择next，注意，这里jdk和scala版本一定要选择，如果jdk没有就手动指定jdk安装目录，scala没有就点击create，然后会出现版本进行选择就行了，没有版本就点击左下角download，一般第一次需要点击坐下家download 创建一个wordcount, 其中我们需要依赖于spark的spark-assembly-1.4.0-hadoop2.6.0.jar包， idea点击File-&gt;Project Structure-&gt;Module, 选择右边的Dependencies,点击右边的+号， 选择jars or direc… 引入spark-assembly-1.4.0-hadoop2.6.0.jar包即可12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091/* * Licensed to the Apache Software Foundation (ASF) under one or more * contributor license agreements. See the NOTICE file distributed with * this work for additional information regarding copyright ownership. * The ASF licenses this file to You under the Apache License, Version 2.0 * (the "License"); you may not use this file except in compliance with * the License. You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed---------- on an "AS IS" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */package cn.kaishun.spark;import scala.Tuple2;import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaPairRDD;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.api.java.JavaSparkContext;import org.apache.spark.api.java.function.FlatMapFunction;import org.apache.spark.api.java.function.Function2;import org.apache.spark.api.java.function.PairFunction;import java.util.Arrays;import java.util.List;import java.util.regex.Pattern;public final class JavaWordCount &#123; private static final Pattern SPACE = Pattern.compile(" "); public static void main(String[] args) throws Exception &#123; if (args.length &lt; 1) &#123; System.err.println("Usage: JavaWordCount &lt;file&gt;"); System.exit(1); &#125; SparkConf sparkConf = new SparkConf().setAppName("JavaWordCount").setMaster("local"); JavaSparkContext ctx = new JavaSparkContext(sparkConf); JavaRDD&lt;String&gt; lines = ctx.textFile(args[0], 1); JavaRDD&lt;String&gt; words = lines.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123; @Override public Iterable&lt;String&gt; call(String s) &#123; return Arrays.asList(SPACE.split(s)); &#125; &#125;); JavaPairRDD&lt;String, Integer&gt; ones = words.mapToPair(new PairFunction&lt;String, String, Integer&gt;() &#123; @Override public Tuple2&lt;String, Integer&gt; call(String s) &#123; return new Tuple2&lt;String, Integer&gt;(s, 1); &#125; &#125;); JavaPairRDD&lt;String, Integer&gt; counts = ones.reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() &#123; @Override public Integer call(Integer i1, Integer i2) &#123; return i1 + i2; &#125; &#125;); counts.saveAsTextFile("G:\\ceshi\\bigdata\\spark\\wordcount\\output\\out1"); List&lt;Tuple2&lt;String, Integer&gt;&gt; output = counts.collect(); for (Tuple2&lt;?,?&gt; tuple : output) &#123; System.out.println(tuple._1() + ": " + tuple._2()); &#125; ctx.stop(); &#125;&#125;``` 创建测试文件，在args进行配置输入和输出路径，点击运行，即可 若有空指针问题，大多情况是hadoop下的winutils.exe的问题，github找一份复制就可以了(注： 我记得之前如果第一次出问题了，以后怎么解决都解决不了，一直报空指针，后来我复制winutils.exe和hadoop.dll,然后重启电脑才解决) ## idea Maven 构建spark程序大多情况下，我不想按照上面的方式，各种jar包的导入，我更多的是想用maven来构建spark程序。### maven下载速度慢的解决办法在maven安装目录的conf目录下，修改settings.xml文件, 添加阿里云的加速即可，参考我的setting &lt;?xml version=”1.0” encoding=”UTF-8”?&gt; central Central Repository http://maven.aliyun.com/nexus/content/repositories/central default false nexus-aliyun * Nexus aliyun http://maven.aliyun.com/nexus/content/groups/public 12pom中增加 &lt;repository&gt; &lt;id&gt;central&lt;/id&gt; &lt;name&gt;Central Repository&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/repositories/central&lt;/url&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132### **spark中的maven配置** 这是我的本身配置，有的不必要可以删除，其中的groupId，artifactId 根据自己的设置而设置**spark1.6的配置**```xml&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;cn.kaishun&lt;/groupId&gt; &lt;artifactId&gt;mvntest&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;spark.version&gt;1.6.0&lt;/spark.version&gt; &lt;scala.version&gt;2.10&lt;/scala.version&gt; &lt;hadoop.version&gt;2.6.0&lt;/hadoop.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_$&#123;scala.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_$&#123;scala.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-hive_$&#123;scala.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_$&#123;scala.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming-kafka_$&#123;scala.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-mllib_$&#123;scala.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.39&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.6.6&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;1.6.6&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.16&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;!-- maven官方 http://repo1.maven.org/maven2/ 或 http://repo2.maven.org/maven2/ （延迟低一些） --&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;central&lt;/id&gt; &lt;name&gt;Central Repository&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/repositories/central&lt;/url&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;central&lt;/id&gt; &lt;name&gt;Maven Repository Switchboard&lt;/name&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;url&gt;http://repo2.maven.org/maven2&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;build&gt; &lt;sourceDirectory&gt;src/main/scala&lt;/sourceDirectory&gt; &lt;testSourceDirectory&gt;src/test/scala&lt;/testSourceDirectory&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;!-- MAVEN 编译使用的JDK版本 --&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.3&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.7&lt;/source&gt; &lt;target&gt;1.7&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt;``` **spark2.10** &lt;?xml version=”1.0” encoding=”UTF-8”?&gt; 4.0.0 &lt;groupId&gt;sparkmaven&lt;/groupId&gt; &lt;artifactId&gt;com.shuner.mvn&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;spark.version&gt;2.11&lt;/spark.version&gt; &lt;scala.version&gt;2.11.8&lt;/scala.version&gt; &lt;hadoop.version&gt;2.6.0&lt;/hadoop.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.10&lt;/artifactId&gt; &lt;version&gt;2.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.10&lt;/artifactId&gt; &lt;version&gt;2.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-hive_2.10&lt;/artifactId&gt; &lt;version&gt;2.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.10&lt;/artifactId&gt; &lt;version&gt;2.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.6.6&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;1.6.6&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.16&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;!-- maven官方 http://repo1.maven.org/maven2/ 或 http://repo2.maven.org/maven2/ （延迟低一些） --&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;central&lt;/id&gt; &lt;name&gt;Central Repository&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/repositories/central&lt;/url&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;build&gt; &lt;sourceDirectory&gt;src/main/scala&lt;/sourceDirectory&gt; &lt;testSourceDirectory&gt;src/test/scala&lt;/testSourceDirectory&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;!-- MAVEN 编译使用的JDK版本 --&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.3&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.7&lt;/source&gt; &lt;target&gt;1.7&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 123最后补充，来一个既可以玩spark，又可以玩hadoop的pom.xml &lt;?xml version=”1.0” encoding=”UTF-8”?&gt; 4.0.0 &lt;groupId&gt;cn.mingtong&lt;/groupId&gt; &lt;artifactId&gt;spark16test&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;spark.version&gt;1.6.0&lt;/spark.version&gt; &lt;scala.version&gt;2.10&lt;/scala.version&gt; &lt;hadoop.version&gt;2.7.3&lt;/hadoop.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_${scala.version}&lt;/artifactId&gt; &lt;version&gt;${spark.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_${scala.version}&lt;/artifactId&gt; &lt;version&gt;${spark.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-hive_${scala.version}&lt;/artifactId&gt; &lt;version&gt;${spark.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_${scala.version}&lt;/artifactId&gt; &lt;version&gt;${spark.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.39&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.6.6&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;1.6.6&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.16&lt;/version&gt; &lt;/dependency&gt; &lt;!-- 配置hadoop的环境 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;${hadoop.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;${hadoop.version}&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;${hadoop.version}&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;!-- maven官方 http://repo1.maven.org/maven2/ 或 http://repo2.maven.org/maven2/ （延迟低一些） --&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;central&lt;/id&gt; &lt;name&gt;Central Repository&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/repositories/central&lt;/url&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;build&gt; &lt;sourceDirectory&gt;src/main/scala&lt;/sourceDirectory&gt; &lt;testSourceDirectory&gt;src/test/scala&lt;/testSourceDirectory&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;!-- MAVEN 编译使用的JDK版本 --&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.3&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; ```]]></content>
      <categories>
        <category>大数据</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark集群环境的搭建]]></title>
    <url>%2Fhadoop%2Fspark%2Fspark-cluster-install.html</url>
    <content type="text"><![CDATA[前提：已经安装好了hadoop, 说明，这次安装的是spark-1.4.0-bin-hadoop2.6.tgz版本，这个版本是兼容hadoop2.7的hdfs和yarn的 软件准备:scala-2.10.4 ,下载地址spark-1.4.0-bin-hadoop2.6.tgz 下载地址 1. 安装scala-2.10.4将下载好的scala-2.11.4.tgz复制到/usr/lib 1.1 解压安装包1234567891011sudo tar -zxf scala-2.11.4.tgz``` ### 1.2 设置scala环境变量```shellsudo vim /etc/profile``` 打开之后在末尾添加```shell#scaleexport SCALA_HOME=/usr/lib/scala-2.10.4export PATH=$SCALA_HOME/bin:$PATH 使配置生效1source /etc/profile 1.3 检查是否安装成功1scala -version 出现Scala code runner version 2.10.4 – Copyright 2002-2013, LAMP/EPFL说明安装成功 2. 安装spark-1.4.0-bin-hadoop2.6将下载的 spark-1.4.0-bin-hadoop2.6.tar.gz 复制到/usr目录 2.1 解压并且改名12sudo tar zxvf spark-1.4.0-bin-hadoop2.6.tgzsudo mv spark-1.4.0-bin-hadoop2.6/ spark-1.4.0-hadoop2.6 2.2 配置环境123456789sudo vi /etc/profile``` 打开之后在末尾添加```shell#sparkexport SPARK_HOME=/usr/spark-1.4.0-hadoop2.6export PATH=$SPARK_HOME/bin:$PATH``` 使资源生效 source /etc/profile123456### 2.3 配置Spark环境#### 配置 spark-env.sh文件进入到 spark的conf目录, 拷贝一份spark-env.sh.template 并且改名为 spark-env.sh```shellcd $SPARK_HOME/confsudo cp spark-env.sh.template spark-env.sh sudo vi spark-env.sh 添加以下内容：12345678910111213141516171819202122232425export JAVA_HOME=/usr/local/java/jdk1.7.0_79export HADOOP_HOME=/home/hadoop/MyCloudera/APP/hadoop/hadoopexport HADOOP_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoopexport SCALA_HOME==/usr/lib/scala-2.10.4export SPARK_HOME=/usr/spark-1.4.0-hadoop2.6export SPARK_MASTER_IP=masterexport SPARK_MASTER_PORT=7077export SPARK_MASTER_WEBUI_PORT=8099#每个Worker使用的CPU核数export SPARK_WORKER_CORES=1#每个Slave中启动几个Worker实例export SPARK_WORKER_INSTANCES=1#每个Worker使用多大的内存export SPARK_WORKER_MEMORY=2G#Worker的WebUI端口号export SPARK_WORKER_WEBUI_PORT=8081#每个Executor使用使用的核数export SPARK_EXECUTOR_CORES=1#每个Executor使用的内存export SPARK_EXECUTOR_MEMORY=1Gexport SPARK_CLASSPATH=$SPARK_HOME/conf/:$SPARK_HOME/lib/*export SPARK_CLASSPATH=$SPARK_CLASSPATH:$CLASSPATHexport LD_LIBRARY_PATH=$&#123;LD_LIBRARY_PATH&#125;:$HADOOP_HOME/lib/nativ 配置Slave文件拷贝一份slaves.template并且改名为slaves1234567891011sudo cp slaves.template slaves``` sudo vim slaves，然后根据自己对节点的设置，按照下面类似的方法添加内容```shellmasternode01node02``` ### 2.4 把配置好的，所有文件，复制一份到node节点的服务器上```shell建议先打包，然后使用scp传到所有的node节点，然后在所有的node的节点的/usr/目录下解压 2.5 启动Spark并且测试启动Spark(前提是先启动hadoop，以后会用到其hdfs的系统) ,先启动master,再启动slave， 操作全部在主节点进行, 或者直接全部启动如果启动失败，根据提示进行处理123456#方法一:cd $SPARK_HOME/sbin/./start-master.sh./start-slaves.sh#方法二./start-all.sh 启动 master机器jps有以下显示,相对于hadoop, 多了一个master,一个worker 1234567814930 ResourceManager16437 Jps15033 NodeManager16324 Worker14510 DataNode16166 Master14753 SecondaryNameNode14409 NameNode slave机器jps有如下显示，多了一个worker12342610 NodeManager2816 Worker2888 Jps2466 DataNode 使用spark shell进行测试在hadoop用户下，进入spark 的bin目录，打开spark-shell命令编程12345cd $SPARK_HOME/binspark-shell 打开后可以看到一个大大的Spark的欢迎图和版本，说明启动成功``` 开始测试 val text= sc.parallelize(Seq(“aa”,”aa”,”bb”,”cc”)) #回车可以看到 text: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at parallelize at :211我们看一看其第一个元素 text.take(1) #回车后最后出现一堆info信息，最后一行出现 res0: Array[String] = Array(aa)1我们统计其元组数量 text.count #回车后出现 res1: Long = 41过滤，统计不含aa的元组数量 注意这条命令的 &gt; 和！之间要有一个空格text.filter(r =&gt; !r.contains(“aa”)).count #回车后出现res3: Long = 2``` 如果想自己写相应的spark测试，可以参考这篇文章，这篇文章使用的是idea的编译器，eclipse的我没用过，估计除了打包这里不一样，其他都是一样的Spark Idea搭建实战 官网给了很多例子，均在 spark-1.4.0-bin-hadoop2.7\examples\src\main 文件夹下，可做参考]]></content>
      <categories>
        <category>大数据</category>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop入门案例（八）之 表 关联]]></title>
    <url>%2Fhadoop%2Fhadoop%2Fhadoop-example-8.html</url>
    <content type="text"><![CDATA[表关联:两个表关联，代码和以后再贴一起在github上贴出来，mapreduce表关联思路很简单。主要如下 思路mapper阶段:(key，value) value中加一个字符，用来区别多个表reduce阶段:reduce时，new 多个数组，例如我们是两个表关联，通过value中的字符，用来判断哪个是哪个表，分别放在不同的数组中，然后数组中进行笛卡尔乘积即可如果是单表关联：找出单表中哪些字段需要关联，理清楚思路，和sql中的单表join一样，和多表关联相同的思路 优化的地方看具体需求如何，如果是大小表，最好不要用这种方法，因为会经过shuffle，导致大量资源浪费。可以在map段就做关联，而不经过reduce。类似于spark中，用flatmap代替join的方法下面是网上常见的方法，我觉得自己的笔记，用这个代码就挺好的了，如果是多表关联，这网上关于爷孙的关联，也是很好的入门 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899public class LeftJoin extends Configured implements Tool&#123; public static final String DELIMITER = &quot;,&quot;; public static class LeftJoinMapper extends Mapper&lt;LongWritable,Text,Text,Text&gt; &#123; protected void map(LongWritable key, Text value, Context context) throws IOException,InterruptedException &#123; /* * 拿到两个不同文件，区分出到底是哪个文件，然后分别输出 */ String filepath = ((FileSplit)context.getInputSplit()).getPath().toString(); String line = value.toString(); if (line == null || line.equals(&quot;&quot;)) return; if (filepath.indexOf(&quot;employee&quot;) != -1) &#123; String[] lines = line.split(DELIMITER); if(lines.length &lt; 2) return; String company_id = lines[0]; String employee = lines[1]; context.write(new Text(company_id),new Text(&quot;a:&quot;+employee)); &#125; else if(filepath.indexOf(&quot;salary&quot;) != -1) &#123; String[] lines = line.split(DELIMITER); if(lines.length &lt; 2) return; String company_id = lines[0]; String salary = lines[1]; context.write(new Text(company_id), new Text(&quot;b:&quot; + salary)); &#125; &#125; &#125; public static class LeftJoinReduce extends Reducer&lt;Text, Text, Text, Text&gt; &#123; protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException&#123; Vector&lt;String&gt; vecA = new Vector&lt;String&gt;(); Vector&lt;String&gt; vecB = new Vector&lt;String&gt;(); for(Text each_val:values) &#123; String each = each_val.toString(); if(each.startsWith(&quot;a:&quot;)) &#123; vecA.add(each.substring(2)); &#125; else if(each.startsWith(&quot;b:&quot;)) &#123; vecB.add(each.substring(2)); &#125; &#125; for (int i = 0; i &lt; vecA.size(); i++) &#123; /* * 如果vecB为空的话，将A里的输出，B的位置补null。 */ if (vecB.size() == 0) &#123; context.write(key, new Text(vecA.get(i) + DELIMITER + &quot;null&quot;)); &#125; else &#123; for (int j = 0; j &lt; vecB.size(); j++) &#123; context.write(key, new Text(vecA.get(i) + DELIMITER + vecB.get(j))); &#125; &#125; &#125; &#125; &#125; public int run(String[] args) throws Exception &#123; Configuration conf = getConf(); GenericOptionsParser optionparser = new GenericOptionsParser(conf, args); conf = optionparser.getConfiguration(); Job job = new Job(conf,&quot;leftjoin&quot;); job.setJarByClass(LeftJoin.class); FileInputFormat.addInputPaths(job, conf.get(&quot;input_dir&quot;)); Path out = new Path(conf.get(&quot;output_dir&quot;)); FileOutputFormat.setOutputPath(job, out); job.setNumReduceTasks(conf.getInt(&quot;reduce_num&quot;,2)); job.setMapperClass(LeftJoinMapper.class); job.setReducerClass(LeftJoinReduce.class); job.setInputFormatClass(TextInputFormat.class); job.setOutputFormatClass(TextOutputFormat.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(Text.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); conf.set(&quot;mapred.textoutputformat.separator&quot;, &quot;,&quot;); return (job.waitForCompletion(true) ? 0 : 1); &#125; public static void main(String[] args) throws Exception&#123; int res = ToolRunner.run(new Configuration(),new LeftJoin(),args); System.exit(res); &#125;&#125;]]></content>
      <categories>
        <category>大数据</category>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop入门案例（七）之TOP K]]></title>
    <url>%2Fhadoop%2Fhadoop%2Fhadoop-example-7.html</url>
    <content type="text"><![CDATA[一. 目的：找出数据集中的top k， 二. 思路2.1 全排序，取前 k 个 最开始是快速排序或者归并排序 其次就是wordcount，然后再进行一遍mapReduce 先排序，再取前k个 2.2 在mapper阶段，找出本地的top k, 然后所有的独立的top k集合在reduce中运算因为k 一般比较小，所以我们只需要一个reduce来处理最后的运算 2.3 流程图如下 三. 代码代码中利用了 treemap 来获取前k个，Treemap 参考 http://blog.csdn.net/chenssy/article/details/26668941其实也可以用其他的一些结构，例如参考这篇文章， 使用的是数组 https://my.oschina.net/u/1378204/blog/343666 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899package com.myhadoop.mapreduce.test;import java.io.IOException;import java.util.StringTokenizer;import java.util.TreeMap;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class TopN &#123; public static class TopTenMapper extends Mapper&lt;Object, Text, NullWritable, IntWritable&gt; &#123; private TreeMap&lt;Integer, String&gt; repToRecordMap = new TreeMap&lt;Integer, String&gt;(); public void map(Object key, Text value, Context context) &#123; int N = 10; //默认为Top10 N = Integer.parseInt(context.getConfiguration().get("N")); StringTokenizer itr = new StringTokenizer(value.toString()); while (itr.hasMoreTokens()) &#123; repToRecordMap.put(Integer.parseInt(itr.nextToken()), " "); if (repToRecordMap.size() &gt; N) &#123; repToRecordMap.remove(repToRecordMap.firstKey()); &#125; &#125; &#125; protected void cleanup(Context context) &#123; for (Integer i : repToRecordMap.keySet()) &#123; try &#123; context.write(NullWritable.get(), new IntWritable(i)); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; public static class TopTenReducer extends Reducer&lt;NullWritable, IntWritable, NullWritable, IntWritable&gt; &#123; private TreeMap&lt;Integer, String&gt; repToRecordMap = new TreeMap&lt;Integer, String&gt;(); public void reduce(NullWritable key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int N = 10; //默认为Top10 N = Integer.parseInt(context.getConfiguration().get("N")); for (IntWritable value : values) &#123; repToRecordMap.put(value.get(), " "); if (repToRecordMap.size() &gt; N) &#123; repToRecordMap.remove(repToRecordMap.firstKey()); &#125; &#125; for (Integer i : repToRecordMap.descendingMap().keySet()) &#123; context.write(NullWritable.get(), new IntWritable(i)); &#125; &#125; &#125; public static void main(String[] args) throws Exception &#123; if (args.length != 3) &#123; throw new IllegalArgumentException( "!!!!!!!!!!!!!! Usage!!!!!!!!!!!!!!: hadoop jar &lt;jar-name&gt; " + "TopN.TopN " + "&lt;the value of N&gt;" + "&lt;input-path&gt; " + "&lt;output-path&gt;"); &#125; Configuration conf = new Configuration(); conf.set("N", args[0]); Job job = Job.getInstance(conf, "TopN"); job.setJobName("TopN"); Path inputPath = new Path(args[1]); Path outputPath = new Path(args[2]); FileInputFormat.setInputPaths(job, inputPath); FileOutputFormat.setOutputPath(job, outputPath); job.setJarByClass(TopN.class); job.setMapperClass(TopTenMapper.class); job.setReducerClass(TopTenReducer.class); job.setNumReduceTasks(1); //reduce Num 设置成1 job.setMapOutputKeyClass(NullWritable.class);// map阶段的输出的key job.setMapOutputValueClass(IntWritable.class);// map阶段的输出的value job.setOutputKeyClass(NullWritable.class);// reduce阶段的输出的key job.setOutputValueClass(IntWritable.class);// reduce阶段的输出的value System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125; 四. 结果上述代码经过测试，能返回Top k条记录 五. 性能分析上述代码使用的是第二种思路，避免了第一种思路的全排序，但是注意到，我们只能用一个reduce，如果数据量特别大，k也非常大，单一的reduce可能会出现一些问题 这个reducer的主机需要通过网络获取大量数据，会造成单一节点工作负荷太大。 reducer的内存中可能会出现java虚拟机内存不足 写文件不是并行的，当数据规模很大的时候，这种思路会导致效率变得很低 参考自 mappreduce 设计模式]]></content>
      <categories>
        <category>大数据</category>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop入门案例（六）之二次排序，全排序基础下的二次排序]]></title>
    <url>%2Fhadoop%2Fhadoop%2Fhadoop-example-6.html</url>
    <content type="text"><![CDATA[前言：对于二次排序，定义是在一个字段排好顺序的前提下，另外一个字段也进行排序。类似于sql中的order by多个字段，然而，网上大多数二次排序，对于partitioner都没有利用，因为网上的partition竟然都是是按照hash分组的，而且也没设置reduceTaskNum，这不能充分利用集群的资源，只有一个reduce，，即使如果有多个reduce，partition按照hash分区也不属于全排序，准确的说属于分组排序。 我对网上的代码做了稍加修改，可以满足全排序的情况下，再进行二次排序。测试数和代码都参考自网上，具体哪个不记得了，反正网上都差不多测试数据12345678910111213141516171819202122232425262728293031323334353637383940414220 21 50 51 50 52 50 53 50 54 60 51 60 53 60 52 60 56 60 57 70 58 60 61 70 54 70 55 70 56 70 57 70 58 1 2 3 4 5 6 7 82 203 21 50 512 50 522 50 53 530 54 40 511 20 53 20 522 60 56 60 57 740 58 63 61 730 54 71 55 71 56 73 57 74 58 12 211 31 42 50 62 7 8 代码如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239package com.myhadoop.mapreduce.test;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.*;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Partitioner;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import java.util.StringTokenizer;/** * Created by Administrator on 2017/6/14. */public class SecondSort &#123; public static class IntPair implements WritableComparable&lt;IntPair&gt; &#123; int first; int second; /** * Set the left and right values. */ public void set(int left, int right) &#123; first = left; second = right; &#125; public int getFirst() &#123; return first; &#125; public int getSecond() &#123; return second; &#125; @Override //反序列化，从流中的二进制转换成IntPair public void readFields(DataInput in) throws IOException &#123; // TODO Auto-generated method stub first = in.readInt(); second = in.readInt(); &#125; @Override //序列化，将IntPair转化成使用流传送的二进制 public void write(DataOutput out) throws IOException &#123; // TODO Auto-generated method stub out.writeInt(first); out.writeInt(second); &#125; @Override //key的比较 public int compareTo(IntPair o) &#123; // TODO Auto-generated method stub if (first != o.first) &#123; return first &lt; o.first ? -1 : 1; &#125; else if (second != o.second) &#123; return second &lt; o.second ? -1 : 1; &#125; else &#123; return 0; &#125; &#125; //新定义类应该重写的两个方法 @Override //The hashCode() method is used by the HashPartitioner (the default partitioner in MapReduce) public int hashCode() &#123; return first * 157 + second; &#125; @Override public boolean equals(Object right) &#123; if (right == null) return false; if (this == right) return true; if (right instanceof IntPair) &#123; IntPair r = (IntPair) right; return r.first == first &amp;&amp; r.second == second; &#125; else &#123; return false; &#125; &#125; &#125; /** * 分区函数类。根据first确定Partition。 */ public static class FirstPartitioner extends Partitioner&lt;IntPair, IntWritable&gt; &#123; @Override public int getPartition(IntPair key, IntWritable value,int numPartitions) &#123; int first = key.getFirst(); if(first&lt;60)&#123; return 0; &#125;else if(first&lt;80)&#123; return 1; &#125; return 2; &#125; &#125; /** * 分组函数类。只要first相同就属于同一个组。 */ public static class GroupingComparator extends WritableComparator &#123; protected GroupingComparator() &#123; super(IntPair.class, true); &#125; @Override //Compare two WritableComparables. public int compare(WritableComparable w1, WritableComparable w2) &#123; IntPair ip1 = (IntPair) w1; IntPair ip2 = (IntPair) w2; int l = ip1.getFirst(); int r = ip2.getFirst(); return l == r ? 0 : (l &lt; r ? -1 : 1); &#125; &#125; // 自定义map public static class Map extends Mapper&lt;LongWritable, Text, IntPair, IntWritable&gt; &#123; private final IntPair intkey = new IntPair(); private final IntWritable intvalue = new IntWritable(); public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String line = value.toString(); StringTokenizer tokenizer = new StringTokenizer(line); int left = 0; int right = 0; if (tokenizer.hasMoreTokens()) &#123; left = Integer.parseInt(tokenizer.nextToken()); if (tokenizer.hasMoreTokens()) right = Integer.parseInt(tokenizer.nextToken()); intkey.set(left, right); intvalue.set(right); context.write(intkey, intvalue); &#125; &#125; &#125; // 自定义reduce // public static class Reduce extends Reducer&lt;IntPair, IntWritable, Text, IntWritable&gt; &#123; private final Text left = new Text(); private static final Text SEPARATOR = new Text("------------------------------------------------"); public void reduce(IntPair key, Iterable&lt;IntWritable&gt; values,Context context) throws IOException, InterruptedException &#123; context.write(SEPARATOR, null); left.set(Integer.toString(key.getFirst())); for (IntWritable val : values) &#123; context.write(left, val); &#125; &#125; &#125; /** * @param args */ public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException &#123; // TODO Auto-generated method stub // 读取hadoop配置 Configuration conf = new Configuration(); // 实例化一道作业 Job job = new Job(conf, "secondarysort"); job.setJarByClass(SecondSort.class); // Mapper类型 job.setMapperClass(Map.class); // 不再需要Combiner类型，因为Combiner的输出类型&lt;Text, IntWritable&gt;对Reduce的输入类型&lt;IntPair, IntWritable&gt;不适用 //job.setCombinerClass(Reduce.class); // Reducer类型 job.setReducerClass(Reduce.class); //设置NumReduceTasks 的数量为三 job.setNumReduceTasks(3); // 分区函数 job.setPartitionerClass(FirstPartitioner.class); // 分组函数 job.setGroupingComparatorClass(GroupingComparator.class); // map 输出Key的类型 job.setMapOutputKeyClass(IntPair.class); // map输出Value的类型 job.setMapOutputValueClass(IntWritable.class); // rduce输出Key的类型，是Text，因为使用的OutputFormatClass是TextOutputFormat job.setOutputKeyClass(Text.class); // rduce输出Value的类型 job.setOutputValueClass(IntWritable.class); // 将输入的数据集分割成小数据块splites，同时提供一个RecordReder的实现。 job.setInputFormatClass(TextInputFormat.class); // 提供一个RecordWriter的实现，负责数据输出。 job.setOutputFormatClass(TextOutputFormat.class); // 输入hdfs路径 FileInputFormat.setInputPaths(job, new Path(args[0])); // 输出hdfs路径 FileOutputFormat.setOutputPath(job, new Path(args[1])); // 提交job System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125;``` ## 运行结果生成三个文件 partition-00000，partition-00001，partition-00002 每个文件第一个字段都按照进行了全排序，满足上一个条件下对第二个字段进行了排序，并且按照第一个字段进行了分组 输出的文件分别为 **partition-00000** 1 2 3 4 5 6 7 8 7 82 12 211 20 2120 53 20 522 31 42 40 511 50 5150 5250 5350 5350 5450 6250 51250 5221**partition-00001** 60 5160 5260 5360 5660 5660 5760 57 60 61 63 61 70 5470 5570 5670 5770 58 70 58 71 55 71 56 73 57 74 581**partion-00003** 203 21 530 54 730 54 740 58``` 分析：网上说的已经够清楚了，我这代码主要也是参考于网上的，主要有三步 自定义一个对象，需要实现WritableComparable接口，这个对象包含要排序的字段，并且内部有一些方法需要重写，具体参考我的代码 自定义分区，网上的hash分区是有小问题的，网上的要么第一个字段不属于全排序，要么只有一个reduce，我们这里参考全排序，自己定义分区，保证整体有序 自定义分组（没要求就不需要了），比如只按照第一个字段分组，参考代码中 正常的map，reduce，map中的key使用我们自定义的对象 感悟：主要代码还是网上的，但是做程序开发还是要有自己的独立思考，网上的并不是真正的二次排序，或者说即使满足二次排序，但是也是只有一个reduce任务]]></content>
      <categories>
        <category>大数据</category>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop入门案例（五）全排序之TotalOrderPartitioner工具类+自动采样]]></title>
    <url>%2Fhadoop%2Fhadoop%2Fhadoop-example-5.html</url>
    <content type="text"><![CDATA[为什么用这种方法我们之前的是自定义分区的，但是如果我们不知道数据的分布，手动分区不太容易，稍有不慎，会导致数据倾斜较大，这时候，我们应该使用采样点进行排序，本文使用的是 Hadoop内置的名为 TotalOrderPartitioner 的全排序，采样器使用的是 InputSampler.Sampler，关键解释已经存在于代码之中。 文章参考了国外 http://blog.ditullio.fr/2016/01/04/hadoop-basics-total-order-sorting-mapreduce/#The_TotalOrderPartitioner国内 http://www.cnblogs.com/one--way/p/5931308.html 代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586package com.myhadoop.mapreduce.test;import org.apache.hadoop.conf.Configured;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapred.lib.InputSampler;import org.apache.hadoop.mapred.lib.TotalOrderPartitioner;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.util.Tool;import org.apache.hadoop.util.ToolRunner;import java.io.IOException;/** * Created by kaishun on 2017/6/10. */public class TotalOrderSort extends Configured implements Tool&#123; public static class myMap extends org.apache.hadoop.mapreduce.Mapper&lt;LongWritable, Text, Text, Text&gt;&#123; public void map(LongWritable key,Text value,Context context) throws IOException,InterruptedException&#123; String[] split = value.toString().split(&quot;\\s+&quot;); for (int i = 0; i &lt;split.length ; i++) &#123; Text word = new Text(split[i]); context.write(word,new Text(&quot;&quot;)); &#125; &#125; &#125; public static class myReduce extends Reducer&lt;Text,Text,Text,Text&gt;&#123; public void reduce(Text key, Iterable&lt;Text&gt; values,Context context) throws IOException,InterruptedException &#123; context.write(key, new Text(&quot;&quot;)); &#125; &#125; @Override public int run(String[] args) throws Exception &#123; Job job = Job.getInstance(getConf()); job.setJarByClass(TotalSort.class); job.setJobName(&quot;TotalSortTest&quot;); job.setInputFormatClass(KeyValueTextInputFormat.class); job.setNumReduceTasks(3); //因为map和reduce的输出是同样的类型，所以输出一个就可以了 job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); job.setMapperClass(myMap.class); job.setReducerClass(myReduce.class); FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 设置分区文件，即采样后放在的文件的文件名，不是完整路径 TotalOrderPartitioner.setPartitionFile(job.getConfiguration(), new Path(args[2])); //采样器：三个参数 /* 第一个参数 freq: 表示来一个样本，将其作为采样点的概率。如果样本数目很大 *第二个参数 numSamples：表示采样点最大数目为，我这里设置10代表我的采样点最大为10，如果超过10，那么每次有新的采样点生成时 * ，会删除原有的一个采样点,此参数大数据的时候尽量设置多一些 * 第三个参数 maxSplitSampled：表示的是最大的分区数：我这里设置100不会起作用，因为我设置的分区只有4个而已 */ InputSampler.Sampler&lt;Text, Text&gt; sampler = new InputSampler.RandomSampler&lt;&gt;(0.01, 10, 100); //把分区文件放在hdfs上，对程序没什么效果，方便我们查看而已 FileInputFormat.addInputPath(job, new Path(&quot;/test/sort&quot;)); //将采样点写入到分区文件中，这个必须要 InputSampler.writePartitionFile(job, sampler); job.setPartitionerClass(TotalOrderPartitioner.class); boolean success = job.waitForCompletion(true); return success ? 0:1; &#125; public static void main(String[] args) throws Exception &#123; int ret = ToolRunner.run(new TotalSortTest(), args); System.exit(ret); &#125;&#125; 注意的地方12 InputSampler.Sampler&lt;Text, Text&gt; sampler = new InputSampler.RandomSampler&lt;&gt;(0.01, 10, 100);中三个参数要注意``` InputSampler.Sampler 只能是Text,Text的类型12```3. TotalOrderPartitioner.setPartitionFile(job.getConfiguration(), new Path(args[2]));用来给TotalOrderPartitioner初始化赋值，job.setPartitionerClass(TotalOrderPartitioner.class); 进行分区，就不需要自己写分区函数了 14. job.setInputFormatClass(KeyValueTextInputFormat.class); 注意里面是KeyValueTextInputFormat.class，而不是TextInputFormat.class。 15. 在集群上，次程序才能体现出来 6.由于我这里，map的输入和输出都是用的(Text,Text),所以我只需要设置12job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); 如果不一样，那么 应该设置4个,前两个为map的输出类型，后两个为reduce的输出类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); job.setOutputKeyClass(IntWritable.class); job.setOutputValueClass(NullWritable.class); 更多文章：自定义分区全排序 http://blog.csdn.net/t1dmzks/article/details/73032796http://blog.csdn.net/t1dmzks/article/details/73028776]]></content>
      <categories>
        <category>大数据</category>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop入门案例（四）全排序之自定义分区 字符串（单词）排序]]></title>
    <url>%2Fhadoop%2Fhadoop%2Fhadoop-example-4.html</url>
    <content type="text"><![CDATA[需求大量文本中有很多单词，需要对这些单词进行排序，排序规则按照字符进行排序 测试文本12345ba bacdf gh hgg dft dfa dfga df fdaf qqq we fsf aa bb abrrty ioo zks huawei mingtong jyzt beijing shanghai shenzhen wuhan nanning guilin zhejiang hanzhou anhui hefei xiaoshan xiaohao anqian zheli guiyang 原理分析和上一篇对数字进行排序是一样的 http://blog.csdn.net/T1DMzks/article/details/73028776 ， 只不过是自定义分区有点变化, 利用mapReduce中map到reduce端的shuffle进行排序，MapReduce只能保证各个分区内部有序，但不能保证全局有序，于是我还自定义了分区，在map后、shuffle之前，我先将小于c的放在0分区，c-f的放在1分区，其余的放在2分区，这样，首先保证了分区与分区之间是整体有序，然后各个分区进行各自的shuffle，使其分区内部有序。 代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182package com.myhadoop.mapreduce.test;import org.apache.hadoop.conf.Configured;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapred.Mapper;import org.apache.hadoop.mapred.OutputCollector;import org.apache.hadoop.mapreduce.Partitioner;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapred.Reporter;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.util.Tool;import org.apache.hadoop.util.ToolRunner;import java.io.IOException;/** * Created by kaishun on 2017/6/10. */public class TotalSortTest extends Configured implements Tool&#123; public static class myMap extends org.apache.hadoop.mapreduce.Mapper&lt;LongWritable, Text, Text, Text&gt;&#123; public void map(LongWritable key,Text value,Context context) throws IOException,InterruptedException&#123; String[] split = value.toString().split("\\s+"); for (int i = 0; i &lt;split.length ; i++) &#123; Text word = new Text(split[i]); context.write(word,new Text("")); &#125; &#125; &#125; public static class myReduce extends Reducer&lt;Text,Text,Text,Text&gt;&#123; public void reduce(Text key, Iterable&lt;Text&gt; values,Context context) throws IOException,InterruptedException &#123; context.write(key, new Text("")); &#125; &#125; public static class Partition extends Partitioner&lt;Text,Text&gt;&#123; @Override public int getPartition(Text value1, Text value2, int i) &#123; if(value1.toString().compareTo("c")&lt;0)&#123; return 0; &#125;else if(value1.toString().compareTo("f")&lt;0)&#123; return 1; &#125; return 2; &#125; &#125; @Override public int run(String[] args) throws Exception &#123; Job job = Job.getInstance(getConf()); job.setJarByClass(TotalSort.class); job.setJobName("TotalSortTest"); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); job.setPartitionerClass(Partition.class); job.setMapperClass(myMap.class); job.setReducerClass(myReduce.class); job.setNumReduceTasks(3); FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); boolean success = job.waitForCompletion(true); return success ? 0:1; &#125; public static void main(String[] args) throws Exception &#123; int ret = ToolRunner.run(new TotalSortTest(), args); System.exit(ret); &#125;&#125; 测试结果生成了三个文件part-r-00000，part-r-00001，part-r-00002各个分区之间有顺序，分区内部也有顺序，分别为12345678aa ab anhui anqian ba bac bb beijing 1234df dfa dfga dft fdaf fsf gh guilin guiyang hanzhou hefei hgg huawei ioo jyzt mingtong nanning qqq rr shanghai shenzhen ty we wuhan xiaohao xiaoshan zhejiang zheli zks 总结mapreduce的shuffle是对key值得hashcode进行排序的，所以单词的全排序也是一样的，类似于数据库中的order by 一样， 利用自定义分区，保证整体有序，利用mapreduce内部的shuffle，对key进行排序，保证了局部有序，从而实现了全排序]]></content>
      <categories>
        <category>大数据</category>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop入门案例（三）全排序之自定义分区 数字排序]]></title>
    <url>%2Fhadoop%2Fhadoop%2Fhadoop-example-3.html</url>
    <content type="text"><![CDATA[需求介绍大量的文本中有大量数字，需要对数字进行全排序,按照升序排序 测试的文本145 95 167 84 6 120 164 195 81 35 63 1 11 89 170 55 58 88 125 173 2 173 129 74 69 24 107 55 149 83 178 159 147 178 53 137 53 132 134 154 174 164 122 108 130 184 28 129 93 157 171 127 192 86 194 41 111 114 190 98 99 99 5 161 146 120 122 80 1 66 171 47 54 121 130 170 125 119 8 52 182 112 146 1 198 0 149 72 56 191 48 172 165 49 73 107 134 179 0 59 16 143 83 92 113 152 109 118 186 186 97 117 193 67 34 152 92 179 52 51 26 163 121 115 72 17 61 107 125 115 163 18 76 2 172 39 190 184 73 108 7 142 68 54 60 169 71 28 141 48 139 182 140 158 102 99 36 158 55 190 176 45 63 126 179 130 95 22 120 109 59 78 38 13 5 88 1 87 184 83 198 47 73 82 94 141 190 184 161 56 141 99 177 107 21 158 71 149 61 137 原理分析利用mapReduce中map到reduce端的shuffle进行排序，MapReduce只能保证各个分区内部有序，但不能保证全局有序，于是我还自定义了分区，在map后、shuffle之前，我先将小于50的放在0分区，50-100的放在1分区，100到150的放在2分区，其余的放在三分区，这样，首先保证了分区与分区之间是整体有序，然后各个分区进行各自的shuffle，使其分区内部有序 代码package com.myhadoop.mapreduce.test; import org.apache.hadoop.conf.Configured; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapreduce.Job; import org.apache.hadoop.mapreduce.Mapper; import org.apache.hadoop.mapreduce.Partitioner; import org.apache.hadoop.mapreduce.Reducer; import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; import org.apache.hadoop.util.Tool; import org.apache.hadoop.util.ToolRunner; import java.io.IOException; public class TotalSort extends Configured implements Tool{ public static class Map extends Mapper&lt;LongWritable, Text, IntWritable, IntWritable&gt; { public void map(LongWritable key,Text value,Context context) throws IOException,InterruptedException { String[] split = value.toString().split("\\s+"); for (int i = 0; i &lt;split.length ; i++) { try{ IntWritable intWritable = new IntWritable(Integer.parseInt(split[i])); context.write(intWritable, intWritable); }catch (Exception e){ e.printStackTrace(); } } } } public static class Reduce extends Reducer&lt;IntWritable, IntWritable, IntWritable, Text&gt; { public void reduce(IntWritable key,Iterable&lt;IntWritable&gt; values,Context context) throws IOException,InterruptedException { for (IntWritable value:values) { context.write(value, new Text("")); } } } /* ·* 重写Partition的方法 */ public static class Partition extends Partitioner&lt;IntWritable, IntWritable&gt;{ @Override public int getPartition(IntWritable key, IntWritable intWritable2, int i) { int i1=key.get(); if(i1&lt;50){ return 0; }else if(i1&lt;100){ return 1; }else if(i1&lt;150){ return 2; } return 3; } } @Override public int run(String[] args) throws Exception { Job job = Job.getInstance(getConf()); job.setJarByClass(TotalSort.class); job.setJobName("TotalSort"); job.setOutputKeyClass(IntWritable.class); job.setOutputValueClass(IntWritable.class); job.setPartitionerClass(Partition.class); job.setMapperClass(Map.class); job.setReducerClass(Reduce.class); job.setNumReduceTasks(4); FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); boolean success = job.waitForCompletion(true); return success ? 0:1; } public static void main(String[] args) throws Exception { int ret = ToolRunner.run(new TotalSort(), args); System.exit(ret); } } 运行结果生成了4个文件，part-r-00000，part-r-00001，part-r-00002，part-r-00003，这四个文件内部都有序part-r-00000内的元素都小于part-r00001的元素，其他的以此类推 总结利用自定义分区，保证整体有序，利用mapreduce内部的shuffle，对key进行排序，保证了局部有序，从而实现了全排序]]></content>
      <categories>
        <category>大数据</category>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop入门案例（二） 单词去重]]></title>
    <url>%2Fhadoop%2Fhadoop%2Fhadoop-example-2.html</url>
    <content type="text"><![CDATA[前言单词去重在很多地方都会进行，其实这个就类似于wordcount 1. 需求说明对指定的一个或者多个文本进行数据去重 1.1 需求输入一个或者多个文本，测试文本内容:1234aa bb cc aa aa aa dd dd ee ee ee ee ff aa bb zksee kksee zz zks 1.2 需求输出输出的内容中单词没有重复 2. 代码如下12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667package com.myhadoop.mapreduce.test;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;import java.io.IOException;import java.util.StringTokenizer;public class Dedup&#123; public static class Map extends Mapper&lt;LongWritable, Text, Text, Text&gt; &#123; private final static IntWritable one = new IntWritable(1); private Text word = new Text(); public void map(LongWritable key,Text value,Context context) throws IOException,InterruptedException &#123; String lines = value.toString(); StringTokenizer tokenizer = new StringTokenizer(lines," "); while(tokenizer.hasMoreElements()) &#123; word.set(tokenizer.nextToken()); context.write(word, new Text("")); &#125; &#125; &#125; public static class Reduce extends Reducer&lt;Text, Text, Text, Text&gt; &#123; public void reduce(Text key,Iterable&lt;Text&gt; values,Context context) throws IOException,InterruptedException &#123; context.write(key, new Text("")); &#125; &#125; public static void main(String[] args) throws Exception &#123; Job job = Job.getInstance(); job.setJarByClass(Dedup.class); job.setJobName("Dedup"); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); job.setMapperClass(Map.class); job.setReducerClass(Reduce.class); job.setInputFormatClass(TextInputFormat.class); job.setOutputFormatClass(TextOutputFormat.class); FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125;``` # **3. 代码输出** aabbccddeeffkkszkszz``` 4. 代码解析整体非常类似于wordcount，先将文本每行内容进行分割，每行都得到一些单词，将单词都转变成map，并且key设置成单词，value设置成空值，经过shuffle，把相同key的放在一组，在reduce中把相同key中的value变成一个空值，然后输出(word,””)的形式Map类：输入： LongWritable, Text输出： Text, TextReduce类：输入：Text, Text —&gt; Text, Interable输出：Text, Text]]></content>
      <categories>
        <category>大数据</category>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop入门案例（一） wordcount]]></title>
    <url>%2Fhadoop%2Fhadoop%2Fhadoop-example-1.html</url>
    <content type="text"><![CDATA[个人感想：虽然现在一直用的是Spark，但是由于目前公司是将Hadoop的程序转移到Spark，这个过程需要用到hadoop，加上现在大多数的大数据平台依旧还是使用的MapReduce的编程模型，所以MapReduce也应该是比较熟悉才行。并且，一般的大数据学习，都是从一些分布式计算框架开始看起，MapReduce一般都是必学的内容，接下来几篇文章，不讲述原理（因为原理网上太多了），只记录一些非常简单，但却非常实用应用案例，以便以后自己查看阅读 1. 需求说明大数据中，经常可能会碰到一些需要单词的出现个数，例如top n 等等。下面介绍一个hadoop的入门案例，对一个或多个文本中的单词进行统计 1.1 需求输入输入一个或者多个文本 测试的文本内容如下1234aa bb cc aa aa aa dd dd ee ee ee ee ff aa bb zksee kksee zz zks 1.2 需求输出将文本中的内容按照单词进行计数，并且将各个单词的统计记录到制定的路径下 2. 代码如下12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667package com.myhadoop.mapreduce.test;import java.io.IOException;import java.util.*;import org.apache.hadoop.fs.Path;import org.apache.hadoop.conf.*;import org.apache.hadoop.io.*;import org.apache.hadoop.mapreduce.lib.input.*;import org.apache.hadoop.mapreduce.lib.output.*;import org.apache.hadoop.mapreduce.*;public class WordCount&#123; public static class Map extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; &#123; private final static IntWritable one = new IntWritable(1); private Text word = new Text(); public void map(LongWritable key,Text value,Context context) throws IOException,InterruptedException &#123; String lines = value.toString(); StringTokenizer tokenizer = new StringTokenizer(lines," "); while(tokenizer.hasMoreElements()) &#123; word.set(tokenizer.nextToken()); context.write(word, one); &#125; &#125; &#125; public static class Reduce extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123; public void reduce(Text key,Iterable&lt;IntWritable&gt; values,Context context) throws IOException,InterruptedException &#123; int sum = 0; for (IntWritable val : values) &#123; sum += val.get(); &#125; context.write(key, new IntWritable(sum)); &#125; &#125; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setJarByClass(WordCount.class); job.setJobName("WordCount"); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); job.setMapperClass(Map.class); job.setReducerClass(Reduce.class); job.setInputFormatClass(TextInputFormat.class); job.setOutputFormatClass(TextOutputFormat.class); FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125;``` # **3. 代码输出** aa 5bb 2cc 1dd 2ee 6ff 1kks 1zks 2zz 1``` 4. 代码解析原理：先将文本每行内容进行分割，每行都得到一些单词，将单词都转变成map，并且key设置成单词，value设置成1，经过shuffle，把相同key的value放在一个list中，reduce过程把相同key中的value进行相加，最后输出key为单词，value为总数Map类：用于将每一行的单词变成map，如 (aa,1),(bb,1)…等。输入是： LongWritable, Text输出是： Text, IntWritableReduce类：用于将map后，并且经过shuffle的数据，进行整合处理输入是：Text，Iterable输出是：Text, IntWritable]]></content>
      <categories>
        <category>大数据</category>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop集群搭建教程]]></title>
    <url>%2Fhadoop%2Fhadoop%2Fhadoop-cluster-install.html</url>
    <content type="text"><![CDATA[hadoop的配置参看github https://github.com/zhaikaishun/hadoop_cluster作者: 翟开顺 关键字:集群环境介绍，Hadoop简介，网络配置，所需软件SSH免密码登陆配置，java环境安装，卸载原有的JDK， 安装jdk17， 配置java环境变量，验证是否成功，Hadoop集群安装，安装Hadoop，验证hadoophadoop错误分析 集群环境介绍1. Hadoop简介 Hadoop是Apache软件基金会旗下的一个开源分布式计算平台。以Hadoop分布式文件系统（HDFS，Hadoop Distributed Filesystem）和MapReduce（Google MapReduce的开源实现）为核心的Hadoop为用户提供了系统底层细节透明的分布式基础架构。 对于Hadoop的集群来讲，可以分成两大类角色：Master和Salve。一个HDFS集群是由一个NameNode和若干个DataNode组成的。其中NameNode作为主服务器，管理文件系统的命名空间和客户端对文件系统的访问操作；集群中的DataNode管理存储的数据。MapReduce框架是由一个单独运行在主节点上的JobTracker和运行在每个集群从节点的TaskTracker共同组成的。主节点负责调度构成一个作业的所有任务，这些任务分布在不同的从节点上。主节点监控它们的执行情况，并且重新执行之前的失败任务；从节点仅负责由主节点指派的任务。当一个Job被提交时，JobTracker接收到提交作业和配置信息之后，就会将配置信息等分发给从节点，同时调度任务并监控TaskTracker的执行。 从上面的介绍可以看出，HDFS和MapReduce共同组成了Hadoop分布式系统体系结构的核心。HDFS在集群上实现分布式文件系统，MapReduce在集群上实现了分布式计算和任务处理。HDFS在MapReduce任务处理过程中提供了文件操作和存储等支持，MapReduce在HDFS的基础上实现了任务的分发、跟踪、执行等工作，并收集结果，二者相互作用，完成了Hadoop分布式集群的主要任务。 2. 环境说明本教程为了简单起见只设置两个节点： master为主节点，node01为数据节点，节点之间局域网连接，相互可以ping通，节点IP分布如下 机器名称 IP地址 master 192.168.200.128 node01 192.168.200.129 两个节点都是centos6.5系统，都有同一个用户，用户名叫Hadoop 给hadoop用户赋予root权限切换到root用户 赋予etc/sudoers777权限，然后打开1234567891011[root@kaishun etc]# chmod 777 /etc/sudoers[root@kaishun etc]# vim /etc/sudoers``` 找到Allows people in group wheel to run all commands，把下面%wheel的#给去掉,在Allow root to run any commands anywhere下，加上hadoop ALL=(ALL) ALL，然后保存```shell## Allow root to run any commands anywhereroot ALL=(ALL) ALL hadoop ALL=(ALL) ALL## Allows people in group wheel to run all commands%wheel ALL=(ALL) ALL 把sudoers的权限改回来成4401234567[root@kaishun etc]# chmod 440 /etc/sudoers``` 测试是否成功 在普通用户下```shell[hadoop@kaishun ~]$ sudo mkdir test输入密码如果可以成功创建文件夹，说明成功 网络配置1. 查看当前机器名在root用户下输入，显示123456[root@kaishun hadoop]# hostname显示 kaishun， 与我们规划的master不符合``` **2. 在root用户下修改当前机器名称**```shell[root@kaishun hadoop]# vim /etc/sysconfig/network 修改HOSTNAME 为 master1HOSTNAME=master 同理，192.168.200.129这台机器修改成node01修改之后，可能不会立即生效，我是重启后才生效的3. 在root用户下配置hosts文件, 每台机器都需要配置（必须）1[root@master hadoop]# vim /etc/hosts 添加123456789101112131415161718127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.200.128 master192.168.200.129 node01``` **测试是否成功**，如果能相互使用ping node01 ping master能成功，说明hosts文件配置成功```shell[hadoop@master ~]$ ping node01PING node01 (192.168.200.129) 56(84) bytes of data.64 bytes from node01 (192.168.200.129): icmp_seq=1 ttl=64 time=0.391 ms64 bytes from node01 (192.168.200.129): icmp_seq=2 ttl=64 time=0.435 ms64 bytes from node01 (192.168.200.129): icmp_seq=3 ttl=64 time=0.442 ms[hadoop@node01 ~]$ ping masterPING master (192.168.200.128) 56(84) bytes of data.64 bytes from master (192.168.200.128): icmp_seq=1 ttl=64 time=0.379 ms64 bytes from master (192.168.200.128): icmp_seq=2 ttl=64 time=0.411 ms64 bytes from master (192.168.200.128): icmp_seq=3 ttl=64 time=0.460 ms 3. 所需软件 JDK版本1.7 hadoop版本hadoop-2.7.1 去官网的华科镜像下载hadoop-2.7.1.tar.gz， 地址 数据传输工具FileZilla， ssh连接工具 secureCRT 4. SSH免密码登陆配置Hadoop运行过程中需要管理远端Hadoop守护进程，在Hadoop启动以后，NameNode是通过SSH（Secure Shell）来启动和停止各个DataNode上的各种守护进程的。这就必须在节点之间执行指令的时候是不需要输入密码的形式，故我们需要配置SSH运用无密码公钥认证的形式，这样NameNode使用SSH无密码登录并启动DataName进程，同样原理，DataNode上也能使用SSH无密码登录到NameNode。安装CentOS6.5时，我们选择了一些基本安装包，所以我们需要两个服务：ssh和rsync已经安装了。可以通过下面命令查看结果显示如下：123456789101112131415161718192021[hadoop@master ~]$ rpm –qa | grep openssh[hadoop@master ~]$ rpm –qa | grep rsync如果有相应的提示，说明这两个是装好了的，我这里是系统自带的``` **4.1 配置master无密码登陆所有的node** 原理请百度在master节点上执行以下命令 然后按几次回车键：```shell[hadoop@master ~]$ ssh-keygen -t rsa``` 出现下图 ![ssh免密码登陆](http://i4.buimg.com/567571/b22c79e94cbc2297.png)我们看到这句话 Your identification has been saved in /home/hadoop/.ssh/id_rsa. Your public key has been saved in /home/hadoop/.ssh/id_rsa.pub. 说明默认目录在 /home/hadoop/.ssh/ 下接着在master节点上做如下配置，把id_rsa.pub追加到授权的key里面去。 ```shell[hadoop@master ~]$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys 现在我们进入~/.ssh目录可以看到123456[hadoop@master ~]$ cd ~/.ssh/[hadoop@master .ssh]$ lltotal 12-rw-rw-r--. 1 hadoop hadoop 395 Apr 2 16:22 authorized_keys-rw-------. 1 hadoop hadoop 1675 Apr 2 16:17 id_rsa-rw-r--r--. 1 hadoop hadoop 395 Apr 2 16:17 id_rsa.pub 4.1.1. 修改文件”authorized_keys权限1[hadoop@master .ssh]$ chmod 600 ~/.ssh/authorized_keys 4.1.2. 设置SSH配置用root用户登录服务器修改SSH配置文件”/etc/ssh/sshd_config”的下列内容。这里找到这些内容，把前面的#去掉即可1234[root@master .ssh]# vim /etc/ssh/sshd_configRSAAuthentication yes # 启用 RSA 认证PubkeyAuthentication yes # 启用公钥私钥配对认证方式AuthorizedKeysFile .ssh/authorized_keys # 公钥文件路径（和上面生成的文件同） 重启SSH服务1234[root@master .ssh]# /etc/rc.d/init.d/sshd restartStopping sshd: [ OK ]Starting sshd: [ OK ][root@master .ssh]# 退出root用户，使用hadoop普通用户验证是否成功, ssh localhost, 如果不需要输入密码，那么验证成功123456[hadoop@master .ssh]$ ssh localhostThe authenticity of host 'localhost (::1)' can't be established.RSA key fingerprint is 48:0b:ee:9b:67:85:4c:19:35:10:d1:1d:e1:5d:fa:c4.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added 'localhost' (RSA) to the list of known hosts.Last login: Sun Apr 2 16:09:39 2017 from 192.168.200.1 4.1.3. 把公钥复制到所有的node机器上从上图中得知无密码登录本级已经设置完毕，接下来的事儿是把公钥复制所有的node机器上。使用下面的命令格式进行复制公钥scp ~/.ssh/id_rsa.pub 远程用户名@远程服务器IP:~/我本地这样使用 scp ~/.ssh/id_rsa.pub hadoop@192.168.200.129:~/ ,然后根据提示输入需要复制的远程服务器的密码，最后出现下面的提示说明复制成功12345678[hadoop@master ~]$ scp ~/.ssh/id_rsa.pub hadoop@192.168.200.129:~/hadoop@192.168.200.129's password: # 这里输入远程密码id_rsa.pub 100% 395 0.4KB/s 00:00 [hadoop@master ~]$ ``` **4.1.4. 对节点机器进行配置**下面就针对IP为"192.168.200.129"的node01的节点进行配置。4.1 ll -a查看是否有.ssh目录，如果没有，我们需要创建一个.ssh目录，并且赋予这个权限 drwx------. 2 hadoop hadoop 4096 Apr 2 16:40 .ssh 具体权限参照master的机器， centos6.5一般都是默认带有.ssh目录的 [hadoop@node01 ~]$ ll -adrwx——. 2 hadoop hadoop 4096 Apr 2 16:40 .ssh1如果有这个目录了，我们把刚才的文件追加到authorized_keys 中去，然后修改authorized_keys文件权限 [hadoop@node01 ~]$ cat ~/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys[hadoop@node01 .ssh]$ chmod 600 ~/.ssh/authorized_keys 123456进入到ssh 目录，ll 看到如下所示说明成功，注意权限是否正确```shell[hadoop@node01 .ssh]$ lltotal 8-rw-------. 1 hadoop hadoop 395 Apr 2 16:52 authorized_keys-rw-r--r--. 1 hadoop hadoop 391 Apr 2 16:40 known_hosts 4.2 用root用户修改/etc/ssh/sshd_config参考前面的master的修改/etc/ssh/sshd_config的方法设置SSH配置用root用户登录服务器修改SSH配置文件”/etc/ssh/sshd_config”的下列内容。这里找到这些内容，把前面的#去掉即可1234[root@master .ssh]# vim /etc/ssh/sshd_configRSAAuthentication yes # 启用 RSA 认证PubkeyAuthentication yes # 启用公钥私钥配对认证方式AuthorizedKeysFile .ssh/authorized_keys # 公钥文件路径（和上面生成的文件同） 重启SSH服务1234[root@master .ssh]# /etc/rc.d/init.d/sshd restartStopping sshd: [ OK ]Starting sshd: [ OK ][root@master .ssh]# 最后记得把”/home/hadoop/“目录下的”id_rsa.pub”文件删除掉 到此为止，我们经过的步骤已经实现了从”master”到”node01”SSH无密码登录 验证master到node01的无密码登陆,在master机器上，使用hadoop用户 ssh node01或者ssh 192.168.200.129, 下面是成功的的结果1234567[hadoop@master ~]$ ssh node01Last login: Sun Apr 2 17:25:50 2017 from localhost[hadoop@node01 ~]$ [hadoop@node01 ~]$ ssh masterLast login: Sun Apr 2 17:26:04 2017 from node01[hadoop@master ~]$ 5 java安装环境5.1 卸载原有的JDK因为有的系统自带有JDK, 安装前先卸载查看所装的JDK1234[hadoop@master ~]$ rpm -qa | grep jdk出现java-1.7.0-openjdk-1.7.0.45-2.4.3.3.el6.x86_64java-1.6.0-openjdk-1.6.0.0-1.66.1.13.0.el6.x86_64 root下卸载前面查出的这两个12345678910111213[root@master hadoop]# yum -y remove java-1.7.0-openjdk-1.7.0.45-2.4.3.3.el6.x86_64 [root@master hadoop]# yum -y remove java-1.6.0-openjdk-1.6.0.0-1.66.1.13.0.el6.x86_64 成功后会出现一个complete``` ### **5.2 安装jdk1.7**首先用root身份登录master后在/usr/local下创建java文件夹```shell[root@master hadoop]# mkdir -p /usr/local/java``` 我们把FTP传来的jdk-7u79-linux-x64.tar.gz复制到/usr/local/java 文件夹下```shell[root@master Downloads]# cp jdk-7u79-linux-x64.tar.gz /usr/local/java/ 解压并且123456[root@master java]# tar zxvf jdk-7u79-linux-x64.tar.gz 解压完成后出现[root@master java]# lltotal 149920drwxr-xr-x. 8 uucp 143 4096 Apr 11 2015 jdk1.7.0_79-rw-r--r--. 1 root root 153512879 Apr 2 18:17 jdk-7u79-linux-x64.tar.gz 给所有者权限1[root@master java]# chown hadoop:hadoop jdk1.7.0_79/ -R 5.3 配置java环境变量编辑”/etc/profile”文件1[root@master java]# vim /etc/profile 在尾部加入12345# set java environmentexport JAVA_HOME=/usr/local/java/jdk1.7.0_79export JRE_HOME=/usr/local/java/jdk1.7.0_79/jreexport PATH=$PATH:/usr/local/java/jdk1.7.0_79/binexport CLASSPATH=./:/usr/local/java/jdk1.7.0_79/lib:/usr/local/java/jdk1.7.0_79/jre/lib 使配置生效1[root@master java]# source /etc/profile 5.4 验证是否成功123java -version 出现 java version &quot;1.7.0_79&quot;javac 有提示java 有提示 确保是按照我上面的步骤，权限不能有错，否则可能会有问题, 同样，在另外的节点上也安装好jdk 6. Hadoop集群安装所有的机器上都要安装hadoop，现在就先在Master服务器安装，然后其他服务器按照步骤重复进行即可。安装和配置hadoop需要以”root”的身份进行。 6.1 安装Hadoop6.1.1 建立一个目录，用来存放hadoop1[root@master Downloads]# mkdir -p /home/hadoop/MyCloudera/APP/hadoop/ 6.1.2 把下载好得hadoop-2.7.1.tar.gz 复制到这个目录下，解压并且命名为hadoop12345678复制到我们建立得目录[root@master Downloads]# cp hadoop-2.7.1.tar.gz /home/hadoop/MyCloudera/APP/hadoop/进入到我们复制得目录[root@master Downloads]# cd /home/hadoop/MyCloudera/APP/hadoop 对此tar.gz解压[root@master hadoop]# tar zxvf hadoop-2.7.1.tar.gz 改名为hadoop[root@master hadoop]# mv hadoop-2.7.1 hadoop 6.1.3 将文件夹得读写权限赋予给hadoop用户12345[root@master APP]# chown -R hadoop:hadoop hadoop ll 查看权限，是这样得[root@master APP]# lltotal 4drwxr-xr-x. 3 hadoop hadoop 4096 Apr 2 23:29 hadoop 6.1.4 配置/etc/profile1[root@master APP]# vim /etc/profile 在末尾加上如下配置，其中HADOOP_HOME填写前面得hadoop存放得位置123456789# set hadoop pathexport HADOOP_HOME=/home/hadoop/MyCloudera/APP/hadoop/hadoop export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/binexport HADOOP_MAPRED_HOME=$HADOOP_HOMEexport HADOOP_COMMON_HOME=$HADOOP_HOMEexport HADOOP_HDFS_HOME=$HADOOP_HOMEexport HADOOP_YARN_HOME=$HADOOP_HOMEexport HADOOP_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoopexport YARN_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoop 让配置生效1[root@master APP]# source /etc/profile 6.2 配置hadoopHadoop配置文件在conf目录下，之前的版本的配置文件主要是Hadoop-default.xml和Hadoop-site.xml。由于Hadoop发展迅速，代码量急剧增加，代码开发分为了core，hdfs和map/reduce三部分，配置文件也被分成了三个core-site.xml、hdfs-site.xml、mapred-site.xml。core-site.xml和hdfs-site.xml是站在HDFS角度上配置文件；core-site.xml和mapred-site.xml是站在MapReduce角度上配置文件。 6.2.1 配置hadoop-env.sh该hadoop-env.sh文件位于/home/hadoop/MyCloudera/APP/hadoop/hadoop/etc/hadoop目录下在文件的末尾添加下面内容12# The java environmentexport JAVA_HOME=/usr/local/java/jdk1.7.0_79 6.2.2 配置core-site.xml文件我们先在本地建立几个目录，用来存放一些hadoop得文件在根目录下，建立一个data1234567根目录路径[root@master /]# pwd/创建一个data目录[root@master /]# mkdir data创建/data/tmpdata/hadoop/data/tmp目录[root@master /]# mkdir -p /data/tmpdata/hadoop/data/tmp 然后对core-site.xml做如下配置, 具体得hadoop.tmp.dir和fs.default.name得功能参看百度google1234567891011&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/data/tmpdata/hadoop/data/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 6.2.3 配置hdfs-site.xml文件我这里配置的比较完整，如果想简单点，有的其实可以默认设置，具体参看其他文章1. 创建namenode和datanode的存放目录,然后对/data目录赋予权限。 注意权限不能有错12345[root@master /]# mkdir -p /data/hadoop/data/name[root@master data]# mkdir -p /data/hadoop/data/data[root@master /]# chown hadoop:hadoop data/ -R[root@master /]# chmod 777 data/ -R 2. 创建SecondaryNameNode的目录在根目录下创建hadoop目录，然后创建/hadoop/SecondaryNameNode/目录，最后赋予hadoop目录权限12345[root@master /]# mkdir hadoop [root@master /]# mkdir -p /hadoop/SecondaryNameNode/ [root@master /]# chown hadoop:hadoop hadoop/ -R [root@master /]# chmod 777 hadoop/ -R hdfs-site.xml配置12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/data/hadoop/data/name/&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/data/hadoop/data/data/&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.dir&lt;/name&gt; &lt;value&gt;/hadoop/SecondaryNameNode/&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.http.address&lt;/name&gt; &lt;value&gt;master:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.secondary.http.address&lt;/name&gt; &lt;value&gt;master:50090&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.du.reserved&lt;/name&gt; &lt;value&gt;0&lt;/value&gt; &lt;description&gt; 每个卷预留的空闲空间数量 &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.max.xcievers&lt;/name&gt; &lt;value&gt;32768&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.socket.write.timeout&lt;/name&gt; &lt;value&gt;0&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.socket.timeout&lt;/name&gt; &lt;value&gt;180000&lt;/value&gt; &lt;description&gt;socket通讯超时时间&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt; 6.2.3 配置mapred-site.xml文件我这里配置的比较完整，网上大多数都是用的默认，具体其中的一些参数可以百度这里先建立几个文件1234567[root@master /]# mkdir -p /hadoop/mapreduce/jobhistory/history/done[root@master /]# mkdir -p /hadoop/mapreduce/jobhistory/history/done_intermediate[root@master /]# mkdir -p /hadoop/hadoop-yarn/staging 赋予权限[root@master /]# chown hadoop:hadoop hadoop/ -R [root@master /]# chmod 777 hadoop/ -R 复制一份 mapred-site.xml1[root@master hadoop]# cp mapred-site.xml.template mapred-site.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102&lt;configuration&gt;&lt;property&gt;&lt;name&gt;mapreduce.framework.name&lt;/name&gt;&lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;mapreduce.jobtracker.address&lt;/name&gt;&lt;value&gt;master:9001&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;mapreduce.jobtracker.http.address&lt;/name&gt;&lt;value&gt;master:50030&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;&lt;value&gt;master:10020&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;&lt;value&gt;master:19888&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;mapreduce.jobhistory.done-dir&lt;/name&gt;&lt;value&gt;/hadoop/mapreduce/jobhistory/history/done&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;mapreduce.jobhistory.intermediate-done-dir&lt;/name&gt;&lt;value&gt;/hadoop/mapreduce/jobhistory/history/done_intermediate&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;yarn.app.mapreduce.am.staging-dir&lt;/name&gt;&lt;value&gt;/hadoop/hadoop-yarn/staging&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;mapred.hosts.exclude&lt;/name&gt;&lt;value&gt;/home/hadoop/MyCloudera/APP/hadoop/hadoop/etc/hadoop/excludes&lt;/value&gt;&lt;final&gt;true&lt;/final&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;mapreduce.tasktracker.map.tasks.maximum&lt;/name&gt;&lt;value&gt;32&lt;/value&gt;&lt;description&gt; 同一时间允许运行的最大map任务数 &lt;/description&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;mapreduce.tasktracker.reduce.tasks.maximum&lt;/name&gt;&lt;value&gt;16&lt;/value&gt;&lt;description&gt; 同一时间允许运行的最大reduce任务数 &lt;/description&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;&lt;value&gt;1000&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;mapreduce.map.memory.mb&lt;/name&gt;&lt;value&gt;512&lt;/value&gt;&lt;description&gt;map阶段申请的container的内存的大小&lt;/description&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;mapreduce.reduce.memory.mb&lt;/name&gt;&lt;value&gt;512&lt;/value&gt;&lt;description&gt;reduce阶段申请的container的内存的大小&lt;/description&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;mapreduce.map.java.opts&lt;/name&gt;&lt;value&gt;-Xmx512M&lt;/value&gt;&lt;description&gt;用户设定的map/reduce阶段申请的container的JVM参数。最大堆设定要比申请的内存少一些，用于JVM的非堆部分使用。 &lt;/description&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;mapreduce.reduce.java.opts&lt;/name&gt;&lt;value&gt;-Xmx1024M&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;mapreduce.task.io.sort.mb&lt;/name&gt;&lt;value&gt;1024&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;mapreduce.reduce.shuffle.parallelcopies&lt;/name&gt;&lt;value&gt;16&lt;/value&gt;&lt;/configuration&gt; 6.2.3 配置yarn-site.xml文件创建一些文件夹，并且赋予权限12345678[root@master /]# clear[root@master /]# mkdir -p /data/nodemanager/tmp/[root@master /]# mkdir -p /hadoop/nodemanager/remote[root@master /]# mkdir -p /data/hadoop/data/nodemanager/logs[root@master /]# chown hadoop:hadoop hadoop/ -R[root@master /]# chmod 777 hadoop/ -R[root@master /]# chown hadoop:hadoop data/ -R[root@master /]# chmod 777 data/ -R 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109&lt;configuration&gt;&lt;!-- Site specific YARN configuration properties --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;master:8030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;master:8031&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;master:8032&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt; &lt;value&gt;master:8033&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.address&lt;/name&gt; &lt;value&gt;master:9999&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.webapp.address&lt;/name&gt; &lt;value&gt;master:8042&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;master:8088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.local-dirs&lt;/name&gt; &lt;value&gt;/data/nodemanager/tmp/&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.remote-app-log-dir&lt;/name&gt; &lt;value&gt;/hadoop/nodemanager/remote&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.log-dirs&lt;/name&gt; &lt;value&gt;/data/hadoop/data/nodemanager/logs&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.log.retain-seconds&lt;/name&gt; &lt;value&gt;604800&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt; &lt;value&gt;24&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;1024&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt; &lt;value&gt;256&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt; &lt;value&gt;1024&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.minimum-allocation-vcores&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.maximum-allocation-vcores&lt;/name&gt; &lt;value&gt;24&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 6.2.3 配置slaves文件这个配置主要记录数据节点的列表，假如集群有3个数据节点，如：node001，node002，node003那么在slave文件里面就可以设置为：node001node002node003 我这里为两个节点，配置如下12masternode01 到此，master的hadoop的配置已经完成，对于其他节点，我们建立好相关的目录，复制过去，稍作配置即可了 需要建立的目录总结 1234567891011121314151617181920[root@node01 /]# mkdir -p /home/hadoop/MyCloudera/APP/hadoop/[root@node01 /]# mkdir data[root@node01 /]# mkdir -p /data/tmpdata/hadoop/data/tmp[root@node01 /]# mkdir -p /data/hadoop/data/name[root@node01 /]# mkdir -p /data/hadoop/data/data[root@node01 /]# mkdir hadoop[root@node01 /]# mkdir -p /hadoop/SecondaryNameNode/ [root@node01 /]# chown hadoop:hadoop hadoop/ -R [root@node01 /]# chmod 777 hadoop/ -R[root@node01 /]# mkdir -p /hadoop/mapreduce/jobhistory/history/done[root@node01 /]# mkdir -p /hadoop/mapreduce/jobhistory/history/done_intermediate[root@node01 /]# mkdir -p /hadoop/hadoop-yarn/staging [root@node01 /]# mkdir -p /data/nodemanager/tmp/[root@node01 /]# mkdir -p /hadoop/nodemanager/remote[root@node01 /]# mkdir -p /data/hadoop/data/nodemanager/logs[root@node01 /]# chown hadoop:hadoop hadoop/ -R[root@node01 /]# chmod 777 hadoop/ -R[root@node01 /]# chown hadoop:hadoop data/ -R[root@node01 /]# chmod 777 data/ -R[root@node01 /]# 为了确保 hadoop目录 权限没有问题，每台机器在hadoop目录下再次执行一下以下命令123chown -R hadoop:hadoop hadoop## 为了保险起见，我给了777的权限， 下面的这一步貌似不做也可以chmod 777 hadoop/ -R 6.3 启动与验证6.3.1 格式化HDFS文件系统在master上使用普通用户hadoop进行操作如果第一次启动需要对hadoop平台进行格式化，记得第一次，假如原来有数据就不需要格式化：1hdfs namenode -format 如果经过多次format之后，一定要把/data/hadoop/data/data /data/hadoop/data/name目录下的文件删除 6.3.2 启动hadoop在启动前关闭集群中所有机器的防火墙，不然会出现datanode开后又自动关闭。记得永久的关闭防火墙chkconfig iptables off1chkconfig iptables off 开始启动,在master的普通用户 hadoop下进行操作1start-all.sh 验证hadoop: 输入jps命令，会出现以下进程说明成功1234567891011121314151617[hadoop@master hadoop]$ jps[hadoop@master hadoop]$ jps4197 ResourceManager3851 DataNode4602 Jps4013 SecondaryNameNode4308 NodeManager3739 NameNode``` #### **6.3.3 测试以下hdfs** 创建一个目录```shell[hadoop@node01 ~]$ hadoop fs -mkdir -p /hive/warehouse``` 传一个文件```shell[hadoop@master hadoop]$ hadoop fs -put slaves /hive/warehouse 查看文件1234[hadoop@master hadoop]$ hadoop fs -cat /hive/warehouse/slaves显示masternode01 经过上面的测试，说明我们集群安装成功 6.4 网页查看集群查看hdfshttp://192.168.200.128:50070显示 验证hadoophttp://192.168.200.128:8088/cluster/nodes显示 7. hadoop 集群碰到错误的解决办法这里的错误，一般都分为几大类，一类是某些文件夹没有创建，一类是某些文件或者文件夹权限不够，一类就是配置错误这些错误都可以去logs目录下查看，我的logs目录在 /home/hadoop/MyCloudera/APP/hadoop/hadoop/logs哪里有问题就对应哪个文件去查看错误，例如resourcemanager没起来或者出问题，就去yarn-hadoop-resourcemanager-master.log1234567891011121314151617181920212223242526272829-rwxrwxrwx. 1 hadoop hadoop 921348 Apr 3 13:19 hadoop-hadoop-datanode-master.log-rw-rw-r--. 1 hadoop hadoop 1434 Apr 3 13:18 hadoop-hadoop-datanode-master.out-rw-rw-r--. 1 hadoop hadoop 0 Apr 3 13:18 hadoop-hadoop-datanode-master.out.1-rw-rw-r--. 1 hadoop hadoop 1434 Apr 3 12:52 hadoop-hadoop-datanode-master.out.2-rwxrwxrwx. 1 hadoop hadoop 1434 Apr 3 12:45 hadoop-hadoop-datanode-master.out.3-rwxrwxrwx. 1 hadoop hadoop 1434 Apr 3 12:42 hadoop-hadoop-datanode-master.out.4-rwxrwxrwx. 1 hadoop hadoop 1434 Apr 3 12:35 hadoop-hadoop-datanode-master.out.5-rwxrwxrwx. 1 hadoop hadoop 371773 Apr 3 13:26 hadoop-hadoop-namenode-master.log-rw-rw-r--. 1 hadoop hadoop 717 Apr 3 13:18 hadoop-hadoop-namenode-master.out-rw-rw-r--. 1 hadoop hadoop 717 Apr 3 13:09 hadoop-hadoop-namenode-master.out.1-rw-rw-r--. 1 hadoop hadoop 717 Apr 3 12:52 hadoop-hadoop-namenode-master.out.2-rwxrwxrwx. 1 hadoop hadoop 717 Apr 3 12:44 hadoop-hadoop-namenode-master.out.3-rwxrwxrwx. 1 hadoop hadoop 717 Apr 3 12:42 hadoop-hadoop-namenode-master.out.4-rwxrwxrwx. 1 hadoop hadoop 717 Apr 3 12:35 hadoop-hadoop-namenode-master.out.5-rwxrwxrwx. 1 hadoop hadoop 0 Apr 3 01:43 SecurityAuth-hadoop.audit-rwxrwxrwx. 1 hadoop hadoop 618506 Apr 3 13:19 yarn-hadoop-nodemanager-master.log-rw-rw-r--. 1 hadoop hadoop 1402 Apr 3 13:19 yarn-hadoop-nodemanager-master.out-rw-rw-r--. 1 hadoop hadoop 0 Apr 3 13:19 yarn-hadoop-nodemanager-master.out.1-rw-rw-r--. 1 hadoop hadoop 1402 Apr 3 13:09 yarn-hadoop-nodemanager-master.out.2-rw-rw-r--. 1 hadoop hadoop 1402 Apr 3 12:52 yarn-hadoop-nodemanager-master.out.3-rwxrwxrwx. 1 hadoop hadoop 1402 Apr 3 12:36 yarn-hadoop-nodemanager-master.out.4-rwxrwxrwx. 1 hadoop hadoop 1402 Apr 3 12:30 yarn-hadoop-nodemanager-master.out.5-rwxrwxrwx. 1 hadoop hadoop 343209 Apr 3 13:19 yarn-hadoop-resourcemanager-master.log-rw-rw-r--. 1 hadoop hadoop 701 Apr 3 13:19 yarn-hadoop-resourcemanager-master.out-rw-rw-r--. 1 hadoop hadoop 701 Apr 3 13:09 yarn-hadoop-resourcemanager-master.out.1-rw-rw-r--. 1 hadoop hadoop 701 Apr 3 12:52 yarn-hadoop-resourcemanager-master.out.2-rwxrwxrwx. 1 hadoop hadoop 701 Apr 3 12:45 yarn-hadoop-resourcemanager-master.out.3-rwxrwxrwx. 1 hadoop hadoop 701 Apr 3 12:36 yarn-hadoop-resourcemanager-master.out.4-rwxrwxrwx. 1 hadoop hadoop 701 Apr 3 12:30 yarn-hadoop-resourcemanager-master.out.5]]></content>
      <categories>
        <category>大数据</category>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS java操作（二）FileStatus 获取文件属性，globStatus 进行路径过滤]]></title>
    <url>%2Fhadoop%2Fhdfs%2Fhdfs-filesystem-2.html</url>
    <content type="text"><![CDATA[前言：本章主要记录了如何使用fileStatus来获取hdfs文件的一些属性，以及如何使用globStatus对路径进行过滤, 列出某个目录下所有文件的信息 filestatus 获取文件状态12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697package hdfs.tutorial;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileStatus;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import java.io.IOException;import java.net.URI;import java.net.URISyntaxException;import java.sql.Timestamp;/** * Created by Administrator on 2017/5/25. */public class ReadHdfs &#123; public static void main(String[] args) &#123; Path path = new Path("hdfs://192.xxx.xxx.xxx:9000/hdfstest"); Path filePath = new Path("hdfs://192.xxx.xxx.xxx:9000/hdfstest/people.txt"); try &#123; //得到 FileSystem 类 FileSystem hdfs = getFileSystem(); //列出某个目录下所有文件的信息 listFilesStatus(path, hdfs); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; /** * * @return 得到hdfs的连接 FileSystem类 * @throws URISyntaxException * @throws IOException * @throws InterruptedException */ public static FileSystem getFileSystem() throws URISyntaxException, IOException, InterruptedException &#123; //获取FileSystem类的方法有很多种，这里只写一种 Configuration config = new Configuration(); URI uri = new URI("hdfs://192.xxx.xxx.xxx:9000"); return FileSystem.get(uri,config,"your_user");// 第一位为uri，第二位为config，第三位是登录的用户 &#125; /** * 列出某个目录下所有文件的信息 * @param path * @param hdfs * @throws IOException */ public static void listFilesStatus(Path path, FileSystem hdfs) throws IOException &#123; // 列出目录下的所有文件 FileStatus[] files = hdfs.listStatus(path); for (int i = 0; i &lt;files.length ; i++) &#123; FileStatus file = files[i]; if(file.isFile())&#123; System.out.println("这是文件"); long len = file.getLen(); //文件长度 String pathSource = file.getPath().toString();//文件路径 String fileName = file.getPath().getName(); // 文件名称 String parentPath = file.getPath().getParent().toString();//文件父路径 Timestamp timestamp = new Timestamp(file.getModificationTime());//文件最后修改时间 long blockSize = file.getBlockSize(); //文件块大小 String group = file.getGroup(); //文件所属组 String owner = file.getOwner(); // 文件拥有者 long accessTime = file.getAccessTime(); //该文件上次访问时间 short replication = file.getReplication(); //文件副本数 System.out.println("文件长度: "+len+"\n"+ "文件路径: "+pathSource+"\n"+ "文件名称: "+fileName+"\n"+ "文件父路径: "+parentPath+"\n"+ "文件最后修改时间: "+timestamp+"\n"+ "文件块大小: "+blockSize+"\n"+ "文件所属组: "+group+"\n"+ "文件拥有者: "+owner+"\n"+ "该文件上次访问时间: "+accessTime+"\n"+ "文件副本数: "+replication+"\n"+ "=============================="); &#125;else if(file.isDirectory())&#123; System.out.println("这是文件夹"); System.out.println("文件父路径: "+file.getPath().toString()); //递归调用 listFilesStatus(file.getPath(),hdfs); &#125;else if(file.isSymlink())&#123; System.out.println("这是链接文件"); &#125; &#125; &#125;&#125; globStatus 路径过滤例如下面的例子，我需要得到/filtertest// 路径中 带有abc的路径globStatus 很灵活，内部甚至可以写一些正则表达式，有时候在处理大数据的预处理的时候可能很有效 123456789101112131415161718192021222324252627282930public class FileListFilter &#123; public static void main(String[] args) &#123; FileSystem hdfs = null; try&#123; Configuration config = new Configuration(); hdfs = FileSystem.get(new URI(&quot;hdfs://192.xxx.xxx.xxx:9000&quot;), config, &quot;hmaster&quot;); Path allPath = new Path(&quot;/filtertest/*/*&quot;); FileStatus[] fileGlobStatuses = hdfs.globStatus(allPath, new PathFilter() &#123; @Override public boolean accept(Path path) &#123; String contidion = &quot;abc&quot;; // 过滤出路径中包含 abc字符串 的路径 return path.toString().contains(contidion); &#125; &#125;); Path[] globPaths = FileUtil.stat2Paths(fileGlobStatuses); for (Path p :globPaths)&#123; System.out.println(&quot;globe过滤后的路径&quot;+p); &#125; &#125;catch(Exception e)&#123; e.printStackTrace(); &#125; &#125;&#125;]]></content>
      <categories>
        <category>大数据</category>
        <category>hdfs</category>
      </categories>
      <tags>
        <tag>hdfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS java操作（一）FileSystem 常用操作]]></title>
    <url>%2Fhadoop%2Fhdfs%2Fhdfs-filesystem-1.html</url>
    <content type="text"><![CDATA[简介:本文主要讲解如何用java去操作hdfs，以下是我整理的常用的一些方法，本文主要介绍的是FileSystem，我把其集合到了一个工具类当中，下面的操作主要有 检查文件是否存在，创建文件，创建文件夹，复制（上传）本地文件到hdfs指定目录，复制（上传）本地文件夹到hdfs指定目录，从hdfs下载文件，移动hdfs上的文件或者文件夹，删除文件或者文件夹，HDFS 到 HDFS 的合并，列出所有DataNode的名字信息，检测是否是备用节点等操作 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509package hdfs.tutorial;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.*;import org.apache.hadoop.hdfs.DistributedFileSystem;import org.apache.hadoop.hdfs.protocol.DatanodeInfo;import java.io.File;import java.io.FileInputStream;import java.io.FileOutputStream;import java.io.IOException;import java.net.URI;import java.net.URISyntaxException;import java.util.Date;import java.util.List;/** * Created by Administrator on 2017/5/25. */public class HdfsOper &#123; private FileSystem hdfs; /** * @return 得到hdfs的连接 FileSystem类 * @throws URISyntaxException * @throws IOException * @throws InterruptedException */ public static FileSystem getFileSystem() throws URISyntaxException, IOException, InterruptedException &#123; //获取FileSystem类的方法有很多种，这里只写一种 Configuration config = new Configuration(); URI uri = new URI("hdfs://192.xxx.x.xxx:xxx"); return FileSystem.get(uri,config,"your user");// 第一位为uri，第二位为config，第三位是登录的用户 &#125; /** * 检查文件或者文件夹是否存在 * @param filename * @return */ public boolean checkFileExist(String filename) &#123; try &#123; Path f = new Path(filename); return hdfs.exists(f); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return false; &#125; /** * 创建文件夹 * @param dirName * @return */ public boolean mkdir(String dirName) &#123; if (checkFileExist(dirName)) return true; try &#123; Path f = new Path(dirName); System.out.println("Create and Write :" + f.getName() + " to hdfs"); return hdfs.mkdirs(f); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return false; &#125; /** * 创建一个空文件 * @param filePath 文件的完整路径名称 * @return */ public boolean mkfile(String filePath) &#123; try &#123; Path f = new Path(filePath); FSDataOutputStream os = hdfs.create(f, true); os.close(); return true; &#125; catch (IllegalArgumentException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return false; &#125; /** * 复制文件到指定目录 * @param srcfile 复制的文件路径 * @param desfile 粘贴的路径 * @return */ public boolean hdfsCopyUtils(String srcfile, String desfile) &#123; Configuration conf = new Configuration(); Path src = new Path(srcfile); Path dst = new Path(desfile); try &#123; FileUtil.copy(src.getFileSystem(conf), src, dst.getFileSystem(conf), dst, false, conf); &#125; catch (IOException e) &#123; return false; &#125; return true; &#125; /** * 移动文件或者文件夹 * @param src 初始路径 * @param dst 移动结束路径 * @throws Exception */ public void movefile(String src, String dst) throws Exception &#123; Path p1 = new Path(src); Path p2 = new Path(dst); hdfs.rename(p1, p2); &#125; /** * 删除文件或者文件夹 * @param src * @throws Exception */ public void delete(String src) throws Exception &#123; Path p1 = new Path(src); if (hdfs.isDirectory(p1)) &#123; hdfs.delete(p1, true); System.out.println("删除文件夹成功: " + src); &#125; else if (hdfs.isFile(p1)) &#123; hdfs.delete(p1, false); System.out.println("删除文件成功: " + src); &#125; &#125; /** * 读取本地文件到HDFS系统, 保证文件格式是utf-8 * @param localFilename * @param hdfsPath * @return */ public boolean copyLocalFileToHDFS(String localFilename, String hdfsPath) &#123; try &#123; // 如果路径不存在就创建文件夹 mkdir(hdfsPath); File file = new File(localFilename); FileInputStream is = new FileInputStream(file); // 如果hdfs上已经存在文件，那么先删除该文件 if (this.checkFileExist(hdfsPath + "/" + file.getName())) &#123; delete(hdfsPath + "/" + file.getName()); &#125; Path f = new Path(hdfsPath + "/" + file.getName()); FSDataOutputStream os = hdfs.create(f, true); byte[] buffer = new byte[10240000]; int nCount = 0; while (true) &#123; int bytesRead = is.read(buffer); if (bytesRead &lt;= 0) &#123; break; &#125; os.write(buffer, 0, bytesRead); nCount++; if (nCount % (100) == 0) System.out.println((new Date()).toLocaleString() + ": Have move " + nCount + " blocks"); &#125; is.close(); os.close(); System.out.println((new Date()).toLocaleString() + ": Write content of file " + file.getName() + " to hdfs file " + f.getName() + " success"); return true; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return false; &#125; /** * 复制本地文件夹到hdfs的文件 * @param localPath * @param hdfsPath * @return */ public boolean CopyLocalDirTohdfs(String localPath, String hdfsPath) &#123; try &#123; File root = new File(localPath); File[] files = root.listFiles(); for (File file : files) &#123; if (file.isFile()) &#123; copyLocalFileToHDFS(file.getPath().toString(), hdfsPath); &#125; else if(file.isDirectory()) &#123; CopyLocalDirTohdfs(localPath+"/"+file.getName(), hdfsPath+"/"+file.getName()); &#125; &#125; return true; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return false; &#125; /** * 从hdfs下载 * @param hdfsFilename * @param localPath * @return */ public boolean downloadFileFromHdfs(String hdfsFilename, String localPath) &#123; try &#123; Path f = new Path(hdfsFilename); FSDataInputStream dis = hdfs.open(f); File file = new File(localPath + "/" + f.getName()); FileOutputStream os = new FileOutputStream(file); byte[] buffer = new byte[1024000]; int length = 0; while ((length = dis.read(buffer)) &gt; 0) &#123; os.write(buffer, 0, length); &#125; os.close(); dis.close(); return true; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return false; &#125; /** * HDFS 到 HDFS 的合并 * hdfs提供了一种FileUtil.copyMerge（）的方法， 注意下面的 false 这个，如果改为true，就会删除这个目录 * @param folder 需要合并的目录 * @param file 要合并成的文件，完整路径名称 */ public void copyMerge(String folder, String file) &#123; Configuration conf = new Configuration(); Path src = new Path(folder); Path dst = new Path(file); try &#123; FileUtil.copyMerge(src.getFileSystem(conf), src, dst.getFileSystem(conf), dst, false, conf, null); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; /** * 列出所有DataNode的名字信息 */ public void listDataNodeInfo() &#123; try &#123; DistributedFileSystem fs =null; fs = (DistributedFileSystem) hdfs; DatanodeInfo[] dataNodeStats = fs.getDataNodeStats(); String[] names = new String[dataNodeStats.length]; System.out.println("List of all the datanode in the HDFS cluster:"); for (int i = 0; i &lt; names.length; i++) &#123; names[i] = dataNodeStats[i].getHostName(); System.out.println(names[i]); &#125; System.out.println(hdfs.getUri().toString()); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; /** * 检测是否是备用节点 * @throws Exception */ public boolean checkStandbyException(String filename) &#123; try &#123; Path f = new Path(filename); hdfs.exists(f); &#125; catch (org.apache.hadoop.ipc.RemoteException e) &#123; if(e.getClassName().equals("org.apache.hadoop.ipc.StandbyException")) &#123; return true; &#125; &#125; catch (Exception e) &#123; &#125; return false; &#125; public boolean mergeDirFiles(List&lt;FileStatus&gt; fileList, String tarPath, String rowTerminateFlag) &#123; //rowTerminateFlag \n FSDataOutputStream tarFileOutputStream = null; FSDataInputStream srcFileInputStream = null; try &#123; Path tarFile = new Path(tarPath); tarFileOutputStream = hdfs.create(tarFile, true); byte[] buffer = new byte[1024000]; int length = 0; long nTotalLength = 0; int nCount = 0; boolean bfirst = true; for(FileStatus file : fileList) &#123; if(file.getPath().equals(tarFile)) &#123; continue; &#125; System.out.println(" merging file from " + file.getPath() + " to " + tarPath); if(!bfirst) &#123; //添加换行符 tarFileOutputStream.write(rowTerminateFlag.getBytes(), 0, rowTerminateFlag.length()); &#125; srcFileInputStream = hdfs.open(file.getPath(), buffer.length); while ((length = srcFileInputStream.read(buffer)) &gt; 0) &#123; nCount++; tarFileOutputStream.write(buffer, 0, length); nTotalLength += length; //System.out.println(" file length " + file.getLen() + " read " + length); if (nCount % 1000 == 0) &#123; tarFileOutputStream.flush(); System.out.println((new Date()).toLocaleString() + ": Have move " + (nTotalLength / 1024000) + " MB"); &#125; &#125; srcFileInputStream.close(); bfirst = false; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); try &#123; delete(tarPath); &#125; catch (Exception e2) &#123; // TODO: handle exception &#125; return false; &#125; finally &#123; try &#123; if(tarFileOutputStream != null) &#123; tarFileOutputStream.flush(); tarFileOutputStream.close(); srcFileInputStream.close(); &#125; &#125; catch (Exception e2) &#123; // TODO: handle exception &#125; &#125; return true; &#125;&#125; /** * 将一个字符串写入某个路径 * * @param text 要保存的字符串 * @param path 要保存的路径 */ public void writerString(String text,String path)&#123; try &#123; Path f = new Path(path); FSDataOutputStream os = fs.create(f, true); BufferedWriter writer = new BufferedWriter(new OutputStreamWriter(os, "utf-8"));// 以UTF-8格式写入文件，不乱码 writer.write(text); writer.close(); os.close(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; /** * 按行读取文件内容，并且防止乱码 * @param hdfsFilename * @return */ public boolean readByLine(String hdfsFilename) &#123; try &#123; Path f = new Path(hdfsFilename); FSDataInputStream dis = hdfs.open(f); BufferedReader bf=new BufferedReader(new InputStreamReader(dis));//防止中文乱码 String line = null; while ((line=bf.readLine())!=null) &#123; System.out.println(new String(line.getBytes(),"utf-8")); &#125; dis.close(); return true; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return false; &#125; public void reNameExistsPath(String srcPath, String tarPath) throws Exception &#123; //检测输出目录是否存在，存在就改名 if(checkFileExist(srcPath)) &#123; tarPath = srcPath.trim(); while(tarPath.charAt(tarPath.length()-1) == '/') &#123; tarPath = tarPath.substring(0, tarPath.length()-1); &#125; Date now = new Date(); SimpleDateFormat dateFormat = new SimpleDateFormat("yyMMddHHmmss"); String nowStr = dateFormat.format(now); tarPath += "_" + nowStr; movefile(srcPath, tarPath); &#125; else &#123; tarPath = srcPath; &#125; &#125;]]></content>
      <categories>
        <category>大数据</category>
        <category>hdfs</category>
      </categories>
      <tags>
        <tag>hdfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS 指令（四）find,help,setfatter,truncate,usage]]></title>
    <url>%2Fhadoop%2Fhdfs%2Fhdfs-operator-4.html</url>
    <content type="text"><![CDATA[关键字：hdfs命令,find,help,setfatter,truncate,usage 的用法 前言在这个HDFS教程中，主要介绍了 如何查看命令帮助，如何设置文件扩展属性，文件的截断 1. find找出能匹配上的所有文件-name pattern 不区分大小写，对大小写不敏感-iname pattern 对大小写敏感。-print 打印。-print0 打印在一行命令用法1hadoop fs -find &lt;path&gt; ... &lt;expression&gt; ... 例子1hadoop fs -find /user/dataflair/dir1/ -name sample -print 2. help查看帮助文档命令用法1hadoop fs -help 例子1hadoop fs -help 3. setfattr为一个文件或者文件夹设置一个扩展属性和值操作-b: 除了ACL的文件不移除，他移除所有的扩展属性。所有文件信息都保留给用户，组和其他用户-n name: 显示扩展的属性的名.-v value: 显示扩展的属性的值。-x name: 移除所有的扩展属性path: 文件或文件夹.命令用法1hadoop fs -setfattr -n name [-v value] | -x name &lt;path&gt; 例子123hdfs dfs -setfattr -n user.myAttr -v myValue /user/dataflair/dir2/purchases.txthdfs dfs -setfattr -n user.noValue /user/dataflair/dir2/purchases.txthdfs dfs -setfattr -x user.myAttr /user/dataflair/dir2/purchases.txt 4. truncate 参考文章：http://blog.csdn.net/androidlushangderen/article/details/52651995类似于linux的truncate命令用法1hadoop fs -truncate [-w] &lt;length&gt; &lt;paths&gt; 例子12hadoop fs -truncate 55 /user/dataflair/dir2/purchases.txt /user/dataflair/dir1/purchases.txthadoop fs -truncate -w 127 /user/dataflair/dir2/purchases.txt 5. usage返回某个命令的帮助命令用法1hadoop fs -usage command 例子1hadoop fs -usage mkdir]]></content>
      <categories>
        <category>大数据</category>
        <category>hdfs</category>
      </categories>
      <tags>
        <tag>hdfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS 指令（三）touchz，test，text，stat，appendToFile，checksum，count，chmod]]></title>
    <url>%2Fhadoop%2Fhdfs%2Fhdfs-operator-3.html</url>
    <content type="text"><![CDATA[关键词：hdfs命令，touchz，test，text，stat，appendToFile，checksum，count，chmod 本章目的在这个Hadoop HDFS命令教程中，我们将学习剩下一些重要并且经常使用的HDFS命令，借助这些命令，我们将能够执行HDFS文件操作，如复制文件，更改文件权限，查看文件内容，更改文件所有权，创建目录等。要了解有关世界上最可靠的存储层的更多信息，请参阅HDFS入门指南。 1. touchz在指定目录创建一个新文件，如果文件存在，则创建失败命令用法1touchz &lt;path&gt; 例子1hdfs dfs -touchz /user/dataflair/dir2 2. test测试hdfs中的某个文件或者目录是否存在-d: 如果测试的路径是一个文件夹, 则返回0，否则返回1。-e: 如果测试的路径存在, 则返回0，否则返回1。-f: 如果测试的路径是一个文件, 则返回0，否则返回1。-s: if 如果测试的路径不是空(文件夹下有文件或者文件夹), 则返回0，否则返回1。-z: 如果测试的是一个文件，并且这个文件不为空, 则返回0，否则返回1。命令用法1hdfs dfs -test -[ezd] URI 例子123&quot;hdfs dfs -test -e /test/file/samplehdfs dfs -test -z /test/file/samplehdfs dfs -test -d /test/file/sample 3. text格式化输出文件的内容，允许的格式化包括zip,和 TextRecordInputStream命令用法1hdfs dfs -text &lt;source&gt; 例子1hdfs dfs -text /user/dataflair/dir1/sample 4. stat打印有关路径的信息，可以加下面的格式化输出%b: 文件大小 %n: 文件名 %o: 块大小 %r: 副本个数 %y, %Y: 修改日期.命令用法1hdfs dfs -stat [format] path 例子12hdfs dfs -stat /user/dataflair/dir1hdfs fs -stat &quot;%o %r&quot; /user/dataflair/dir1 5. appendToFileappendToFile命令是将一个或者多个文件添加到HDFS系统中，他也是从标准输入中读取，然后添加到目标文件系统汇总命令用法1hadoop fs -appendToFile &lt;localsource&gt; ... &lt;dst&gt; 例子1hadoop fs -appendToFile /home/dataflair/Desktop/sample /user/dataflair/dir1 6. checksumDatanode在把数据实际存储之前会验证数据的校验和（checksum的初始值？）如果某个client在读取数据时检测到数据错误, 在抛出ChecksumException参考： http://blog.csdn.net/oh_mourinho/article/details/52524442命令用法1hadoop fs -checksum URI 例子1hadoop fs -checksum /user/dataflair/dir1/sample 7. count统计hdfs对应路径下的目录个数，文件个数，文件总计大小显示为目录个数，文件个数，文件总计大小，输入路径命令用法1hdfs dfs -count [-q] &lt;paths&gt; 例子1hdfs dfs -count /user/dataflair 8. chmod类似于linux，用来修改权限命令用法1chmod [-R] mode,mode,... &lt;path&gt;... 例子1hdfs dfs -chmod 777 /user/dataflair/dir1/sample 翻译原文：http://data-flair.training/blogs/hadoop-hdfs-commands-tutorial/]]></content>
      <categories>
        <category>大数据</category>
        <category>hdfs</category>
      </categories>
      <tags>
        <tag>hdfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS 指令（一）version，mkdir，ls，put，copyFromLocal，get，copyToLocal，cat，mv，cp]]></title>
    <url>%2Fhadoop%2Fhdfs%2Fhdfs-operator-1.html</url>
    <content type="text"><![CDATA[1. 简介本文主要简单介绍一些常用的hdfs，包括复制文件，修改文件权限，查看文件内容，改变文件的所有权，创建目录，等HDFS文件操作所有的Hadoop文件系统shell命令是由在bin / HDFS脚本调用 2. version 查看版本指令用法1hdfs version 例子1[hadoop@master ~]$ hdfs version 输出hadoop版本号 Hadoop 2.7.1 3. mkdir 创建目录指令用法1mkdir &lt;path&gt; 例子 , 创建testdir目录1[hadoop@master ~]$ hdfs dfs -mkdir /testdir 4. ls 查看目录文件能显示指定路径的文件或文件夹列表，显示每个条目 的名称，权限，拥有者，大小和修改日期，路径。 指令用法1ls &lt;path&gt; 例子，查看 /test目录 (我之前创建了test目录，并且放了一个文件在里面)1[hadoop@master ~]$ hdfs dfs -ls /test 输出12Found 1 items-rw-r--r-- 2 hadoop supergroup 14 2017-04-03 14:45 /test/slaves 5. put 复制文件到hdfs可以复制本地文件到hdfs，也可以复制hdfs上的文件，到hdfs的另外一个地方指令用法1put &lt;localSrc&gt; &lt;dest&gt; 例子，把hdfs.site文件放到 /test目录下1[hadoop@master hadoop]$ hdfs dfs -put /home/hadoop/MyCloudera/APP/hadoop/hadoop/etc/hadoop/hdfs-site.xml /test 6. copyFromLocal和put很像，但是指定了只能是从本地复制到hdfs指令用法1copyFromLocal &lt;localSrc&gt; &lt;dest&gt; 例子，把hadoop-env.sh复制到 /test中1[hadoop@master hadoop]$ hdfs dfs -copyFromLocal /home/hadoop/MyCloudera/APP/hadoop/hadoop/etc/hadoop/hadoop-env.sh /test 7. get 复制hdfs文件到本地指令用法1get [-crc] &lt;src&gt; &lt;localDest&gt; 例如我将hdfs 上的文件拷贝到本地1[hadoop@master Downloads]$ hdfs dfs -get /test/slaves /home/hadoop/Downloads/ 8. copyToLocal 复制hdfs文件到本地类似于get命令，唯一的区别是，在这个目的路径被限制到本地 指令用法1copyToLocal &lt;src&gt; &lt;localDest&gt; 例子, 复制slaves到本地,Downloads目录1[hadoop@master Downloads]$ hdfs dfs -copyToLocal /test/slaves /home/hadoop/Downloads/ 9. cat 查看文件内容将文件的内容输出到console或者stdout控制台指令用法1cat &lt;file-name&gt; 例如，查看/test/slaves文件1[hadoop@master Downloads]$ hdfs dfs -cat /test/slaves 控制台打印12masternode01 10. mv 移动类似于linux的mv命令，移动hdfs中的一个地方到另外一个地方指定用法1mv &lt;src&gt; &lt;dest&gt; 把/test/slaves 复制到 /testdir1[hadoop@master Downloads]$ hdfs dfs -mv /test/slaves /testdir 11. copy复制类似于linux中的cp方法，复制hdfs中的一个文件到另外一个文件目录 指令用法1cp &lt;src&gt; &lt;dest&gt; 例子 [hadoop@master Downloads]$ hdfs dfs -cp /testdir/slaves /test 翻译原文：http://data-flair.training/blogs/top-hadoop-hdfs-commands-tutorial/]]></content>
      <categories>
        <category>大数据</category>
        <category>hdfs</category>
      </categories>
      <tags>
        <tag>hdfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS 指令（二）moveFromLocal，moveToLocal，tail，rm，expunge，chown，chgrp，setrep，du，df]]></title>
    <url>%2Fhadoop%2Fhdfs%2Fhdfs-operator-2.html</url>
    <content type="text"><![CDATA[目的本文主要学习hadoop hdfs 从hdfs移动到本地，从本地移动到hdfs，tail查看最后，rm删除文件，expunge清空 trash,chown 改变拥有者，setrep 改变文件副本数，chgrp改变所属组，，du,df磁盘占用情况 moveFromLocal复制一份本地文件到hdfs，当成功后，删除本地文件指令用法1moveFromLocal &lt;localSrc&gt; &lt;dest&gt; 例子1hdfs dfs -moveFromLocal /home/dataflair/Desktop/sample /user/dataflair/dir1 moveToLocal类似于-get，但是当复制完成后，会删除hdfs上的文件指令用法1moveToLocal &lt;src&gt; &lt;localDest&gt; 例子1hdfs dfs -moveToLocal /user/dataflair/dir2/sample /user/dataflair/Desktop tail类似于linux中的tail，把文件最后1kb给打印到console 或者 stdout.指令用法1hdfs dfs -tail [-f] &lt;filename&gt; 例子12hdfs dfs -tail /user/dataflair/dir2/purchases.txthdfs dfs -tail -f /user/dataflair/dir2/purchases.txt rm移除文件或者清空路径下的数据指令用法 1rm &lt;path&gt; 例子 1hdfs dfs -rm /user/dataflair/dir2/sample 递归删除 1hdfs dfs -rm -r /user/dataflair/dir2 expunge清空 trashHDFS中的数据删除也是比较有特点的，并不是直接删除，而是先放在一个类似回收站的地方（/trash），可供恢复对于用户或者应用程序想要删除的文件，HDFS会将它重命名并移动到/trash中 ，当被覆盖或者到了一定的生命周期（现在默认为6小时），hdfs才会从文件中删除，并且只有这个时候，Datanode上相关的磁盘空间才能节省出而 expunge 就类会清空/trash，类似于清空回收站指令用法1hdfs dfs -expunge 例子1hdfs dfs -expunge chown类似于linux中的chown，用户应该是超级用户才能做次操作改变文件拥有者指令用法1hdfs dfs -chown [-R] [OWNER][:[GROUP]] URI [URI ] 例子1hdfs dfs -chown -R dataflair /opt/hadoop/logs chgrp类似于linux中的chgrp，用户应该是超级用户才能做次操作改变文件所有组指令用法1hdfs dfs -chgrp [-R] &lt;NewGroupName&gt; &lt;file or directory name&gt; 例子1hdfs dfs -chgrp [-R] New Group sample setrep指令用法用来改变文件的副本数，如果是文件夹，那么次命令会针对该文件夹下的所有文件都会改变副本数-w 表示副本数改为多少使用-R选项可以对一个目录下的所有目录+文件递归执行改变副本个数的操作1setrep [-R] [-w] rep &lt;path&gt; 例子1hdfs dfs -setrep -w 3 /user/dataflair/dir1 du类似于linux中的du ， 统计个目录下各个文件大小，可以加-h 提高文件可读性指令用法1du &lt;path&gt; 例子1hdfs dfs -du /user/dataflair/dir1/sample df类似于linux中的du ，查询某个目录的空间大小，可以加-h 提高文件可读性指令用法1hdfs dfs -df [-h] URI [URI ...] 例子1hdfs dfs -df -h 翻译原文：http://data-flair.training/blogs/most-used-hdfs-commands-tutorial-examples/]]></content>
      <categories>
        <category>大数据</category>
        <category>hdfs</category>
      </categories>
      <tags>
        <tag>hdfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS获取目录大小API]]></title>
    <url>%2Fhadoop%2Fhdfs%2Fhdfs-ContentSummary-directory.html</url>
    <content type="text"><![CDATA[获取文件大小，在命令行上，使用hadoop fs -du 命令可以，但是通过javaAPI怎么获取呢，最开始我想到的是递归的方法，这个方法很慢，后来发现FileSystem.getContentSummary的方法 最慢的一个方法–递归1网上很多类似的方法，不建议 使用FileSystem.getContentSummary方法下面是api的一句话： The getSpaceConsumed() function in the ContentSummary class will return the actual space the file/directory occupies in the cluster i.e. it takes into account the replication factor set for the cluster. For instance, if the replication factor in the hadoop cluster is set to 3 and the directory size is 1.5GB, the getSpaceConsumed() function will return the value as 4.5GB. Using getLength() function in the ContentSummary class will return you the actual file/directory size.示例代码如下 12345678910111213141516171819202122public static void main(String[] args) &#123; FileSystem hdfs = null; Configuration conf = new Configuration(); try &#123; hdfs = FileSystem.get(new URI("hdfs://192.xxx.xx.xx:9000"),conf,"username"); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; Path filenamePath = new Path("/test/input"); try &#123; //会根据集群的配置输出，例如我这里输出3G System.out.println("SIZE OF THE HDFS DIRECTORY : " + hdfs.getContentSummary(filenamePath).getSpaceConsumed()); // 显示实际的输出，例如这里显示 1G System.out.println("SIZE OF THE HDFS DIRECTORY : " + hdfs.getContentSummary(filenamePath).getLength()); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;]]></content>
      <categories>
        <category>大数据</category>
        <category>hdfs</category>
      </categories>
      <tags>
        <tag>hdfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS合并文件]]></title>
    <url>%2Fhadoop%2Fhdfs%2Fhdfs-merge-file.html</url>
    <content type="text"><![CDATA[HDFS到HDFS的合并hdfs提供了一种FileUtil.copyMerge（）的方法， 注意下面的 false 这个，如果改为true，就会删除这个目录，public void copyMerge(String folder, String file) {Path src = new Path(folder)…….;12345678910111213public void copyMerge(String folder, String file) &#123; Path src = new Path(folder); Path dst = new Path(file); try &#123; FileUtil.copyMerge(src.getFileSystem(conf), src, dst.getFileSystem(conf), dst, false, conf, null); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125;&#125; 上传文件到HDFS并且合并123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111/** * 读取本地文件到HDFS系统&lt;br&gt; * 请保证文件格式一直是UTF-8，从本地-&gt;HDFS * *//** * * @param localDirname 源文件所在位置 * @param hdfsPath 要放在服务器的位置 * @param destFileName 要合并成的文件名称 * @param filter * @return */public boolean putMerge(String localDirname, String hdfsPath,String destFileName, String filter )&#123; try &#123; File dir = new File(localDirname); if(!dir.isDirectory()) &#123; System.out.println(localDirname+"不是目录 "); return false; &#125; File[] files = dir.listFiles(); if(files.length ==0) return false; System.out.println("Begin move " + localDirname + " to " + hdfsPath); while(checkFileExist(hdfsPath+"/"+destFileName)) &#123; if(destFileName.contains(".x")) destFileName += "x"; else destFileName += ".x"; &#125; mkdir(hdfsPath); Path f = new Path(hdfsPath+"/"+destFileName); FSDataOutputStream os = fs.create(f, true); byte[] buffer = new byte[10240000]; for(int i=0; i&lt;files.length; i++) &#123; if(MainSvr.bExitFlag == true) break; File file = files[i]; if(!file.getName().toLowerCase().contains(filter.toLowerCase())) continue; FileInputStream is = new FileInputStream(file); GZIPInputStream gis = null; if(file.getName().toLowerCase().endsWith("gz")) gis=new GZIPInputStream(is); while(MainSvr.bExitFlag != true) &#123; int bytesRead =0; if(gis == null) bytesRead= is.read(buffer); else bytesRead= gis.read(buffer,0,buffer.length); if (bytesRead &gt;= 0) &#123; os.write(buffer, 0, bytesRead); &#125; else &#123; break; &#125; &#125; if(gis != null) gis.close(); is.close(); &#125; os.close(); if(MainSvr.bExitFlag) return false; System.out.println("Success move " + localDirname + " to " + hdfsPath); return true; &#125; catch (Exception e) &#123; e.printStackTrace(); log.info("putMerge error:" + e.getMessage()); &#125; return false;&#125;public boolean mkdir(String dirName)&#123; try &#123; if (checkFileExist(dirName)) return true; Path f = new Path(dirName); System.out.println("Create and Write :" + dirName + " to hdfs"); return hdfs.mkdirs(f); &#125; catch (Exception e) &#123; System.out.println("mkdir Fail:" + e.getMessage()); e.printStackTrace(); &#125; return false;&#125;]]></content>
      <categories>
        <category>大数据</category>
        <category>hdfs</category>
      </categories>
      <tags>
        <tag>hdfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[github+hexo搭建免费博客]]></title>
    <url>%2Fprogramme%2Flearn-hexo1.html</url>
    <content type="text"><![CDATA[之前做了很多网站，用的都是wordpress，这里我记录一下自己免费搭建博客的一种方法，主要是免费，放一些静态页面是比较好的，丰富程度当然比不上wordpress了。话不多说，进入主题 前提：安装好了git,有github账号 安装hexo12345678910111213141516171819202122$ npm install -g hexo出现npm WARN deprecated swig@1.4.2: This package is no longer maintainedC:\Users\kaishun\AppData\Roaming\npm\hexo -&gt; C:\Users\kaishun\AppData\Roaming\npm\node_modules\hexo\bin\hexo&gt; dtrace-provider@0.8.3 install C:\Users\kaishun\AppData\Roaming\npm\node_modules\hexo\node_modules\dtrace-provider&gt; node scripts/install.js&gt; hexo-util@0.6.1 postinstall C:\Users\kaishun\AppData\Roaming\npm\node_modules\hexo\node_modules\hexo-util&gt; npm run build:highlight&gt; hexo-util@0.6.1 build:highlight C:\Users\kaishun\AppData\Roaming\npm\node_modules\hexo\node_modules\hexo-util&gt; node scripts/build_highlight_alias.js &gt; highlight_alias.jsonnpm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.1.2 (node_modules\hexo\node_modules\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.1.2: wanted &#123;"os":"darwin","arch":"any"&#125; (current: &#123;"os":"win32","arch":"x64"&#125;)+ hexo@3.3.8added 371 packages in 121.308s 创建hexo文件夹在某个目录下执行下面一句，会在该目录下创建一系列文件和文件夹12345678910111213141516171819$ npm installnpm WARN deprecated swig@1.4.2: This package is no longer maintained&gt; dtrace-provider@0.8.3 install G:\npm\node_modules\dtrace-provider&gt; node scripts/install.js&gt; hexo-util@0.6.1 postinstall G:\npm\node_modules\hexo-util&gt; npm run build:highlight&gt; hexo-util@0.6.1 build:highlight G:\npm\node_modules\hexo-util&gt; node scripts/build_highlight_alias.js &gt; highlight_alias.jsonnpm notice created a lockfile as package-lock.json. You should commit this file.npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.1.2 (node_modules\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.1.2: wanted &#123;&quot;os&quot;:&quot;darwin&quot;,&quot;arch&quot;:&quot;any&quot;&#125; (current: &#123;&quot;os&quot;:&quot;win32&quot;,&quot;arch&quot;:&quot;x64&quot;&#125;) 安装依赖包1234567891011121314151617181920212223242526272829303132$ hexo generateINFO Start processingINFO Files loaded in 323 msINFO Generated: index.htmlINFO Generated: archives/index.htmlINFO Generated: fancybox/fancybox_loading.gifINFO Generated: fancybox/jquery.fancybox.cssINFO Generated: fancybox/jquery.fancybox.pack.jsINFO Generated: fancybox/jquery.fancybox.jsINFO Generated: fancybox/fancybox_loading@2x.gifINFO Generated: fancybox/fancybox_sprite.pngINFO Generated: fancybox/blank.gifINFO Generated: fancybox/fancybox_overlay.pngINFO Generated: archives/2017/index.htmlINFO Generated: fancybox/fancybox_sprite@2x.pngINFO Generated: archives/2017/08/index.htmlINFO Generated: css/fonts/fontawesome-webfont.eotINFO Generated: fancybox/helpers/jquery.fancybox-buttons.cssINFO Generated: js/script.jsINFO Generated: fancybox/helpers/jquery.fancybox-buttons.jsINFO Generated: fancybox/helpers/jquery.fancybox-thumbs.cssINFO Generated: css/fonts/fontawesome-webfont.woffINFO Generated: fancybox/helpers/fancybox_buttons.pngINFO Generated: fancybox/helpers/jquery.fancybox-media.jsINFO Generated: fancybox/helpers/jquery.fancybox-thumbs.jsINFO Generated: css/style.cssINFO Generated: css/fonts/FontAwesome.otfINFO Generated: 2017/08/02/hello-world/index.htmlINFO Generated: css/fonts/fontawesome-webfont.ttfINFO Generated: css/fonts/fontawesome-webfont.svgINFO Generated: css/images/banner.jpgINFO 28 files generated in 929 ms 创建github账号：略git上创建io仓库比如我的git名字是翟开顺那么我git的仓库为:zhaikaishun/zhaikaishun.github.io 本地添加SSH key： 略去廖雪峰看 全局配置 _config.yml12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182# Hexo Configuration## Docs: https://hexo.io/docs/configuration.html## Source: https://github.com/hexojs/hexo/# Sitetitle: 翟开顺的个人博客subtitle: 学有所思，思有所感description: 翟开顺的个人博客author: 翟开顺language: zh-CNtimezone: # URL## If your site is put in a subdirectory, set url as &apos;http://yoursite.com/child&apos; and root as &apos;/child/&apos;url: http://yoursite.comroot: /permalink: :year/:month/:day/:title/permalink_defaults:# Directorysource_dir: sourcepublic_dir: publictag_dir: tagsarchive_dir: archivescategory_dir: categoriescode_dir: downloads/codei18n_dir: :langskip_render:# Writingnew_post_name: :title.mddefault_layout: posttitlecase: false # Transform title into titlecaseexternal_link: true # Open external links in new tabfilename_case: 0render_drafts: falsepost_asset_folder: falserelative_link: falsefuture: truehighlight: enable: true line_number: true auto_detect: false tab_replace: # Home page setting# path: Root path for your blogs index page. (default = &apos;&apos;)# per_page: Posts displayed per page. (0 = disable pagination)# order_by: Posts order. (Order by date descending by default)index_generator: path: &apos;&apos; per_page: 10 order_by: -date # Category &amp; Tagdefault_category: uncategorizedcategory_map:tag_map:# Date / Time format## Hexo uses Moment.js to parse and display date## You can customize the date format as defined in## http://momentjs.com/docs/#/displaying/format/date_format: YYYY-MM-DDtime_format: HH:mm:ss# Pagination## Set per_page to 0 to disable paginationper_page: 10pagination_dir: page# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: landscape# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repo: https://github.com/zhaikaishun/zhaikaishun.github.io.git branch: master 上面配置最重要的是：（填写自己的）,相关参数的意思自己网上搜索1234deploy: type: git repo: https://github.com/zhaikaishun/zhaikaishun.github.io.git branch: master 注意： 配置文件的冒号“:”后面有一个空格 否则会报错，报错的时候，去对应的行看就知道了 123$ hexo serverINFO Start processingINFO Hexo is running at http://localhost:4000/. Press Ctrl+C to stop. 把本地这些文件放在git上创建git仓库1$ git init 添加到远程1$ git remote add origin git@github.com:zhaikaishun/zhaikaishun.github.io.git 先拉一下1$ git pull origin master 添加，我这里不懂为什么第一次要加-f1$ git add .gitignore 1$ git commit -m &quot;第一次提交&quot; 1$ git push -u origin master 我们的github的仓库就有了这些文件 解析域名，绑定主机添加一个域名 1ping zhaikaishun.github.io 创建CNAME文件在source目录下创建CNAME文件，填写内容, 每次部署的时候，其实都会替换source的内容到git上1zhaikaishun.top 当然还有一种方法： 网上的这个方法很麻烦，问题比较多，就是在Repository的根目录下面，新建一个名为CNAME的文本文件，里面写入你要绑定的域名.我的是, 如果是现在创建，可能要等好几分钟才生效，短暂的出现404，所以不推荐这种方法 git执行部署12hexo generatehexo deploy 若出现:1ERROR Deployer not found: git 则先要执行1npm install hexo-deployer-git --save 解析域名先ping zhaikaishun.github.io得到ip的值:解析A记录 主机记录为@ ip 为刚才ping返回的ip解析CANME， 主机记录为www, 记录值为zhaikaishun.github.io阿里云解析等待几分钟这时候，登录zhaikaishun.top，就已经创建好了 参考了http://www.jianshu.com/p/701b1095da11 hexo的配置与教程参考： http://blog.csdn.net/xuezhisdc/article/details/53130383 更多内容参考http://zhaikaishun.top/2017/08/02/hello-world/]]></content>
      <categories>
        <category>programme</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
</search>
